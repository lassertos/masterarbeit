@inproceedings{10.1145/2676723.2693622,
author = {Weintrop, David},
title = {Minding the Gap Between Blocks-Based and Text-Based Programming (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2693622},
doi = {10.1145/2676723.2693622},
abstract = {Graphical blocks-based programming environments, such as Scratch and Snap!, are becoming increasingly popular tools for introducing learners to programming in formal educational settings. However, a growing body of research is finding that students struggle when transitioning from these tools to more conventional, text-based programming languages. To better understand students' difficulties and begin to explore potential solutions to facilitate this transition, a 10-week, quasi-experimental study was conducted with 80 students across three high-school introductory programming classes. Each class spent five weeks working with different version of a blocks-based programming tool, each of which integrated text-based programming in a different way. After working in the introductory environments, students transitioned to Java for the remainder of the study. The goal of this project is to understand the affects of blocks-based programming on students' emerging understandings, document challenges students face in transitioning from blocks-based to text-based programming, and investigate potential ways to bridge these two modalities. To answer these questions, a mixed-method approach was taken that included cognitive interviews with learners, automated collection of student authored programs, and pre/mid/post attitudinal and content assessments.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {720},
numpages = {1},
keywords = {introductory programming environments, high school computer science, blocks-based programming},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@article{10.1007/s10639-023-12024-9,
author = {Vrban\v{c}i\v{c}, Franc and Kocijan\v{c}i\v{c}, Slavko},
title = {Strategy for learning microcontroller programming—a graphical or a textual start?},
year = {2023},
issue_date = {Mar 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-023-12024-9},
doi = {10.1007/s10639-023-12024-9},
abstract = {Microcontroller programming competencies contribute to the sustainable employability of engineering graduates of both higher and secondary education. To develop the required programming skills, one of the challenges for educators is to determine which programming environments should be implemented in introductory programming courses. Conceptually, graphical (e.g. iconic or diagrammatic) environments appear to be very different from textual environments. Our study focused on a programming course in a mechatronics vocational training programme at the secondary school level in Slovenia. To investigate the expectations of potential employers towards our graduates, we surveyed local companies. Out of 104 respondents, 90 (86.5%) expected graduates to be able to use various programming environments, including upcoming ones. In our study, we divided 114 students into two groups of equal prior knowledge. Group A started the course with a textual programming environment and switched to a graphical one in the second sequence, while group B followed a reverse sequence. Group A achieved better test results after both sequences. Knowledge transfer, as measured by normalised learning gains, was also in favour of group A. We concluded that it is more efficient to start with the textual environment and then continue with the graphical environment. The results of this study open up some challenges for further research to investigate the effectiveness of introductory programming courses based on programmable electronics with students of similar ages. Further research should consider the specific requirements of the different courses but should also the effectiveness of knowledge transfer between different programming environments.},
journal = {Education and Information Technologies},
month = {jul},
pages = {5115–5137},
numpages = {23},
keywords = {Introductory programming, Secondary education, Textual vs graphical environment, Teaching/learning strategies, Improving classroom teaching}
}

@inbook{10.1007/978-3-540-74024-7_12,
author = {Enderle, Stefan},
title = {The Robotics and Mechatronics Kit "qfix"},
year = {2006},
isbn = {9783540740230},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-74024-7_12},
abstract = {Robot building projects are increasingly used in schools and universities to raise the interest of students in technical subjects. They can especially be used to teach the three mechatronics areas at the same time: mechanics, electronics, and software. However, it is hard to find reusable, robust, modular and cost-effective robot development kits in the market. Here, we present &lt;em&gt;qfix&lt;/em&gt;, a modular construction kit for edutainment robotics and mechatronics experiments which fulfills all of the above requirements and receives strong interest from schools and universities. The outstanding advantages of this kit family are the solid aluminium elements, the modular controller boards, and the programming tools which reach from an easy-to-use graphical programming environment to a powerful C++ library for the GNU compiler collection.},
booktitle = {RoboCup 2006: Robot Soccer World Cup X},
pages = {134–145},
numpages = {12}
}

@inproceedings{10.1007/978-3-319-92049-8_49,
author = {Zubair, Misbahu S. and Brown, David and Hughes-Roberts, Thomas and Bates, Matthew},
title = {Evaluating the Accessibility of Scratch for Children with Cognitive Impairments},
year = {2018},
isbn = {978-3-319-92048-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-92049-8_49},
doi = {10.1007/978-3-319-92049-8_49},
abstract = {Research on the use of interactive media as learning tools for children with cognitive impairments has focused mainly on employing predesigned content, rather than constructing new content. Visual programming tools could potentially provide cognitively impaired children with a platform that can enable them to create their own interactive media. However, very little is known about the accessibility of the tools. This study uses a novel approach to evaluate the accessibility of Scratch (a visual programming tool) for children with cognitive impairments by employing a Grounded Theory research method. The study was conducted with 9 participants: 2 special education teachers and 7 cognitively impaired children over a period of ten weeks. The children’s usage of Scratch was documented through screen capturing. In addition, semi structured interviews were conducted with the two teachers. Grounded Theory based analysis was performed using QSR NVivo, which led to the identification of: accessibility issues; causal conditions; contexts; strategies employed to tackle issues; and consequences. Thus, the findings of this research contribute to existing knowledge on the accessibility of visual programming tools and elucidate the experience of cognitively impaired children while using the tools.},
booktitle = {Universal Access in Human-Computer Interaction. Methods, Technologies, and Users:  12th International Conference, UAHCI 2018, Held as Part of  HCI International 2018, Las Vegas, NV, USA, July 15-20, 2018, Proceedings, Part I},
pages = {660–676},
numpages = {17},
keywords = {Grounded Theory, Scratch, Cognitive impairments, Visual programming, Accessibility},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/3629296.3629297,
author = {Fischer, Bj\"{o}rn and Birk, Fabian and Iwer, Eva-Maria and Panitz, Sven Eric and D\"{o}rner, Ralf},
title = {Addressing Misconceptions in Introductory Programming: Automated Feedback in Integrated Development Environments},
year = {2024},
isbn = {9798400709111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629296.3629297},
doi = {10.1145/3629296.3629297},
abstract = {In recent years, numerous approaches to automated feedback have been presented in the field of programming education. Often, these methods deliver feedback through standalone web-based environments or educational programming environments. However, only few works have explored how such feedback can be provided within Integrated Development Environments (IDEs). We propose MINDFIX, an approach for integrating alternative feedback mechanisms for addressing programming language misconceptions about Java into the Eclipse IDE. These mechanisms include textual hints, code examples, and a personalized pedagogical agent. A laboratory experiment was conducted to investigate their impact on novice programmers in higher education. The results show that MINDFIX addresses missing feedback mechanisms and features. Additionally, there are initial insights that our feedback addresses programming language misconceptions while being perceived as useful and comprehensible. Our findings also suggest that novice programmers with low self-efficacy expectations perceive pedagogical agents as more motivating, useful, and less disruptive compared to their peers.},
booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers},
pages = {1–8},
numpages = {8},
keywords = {affective computing, example-based feedback, intelligent tutoring system, pedagogical agent, programming environment},
location = {<conf-loc>, <city>Barcelona</city>, <country>Spain</country>, </conf-loc>},
series = {ICETC '23}
}

@inproceedings{10.1109/FIE44824.2020.9273982,
author = {Carlos Begosso, Luiz and Ricardo Begosso, Luiz and Aragao Christ, Natalia},
title = {An analysis of block-based programming environments for CS1},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE44824.2020.9273982},
doi = {10.1109/FIE44824.2020.9273982},
abstract = {This Research Full Paper presents our experience in analyzing and selecting block-based programming environments to support the teaching of algorithms for the students starting the introductory courses of a Computer Science major. The teaching of algorithms and programming concepts to students of the first years of Computer Science and Engineering courses has been a major challenge because students often have difficulty understanding the logic and abstraction, leading to a high dropout rate. Some strategies have been conducted to further the mission of helping students understand better those basic concepts, but this topic still remains a major problem for students in the initial grades of those courses. In previous projects developed at our university, we have already proposed the use of learning objects and gamification, with very positive results. One of the questions that arise when we adopt new teaching approaches is to know how this new path will contribute to the student’s learning. In this project, we conducted a study on eight block-based programming environments and sought to identify which aspects of those environments comply with the Computer Science reference curriculum. Our work was based on the joint task force on Computing Curricula conducted by the ACM and IEEE Computer Society CS2013 curriculum guidelines for undergraduate programs in Computer Science. We studied the virtual programming environments Alice, MIT App Inventor, Blockly Games, Code.org, Gameblox, Pencil Code, Microsoft MakeCode and Scratch. Then, we crossed the characteristics of each, identified the positive and negative points of each teaching environment in relation to the topics established by the guidelines. We have classified the main characteristics of those programming environments, establishing criteria such as: prior programming knowledge requirements; ease of interaction with users; programming language code; availability of documentation for learning; programming practices addressed by the environment; and ease of learning programming. We believe that this work can contribute to the selection process of a suitable programming environment to be adopted in an introductory course of computer programming.},
booktitle = {2020 IEEE Frontiers in Education Conference (FIE)},
pages = {1–5},
numpages = {5},
location = {Uppsala}
}

@inproceedings{10.5555/319568.319572,
author = {Madhavji, Nazim H.},
title = {Operations for programming in the all},
year = {1985},
isbn = {0818606207},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {A primary goal of Software Engineering is to improve the process of software development. It is being recognised that recent integrated programming environments have made significant progress towards this aim. This paper describes new operations, suitable for such environments, which are applicable in a much wider scope of programming, termed here as programming in the all. Development of software in this new scope is carried out incrementally in program fragments of various types, called fragtypes. Fragtypes range from a simple Expression type to a complete Subsystem type, and therefore are suited to the development of non-trivial software. The proposed operations on fragtypes have been incorporated in the design of the programming environment MUPE-2 for Modula-2, which is currently under development at McGill University.},
booktitle = {Proceedings of the 8th International Conference on Software Engineering},
pages = {15–25},
numpages = {11},
location = {London, England},
series = {ICSE '85}
}

@article{10.1145/988316.988319,
author = {Glass, Harvey},
title = {Threaded interpretive systems and functional programming environments},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/988316.988319},
doi = {10.1145/988316.988319},
abstract = {While languages based upon threaded interpretive systems have been used for a variety of applications, these systems have been generally ignored by serious students of programming languages. We describe research here at the University of South Florida where we are investigating the suitability of these systems for implementing a programming environment - specifically an environment to support programming in a functional style.We hypothesize that threaded interpretive systems may have merit as the basis for more ambitious language implementations than have yet been attempted - and that such languages may offer a reasonable compromise between the flexability of more interpretive systems and the efficiency of compilers that generate native code. We describe extensions to a threaded language which provide the kernel of a functional style language. Our goal is to gain insight into the real and apparent capabilities of threaded languages and to evaluate the potential of such systems for support of functional programming environments.},
journal = {SIGPLAN Not.},
month = {apr},
pages = {24–32},
numpages = {9}
}

@inproceedings{10.1145/268084.268167,
author = {McDonald, Chris and Kazemi, Kamran},
title = {Improving the PVM teaching environment},
year = {1997},
isbn = {0897918894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/268084.268167},
doi = {10.1145/268084.268167},
abstract = {The parallel programming community has long recognized the need for a simple programming environment offering interprocess communication between heterogeneous systems. As the Parallel Virtual Machine environment, PVM, has emerged to meet this goal, an increasing number of educational institutions are choosing PVM to support their teaching of parallel and distributed computing using networks of workstations. However, it is often the nature of PVM's design and implementation that can severely limit its success in a teaching environment. This paper first motivates and then describes improvements to the PVM environment which increase both robustness and efficiency in an educational setting.},
booktitle = {Proceedings of the Twenty-Eighth SIGCSE Technical Symposium on Computer Science Education},
pages = {219–223},
numpages = {5},
location = {San Jose, California, USA},
series = {SIGCSE '97}
}

@article{10.1016/j.compedu.2016.01.010,
author = {Chao, Po-Yao},
title = {Exploring students' computational practice, design and performance of problem-solving through a visual programming environment},
year = {2016},
issue_date = {April 2016},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {95},
number = {C},
issn = {0360-1315},
url = {https://doi.org/10.1016/j.compedu.2016.01.010},
doi = {10.1016/j.compedu.2016.01.010},
abstract = {This study aims to advocate that a visual programming environment offering graphical items and states of a computational problem could be helpful in supporting programming learning with computational problem-solving. A visual problem-solving environment for programming learning was developed, and 158 college students were conducted in a computational problem-solving activity. The students' activities of designing, composing, and testing solutions were recorded by log data for later analysis. To initially unveil the students' practice and strategies exhibited in the visual problem-solving environment, this study proposed several indicators to quantitatively represent students' computational practice (Sequence, Selection, Simple iteration, Nested iteration, and Testing), computational design (Problem decomposition, Abutment composition, and Nesting composition), and computational performance (Goal attainment and Program size). By the method of cluster analysis, some empirical patterns regarding the students' programming learning with computational problem-solving were identified. Furthermore, comparisons of computational design and computational performance among the different patterns of computational practice were conducted. Considering the relations of students' computational practice to computational design and performance, evidence-based suggestions on the design of supportive programming environments for novice programmers are discussed. A visual problem-solving environment was proposed to support programming learning.Students exhibited different patterns of computational practice in the environment.Patterns of computational practice were correlated with computational design and performance.},
journal = {Comput. Educ.},
month = {apr},
pages = {202–215},
numpages = {14},
keywords = {Visual problem solving, Students programming patterns, Computer programming}
}

@article{10.1016/j.compedu.2019.103646,
author = {Weintrop, David and Wilensky, Uri},
title = {Transitioning from introductory block-based and text-based environments to professional programming languages in high school computer science classrooms},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0360-1315},
url = {https://doi.org/10.1016/j.compedu.2019.103646},
doi = {10.1016/j.compedu.2019.103646},
journal = {Comput. Educ.},
month = {dec},
numpages = {17},
keywords = {Teaching/learning strategies, Secondary education, Programming and programming languages, Interactive learning environments, Evaluation of CAL systems}
}

@phdthesis{10.5555/AAI28087160,
author = {Blanchard, Jeremiah J. and Boyer, Kristy and Huggins-Manley, Anne and Weintrop, David and Wilson, Joseph},
advisor = {P., Anthony, Lisa and M, Gardner-McCune, Christina},
title = {Building Bridges: Dual-Modality Instruction and Introductory Programming Coursework},
year = {2020},
isbn = {9798728297147},
publisher = {University of Florida},
address = {USA},
abstract = {Blocks-based programming environments have become commonplace in introductory computing courses in K-12 schools and some college level courses. In comparison, most college-level introductory computer science courses teach students text-based languages which are more commonly used in industry and research. However, the literature provides evidence that students may face difficulty moving to text-based programming environments even when moving from blocks-based environments, and some perceive blocks-based environments as inauthentic. Bi-directional dual-modality programming environments, which provide multiple representations of programming language constructs (such as blocks and text) and allow students to transition between them freely, offer a potential solution to issues of authenticity and syntax challenges for novices and those with prior experience in blocks by making clear the connection between blocks and text representations of programs. While previous research has investigated transition from blocks-based to textual environments, there is limited research on dual-modality programming environments.The goal of my dissertation work is to identify how use of bi-directional dual-modality programming environments connects with learning in introductory programming instruction at the college level. I have developed a bi-directional dual-modality Java language plugin and evaluated the use of said tool within an introductory computer science (CS1) course. In my work I analyzed understanding and retention of specific computing / programming concepts, how any connections vary according to prior programming experience, and in what ways dual-modality programming environments affect the classroom learning experience.},
note = {AAI28087160}
}

@inproceedings{10.5555/648138.746794,
author = {Laforenza, Domenico},
title = {Programming High Performance Applications in Grid Environments},
year = {2001},
isbn = {3540426094},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The need for realistic simulations of complex systems relevant to the modeling of several modern technologies and environmental phenomena increasingly stimulates the development of advanced computing approaches. Nowadays it is possible to cluster or couple a wide variety of resources including supercomputers, storage systems, data sources, and special classes of devices distributed geographically and use them as a single unified resource, thus forming what is popularly known as a "computational grid" [1,2].Grid Computing enables the development of large scientific applications on an unprecedented scale. Grid-aware applications (meta-applications, multidisciplinary applications) make use of coupled computational resources that cannot be replicated at a single site. In this light, grids let scientists solve larger or new problems by pooling together resources that could not be coupled easily before. Designing and implementing grid-aware applications often require interdisciplinary collaborations involving aspects of scientific computing, visualization, and data management [3]. Multi-disciplinary applications are typically composed of large and complex components, and some of them are characterized by huge high performance requirements [4,5,6,7]. In order to get better performance, the challenge is to map each component onto the best candidate computational resource having a high degree of affinity with the software component. This kind of mapping is a non-trivial task. Moreover, it is well known that, in general, the programmer's productivity in designing and implementing efficient parallel applications on high performance computers remains a very time-consuming task. Grid computing makes the situation worse as heterogeneous computing environments are combined so that the programmer must manage an enormous amount of details. Consequently, the development of grid programming environments that would enable programmers to efficiently exploit this technology is an important and hot research issue. A grid programming environment should include interfaces, APIs, utilities and tools so as to provide a rich development environment. Common scientific languages such as C, C++, Java and Fortran should be available, as should application-level interfaces like MPI and PVM. A range of programming paradigms should be supported, such as message passing and distributed shared memory. In addition, a suite of numerical and other commonly used libraries should be available.Today, an interesting discussion is opened about the need to thinkat new abstract programming models and develop novel programming techniques addressing specifically the grid, which would deal with the heterogeneity and distributed computing aspects of grid programming [8].In this talk, after an introduction on the main grid programming issues, an overview of the most important approaches/projects conducted in this field worldwide will be presented. In particular, the speaker's contribution in designing some grid extension for a new programming environment will be shown. This workconstitutes a joint effort conducted by some academic and industrial Italian partners, in particular the Department of Computer Science of the Pisa University and CNUCE-CNR, in the frameworkof the ASI-PQE2000 National Project aimed at building ASSIST (A Software development System based on Integrated Skeleton Technology) [9,10,11]. The main target for the ASSIST Team is to build of a new programming environment for the development of high performance applications, based on the integration of the structured parallel programming model and the objects (components) model. In this way, ASSIST should be available for a wide range of hardware platforms from the homogeneous parallel computers (MPP, SMP, CoWs) to the heterogeneous ones (Grids).},
booktitle = {Proceedings of the 8th European PVM/MPI Users' Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface},
pages = {8–9},
numpages = {2}
}

@proceedings{10.1145/2688471,
title = {PROMOTO '14: Proceedings of the 2nd Workshop on Programming for Mobile &amp; Touch},
year = {2014},
isbn = {9781450322959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the proceedings of the PROMOTO'14. The 2nd Workshop on Programming with Mobile and Touch (PROMOTO'14) was held in Portland, OR on October 22, 2014, in conjunction with SPLASH/OOPSLA 2014. The goals of the workshop were to discuss the issues surrounding touch and mobile programming and to plan future directions.Workshop Overview Today, easy-to-use mobile devices like smartphones and tablets are becoming more prevalent than traditional PCs and laptops. New programming languages are emerging to enable programmers to develop software easily, leveraging the exciting advances in existing hardware, and providing abstractions that fit the capabilities of target platforms with multiple sensors, touch and cloud capabilities. PROMOTO'14 brought together researchers who have been exploring new programming paradigms, embracing the new realities of always connected, touch-enabled mobile devices. Specific areas of interest were the technical aspects of cross-platform computing, cloud computing, social applications, and education.Submissions for this event were invited in the general area of mobile and touch-oriented programming languages and programming environments, and teaching of programming for mobile devices. Topics of interest included: Mobile and touch-oriented programming languagesProgramming languages using innovative input mechanismsProgramming environments on or for mobile devicesTeaching of programming on or for mobile devicesProgramming tools such as debuggers on or for mobiles devicesLibraries and programming frameworks that simplify programming for mobile devicesThe workshop received 11 submissions from all over the world. Each paper was reviewed by three members of the program committee and 6 were chosen for presentation as full papers, short papers or tool demos. We also had three additional stimulating sessions: A keynote on "Programming gadgets with gadgets" presented by Jonathan de Halleux of Microsoft Research.A group hands-on session, were participants were challenged to create an app in an hour, and compare results.A lively panel on "Mobile Computing and Education"The Keynote The keynote by de Halleux son "Programming gadgets with gadgets", not reported on elsewhere, was a lively presentation with an array of gadgets on display. Hardware 2.0 is upon us: cheap micro-controller boards like Arduino have gained massive adoption in recent years. Paired with 3D printers, cheap sensors and actuators, Hardware 2.0 allows anyone to prototype the next hot gadget. And yet, the maker will have to learn a soup of software language and framework to build a connected IoC solution: C++ for the micro controller code, HTML + javascript for the client, some backend language and a communication layer to interact with the devices. In this keynote, de Halleux showed a unified approach for compilation of web server code, rich client and embedded firmware under a simple mobile friendly language and IDE.},
location = {Portland, Oregon, USA}
}

@inproceedings{10.1145/2811681.2811683,
author = {Dietrich, Jens and Tandler, Johannes and Sui, Li and Meyer, Manfred},
title = {The PrimeGame Revolutions: A cloud-based collaborative environment for teaching introductory programming},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811683},
doi = {10.1145/2811681.2811683},
abstract = {The PrimeGame is an established mathematical programming game that has been used successfully in undergraduate computer science teaching since 2003. To meet the increasing demand for innovative programming tools in undergraduate tertiary and secondary education, we have created SoGaCo, a novel platform to deliver the PrimeGame and similar games to a wide audience via standard web browsers. SoGaCo is designed to have a very low total cost of ownership. This is achieved by enabling teachers to provision a customised collaborative development environment on commodity cloud computing infrastructure. Amongst the unique features of the platform are its social networking features and support for polyglot programming.In this paper, we describe the requirements for this system, its design and implementation. We focus on how the scalability and security challenges of an open web-based development environment are addressed. This includes a discussion of the sandboxing and verification techniques we have developed in order to safeguard server-side code execution on the Java Virtual Machine.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {8–12},
numpages = {5},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/1593105.1593135,
author = {Gallant, Randy J. and Mahmoud, Qusay H.},
title = {Using Greenfoot and a Moon Scenario to teach Java programming in CS1},
year = {2008},
isbn = {9781605581057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1593105.1593135},
doi = {10.1145/1593105.1593135},
abstract = {In this paper we describe a novel concept for teaching introductory Java programming to post-secondary students in their first year of higher education. The concept includes labs and a capstone project all linked together and all utilizing the Java-based Greenfoot programming environment. The concept is designed with two goals in mind: to improve the students experience in their first computer programming course by making it more entertaining; and to increase retention in the diploma or degree programs by peaking the student's interest early in their studies. This is accomplished through a Going to the Moon scenario we have designed and implemented into the Greenfoot programming environment.},
booktitle = {Proceedings of the 46th Annual Southeast Regional Conference on XX},
pages = {118–121},
numpages = {4},
keywords = {simple data types, programming for fun, programming environments, Moon Scenario, Greenfoot},
location = {Auburn, Alabama},
series = {ACM-SE 46}
}

@inproceedings{10.1145/1595356.1595361,
author = {Kiesm\"{u}ller, Ulrich},
title = {Diagnosing learners' problem solving strategies using learning environments with algorithmic problems in secondary education},
year = {2008},
isbn = {9781605583853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595356.1595361},
doi = {10.1145/1595356.1595361},
abstract = {At schools special learning and programming environments are often used in the field of algorithm. Particularly with regard to informatics lessons in secondary education they should help novices to learn the basics of programming. In several parts of Germany (e. g. Bavaria) these fundamentals are even taught in the 7th grade, when pupils are 12 to 13 years old. Age-based designed learning and programming environments such as Karel, the robot and Kara, the programmable ladybug, are employed there, however learners still underachieve. One possible approach to improve both teaching and learning process is specifying the knowledge concerning the learners' individual problem solving strategies, when they create their solutions in consideration of the solution attempt's quality.A goal of the research project described here is being able to identify and categorise several problem solving strategies automatically. Due to this knowledge learning and programming environments can be improved which will optimise the informatics lessons, in which they are applied. Therefore the environments must be enhanced with special analytic and diagnostic modules, whose results can be given to the learner in the form of individualized system feedback messages in the future.In this text preliminary considerations are demonstrated. The research methodology as well as the design and the implementation of the research instruments are explained. We describe first studies, whose results are presented and discussed.},
booktitle = {Proceedings of the 8th International Conference on Computing Education Research},
pages = {16–24},
numpages = {9},
keywords = {tool-based analysis, secondary computer science education, problem solving process, didactics of informatics, algorithms, Kara},
location = {Koli, Finland},
series = {Koli '08}
}

@article{10.1007/s10664-020-09895-8,
author = {Santos, Adrian and Vegas, Sira and Dieste, Oscar and Uyaguari, Fernando and Tosun, Ay\c{s}e and Fucci, Davide and Turhan, Burak and Scanniello, Giuseppe and Romano, Simone and Karac, Itir and Kuhrmann, Marco and Mandi\'{c}, Vladimir and Rama\v{c}, Robert and Pfahl, Dietmar and Engblom, Christian and Kyykka, Jarno and Rungi, Kerli and Palomeque, Carolina and Spisak, Jaroslav and Oivo, Markku and Juristo, Natalia},
title = {A family of experiments on test-driven development},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09895-8},
doi = {10.1007/s10664-020-09895-8},
journal = {Empirical Softw. Engg.},
month = {may},
numpages = {53},
keywords = {Quality, Academia, Industry, Test-driven development, Family of experiments}
}

@article{10.1145/1594399.1594402,
author = {Kiesm\"{u}ller, Ulrich},
title = {Diagnosing Learners’ Problem-Solving Strategies Using Learning Environments with Algorithmic Problems in Secondary Education},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/1594399.1594402},
doi = {10.1145/1594399.1594402},
abstract = {At schools special learning and programming environments are often used in the field of algorithms. Particularly with regard to computer science lessons in secondary education, they are supposed to help novices to learn the basics of programming. In several parts of Germany (e.g., Bavaria) these fundamentals are taught as early as in the seventh grade, when pupils are 12 to 13 years old. Designed age-based learning and programming environments such as Karel the robot and Kara, the programmable ladybug, are used, but learners still underachieve. One possible approach to improving both the teaching and the learning process is to specify the knowledge concerning the learners’ individual problem solving strategies, their solutions, and their respective quality.A goal of the research project described here is to design the learning environment so that it can identify and categorize several problem-solving strategies automatically. Based on this knowledge, learning and programming environments can be improved, which will optimize the computer science lessons in which they are applied. Therefore, the environments must be enhanced with special analytic and diagnostic modules, the results of which can be given to the learner in the form of individualized system feedback messages in the future.In this text preliminary considerations are demonstrated. The research methodology as well as the design and the implementation of the research instruments are explained. We describe first studies, whose results are presented and discussed.},
journal = {ACM Trans. Comput. Educ.},
month = {sep},
articleno = {17},
numpages = {26},
keywords = {tool-based analysis, problem solving process, didactics of informatics, algorithms, Secondary computer science education, Kara}
}

@phdthesis{10.5555/75966,
author = {Jayaram, S.},
title = {CADMADE: an approach towards a device-independent standard for CAD/CAM software development},
year = {1989},
publisher = {Virginia Polytechnic Institute &amp; State University},
address = {USA},
abstract = {Every year thousands of specialized CAD/CAM applications programs are developed to meet the needs of industry, education and research. The international 3-D graphics standard, PHIGS, has proven to be very useful in the creation of custom CAD/CAM software. Although PHIGS+ promises to deliver some geometric modeling procedures, not nearly enough is being done to support the writing of CAD/CAM software. CAD/CAM applications programmers should have available a standardized high level applications programming environment which supports the creation of device-dependent and portable design and manufacturing software.In this dissertation, one approach towards the establishment of a CAD/CAM programming standard has been presented. This programming environment is called CADMADE--Computer-Aided Design and Manufacturing Applications Development Environment. CADMADE includes not only graphics programming support, but also high level procedures to support the creation of geometric modeling, mechanical design, manufacturing, expert systems and user interface software. The requirements of CADMADE have been created. CADMADE consists of five environments: the User Interface Environment (UIE), the Design and Modeling Environment (DME), the Virtual Manufacturing Environment (VME), the Expert Consultation Environment (ECE) and the PHIGS+ Environment. The User Interface Environment has been designed in great detail. A prototype of the User Interface Environment has been created using PHIGS. Examples of applications programs which use the prototype User Interface Environment are presented. The Design and Modeling Environment has also been designed. A new set of logical input/output devices has been created for the Design and Modeling Environment. The requirements of the Expert Consultation Environment and some new concepts in expert system consultation are discussed.},
note = {UMI order no: GAX89-21155}
}

@inproceedings{10.1109/DASC.2014.41,
author = {Liang, Tyng Yeu and Li, Hung Fu and Chen, Yu Chih},
title = {A Ubiquitous Integrated Development Environment for C Programming on Mobile Devices},
year = {2014},
isbn = {9781479950799},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DASC.2014.41},
doi = {10.1109/DASC.2014.41},
abstract = {C is a popular programming language because of high portability and simple syntax. As a result, it is the first course for most of university students who major in computer, electronics, and electrical engineering and so on. To obtain a good learning achievement, students usually need to put their effort and time on programming as much as possible. Therefore, a ubiquitous integrated development environment is helpful for students to practice programming anytime and anywhere. To achieve this goal, we develop a novel integrated development environment called Ubi-C for C programming based on Clang and LLVM in this paper. Using this IDE, users can directly write, compile, execute and debug their C programs on Android-based smart phones or tablets. Moreover, this IDE provides the API of multimedia programming, and integrates with cloud services such as Dropbox for storing user programs and data. Consequently, users can easily develop multimedia applications by using C on mobile devices, and can switch their working environments between mobile devices and fixed computers with the support of Dropbox.},
booktitle = {Proceedings of the 2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing},
pages = {184–189},
numpages = {6},
keywords = {mobile devices, LLVM, Clang, C programming, Android},
series = {DASC '14}
}

@article{10.1609/aimag.v7i1.531,
author = {Nute, Donald and Covington, Michael and Rankin, Terry},
title = {The Advanced Computational Methods Center, University of Georgia},
year = {1986},
issue_date = {Spring 1986},
publisher = {American Association for Artificial Intelligence},
address = {USA},
volume = {7},
number = {1},
issn = {0738-4602},
url = {https://doi.org/10.1609/aimag.v7i1.531},
doi = {10.1609/aimag.v7i1.531},
abstract = {The Advanced Computational Methods Center (ACMC), established at the University of Georgia in 1984, supports several research projects in artificial intelligence. The primary goal of AI research at ACMC is the design and installation of a logic‐programming environment with advanced natural language processing and knowledge‐acquisition capabilities on the university's highly parallel CYBERPLUS system from Control Data Corporation. This article briefly describes current research projects in artificial intelligence at ACMC.},
journal = {AI Mag.},
month = {mar},
pages = {74–76},
numpages = {3}
}

@article{10.1145/70593.70599,
author = {Goldenson, D. R.},
title = {The impact of structured editing on introductory computer science education: the results so far},
year = {1989},
issue_date = {Sep. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {0097-8418},
url = {https://doi.org/10.1145/70593.70599},
doi = {10.1145/70593.70599},
abstract = {Properly done, introductory computer science courses have great potential, both for preparing future computing professionals and for the broad goals of general education. Yet the performance all too often lags well behind the promise. Poor grades, high failure and drop out rates are all too common at both the collegiate and pre-collegiate levels. However the advent of seamless programming environments based on structure editing provides us with an opportunity to change the situation in fundamental ways. Initial studies show dramatic differences between students who do and do not use a structure editor based environment.},
journal = {SIGCSE Bull.},
month = {sep},
pages = {26–29},
numpages = {4}
}

@article{10.1145/1868358.1868361,
author = {K\"{o}lling, Michael},
title = {The Greenfoot Programming Environment},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
url = {https://doi.org/10.1145/1868358.1868361},
doi = {10.1145/1868358.1868361},
abstract = {Greenfoot is an educational integrated development environment aimed at learning and teaching programming. It is aimed at a target audience of students from about 14 years old upwards, and is also suitable for college- and university-level education. Greenfoot combines graphical, interactive output with programming in Java, a standard, text-based object-oriented programming language. This article first describes Greenfoot and then goes on to discuss design goals and motivations, strengths and weaknesses of the system, and its relation to two environments with similar goals, Scratch and Alice.},
journal = {ACM Trans. Comput. Educ.},
month = {nov},
articleno = {14},
numpages = {21},
keywords = {programming environment, programming education, Greenfoot}
}

@article{10.1145/1045283.1045333,
author = {Brown, D. C. and Buttelmann, H. W. and Chandrasekaran, B. and Kwasny, S. C. and Sondheimer, N. K.},
title = {Natural language graphics},
year = {1977},
issue_date = {February 1977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {61},
issn = {0163-5719},
url = {https://doi.org/10.1145/1045283.1045333},
doi = {10.1145/1045283.1045333},
abstract = {The Natural Language Graphics Project (NLG) at The Ohio State University is concerned with natural language programming in an interactive graphics environment. Natural interaction between men depends on a combination of both graphical and linguistic modes of communication [1]. The goals of the project are to study the relationships between graphical and linguistic processing in a programming environment. To achieve these goals, associated practical and theoretical issues in man-machine interaction, computer graphics, computational linguistics, and knowledge representation will be investigated.},
journal = {SIGART Bull.},
month = {feb},
pages = {57–58},
numpages = {2}
}

@inproceedings{10.1145/3408877.3439625,
author = {Yan, Wei and Israel, Maya and Luo, Feiya and Liu, Ruohan},
title = {Exploring Elementary Students' Debugging Behaviors in Puzzle-based Programming: A Learning Trajectory Approach},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3439625},
doi = {10.1145/3408877.3439625},
abstract = {Debugging has been an expanding topic in K-12 computer science (CS) education research. However, few studies have focused on in-depth analysis of elementary students' debugging in block-based visual programming environments. Thus, using the video analysis technique, this basic interpretive qualitative study aimed to explore what debugging behaviors students exhibited and how these debugging behaviors mapped with an existing K-8 debugging learning trajectory (LT). Findings revealed five types of debugging behaviors and four primary challenges. These debugging behaviors mapped to five consensus goals in the K-8 debugging learning trajectory. Future research will focus on students' efficiency in using debugging strategies and understanding of debugging.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {1308},
numpages = {1},
keywords = {puzzle-based programming, learning trajectories, debugging},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/3545947.3576265,
author = {Geleta, Margarita and Xu, Jiacen and Loya, Manikanta and Wang, Junlin and Singh, Sameer and Li, Zhou and Gago-Masague, Sergio},
title = {Design Factors of Maestro: A Serious Game for Robust AI Education},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3576265},
doi = {10.1145/3545947.3576265},
abstract = {Training tools targeting robust AI are still in their infancy. We present Maestro, an effective open-source game-based platform for robust AI training in higher education, which includes counter- measures and prevention of AI vulnerabilities. Maestro provides goal-based scenarios (GBSs) where students are exposed to challenging life-inspired assignments in a competitive programming environment. The assessment of Maestro showed that its leader-board, a key gamification element, has been crucial for effective student learning. Students who felt the acquisition of new skills in robust AI tended to appreciate highly Maestro and scored highly on material consolidation, curiosity and maestry in robust AI.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1318},
numpages = {1},
keywords = {robust ai, leaderboard, gamification, education, adversarial ai},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.5555/1625015.1625060,
author = {Hirsh, Haym},
title = {Explanation-based generalization in a logic-programming environment},
year = {1987},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper describes a domain-independent implementation of explanation-based generalization (EBG) within a logic-programming environment. Explanation is interleaved with generalization, so that as the training instance is proven to be a positive example of the goal concept, the generalization is simultaneously created. All aspects of the EBG task are viewed in logic, which provides a clear semantics for EBG, and allows its integration into the logic-programming system. In this light operationally becomes a property requiring explicit reasoning. Additionally, viewing EBG in logic clarifies the relation of learning search-control to EBG, and suggests solutions for dealing with imperfect domain theories.},
booktitle = {Proceedings of the 10th International Joint Conference on Artificial Intelligence - Volume 1},
pages = {221–227},
numpages = {7},
location = {Milan, Italy},
series = {IJCAI'87}
}

@inproceedings{10.1145/2485760.2485785,
author = {Flannery, Louise P. and Silverman, Brian and Kazakoff, Elizabeth R. and Bers, Marina Umaschi and Bont\'{a}, Paula and Resnick, Mitchel},
title = {Designing ScratchJr: support for early childhood learning through computer programming},
year = {2013},
isbn = {9781450319188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485760.2485785},
doi = {10.1145/2485760.2485785},
abstract = {ScratchJr is a graphical programming language based on Scratch and redesigned for the unique developmental and learning needs of children in kindergarten to second grade. The creation of ScratchJr addresses the relative lack of powerful technologies for digital creation and computer programming in early childhood education. ScratchJr will provide software for children to create interactive, animated stories as well as curricula and online resources to support adoption by educators. This paper describes the goals and challenges of creating a developmentally appropriate programming tool for children ages 5-7 and presents the path from guiding principles and studies with young children to current ScratchJr designs and plans for future work.},
booktitle = {Proceedings of the 12th International Conference on Interaction Design and Children},
pages = {1–10},
numpages = {10},
keywords = {graphical programming, education, early childhood, STEM},
location = {New York, New York, USA},
series = {IDC '13}
}

@article{10.1145/1121802.1121809,
author = {Mor\'{o}n, C\'{e}lio E. and Faria, Lilian N. and Mascarenhas, Nelson D. A. and Saito, Jos\'{e} H. and Rosa, Reinaldo R. and Sawant, Hanumant S.},
title = {Visual environment for high performance real-time 3D reconstruction},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/1121802.1121809},
doi = {10.1145/1121802.1121809},
abstract = {We are carrying out a development of a whole system for processing solar images in a high performance parallel system. The main objective is to create the initial conditions for studying and forecasting the solar explosions in real time. Due to the high computational costs involved in the processing, visualization and analysis of a great amount of solar images, a high performance computer system becomes necessary to carry out the forecast of solar explosions. As a joint effort between the Department of Computer Science at Federal University of S\~{a}o Carlos (UFSCar), the Astrophysics Division (DAS) and Associated Laboratory for Computing and Applied Mathematics (LAC) at National Institute for Space Research - INPE, a high performance parallel system was developed with capacity to support realistic applications, involving a reasonable amount of parallel processing. The forecast of solar explosions is important as they may cause serious perturbations in terrestrial communication systems. A significant limitation for the development of parallel real-time systems is the lack of adequate programming tools, mainly for supporting the final stages of the development life cycle. This work presents a development environment, called Visual Environment for the Development of Parallel Real-Time Programs, that supports the design and implementation of parallel real-time applications executed with the support of a parallel kernel. This paper shows how this Environment was used to carry out the 3D Reconstruction of Solar Images.},
journal = {SIGBED Rev.},
month = {jul},
pages = {30–35},
numpages = {6}
}

@inproceedings{10.1145/1099203.1099229,
author = {\v{C}ubrani\'{c}, Davor and Storey, Margaret Anne D.},
title = {Collaboration support for novice team programming},
year = {2005},
isbn = {1595932232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1099203.1099229},
doi = {10.1145/1099203.1099229},
abstract = {Learning computer programming in a modern university course is rarely an individual activity; however, IDEs used in introductory programming classes do not support collaboration at a level appropriate for novices. The goal of our research is to make it easier for first-year students to experience working in a team in their programming assignments. Based on our previous work developing and evaluating IDEs for novice programmers, we have identified two main areas of required functionality: 1) features for code sharing and coordination; and 2) features to support communication. We have extended an existing teaching-oriented integrated development environment (called Gild) with features to support code sharing and coordination. We report on a preliminary study in which pairs of students used a prototype of our collaborative IDE to work on a programming assignment. The goals of this study were to evaluate the effectiveness and usability of the new features and to determine requirements for future communication support.},
booktitle = {Proceedings of the 2005 ACM International Conference on Supporting Group Work},
pages = {136–139},
numpages = {4},
keywords = {teaching programming, gild},
location = {Sanibel Island, Florida, USA},
series = {GROUP '05}
}

@inproceedings{10.1145/3341525.3394000,
author = {Seraj, Mazyar},
title = {Impacts of Block-based Programming on Young Learners' Programming Skills and Attitudes in the Context of Smart Environments},
year = {2020},
isbn = {9781450368742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341525.3394000},
doi = {10.1145/3341525.3394000},
abstract = {In computer programming education, showing the application of programming in reality has become a common way to introduce it to young learners. However, we have limited knowledge of how best to utilize smart objects and environments to foster the learners' programming skills and develop a positive attitude towards programming. My research focuses on filling this gap by presenting an educational block-based programming tool that brings together the hot topic of smart environments and the visual programming paradigm. The end goal is empirically investigating the impacts of state-of-the-art smart technologies together with block-based programming on the learners' programming skills and attitudes towards programming in non-formal learning environments.},
booktitle = {Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {569–570},
numpages = {2},
keywords = {young learners, smart homes, programming skills, experiment, block-based programming, attitudes towards programming},
location = {Trondheim, Norway},
series = {ITiCSE '20}
}

@inproceedings{10.1145/1275620.1275625,
author = {Benjamin, Michael and Kaeli, David and Platcow, Richard},
title = {Experiences with the Blackfin architecture in an embedded systems lab},
year = {2006},
isbn = {9781450347358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1275620.1275625},
doi = {10.1145/1275620.1275625},
abstract = {At Northeastern University we are building a number of courses upon a common embedded systems platform. The goal is to reduce the learning curve associated with new architectures and programming environments. The platform selected is based on the Analog Devices Blackfin digital signal processor.In this paper we discuss our recent experience developing anew undergraduate embedded systems lab. Students learn to utilize the embedded DSP platform to address a number of different applications, including controller design, RS-232 communication, encryption, and image processing. This platform provides a rich design exploration sandbox replete with programming and simulation tools. We describe our use of this platform in our Microprocessor-based Design Laboratory and discuss how this platform can be used in a range of classes.},
booktitle = {Proceedings of the 2006 Workshop on Computer Architecture Education: Held in Conjunction with the 33rd International Symposium on Computer Architecture},
pages = {2–es},
location = {Boston, Massachusetts},
series = {WCAE '06}
}

@inproceedings{10.1145/800225.806824,
author = {Bahlke, Rolf and Snelting, Gregor},
title = {The PSG - Programming System Generator},
year = {1985},
isbn = {0897911652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800225.806824},
doi = {10.1145/800225.806824},
abstract = {The programming system generator developed at the Technical University of Darmstadt generates sophisticated interactive programming environments from formal language definitions. From a formal, entirely non-procedural definition of the language's syntax, context conditions and denotational semantics, it produces a hybrid editor, an interpreter and a library system. The editor allows both structure editing and text editing, guaranteeing immediate recognition of syntactic and semantic errors. The generator has been used to generate environments for PASCAL, MODULA-2 and the formal language definition language itself. A brief description of the generated environments and the definition language is given, and our experiences with formal language definitions are discussed from the language definer's point of view as well as from the programmer's point of view using the generated environments.},
booktitle = {Proceedings of the ACM SIGPLAN 85 Symposium on Language Issues in Programming Environments},
pages = {28–33},
numpages = {6},
location = {Seattle, Washington, USA},
series = {SLIPE '85}
}

@inproceedings{10.1145/3162087.3162092,
author = {G\"{u}lbahar, Yasemin and Kalelio\u{g}lu, Filiz},
title = {Competencies of High School Teachers and Training Needs for Computer Science Education},
year = {2017},
isbn = {9781450363389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3162087.3162092},
doi = {10.1145/3162087.3162092},
abstract = {The computer science discipline is evolving with problems in both technological and pedagogical aspects almost worldwide. With the advent of new technologies and approaches for teaching programming at all ages, many countries including Turkey have revised their computer science curriculum. These revisions have resulted in serious training needs being highlighted for teachers with inadequate competencies to meet the expected learning outcomes. Hence, the purpose of this study was to explore; (a) the self-perceived competencies of teachers about the topics in the curriculum, (b) perceptions about programming, programming tools and approaches, and (c) contribution of university education to their teaching profession. The findings revealed that most teachers believe they are not sufficiently competent to be an effective computer science teacher. Related to this finding, most of them especially mentioned their training needs for programming, emerging tools and technologies. Plus more than half of the participants think that the higher education curriculum is inadequate to meet teacher expectations and to create competent teachers.},
booktitle = {Proceedings of the 6th Computer Science Education Research Conference},
pages = {26–31},
numpages = {6},
keywords = {computer science teacher training, Computer science teacher competencies},
location = {Helsinki, Finland},
series = {CSERC '17}
}

@article{10.1145/6465.6489,
author = {Cooper, Keith D. and Kennedy, Ken and Torczon, Linda},
title = {The impact of interprocedural analysis and optimization in the Rn programming environment},
year = {1986},
issue_date = {Oct. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/6465.6489},
doi = {10.1145/6465.6489},
abstract = {In spite of substantial progress in the theory of interprocedural data flow analysis, few practical compiling systems can afford to apply it to produce more efficient object programs. To perform interprocedural analysis, a compiler needs not only the source code of the module being compiled, but also information about the side effects of every procedure in the program containing that module, even separately compiled procedures. In a conventional batch compiler system, the increase in compilation time required to gather this information would make the whole process impractical. In an integrated programming environment, however, other tools can cooperate with the compiler to compute the necessary interprocedural information incrementally. as the program is being developed, decreasing both the overall cost of the analysis and the cost of individual compilations.A central goal of the Rn project at Rice University is to construct a prototype software development environment that is designed to build whole programs, rather than just individual modules. It employs interprocedural analysis and optimization to produce high-quality machine code for whole programs. This paper presents an overview of the methods used by the environment to accomplish this task and discusses the impact of these methods on the various environment components. The responsibilities of each component of the environment for the preparation and use of interprocedural information are presented in detail.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {aug},
pages = {491–523},
numpages = {33}
}

@inproceedings{10.1145/1176617.1176635,
author = {Burke, Michael G. and Morris, Cheryl and Orso, Alessandro and Robillard, Martin},
title = {Eclipse technology eXchange (ETX) workshop},
year = {2006},
isbn = {159593491X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176617.1176635},
doi = {10.1145/1176617.1176635},
abstract = {The Eclipse platform (http://www.eclipse.org) is designed for building integrated development environments (IDEs) for object-oriented application development. Building on the success of the Eclipse Technology eXchange workshops at OOPSLA 2003, 2004, and 2005, we invite original papers that describe potential new uses of Eclipse and how the core Eclipse technology can be leveraged, improved and/or extended for research and teaching projects. Accepted papers will be presented at the workshop. Due to the popularity of this workshop in the past, this year's ETX will be a 1.5 day event. Workshop topics include (but are not limited to) the use of Eclipse for: IDEs, supporting the software development process, debugging or testing, design requirements/specification, modeling environments or frameworks, aspect-oriented programming, program analysis and transformation, such as for refactoring, optimization, or obfuscation, computer-based learning, software engineering education, teaching foundations of object-oriented programming courseware, teaching an introductory undergraduate programming course, web service applications, rich client application.},
booktitle = {Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications},
pages = {619},
numpages = {1},
location = {Portland, Oregon, USA},
series = {OOPSLA '06}
}

@inproceedings{10.1145/1238844.1238853,
author = {Ungar, David and Smith, Randall B.},
title = {Self},
year = {2007},
isbn = {9781595937667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1238844.1238853},
doi = {10.1145/1238844.1238853},
abstract = {The years 1985 through 1995 saw the birth and development of the language Self, starting from its design by the authors at Xerox PARC, through first implementations by Ungar and his graduate students at Stanford University, and then with a larger team formed when the authors joined Sun Microsystems Laboratories in 1991. Self was designed to help programmers become more productive and creative by giving them a simple, pure, and powerful language, an implementation that combined ease of use with high performance, a user interface that off-loaded cognitive burden, and a programming environment that captured the malleability of a physical world of live objects. Accomplishing these goals required innovation in several areas: a simple yet powerful prototype-based object model for mainstream programming, many compilation techniques including customization, splitting, type prediction, polymorphic inline caches, adaptive optimization, and dynamic deoptimization, the application of cartoon animation to enhance the legibility of a dynamic graphical interface, an object-centered programming environment, and a user-interface construction framework that embodied a uniform use-mention distinction. Over the years, the project has published many papers and released four major versions of Self.Although the Self project ended in 1995, its implementation, animation, user interface toolkit architecture, and even its prototype object model impact computer science today (2006). Java virtual machines for desktop and laptop computers have adopted Self's implementation techniques, many user interfaces incorporate cartoon animation, several popular systems have adopted similar interface frameworks, and the prototype object model can be found in some of today's languages, including JavaScript. Nevertheless, the vision we tried to capture in the unified whole has yet to be achieved.},
booktitle = {Proceedings of the Third ACM SIGPLAN Conference on History of Programming Languages},
pages = {9–1–9–50},
keywords = {virtual machine, prototype-based programming language, programming environment, object-oriented language, morphic, history of programming languages, exploratory programming, dynamic optimization, dynamic language, cartoon animation, adaptive optimization, Self},
location = {San Diego, California},
series = {HOPL III}
}

@inproceedings{10.1145/1227310.1227388,
author = {Malan, David J. and Leitner, Henry H.},
title = {Scratch for budding computer scientists},
year = {2007},
isbn = {1595933611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1227310.1227388},
doi = {10.1145/1227310.1227388},
abstract = {Scratch is a "media-rich programming environment" recently developed by MIT's Media Lab that "lets you create your own animations, games, and interactive art." Although Scratch is intended to "enhance the development of technological fluency [among youths] at after-school centers in economically disadvantaged communities," we find rarkable potential in this programming environment for higher education as well.We propose Scratch as a first language for first-time programmers in introductory courses, for majors and non-majors alike. Scratch allows students to program with a mouse: programmatic constructs are represented as puzzle pieces that only fit together if "syntactically" appropriate. We argue that this environment allows students not only to master programmatic constructs before syntax but also to focus on probls of logic before syntax. We view Scratch as a gateway to languages like Java.To validate our proposal, we recently deployed Scratch for the first time in higher education via harvard Summer School's Computer Science S-1: Great Ideas in Computer Science, the summertime version of a course at harvard College. Our goal was not to improve scores but instead to improve first-time programmers' experiences. We ultimately transitioned to Java, but we first introduced programming itself via Scratch. We present in this paper the results of our trial.We find that, not only did Scratch excite students at a critical time (i.e.,, their first foray into computer science), it also familiarized the inexperienced among th with fundamentals of programming without the distraction of syntax. Moreover, when asked via surveys at term's end to reflect on how their initial experience with Scratch affected their subsequent experience with Java, most students (76%) felt that Scratch was a positive influence, particularly those without prior background. Those students (16%) who felt that Scratch was not an influence, positive or negative, all had prior programming experience.},
booktitle = {Proceedings of the 38th SIGCSE Technical Symposium on Computer Science Education},
pages = {223–227},
numpages = {5},
keywords = {scratch, programming, languages, Java},
location = {Covington, Kentucky, USA},
series = {SIGCSE '07}
}

@inproceedings{10.1109/ICCIMA.2005.29,
author = {Kramer, Klaus-Dietrich and Blankenberg, Christian},
title = {Fuzzy Control Design Tool to Apply in FC Teaching Models},
year = {2005},
isbn = {0769523587},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCIMA.2005.29},
doi = {10.1109/ICCIMA.2005.29},
abstract = {Computational Intelligence (CI) based model approaches, realized by Fuzzy Control models (FC) or models with Artificial Neural Networks (NN), are used as alternativ concepts to classical approaches. In university education or further training are on the one hand models necessary which represent the technical system transparent and easy cognizable and on the other hand a programming tool is required that supports an easy development process. That includes tools to verify the results and tuning the system with graphic functions under real time conditions. A Fuzzy Control Design Tool (FHFCE-Tool) and four technical modells will be presented. The methodical and didactical objective in the utilization of this teaching models is to develop solution strategies in CI applications, for example Fuzzy Controller, special to analyse different algorithms of inference or defuzzyfication and to verify and tune those systems.},
booktitle = {Proceedings of the Sixth International Conference on Computational Intelligence and Multimedia Applications},
pages = {335–336},
numpages = {2},
series = {ICCIMA '05}
}

@inproceedings{10.5555/647912.740668,
author = {Arge, Lars and Procopiuc, Octavian and Vitter, Jeffrey Scott},
title = {Implementing I/O-efficient Data Structures Using TPIE},
year = {2002},
isbn = {3540441808},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In recent years, many theoretically I/O-efficient algorithms and data structures have been developed. The TPIE project at Duke University was started to investigate the practical importance of these theoretical results. The goal of this ongoing project is to provide a portable, extensible, flexible, and easy to use C++ programming environment for efficiently implementing I/O-algorithms and data structures. The TPIE library has been developed in two phases. The first phase focused on supporting algorithms with a sequential I/O pattern, while the recently developed second phase has focused on supporting on-line I/O-efficient data structures, which exhibit a more random I/O pattern. This paper describes the design and implementation of the second phase of TPIE.},
booktitle = {Proceedings of the 10th Annual European Symposium on Algorithms},
pages = {88–100},
numpages = {13},
series = {ESA '02}
}

@inproceedings{10.5555/1287073.1287083,
author = {Watson, Terri and Bershad, Brian N.},
title = {Local area mobile computing on stock hardware and mostly stock software},
year = {1993},
publisher = {USENIX Association},
address = {USA},
abstract = {In the Fall of 1992, the graduate Operating Systems class at Carnegie Mellon University implemented the necessary components to provide applications with a programmable interface to a mobile palmtop computer. The goal of the project was to expose project members to the area of mobile computing through "shock immersion." Over the course of two months, students designed and implemented the infrastructure for a simple mobile computing environment for low-end palmtop machines. This programming environment was used to develop a suite of mobile applications such as a mailer, graphical locator map, the game tetris, and a scheme interpreter that allowed functions to be remotely executed on the palmtop. In this paper, we describe the results of the course project and the lessons learned.},
booktitle = {Mobile &amp; Location-Independent Computing Symposium on Mobile &amp; Location-Independent Computing Symposium},
pages = {10},
numpages = {1},
location = {Cambridge, Massachusetts},
series = {MLCS}
}

@inproceedings{10.1145/2616498.2616568,
author = {Feldhausen, Russell and Bell, Scott and Andresen, Daniel},
title = {Minimum Time, Maximum Effect: Introducing Parallel Computing in CS0 and STEM Outreach Activities Using Scratch},
year = {2014},
isbn = {9781450328937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2616498.2616568},
doi = {10.1145/2616498.2616568},
abstract = {This paper presents our experiences and outcomes using Scratch to teach parallel computing concepts to students just learning about computer science. We presented versions of this material to middle school and high school girls during a STEM workshop and then to undergraduate university students enrolled in an introductory computer science course. Using the Scratch development environment, students are able to build, modify and observe the changes in the performance of applications which utilize multi-threaded, concurrent, operations. This includes scenarios which involve more advanced topics such as race conditions and mutex locks.Developing these materials has allowed us to introduce these concepts in a programming environment much earlier than we have previously, giving instructors in down-stream courses the ability to build upon this early exposure. Survey results show that this approach resulted in a significant increase in both of these areas. For example, the number of students in our CS0 course who felt they could apply parallel programming to other problems using Scratch more than doubled, rising from 25 to 55 (out of 61 students that responded to both surveys). Likewise, the number of students who felt they understood what parallel programming means rose from 27 to 56. These results were achieved after just one class period. Similarly, 27 of the 37 girls responding to the workshop survey felt that they were capable of learning to write computer programs and 22 of 41 indicated they had an interest in a job using HPC to solve problems.},
booktitle = {Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment},
articleno = {75},
numpages = {7},
location = {Atlanta, GA, USA},
series = {XSEDE '14}
}

@article{10.1155/ASP.2005.1005,
author = {Coudarcher, R\'{e}mi and Duculty, Florent and Serot, Jocelyn and Jurie, Fr\'{e}d\'{e}ric and Derutin, Jean-Pierre and Dhome, Michel},
title = {Managing algorithmic skeleton nesting requirements in realistic image processing applications: the case of the SKiPPER-II parallel programming environment's operating model},
year = {2005},
issue_date = {1 January 2005},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2005},
issn = {1110-8657},
url = {https://doi.org/10.1155/ASP.2005.1005},
doi = {10.1155/ASP.2005.1005},
abstract = {SKiPPER is a Skeleton-based Parallel Programming EnviRonment being developed since 1996 and running at LASMEA Laboratory, the Blaise-Pascal University, France. The main goal of the project was to demonstrate the applicability of skeleton-based parallel programming techniques to the fast prototyping of reactive vision applications. This paper deals with the special features embedded in the latest version of the project: algorithmic skeleton nesting capabilities and a fully dynamic operating model. Throughout the case study of a complete and realistic image processing application, in which we have pointed out the requirement for skeleton nesting, we are presenting the operating model of this feature. The work described here is one of the few reported experiments showing the application of skeleton nesting facilities for the parallelisation of a realistic application, especially in the area of image processing. The image processing application we have chosen is a 3D face-tracking algorithm from appearance.},
journal = {EURASIP J. Adv. Signal Process},
month = {jan},
pages = {1005–1023},
numpages = {19},
keywords = {parallel programming, nesting, image processing, algorithmic skeleton, 3D face tracking}
}

@inproceedings{10.1145/1565799.1565822,
author = {Wellman, Briana Lowe and Davis, James and Anderson, Monica},
title = {Alice and robotics in introductory CS courses},
year = {2009},
isbn = {9781605582177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1565799.1565822},
doi = {10.1145/1565799.1565822},
abstract = {Statistics for underrepresented minority groups and women continue to show low numbers in enrollment and rates of retention in academic computer science programs. A new approach to increase student interest in computer science in a first year program is introduced.Laboratory modules for an introductory programming course have been developed at the University of Alabama with the goal to increase student motivation and understanding of fundamental programming concepts. The course utilizes robots and Alice, a 3D graphical programming environment. The drag and drop interface of Alice allows students to program real robots using instructions that correspond to statements of programming languages such as Java, C++, and C#. Students gain programming experience that is transferable to upper level courses by engaging in a stimulating and less frustrating environment using Alice interfaced with robots.},
booktitle = {The Fifth Richard Tapia Celebration of Diversity in Computing Conference: Intellect, Initiatives, Insight, and Innovations},
pages = {98–102},
numpages = {5},
keywords = {education, diversity, computer science},
location = {Portland, Oregon},
series = {TAPIA '09}
}

@inproceedings{10.1145/3567512.3567527,
author = {van Binsbergen, L. Thomas and Fr\"{o}lich, Damian and Verano Merino, Mauricio and Lai, Joey and Jeanjean, Pierre and van der Storm, Tijs and Combemale, Benoit and Barais, Olivier},
title = {A Language-Parametric Approach to Exploratory Programming Environments},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567512.3567527},
doi = {10.1145/3567512.3567527},
abstract = {Exploratory programming is a software development style in which code is a medium for prototyping ideas and solutions, and in which even the end-goal can evolve over time. Exploratory programming is valuable in various contexts such as programming education, data science, and end-user programming. However, there is a lack of appropriate tooling and language design principles to support exploratory programming. This paper presents a host language- and object language-independent protocol for exploratory programming akin to the Language Server Protocol. The protocol serves as a basis to develop novel (or extend existing) programming environments for exploratory programming such as computational notebooks and command-line REPLs. An architecture is presented on top of which prototype environments can be developed with relative ease, because existing (language) components can be reused. Our prototypes demonstrate that the proposed protocol is sufficiently expressive to support exploratory programming scenarios as encountered in literature within the software engineering, human-computer interaction and data science domains.},
booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {175–188},
numpages = {14},
keywords = {protocol, notebooks, interpreters, REPLs, IDEs, Exploratory programming},
location = {Auckland, New Zealand},
series = {SLE 2022}
}

@inproceedings{10.1145/2538862.2544312,
author = {Anton, Gabriella and Berland, Matthew},
title = {Studio K: a game development environment designed for gains in computational thinking (abstract only)},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2544312},
doi = {10.1145/2538862.2544312},
abstract = {Studio K is a game design curriculum constructed to provide middle school students an engaging entry into computer science and programming. Developed at the University of Wisconsin-Madison, the program employs Microsoft's 3D programming environment, Kodu, in tandem with support and analytic tools for facilitators, and an online community that provides players with a support system. These key features are bolstered with incorporation of telemetry data gathered through design sessions that are used to improve site functioning, curriculum relevancy, and administrative tools. This program is applied across contexts, with applications in formal classrooms, informal extracurricular clubs or camps, and alternative learning environments such as library systems or home schools. Studio K provides a well-supported, unique entry into computer science in which programming knowledge becomes a tool that supports learners' goals in designing and developing games.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {723},
numpages = {1},
keywords = {game design, computer science education},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/1384271.1384341,
author = {Pedroni, Michela and Oriol, Manuel and Meyer, Bertrand and Albonico, Enrico and Angerer, Lukas},
title = {Course management with TrucStudio},
year = {2008},
isbn = {9781605580784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1384271.1384341},
doi = {10.1145/1384271.1384341},
abstract = {Ever growing expectations from students, university management and other stakeholders make course preparation increasingly time-consuming. Setting up a course from scratch requires producing many supporting documents such as syllabi, schedules, and course web sites listing the concepts being taught. This can be a considerable effort, taking time away from tasks with a more immediate pedagogical value, such as answering student questions and refining the concepts themselves.The TrucStudio course development framework supports a systematic approach to these necessary but arduous tasks. TrucStudio is organized like a modern programming environment, but its elements of discourse, rather than software modules, are units of knowledge such as notions, Trucs and clusters.In addition to course development, applications of TrucStudio include checking sound coverage of topics and comparing courses on an objective basis. This presentation focuses on two novel features of TrucStudio: version management of knowledge units and course information; and generation of output documents in various formats from knowledge units and other material managed by TrucStudio.},
booktitle = {Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education},
pages = {260–264},
numpages = {5},
keywords = {versioning, output generation, knowledge modeling, curriculum design, course design},
location = {Madrid, Spain},
series = {ITiCSE '08}
}

@inproceedings{10.1145/2808580.2808628,
author = {Ram\'{\i}rez-Benavides, Kryscia and Garc\'{\i}a, Franklin and Guerrero, Luis A.},
title = {Creating a protocol for collaborative mobile applications for kids between 4 and 6 years old},
year = {2015},
isbn = {9781450334426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808580.2808628},
doi = {10.1145/2808580.2808628},
abstract = {One of the basic requirements in education is to prepare students for participation in an information society in which knowledge will be the most important resource for development. Computer-Supported Collaborative Learning is one of the most promising approaches to enhance the learning process with the help of information and communication technology. At the same time, advances in technology and mobile devices in the last decade have increased the number of educational institutions adopting mobile tools in the learning process. This paper describes the design and implementation of a protocol for a collaborative mobile application. TITIBOTS Colab is a programming environment for kids between 4 and 6 years old. During the implementation of TITIBOTS Colab, while creating a working version of the protocol, our team found that all messages defined in the design worked properly in order to provide the communication rules for the client and server applications.},
booktitle = {Proceedings of the 3rd International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {317–324},
numpages = {8},
keywords = {robotics, protocol architecture, programming learning, mobile devices, early childhood education, collaborative work},
location = {Porto, Portugal},
series = {TEEM '15}
}

@article{10.1109/2.58220,
author = {Computer Staff},
title = {Gigabit Network Testbeds},
year = {1990},
issue_date = {September 1990},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {23},
number = {9},
issn = {0018-9162},
url = {https://doi.org/10.1109/2.58220},
doi = {10.1109/2.58220},
abstract = {A description is given of five testbeds developed to examine gigabit applications and network technologies: Aurora, Blanca, Casa, Nectar, and Vistanet. Aurora is an experimental wide-area network testbed whose main objective is to explore and evaluate technologies for phase three of the proposed gigabit National Research and Education Network. The goals of the Blanca network research program are to develop technologies supporting gigabit/second networks, to develop programming tools supporting advanced network-based applications, and to explore the relationships between network technology paradigms and application requirements. The intent of the Casa testbed is to demonstrate that distributed supercomputing using wide-area high-speed networks can provide new levels of computational resources for leading-edge scientific problems. For the Nectar testbed project, a gigabit/second or higher experimental network will be developed to connect a variety of high-performance hosts. The Vistanet research project is intended to help determine whether networks based on emerging public-network standards will satisfy the goals of the National Research and Education Network and to provide information on specifications for those standards.},
journal = {Computer},
month = {sep},
pages = {77–80},
numpages = {4}
}

@inproceedings{10.1145/2839509.2844598,
author = {Escherle, Nora A. and Ramirez-Ramirez, Silvia I. and Basawapatna, Ashok R. and Assaf, Dorit and Repenning, Alexander and Maiello, Carmine and Endo, Yasko Ch. and Nolazco-Flores, Juan A.},
title = {Piloting Computer Science Education Week in Mexico},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2844598},
doi = {10.1145/2839509.2844598},
abstract = {Computer Science Education Week activities, featuring online? programming tools embedded with tutorials, report large participation numbers. However, to truly broaden participation, activities need to be made accessible in international contexts. In 2014, Tecnol\'{o}gico de Monterrey and Instituto de Innovaci\'{o}n y Transferencia de Tecnolog\'{\i}a de Nuevo Le\'{o}n, modified the Scalable Game Design CS Ed Week activity to include a Mexican feasibility pilot study. The goal of the pilot was to broaden participation in Computer Science in Mexico by creating interest and demand in further activities, including launching of 2015 Mexico CS Ed Week. This paper reviews the initial results of this 2014 pilot, including the discussion of the unique challenges faced in this context, and examines efforts to make this activity more accessible and successful. In addition to pilot data highlighting future activity improvements, initial retention results show that despite challenges, Mexican students were able to effectively use the modified activity to create games on par with U.S. students.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {431–436},
numpages = {6},
keywords = {international research, globalization of programming activities., experience report, computer science education week, computer science education in mexico, computer science education, broadening participation},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1109/ISCSCT.2008.146,
author = {Xiajiong, Shen and Ge, Wang and Jun, Gu and Xinfa, Dong},
title = {A Novel Visual Programming Method Designed for Error Rate Reduction},
year = {2008},
isbn = {9780769534985},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISCSCT.2008.146},
doi = {10.1109/ISCSCT.2008.146},
abstract = {Utilize the programming environment at present, it’s effortless for us to design intuitionistic visual program. It only implements the visualization of programming ending. The process of programming is not fully visualization and it’s not visual programming language in its true sense. According to the related research on visual programming languages in the past, the language is designed for special area or education, which is of pertinence and ordinary. If the programming language is made up of a series of graphic expressions it’s under the name of visual programming language. So the goal of our research is to construct a kind of visual programming language which is propitious to program in graphic components, is convenient for comprehension to programmers and nonprogrammer and the most important is it can reduce error rate which is caused by former textual input and recede the workload in the course of lexical analysis and semantic parsing.},
booktitle = {Proceedings of the 2008 International Symposium on Computer Science and Computational Technology - Volume 01},
pages = {280–283},
numpages = {4},
keywords = {visualization, visual programming language, error},
series = {ISCSCT '08}
}

@inproceedings{10.1145/3545945.3569736,
author = {Friend, Michelle and Swift, Andrew W. and Love, Betty and Winter, Victor},
title = {A Wolf in Lamb's Clothing: Computer Science in a Mathematics Course},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569736},
doi = {10.1145/3545945.3569736},
abstract = {If computer science programs face a challenge of convincing students that programming is fun and achievable, they have nothing on mathematics departments who face societal beliefs that math is hard and scary. Several movements in computer science education have focused on broadening participation within computer science and across disciplines. The "CS + X" efforts have focused on helping computer science integrate into other disciplines. The "CS For All" movement has highlighted the importance of providing high quality computing education for all students. Simultaneously, there is increasing attention to the need to provide general education alternatives to college algebra. This paper describes a course designed to combine these goals: a course that uses programming to introduce students to functions, patterns, and spatial and computational thinking in order to meet quantitative reasoning goals set by the university. The course initially used Bricklayer as the programming environment, then transitioned to Processing. Students were successful in writing programs that created art, demonstrated mastery of quantitative literacy, and had improved attitudes following the course. This project suggests that in addition to the creation of introductory computer science classes, courses which embed computer science into disciplinary requirements can be a successful pathway to expand opportunities for students to learn computing.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {256–262},
numpages = {7},
keywords = {cs+x, interdisciplinary, mathematics},
location = {<conf-loc>, <city>Toronto ON</city>, <country>Canada</country>, </conf-loc>},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/2047594.2047649,
author = {Manzo, V.J. and Halper, Matthew and Halper, Michael},
title = {Multimedia-based visual programming promoting core competencies in IT education},
year = {2011},
isbn = {9781450310178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047594.2047649},
doi = {10.1145/2047594.2047649},
abstract = {Programming constitutes one of the core competencies demanded of any IT education. However, some students within certain specializations of this diverse discipline are inclined to question the need for programming. The use of a visual programming environment in the development of interactive multimedia applications can serve the dual purposes of getting students excited about programming and giving them the core knowledge they need. The visual language Max/MSP/Jitter ("Max"), geared toward music, audio, and video application programming, is introduced as an excellent vehicle toward achieving this goal. The foundational constructs of Max are introduced in a series of example programs dealing with music applications. Some details of an undergraduate IT course called "Interactive Music System Technology" that utilizes Max are presented. Overall, the use of Max in the undergraduate IT curriculum can enhance the student's experience (both in multimedia and in IT in general) and promote better programming skills.},
booktitle = {Proceedings of the 2011 Conference on Information Technology Education},
pages = {203–208},
numpages = {6},
keywords = {visual programming language, multimedia, digital audio, computer music, Max/MSP/Jitter, IT programming},
location = {West Point, New York, USA},
series = {SIGITE '11}
}

@inproceedings{10.1145/3230977.3231013,
author = {Fagerlund, Janne},
title = {A Study on the Assessment of Introductory Computational Thinking via Scratch Programming in Primary Schools},
year = {2018},
isbn = {9781450356282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230977.3231013},
doi = {10.1145/3230977.3231013},
abstract = {Computational thinking (CT), a transversal intellectual foundation integral to computer science, is making its way into compulsory comprehensive education worldwide. Students are expected to attain skills and knowledge in such interdisciplinary CT principles as Algorithmic thinking, Data representation, and Debugging. Problem-solving by designing and manipulating interactive media with Scratch, a graphical programming tool, is popular especially at the primary school level. However, there has been confusion regarding how introductory CT can be operationalized for educational practice. Teachers and students need research-based knowledge for setting appropriate learning goals in addition to instruments for formative assessment that potentially improve the quality of learning. This study contributes to these issues by developing the assessment for learning of CT via Scratch in primary school settings. A review on prior studies involving the assessment of CT-related computational ideas in Scratch has led to the conceptualization of a revised assessment framework. Next steps in the study are analyzing fourth grade students' (N=58) Scratch projects and exploring complementary methods for analyzing CT in video recordings of the students' programming processes.},
booktitle = {Proceedings of the 2018 ACM Conference on International Computing Education Research},
pages = {264–265},
numpages = {2},
keywords = {scratch, primary school, graphical programming, education, computational thinking, assessment},
location = {Espoo, Finland},
series = {ICER '18}
}

@techreport{10.5555/896309,
author = {Schroedl, Stefan},
title = {An Extension of Explanation-Based Generalization to Negation as Failure},
year = {1995},
publisher = {Albert-Ludwigs University at Freiburg},
abstract = {Implementations of {em Explanation-Based Generalization (EBG)} within a logic-programming environment, as e.g. the well-known PROLOG-EBG algorithm, are able to generalize the proof of a goal from a {em definite} (i.e. Horn clause) domain theory. However, it is a fact that practical applications frequently require the enhanced expressiveness of {em negations} in rule bodies. Specifically, this is the case for the domain of game playing, where traditional EBG has turned out to be inadequate. In this paper we present an approach which extends EBG to this more general setting; it is described in the form of a transformation system, and comprises Siqueira and Puget''s method of {em Explanation-Based Generalization of Failures} for definite programs. For the case that both domain theory and training example are represented as {em allowed} normal programs, we prove that the derived clause satisfies the standard requirements for EBG, namely {em operationality}, {em sufficiency}, and {em correctness}. Furthermore, a meta-interpreter implementation of the transformation system is provided.}
}

@inproceedings{10.1145/3159450.3162177,
author = {Shaffer, Clifford A. and Brusilovsky, Peter and Koedinger, Kenneth R. and Edwards, Stephen H.},
title = {CS Education Infrastructure for All: Interoperability for Tools and Data Analytics (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162177},
doi = {10.1145/3159450.3162177},
abstract = {CS Education makes heavy use of online educational tools like IDEs, Learning Management Systems, eTextbooks, interactive programming environments, and other smart content. Instructors and students would benefit from greater interoperability between tools. CS Ed researchers increasingly make use of the large collections of data generated by click streams coming from them. However, we all face barriers that slow progress: (1) Educational tools do not integrate well. (2) Information about CS learning process and outcome data generated by one system is not compatible with that from other systems. (3) CS problem solving and learning (e.g., coding solutions) is different from the type of data (discrete answers to questions or verbal responses) that current educational data mining focuses on. This BOF will discuss ways that we might support and better coordinate efforts to build community and capacity among CS Ed researchers, data scientists, and learning scientists toward reducing these barriers. CS Ed infrastructure should support broader re-use of innovative learning content that is instrumented for rich data collection, formats and tools for analysis of learner data, and best practices to make large collections of learner data available to researchers. Achieving these goals requires engaging a large community of researchers to define, develop, and use critical elements of this infrastructure to address specific data-intensive research questions.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1063},
numpages = {1},
keywords = {student analytics, smart content, interoperability, infrastructure, computer science education research, LTI},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/3397617.3397833,
author = {Quayyum, Farzana and Bueie, Jonas and Vidal, Juan Carlos Torrado and Jaccheri, Letizia},
title = {Understanding coding activities for teens: a focus on school teachers' perspectives},
year = {2020},
isbn = {9781450380201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397617.3397833},
doi = {10.1145/3397617.3397833},
abstract = {Over the last few years, researchers, teachers, parents, volunteers, and even IT companies have joined efforts to develop coding activities for children in K-12 education. These efforts include technological tools and programming environments as well as activities descriptions. Kodel\o{}ypa is a coding activity offered by NTNU, which focuses on engaging teens in creative programming. In this paper, we report about the design and implementation of an empirical investigation with 13 teachers who attended Kodel\o{}ypa as associated school teachers of the pupils from their respective schools. In this study, we have addressed the following research question: What are the teachers' understandings of coding activities for teens outside the schools? The goal of this study was to identify various factors that will help us to acquire knowledge on this important kind of stakeholders, and improve the design and implementation of Kodel\o{}ypa and other similar efforts. We have conducted a thematic analysis with the data and we expect the results of this study will help teachers and researchers to design and organize computer science learning activities more efficiently and collaboratively.},
booktitle = {Proceedings of the 2020 ACM Interaction Design and Children Conference: Extended Abstracts},
pages = {187–192},
numpages = {6},
keywords = {teenagers, programming for teens, kodel\o{}ypa, computational thinking, coding activities},
location = {London, United Kingdom},
series = {IDC '20}
}

@inproceedings{10.1145/1384271.1384289,
author = {van Tonder, Martin and Naude, Kevin and Cilliers, Charmain},
title = {Jenuity: a lightweight development environment for intermediate level programming courses},
year = {2008},
isbn = {9781605580784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1384271.1384289},
doi = {10.1145/1384271.1384289},
abstract = {The complexity and resource requirements of professional IDEs mean that they are unsuitable for use in intermediate level programming courses. Jenuity is an efficient development environment for the Java programming language. Efficiency is essential as students often have outdated hardware unable to run mainstream development environments. This is of particular relevance in the context of a developing country. Jenuity provides advanced features usually associated with more resource intensive tools. It provides a simple and intuitive interface, which is well suited to intermediate level programming courses. Jenuity has been used successfully in the teaching of these courses at the authors' institution since 2004. The requirements, development and optimisation of this tool are discussed. Techniques used to optimise Jenuity for low specification student hardware, some of which are novel, are presented. Experiences using Jenuity in a university environment are also reported. The efficiency of Jenuity is also demonstrated by means of a comparison to mainstream development environments.},
booktitle = {Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education},
pages = {58–62},
numpages = {5},
keywords = {visual interface builder, jenuity, intermediate level programming, integrated development environment, cs2},
location = {Madrid, Spain},
series = {ITiCSE '08}
}

@article{10.1007/s00146-021-01328-4,
author = {Johansen, Johanna and Pedersen, Tore and Johansen, Christian},
title = {Studying human-to-computer bias transference},
year = {2022},
issue_date = {Aug 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {38},
number = {4},
issn = {0951-5666},
url = {https://doi.org/10.1007/s00146-021-01328-4},
doi = {10.1007/s00146-021-01328-4},
abstract = {It is generally agreed that one origin of machine bias is resulting from characteristics within the dataset on which the algorithms are trained, i.e., the data does not warrant a generalized inference. We, however, hypothesize that a different ‘mechanism’ may also be responsible for machine bias, namely that biases may originate from (i) the programmers’ cultural background, including education or line of work, or (ii) the contextual programming environment, including software requirements or developer tools. Combining an experimental and comparative design, we study the effects of cultural and contextual metaphors, and test whether each of these are ‘transferred’ from the programmer to the program, thus constituting a machine bias. Our results show that (i) cultural metaphors influence the programmer’s choices and (ii) contextual metaphors induced through priming can be used to moderate or exacerbate the effects of the cultural metaphors. Our studies are purposely performed with users of varying educational backgrounds and programming skills stretching from novice to proficient.},
journal = {AI Soc.},
month = {dec},
pages = {1659–1683},
numpages = {25},
keywords = {Randomized controlled trial, Priming, Metaphors, Cultural background, AI, Programmers, Biases}
}

@article{10.1145/122050.122057,
author = {Richard, Philippe},
title = {Research at Alta\"{\i}r},
year = {1991},
issue_date = {March 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/122050.122057},
doi = {10.1145/122050.122057},
abstract = {Alta\"{\i}r is a five year project which began in September of 1986. Its goal is to design an implement a next generation database system. The five year project was divided in two phases: a three year prototyping phase and a two year phase devoted for one part to the development of a product from the prototype and for the other part to a new research effort.The three year phase ended by the demonstration of the V1 prototype of the O2 object-oriented database system which has been distributed for experimentation to more than 40 universities and about 13 industrial partners.The contribution of the Alta\"{\i}r group to the database research community has been mainly in three areas: data model and database languages, object stores, and programming environments. Furthermore, all these efforts have been integrated in a consistent way into the V1 prototype. The results of this research and development effort are being summarized in a book [BDK91] which is a commented collection of papers, most of them already published in the proceedings of a number of internationally recognized conferences. We briefly survey in the following the O2 activities.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {53–59},
numpages = {7}
}

@article{10.1145/6465.20890,
author = {Bahlke, Rolf and Snelting, Gregor},
title = {The PSG system: from formal language definitions to interactive programming environments},
year = {1986},
issue_date = {Oct. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/6465.20890},
doi = {10.1145/6465.20890},
abstract = {The PSG programming system generator developed at the Technical University of Darmstadt produces interactive, language-specific programming environments from formal language definitions. All language-dependent parts of the environment are generated from an entirely nonprocedural specification of the language's syntax, context conditions, and dynamic semantics. The generated environment consists of a language-based editor, supporting systematic program development by named program fragments, an interpreter, and a fragment library system. The major component of the environment is a full-screen editor, which allows both structure and text editing. In structure mode the editor guarantees prevention of both syntactic and semantic errors, whereas in textual mode it guarantees their immediate recognition. PSG editors employ a novel algorithm for incremental semantic analysis which is based on unification. The algorithm will immediately detect semantic errors even in incomplete program fragments. The dynamic semantics of the language are defined in denotational style using a functional language based on the lambda calculus. Program fragments are compiled to terms of the functional language which are executed by an interpreter. The PSG generator has been used to produce environments for Pascal, ALGOL 60, MODULA-2, and the formal language definition language itself.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {aug},
pages = {547–576},
numpages = {30}
}

@inproceedings{10.1109/IPDPSW.2015.51,
author = {Finlayson, Ian and Mueller, Jerome and Rajapakse, Shehan and Easterling, Daniel},
title = {Introducing Tetra: An Educational Parallel Programming System},
year = {2015},
isbn = {9781467376846},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPSW.2015.51},
doi = {10.1109/IPDPSW.2015.51},
abstract = {Despite the fact that we are firmly in the multicore era, the use of parallel programming is not as widespread as it could be - in the software industry or in education. There have been many calls to incorporate more parallel programming content into undergraduate computer science education. One obstacle to doing this is that the programming languages most commonly used for parallel programming are detailed, low-level languages such as C, C++, Fortran (with OpenMP or MPI), OpenCL and CUDA. These languages allow programmers to write very efficient code, but that is not so important for those whose goal is to learn the concepts of parallel computing. This paper introduces a parallel programming language called Tetra which provides parallel programming features as first class language features, and also provides garbage collection and is designed to be as simple as possible. Tetra also includes an integrated development environment which is specifically geared for debugging parallel programs and visualizing program execution across multiple threads.},
booktitle = {Proceedings of the 2015 IEEE International Parallel and Distributed Processing Symposium Workshop},
pages = {746–751},
numpages = {6},
keywords = {Parallel Programming, Education, Debugging},
series = {IPDPSW '15}
}

@inproceedings{10.5555/3618408.3619075,
author = {Khalili, Mohammad Mahdi and Zhang, Xueru and Abroshan, Mahed},
title = {Loss balancing for fair supervised learning},
year = {2023},
publisher = {JMLR.org},
abstract = {Supervised learning models have been used in various domains such as lending, college admission, face recognition, natural language processing, etc. However, they may inherit pre-existing biases from training data and exhibit discrimination against protected social groups. Various fairness notions have been proposed to address unfairness issues. In this work, we focus on Equalized Loss (EL), a fairness notion that requires the expected loss to be (approximately) equalized across different groups. Imposing EL on the learning process leads to a non-convex optimization problem even if the loss function is convex, and the existing fair learning algorithms cannot properly be adopted to find the fair predictor under the EL constraint. This paper introduces an algorithm that can leverage off-the-shelf convex programming tools (e.g., CVXPY (Diamond and Boyd, 2016; Agrawal et al., 2018)) to efficiently find the global optimum of this non-convex optimization. In particular, we propose the ELminimizer algorithm, which finds the optimal fair predictor under EL by reducing the non-convex optimization to a sequence of convex optimization problems. We theoretically prove that our algorithm finds the global optimal solution under certain conditions. Then, we support our theoretical results through several empirical studies.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {667},
numpages = {20},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}

@article{10.1016/S0167-8191(02)00189-8,
author = {S\'{e}rot, Jocelyn and Ginhac, Dominique},
title = {Skeletons for parallel image processing: an overview of the SKIPPER project},
year = {2002},
issue_date = {December 2002},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {28},
number = {12},
issn = {0167-8191},
url = {https://doi.org/10.1016/S0167-8191(02)00189-8},
doi = {10.1016/S0167-8191(02)00189-8},
abstract = {This paper is a general overview of the SKIPPER project, run at Blaise Pascal University between 1996 and 2002. The main goal of the SKIPPER project was to demonstrate the applicability of skeleton-based parallel programming techniques to the fast prototyping of reactive vision applications. This proiect has produced several versions of a full-fledged integrated parallel programming environment (PPE). These PPEs have been used to implement realistic vision applications, such as road following or vehicle tracking for assisted driving, on embedded parallel platforms embarked on semi-autonomous vehicles. All versions of SKIPPER share a common front-end and repertoire of skeletons--presented in previous papers--but differ in the techniques used for implementing skeletons. This paper focuses on these implementation issues, by making a comparative survey, according to a set of four criteria (efficiency, expressivity, portability, predictability), of these implementation techniques. It also gives an account of the lessons we have learned, both when dealing with these implementation issues and when using the resulting tools for prototyping vision applications.},
journal = {Parallel Comput.},
month = {dec},
pages = {1685–1708},
numpages = {24},
keywords = {parallelism, fast prototyping, data-flow, computer vision, Skeleton}
}

@article{10.5555/1629036.1629063,
author = {Wellman, Briana Lowe and Anderson, Monica and Vrbsky, Susan V.},
title = {PREOP as a tool to increase student retention in CS},
year = {2009},
issue_date = {December 2009},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {25},
number = {2},
issn = {1937-4771},
abstract = {The demand for computer scientists is expected to continue to increase irrespective of the current state of the economy. Unfortunately, the supply is not expected to match the demand as the number of computer science majors has decreased substantially since the year 2000. As a result, universities and colleges are attempting to identify new ways to attract and retain prospective students into the field of computer science in order to increase the number of majors. In this paper we describe our approach to increase participation and retention through the use of PREOP (Providing Robotic Experiences through Object-Based Programming), an approach that combines the Alice interface and robots for a CS1 Laboratory. PREOP is an interactive 3D animation programming environment, that allows students to program real robots using a drag-and-drop, syntax-free interface. The goal is to foster student motivation and increase student understanding of the fundamental concepts within the first-year curriculum. Initial results indicate that the students in the PREOP Lab who are eligible for CS2 are more likely to rate their skills and knowledge above average than the students in the non-PREOP Labs, and more likely to be registered for the CS2 course than the students in the non-PREOP Labs.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {167–175},
numpages = {9}
}

@inproceedings{10.5555/788012.788356,
author = {Lin, Mengjou and Singer, D.},
title = {An experimental ATM-based video-on-demand system on a Macintosh platform},
year = {1996},
isbn = {0818676175},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Video-on-demand (VOD) systems can support multiple concurrent video accesses with guaranteed user-controlled quality of service (QoS). The growth of VOD systems is driven by the increasing need for distributed multimedia applications such as company training, distance learning, telemedicine, and home entertainment. We propose and demonstrate an ATM-based (asynchronous transfer mode) VOD system (including VOD servers, clients and connection networks) based on a Macintosh platform. ATM is a promising network standard which may satisfy various requirements of VOD systems. In this VOD system, time-based data are encoded and decoded using the MPEG (Moving Picture Experts Group) standard. Issues related to VOD system design, including the multimedia programming environment, the required system modification to support remote MPEG playback, ATM networking, process scheduling, and buffer management for both VOD servers and clients, are discussed and analyzed based on the Macintosh platform. A prototype VOD system is used to explore the design issues proposed. The Macintosh platform provided an efficient and jitter-free VOD system supporting MPEG movie playback through ATM networks.},
booktitle = {Proceedings of the 21st Annual IEEE Conference on Local Computer Networks},
pages = {59},
keywords = {video on demand system, telemedicine, remote MPEG playback, process scheduling, network standard, multimedia programming environment, home entertainment, distributed multimedia applications, distance learning, connection networks, company training, buffer management, asynchronous transfer mode, VOD systems, VOD system design, VOD servers, Moving Picture Experts Group, Macintosh platform, MPEG standard, MPEG movie playback, ATM networks, ATM networking},
series = {LCN '96}
}

@inproceedings{10.5555/1647636.1647741,
author = {Sanchez, Daniel and Shirk, Andrew and Sanchez, Danmary and Marrero, Adrian},
title = {Introducing the data mining utility: a web-based application for data mining experimentation on neurological datasets},
year = {2007},
isbn = {9780889867062},
publisher = {ACTA Press},
address = {USA},
abstract = {Data mining applications have facilitated knowledge discovery in a multitude of areas including marketing, biological and geological sciences, and image processing. Through well established mathematical algorithms related to pattern recognition, predictive modeling, and anomaly detection, warehouses of data have been explored and interpreted with the purposes of helping researchers unveil and understand complex patterns. In this paper, we present the development of a web-based Data Mining Utility, created with the ultimate goal of allowing medical researchers to detect new artifacts that have been previously obscured through conventional signal processing techniques in neurological datasets. This web tool is powered by Data to Knowledge, which was created by the Automated Learning Group of the National Center for Supercomputing Applications at the University of Illinois, by integrating a vast library of customizable data mining techniques into a unified programming environment. Using this framework, along with the D2K Web Service, this Data Mining Utility will enhance an established Multi-Site Pediatric Network for fMRI in Childhood Epilepsy, as a means for researchers to gain further insight on medical case studies.},
booktitle = {Proceedings of the 11th IASTED International Conference on Software Engineering and Applications},
pages = {603–608},
numpages = {6},
keywords = {web services, distributed computing, data mining},
location = {Cambridge, Massachusetts},
series = {SEA '07}
}

@inproceedings{10.1109/ICST.2010.19,
author = {Sinha, Avik and Jr., Stanley M. Sutton and Paradkar, Amit},
title = {Text2Test: Automated Inspection of Natural Language Use Cases},
year = {2010},
isbn = {9780769539904},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICST.2010.19},
doi = {10.1109/ICST.2010.19},
abstract = {The modularity and customer centric approach of use cases make them the preferred methods for requirement elicitation, especially in iterative software development processes as in agile programming. Numerous guidelines exist for use case style and content, but enforcing compliance to such guidelines in the industry currently requires specialized training and a strongly managed requirement elicitation process. However, often due to aggressive development schedules, organizations shy away from such extensive processes and end up capturing use cases in an ad-hoc fashion with little guidance. This results in poor quality use cases that are seldom fit for any downstream software activities. We have developed an approach for automated and “edittime”inspection of use cases based on the construction and analysis of models of use cases. Our models contain linguistic properties of the use case text along with the functional properties of the system under discussion. In this paper, we present a suite of model analysis techniques that leverage such models to validate uses cases simultaneously for their style and content. Such model analysis techniques can be combined with a robust NLP techniques to develop integrated development environments for use case authoring, as we do in Text2Test.When used in an industrial setting, Text2Test resulted in better compliance of use cases, in enhanced productivity},
booktitle = {Proceedings of the 2010 Third International Conference on Software Testing, Verification and Validation},
pages = {155–164},
numpages = {10},
keywords = {Use Cases, Text2Test, Testing, Requirements, Automated Inspection, Analysis},
series = {ICST '10}
}

@inproceedings{10.1145/3484272.3484970,
author = {Anderson, Boyd and Henz, Martin and Low, Kok-Lim and Tan, Daryl},
title = {Shrinking JavaScript for CS1},
year = {2021},
isbn = {9781450390897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3484272.3484970},
doi = {10.1145/3484272.3484970},
abstract = {In teaching and learning programming at first-year-university level, simple languages with small feature sets are preferable over industry-strength languages with extensive feature sets, to reduce the learners' cognitive load. At the same time, there is increasing pressure to familiarise students with mainstream languages early in their learning journey, and these languages accumulate features as years go by. In response to these competing requirements, we developed Source, a collection of JavaScript sublanguages with feature sets just expressive enough to introduce first-year computer science students to the elements of computation. These languages are supported by a web-based programming environment custom-built for learning at beginner's level, which provides transpiler, interpreter, virtual machine, and algebraic-stepper-based implementations of the languages, and includes tracing, debugging, visualization, type-inference, and smart-editor features. This paper motivates the choice of JavaScript as starting point and describes the syntax and semantics of the Source languages compared to their parent language, and their implementations in the system. We report our experiences in developing and improving the languages and implementations over a period of three years, teaching a total of 1561 computer science first-year students at a university.},
booktitle = {Proceedings of the 2021 ACM SIGPLAN International Symposium on SPLASH-E},
pages = {87–96},
numpages = {10},
keywords = {teaching programming, learning tools, learning environments, JavaScript},
location = {Chicago, IL, USA},
series = {SPLASH-E 2021}
}

@inproceedings{10.1609/aaai.v37i13.26878,
author = {Geleta, Margarita and Xu, Jiacen and Loya, Manikanta and Wang, Junlin and Singh, Sameer and Li, Zhou and Gago-Masague, Sergio},
title = {Maestro: a gamified platform for teaching AI robustness},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i13.26878},
doi = {10.1609/aaai.v37i13.26878},
abstract = {Although the prevention of AI vulnerabilities is critical to preserve the safety and privacy of users and businesses, educational tools for robust AI are still underdeveloped worldwide. We present the design, implementation, and assessment of Maestro. Maestro is an effective open-source game-based platform that contributes to the advancement of robust AI education. Maestro provides goal-based scenarios where college students are exposed to challenging life-inspired assignments in a competitive programming environment. We assessed Maestro's influence on students' engagement, motivation, and learning success in robust AI. This work also provides insights into the design features of online learning tools that promote active learning opportunities in the robust AI domain. We analyzed the reflection responses (measured with Likert scales) of 147 undergraduate students using Maestro in two quarterly college courses in AI. According to the results, students who felt the acquisition of new skills in robust AI tended to appreciate highly Maestro and scored highly on material consolidation, curiosity, and maestry in robust AI. Moreover, the leaderboard, our key gamification element in Maestro, has effectively contributed to students' engagement and learning. Results also indicate that Maestro can be effectively adapted to any course length and depth without losing its educational quality.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1809},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{10.1145/800227.806922,
author = {Wasserman, Anthony I.},
title = {The extension of data abstraction to database management},
year = {1980},
isbn = {0897910311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800227.806922},
doi = {10.1145/800227.806922},
abstract = {The long-term goal of the User Software Engineering (USE) project at the University of California, San Francisco, is to provide an integrated homogeneous programming environment for the design and development of interactive information systems. Realization of this goal involves the development of new software tools, their integration with existing tools, and the creation of an information system development methodology in which these tools are systematically used [1,2].The successful construction of interactive information systems requires the utilization of principles of user-centered design [3,4,5], combined with features traditionally associated with the separate areas of programming languages, operating systems, and data base management [6]. It has become increasingly clear that the key to being able to provide such a unified view lies in providing a unified view of data [7]. The potential benefits of such a unification are considerable, including:1) conceptual simplification of the system structure permitting, for example, joint design of data structures and data bases2) the elimination of duplication or inconsistencies among diverse software components3) the ability to achieve greater reliability in systems because of reduced dependence upon multiple software systems},
booktitle = {Proceedings of the 1980 Workshop on Data Abstraction, Databases and Conceptual Modeling},
pages = {198–200},
numpages = {3},
location = {Pingree Park, Colorado, USA}
}

@book{10.5555/558225,
author = {Light, Ann and Publishing, Dorling Kindersley},
title = {Essential Computers: Enhancing Your Website},
year = {2001},
isbn = {0789472880},
publisher = {DK Publishing, Inc.},
abstract = {From the Publisher:World-renowned for its digital expertise, Dorling Kindersley shares its know-how in a series of user-friendly visual guides to all the computer skills you'll ever need.  Master state-of-the-art computer skills quickly and confidently with the Essential Computers series. These clear and concise step-by-step visual guides are designed to help beginners acquire all the techniques necessary to use today's information technology, from word processing to desktop publishing to setting up e-commerce and researching on the Internet. Enhancing Your Website helps you master the basic skills to tailor your website to your needs, and includes: Identifying your requirements, Assessing website design and effectiveness, Examining websites for inspiration, Understanding HTML, Viewing website source code, Importing source code, Achieving browser compatibility, Discovering other programming tools, such as Perl, and JavaScript, and Integrating your site into the web. Author Biography: Ann Light and Des Watson are both members of staff at the School of Cognitive and Computing Sciences at the University of Sussex, England. Ann Light is researching into factors affecting website effectiveness, and Des Watson is a senior lecturer in software systems, working primarily in the field of high-level language implementation.}
}

@inproceedings{10.1145/3159450.3162299,
author = {Micka, Samuel Adam and Fasy, Brittany Terese and Hancock, Stacey A. and Madubuko, Jachiike C. and Theobold, Allison Shay},
title = {American Indian Storytelling with Alice: (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162299},
doi = {10.1145/3159450.3162299},
abstract = {Montana is home to a large American Indian population and a rich history. The Indian Education for All (IEFA) Act, passed in 1999, reinforces the educational goals stated in Montana's 1972 Constitution that "every Montanan, whether Indian or non-Indian, be encouraged to learn about the distinct and unique heritage of American Indians in a culturally responsive manner." IEFA requires that American Indian education be integrated into "the education of each Montana citizen," making Montana the only state to mandate Indian education by law. We propose an integration of CS concepts into existing content standards using the IEFA curricula. To make these concepts approachable, we utilize Alice, a drag-and-drop programming environment. This software allows students to animate stories while learning programming techniques in a user-friendly way. Furthermore, Alice 2 allows customized models; in particular, we can create models specific to American Indian culture. In this poster, we present an overview of the Storytelling project and preliminary results, an example lesson plan, evaluation techniques, and a description of the 3D model creation process. With these lesson plans and customized models, we strive to broaden participation of students from rural and American Indian communities in CS and related fields.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1098},
numpages = {1},
keywords = {middle school education, computational thinking, Indian education for all},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/1411260.1411264,
author = {Page, Rex and Eastlund, Carl and Felleisen, Matthias},
title = {Functional programming and theorem proving for undergraduates: a progress report},
year = {2008},
isbn = {9781605580685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1411260.1411264},
doi = {10.1145/1411260.1411264},
abstract = {For the past five years, the University of Oklahoma has used the ACL2 theorem prover for a year-long sequence on software engineering. The goal of the course is to introduce students to functional programming with "Applicative Common Lisp" (ACL) and to expose them to defect recognition at all levels, including unit testing, randomized testing of conjectures, and formal theorem proving in "a Computational Logic" (ACL2).Following Page's example, Northeastern University has experimented with the introduction of ACL2 into the freshman curriculum for the past two years. Northeastern's goal is to supplement an introductory course on functional program design with a course on logic and theorem proving that integrates the topic with programming projects.This paper reports on our joint project's progress. On the technical side, the paper presents the Scheme-based integrated development environment, its run-time environment for functional GUI programming, and its support for different forms of testing. On the experience side, the paper summarizes the introduction of these tools into the courses, the reaction of industrial observers of Oklahoma's software engineering course, and the feedback from a first outreach workshop.},
booktitle = {Proceedings of the 2008 International Workshop on Functional and Declarative Programming in Education},
pages = {21–30},
numpages = {10},
keywords = {test-driven development, predicate-based testing, mechanical logic, formal methods, drscheme, dracula},
location = {Victoria, BC, Canada},
series = {FDPE '08}
}

@inproceedings{10.1145/3241815.3242586,
author = {Rahman, Farzana},
title = {Leveraging Visual Programming Language and Collaborative Learning to Broaden Participation in Computer Science},
year = {2018},
isbn = {9781450359542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241815.3242586},
doi = {10.1145/3241815.3242586},
abstract = {Engaging underrepresented populations of women and minorities in Computer Science (CS) represents our greatest untapped resource for increasing the STEM workforce. In recent years, tremendous efforts have been geared towards developing learning materials to increase the interest of underrepresented students in CS. More recently, CS education researchers are beginning to recognize the need to apply the learning sciences to develop age- and grade-appropriate curricula and pedagogies for developing computing competencies among children. One effective approach to build learning competencies among young underrepresented students is through Collaborative Learning, which is an educational approach that involves groups of learners working together to solve a problem or create a product. Our goal, in this paper, is to report our experiences on designing and delivering a curriculum that teaches programming to middle school students using App Inventor through collaborative learning. Our curriculum is developed on the hypothesis that visual programming environment, in this case, App Inventor, present an alternative way of learning programming, which in the collaborative learning environment can enhance programming competencies and interests in underrepresented students. In this experience report, we will describe how we implemented this curriculum as a block course; present our lessons learned, and few findings from the evaluation.},
booktitle = {Proceedings of the 19th Annual SIG Conference on Information Technology Education},
pages = {172–177},
numpages = {6},
keywords = {visual programming language, programming fundamentals, collaborative learning, appinventor},
location = {Fort Lauderdale, Florida, USA},
series = {SIGITE '18}
}

@book{10.5555/3265452,
author = {Felleisen, Matthias and Findler, Robert Bruce and Flatt, Matthew and Krishnamurthi, Shriram},
title = {How to Design Programs: An Introduction to Programming and Computing},
year = {2018},
isbn = {0262534800},
publisher = {The MIT Press},
abstract = {A completely revised edition, offering new design recipes for interactive programs and support for images as plain values, testing, event-driven programming, and even distributed programming. This introduction to programming places computer science at the core of a liberal arts education. Unlike other introductory books, it focuses on the program design process, presenting program design guidelines that show the reader how to analyze a problem statement, how to formulate concise goals, how to make up examples, how to develop an outline of the solution, how to finish the program, and how to test it. Because learning to design programs is about the study of principles and the acquisition of transferable skills, the text does not use an off-the-shelf industrial language but presents a tailor-made teaching language. For the same reason, it offers DrRacket, a programming environment for novices that supports playful, feedback-oriented learning. The environment grows with readers as they master the material in the book until it supports a full-fledged language for the whole spectrum of programming tasks. This second edition has been completely revised. While the book continues to teach a systematic approach to program design, the second edition introduces different design recipes for interactive programs with graphical interfaces and batch programs. It also enriches its design recipes for functions with numerous new hints. Finally, the teaching languages and their IDE now come with support for images as plain values, testing, event-driven programming, and even distributed programming.}
}

@inproceedings{10.1145/3638067.3638076,
author = {Zen, Eliana and Da Costa, Vinicius Kruger and Tavares, Tatiana Aires},
title = {Understanding the Accessibility Barriers Faced by Learners with Visual Impairments in Computer Programming},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638076},
doi = {10.1145/3638067.3638076},
abstract = {Assistive Technology (AT) has facilitated the integration and inclusion of people with disabilities in society. Regarding the visually impaired, the use of AT can be crucial in ensuring access to Information and Communication Technologies. However, in technical and higher-level courses related to computing, the use of AT may not be sufficient to ensure that these students comprehend the concepts addressed in the subjects and have access to all the resources provided by the tools necessary for their professional training. This is because screen readers, the main accessibility resource used by the visually impaired to interact with digital systems, generally perform a linear reading of the content available in the graphical interface, which can demand more time and effort from users. Additionally, this approach may limit access to information. This is particularly relevant in the case of Integrated Development Environment (IDEs), which have complex visual interfaces that may not be accessible to screen readers if accessibility requirements are not correctly implemented. Therefore, this study seeks to identify the barriers to interaction with IDEs faced by students in Computer Programming classes. The research involved surveying 12 professors of disciplines related to the area of Computer Programming who taught students with visual impairment. Additionally, interviews were conducted with 6 students and graduates with visual impairments from Technical and Higher Education courses in the area of Computing. The data were analyzed using Content Analysis. The results confirmed those already identified in the literature and also revealed new barriers.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {65},
numpages = {11},
keywords = {Accessibility, Assistive Technology, Computer Programming, Human-Computer Interaction, Visual Impairment},
location = {<conf-loc>, <city>Macei\'{o}</city>, <country>Brazil</country>, </conf-loc>},
series = {IHC '23}
}

@inproceedings{10.1145/3428029.3428059,
author = {Grover, Shuchi and Catet\'{e}, Veronica and Barnes, Tiffany and Hill, Marnie and Ledeczi, Akos and Broll, Brian},
title = {FIRST Principles to Design for Online, Synchronous High School CS Teacher Training and Curriculum Co-Design},
year = {2020},
isbn = {9781450389211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428029.3428059},
doi = {10.1145/3428029.3428059},
abstract = {The Covid-19 pandemic has offered new challenges and opportunities for teaching and research. It has forced constraints on in-person gathering of researchers, teachers, and students, and conversely, has also opened doors to creative instructional design. This paper describes a novel approach to designing an online, synchronous teacher professional development (PD) and curriculum co-design experience. It shares our work in bringing together high school teachers and researchers in four US states. The teachers participated in a 3-week summer PD on ideas of Distributed Computing and how to teach this advanced topic to high school students using NetsBlox, an extension of the Snap! block-based programming environment. The goal of the PD was to prepare teachers to engage in collaborative co-design of a 9-week curricular module for use in classrooms and schools. Between their own training and the co-design process, teachers co-taught a group of high school students enrolled in a remote summer internship at a university in North Carolina to pilot the learned units and leverage ideas from their teaching experience for subsequent curricular co-design. Formative and summative feedback from teachers suggest that this PD model was successful in meeting desired outcomes. Our generalizable FIRST principles—Flexibility, Innovativeness, Responsiveness (and Respect), Supports, and Teamwork (collaboration)—that helped make this unique PD successful, can help guide future CS teacher PD designs.},
booktitle = {Proceedings of the 20th Koli Calling International Conference on Computing Education Research},
articleno = {21},
numpages = {5},
location = {Koli, Finland},
series = {Koli Calling '20}
}

@inproceedings{10.1145/3408877.3439661,
author = {Tabassum, Moumita and Gray, Jeff and Smith, Derrick},
title = {Designing Accessibility into Blocks Languages},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3439661},
doi = {10.1145/3408877.3439661},
abstract = {The growing interest in teaching Computer Science (CS) to K-12 students has led to the development of blocks-based programming languages (BBPLs) for young students where a program can be created by connecting blocks in a structured way. One of the benefits of a BBPL is removing the necessity to recall the syntax rules of a language. However, the visual nature of these languages makes learning and creating code difficult for children with visual impairments and blindness (VIB). Although a few BBPLs were designed for children with VIB, such environments are rarely used by sighted children. Therefore, to investigate the requirements for designing and implementing an accessible blocks-based programming environment that can be used by both students with and without VIB, we developed a BBPL environment for middle school children. The environment consists of a two-dimensional grid where a character can be moved by creating a simple blocks program. Our future plans include performing several empirical experiments to evaluate our BBPL, including the educational impact of the environment on all students and accessibility support for students with VIB. To perform the study, we also plan to prepare lessons for training the students. The results of the study will inform the design of future BBPL environments, as well as contribute toward broadening participation in computing by enabling children with visual impairments to participate fully in CS education opportunities.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {1309},
numpages = {1},
keywords = {visual impairment, blocks-based programming environment},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/800174.809773,
author = {Saib, Sabina H.},
title = {Issues in Ada's future sponsored by ACM/adatc (Panel Discussion)},
year = {1982},
isbn = {0897910850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800174.809773},
doi = {10.1145/800174.809773},
abstract = {The Ada programming language has gone public. The Department of Defense originally sponsored the effort in order to develop a high level language which could be used in real time computer systems. Such systems are written in assembly language or in obscure languages such as CMS (Navy), JOVIAL (Air Force), and TACPOL (Army). Each of the high level languages in current use by the services has deficiencies which require the use of assembly language in almost all applications. In fact, most of the real time applications which are written supposedly in a high level language are written actually in assembly language because the compilers accept assembly language statements. This sad state of affairs has resulted because none of the current language can handle all of the requirements of real time applications. For example, there exist at least four dialects of JOVIAL (J2, J3, J3B, and J73) none of which have a basic input or output capability.Some of the problems which result from the use of assembly language and obscure languages are logistical rather than technical: a small number of people know these languages; there is a dearth of training material and courses; there is no portability of programs or people between systems which use these languages; the programming environments for each of the languages is poor; the languages are not available on many computer systems.},
booktitle = {Proceedings of the ACM '82 Conference},
pages = {118–120},
numpages = {3},
series = {ACM '82}
}

@inproceedings{10.1145/3408877.3432554,
author = {Branco, Andr\'{e} and Dutra, Claudia and Zumpichiatti, D\'{e}bora and Campos, Francisco Augusto and SantClair, Gabriel and Mello, Jhulian and Moreira, Jo\~{a}o Victor and Godinho, Julia and Marotti, Julia and Gomide, Janaina},
title = {Programming for Children and Teenagers in Brazil: A 5-year Experience of an Outreach Project},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432554},
doi = {10.1145/3408877.3432554},
abstract = {There has been a worldwide surge in programming education initiatives for children and teenagers. In Brazil, this trend faces some challenges, namely inadequate infrastructure of most schools, notably public ones, that lack access to computers and tablets, and basic education curricular requirements not contemplating computer science concepts. This article reports on the five-year experience of an outreach project from a public university in Brazil. The project aims to promote computer science education and to teach programming to children and teenagers. Undergraduate engineering students who participate in the project as members engage in activities such as planning the courses and their schedules, creating partnerships with local schools and other educational projects, giving lectures, producing scientific research and educational materials, as well as promoting the project on social media. The courses use free online programming tools, Python, MIT App Inventor, and Arduino to cover fundamental concepts of programming and computational thinking. They vary approaches and tools according to the age range and available technological resources of the target audience. The use of unplugged activities means to assist in learning and to circumvent computer access problems. Furthermore, they serve for introducing basic programming concepts in classes and motivating students with dynamic activities. Over its five-year existence, the project has achieved its purpose, by reaching a total of 2639 students through 45 workshops and 94 courses. It has provided courses in eleven public schools, created two booklets and one app as free educational material, along with presented papers and posters in scientific conferences.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {411–417},
numpages = {7},
keywords = {programming, k-12, didactic strategies, computer science education, computational thinking},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1007/978-3-031-16681-5_20,
author = {Dr\u{a}mnesc, Isabela and \'{A}brah\'{a}m, Erika and Jebelean, Tudor and Kusper, G\'{a}bor and Stratulat, Sorin},
title = {Experiments with&nbsp;Automated Reasoning in&nbsp;the&nbsp;Class},
year = {2022},
isbn = {978-3-031-16680-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-16681-5_20},
doi = {10.1007/978-3-031-16681-5_20},
abstract = {The European Erasmus+ project ARC – Automated Reasoning in the Class aims at improving the academic education in disciplines related to Computational Logic by using Automated Reasoning tools. We present the technical aspects of the tools as well as our education experiments, which took place mostly in virtual lectures due to the COVID pandemics. Our education goals are: to support the virtual interaction between teacher and students in the absence of the blackboard, to explain the basic Computational Logic algorithms, to study their implementation in certain programming environments, to reveal the main relationships between logic and programming, and to develop the proof skills of the students. For the introductory lectures we use some programs in C and in Mathematica in order to illustrate normal forms, resolution, and DPLL (Davis-Putnam-Logemann-Loveland) with its Chaff version, as well as an implementation of sequent calculus in the Theorema system. Furthermore we developed special tools for SAT (propositional satisfiability), some based on the original methods from the partners, including complex tools for SMT (Satisfiability Modulo Theories) that allow the illustration of various solving approaches. An SMT related approach is natural-style proving in Elementary Analysis, for which we developed and interesting set of practical heuristics. For more advanced lectures on rewrite systems we use the Coq programming and proving environment, in order on one hand to demonstrate programming in functional style and on the other hand to prove properties of programs. Other advanced approaches used in some lectures are the deduction based synthesis of algorithms and the techniques for program transformation.},
booktitle = {Intelligent Computer Mathematics: 15th International Conference, CICM 2022, Tbilisi, Georgia, September 19–23, 2022, Proceedings},
pages = {287–304},
numpages = {18},
keywords = {Computer aided teaching, Computational logic, Automated reasoning},
location = {Tbilisi, Georgia}
}

@inproceedings{10.1145/2839509.2850525,
author = {Wagner, Amber and Gray, Jeff and Marghitu, Daniela and Stefik, Andreas},
title = {Raising the Awareness of Accessibility Needs in Block Languages (Abstract Only)},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2850525},
doi = {10.1145/2839509.2850525},
abstract = {Block languages (e.g., Scratch, Snap!, Alice, App Inventor, Blockly) offer a gentle introduction to programming and have been adopted widely in both K-12 and CS0 courses. However, block languages often are dependent on the mouse/keyboard for input and typically are visual in their output and representation. Because of these dependencies, students with a disability (e.g., mobility limitations or vision impairment) generally are unable to use block languages, thereby reducing the opportunities for broader participation in computational learning activities. Given the increasing need to broaden the participation of computing to those with diverse skills and backgrounds, it is important that the tools used to initiate the earliest entre into computing do not erect immediate roadblocks that impede initial interest and opportunity. There are many variations of user interfaces and assistive technologies that benefit those who may have difficulties utilizing traditional Graphical User Interfaces (GUIs), but these tools often cannot be used universally across block languages. As more block languages are being developed and integrated into K12 and University curriculum, it is imperative that accessible solutions are discussed and implemented. These discussions require participation from the block language developer community, accessible computing community, and those educators who encounter accessibility needs among the students in their classrooms. The goal of this lightning talk is to call attention to the need for more accessible block-based programming environments and to spark conversation surrounding possible standard accessibility APIs that could possibly be supported by block language environment tool developers.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {497},
numpages = {1},
keywords = {broadening participation, block languages, accessibility},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/62402.62434,
author = {Teran, Marcelo and Rasure, John and Argiro, Danielle and Hallett, Stephanie and Neher, Ron and Young, Mark and Wilson, Scott},
title = {XVISION: a comprehensive software system for image processing research, education, and applications},
year = {1988},
isbn = {0897912837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/62402.62434},
doi = {10.1145/62402.62434},
abstract = {XVision is a software system for image processing research, education and applications. Xyvision utilizes the X Window System Version 11, which provides a network transparent windowing environment and software portability. Xyvision is designed to facilitate:
data and algorithm exchange of new computer vision/image processing techniques,image processing training and education,development of turn key vision solutions for various application areas (automation, medicine, biology, astronomy, etc).XVision is a comprehensive system because it supports generation of new programs (extendibility), and integration, maintenance, modification and documentation of existing programs; and it includes:
three user interfaces; a menuing system, a quick command line interface that can be customized and a standardized UNIX‡-like command line interface. A visual programming language, xvglyph, is under development.tutorials, manual pages, experiments, automated demonstrations and other supplemental documentation.an image processing library written in C,interactive image display and enhancement, image editing and creation, 2D, 3D, and contour plotting, and data creation/display via user specified functions.The Xyvision project started in February of 1987 with its first release in August of 1987. This paper describes the second version which incorporates changes suggested by many of the users (over 30 different institutions) of Xyvision Version 1.0 [1]. One of the most important design goals of the Xyvision project is to provide for easy growth and extendibility. This has been accomplished by clearly defining software levels, software systems and their standard interfaces, and by providing programming tools and a variety of user interfaces for the Xyvision user/maintainer.},
booktitle = {Proceedings of the 1st Annual ACM SIGGRAPH Symposium on User Interface Software},
pages = {203–210},
numpages = {8},
location = {Alberta, Canada},
series = {UIST '88}
}

@phdthesis{10.5555/932896,
author = {Kerkiz, Nabil Fouad and Bouldin, Dan},
title = {Development and experimental evaluation of partitioning algorithms for adaptive computing systems},
year = {2000},
isbn = {0493035788},
publisher = {The University of Tennessee},
abstract = {Multi-FPGA systems offer the potential to deliver higher performance solutions than traditional computers for some low-level computing tasks. This requires a flexible hardware substrate and an automated mapping system. CHAMPION is an automated mapping system for implementing image processing applications in multi-FPGA systems under development at the University of Tennessee. CHAMPION will map applications in the Khoros Cantata graphical programming environment to hardware. The work described in this dissertation involves the automation of the CHAMPION back-end design flow, which includes the partitioning problem, netlist to structural VHDL conversion, synthesis and placement and routing, and host code generation. The primary goal is to investigate the development and evaluation of three different k-way partitioning approaches. In the first and the second approaches, we discuss the development and implementation of two existing algorithms. The first approach is a hierarchical partitioning method based on topological ordering (HP). The second approach is a recursive algorithm based on the Fiduccia and Mattheyses bipartitioning heuristic (RP). We extend these algorithms to handle the multiple constraints imposed by adaptive computing systems. We also introduce a new recursive partitioning method based on topological ordering and levelization (RPL). In addition to handling the partitioning constraints, the new approach efficiently addresses the problem of minimizing the number of FPGAs used and the amount of computation, thereby overcoming some of the weaknesses of the HP and RP algorithms.},
note = {AAI9996361}
}

@techreport{10.5555/867115,
author = {Presberg, David and Jaeger, Christopher},
title = {Porting the Distributed Array Query and Visualization Tool for High Performance Fortran to the SP2},
year = {1996},
publisher = {Cornell University},
address = {USA},
abstract = {Distributed Array Query and Visualization (DAQV) is a Parallel Tools Consortium sponsored project to create a tool for visualizing distributed data in High Performance Fortran (HPF). The DAQV tool is currently maintained at the University of Oregon, and our goals here at the Cornell Theory Center (CTC) are to verify the work done by the people there, and port the DAQV tool and its associated visualization clients to our IBM SP2 and IBM''s HPF compiler, XL HPF. We describe in this paper the installation of the DAQV tool first on an SGI Onyx, using Portland Group Inc.''s HPF compiler, pghpf, which was already supported by DAQV. We make various modifications to the distribution to generalize the installation, and then port the DAQV tool to the IBM SP2, also using the pghpf compiler. Finally, we accomplish the port of DAQV to IBM''s XL HPF compiler. We describe our approaches to overcoming various obstacles encountered in the porting process. These include re-analyzing the DAQV design to accommodate distinctions between the pghpf and XL HPF compiler run time implementations. The end result is not a single portable reference implementation of the DAQV tool, as was originally planned, but rather two different portable implmentations that demonstrate different ways in which a vendor may choose to perform the gathering of distributed data from HPF programs in a DAQV-like tool. Keywords: multiprocessors, parallel programming tools, data access, DAQV, data distribution, data parallel, Fortran, HPF, SP2, SPMD}
}

@article{10.1007/s100090100054,
author = {Engblom, Jakob and Ermedahl, Andreas and Sj\"{o}din, Mikael and Gustafsson, Jan and Hansson, Hans},
title = {Worst-case execution-time analysis for embedded real-time systemsTHANKSREF="*"ID="*"This work was performed within the Advanced Software Technology (ASTEC, http: //www.astec.uu.se) competence center, supported by the Swedish National Board for Industrial and Technical Development (NUTEK, http://www.nutek.se).},
year = {2003},
issue_date = {August    2003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {4},
number = {4},
issn = {1433-2779},
url = {https://doi.org/10.1007/s100090100054},
doi = {10.1007/s100090100054},
abstract = {In this article we give an overview of the worst-case execution time (WCET) analysis research performed by the WCET group of the ASTEC Competence Centre at Uppsala University. Knowing the WCET of a program is necessary when designing and verifying real-time systems. The WCET depends both on the program flow, such as loop iterations and function calls, and on hardware factors, such as caches and pipelines. WCET estimates should be both safe (no underestimation allowed) and tight (as little overestimation as possible). We have defined a modular architecture for a WCET tool, used both to identify the components of the overall WCET analysis problem, and as a starting point for the development of a WCET tool prototype. Within this framework we have proposed solutions to several key problems in WCET analysis, including representation and analysis of the control flow of programs, modeling of the behavior and timing of pipelines and other low-level timing aspects, integration of control flow information and low-level timing to obtain a safe and tight WCET estimate, and validation of our tools and methods. We have focussed on the needs of embedded real-time systems in designing our tools and directing our research. Our long-term goal is to provide WCET analysis as a part of the standard tool chain for embedded development (together with compilers, debuggers, and simulators). This is facilitated by our cooperation with the embedded systems programming-tools vendor IAR Systems.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = {aug},
pages = {437–455},
numpages = {19},
keywords = {WCET analysis, Software architecture, Programming tools, Hard real-time, Embedded systems}
}

@article{10.1007/s10639-023-12325-z,
author = {Cheng, Miaoting and Lai, Xiaoyan and Tao, Da and Lai, Juntong and Yang, Jun},
title = {Children’s programming environment acceptance: extending the boundary conditions to programming competition, computational thinking, and programming modality},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-023-12325-z},
doi = {10.1007/s10639-023-12325-z},
abstract = {While numerous studies have highlighted the potential benefits of programming environment (PE) use for children’s learning, the boundary conditions of children’s PE acceptance within the programming education context are less clear. This study fills this gap in the literature by investigating the critical determinants of children’s PE use intention and extending the boundary conditions to programming competition, computational thinking, and programming modality. A total of 1527 primary students participated in this study. Using structural equation modelling (SEM) analyses, the measurement model was validated, and the configural, metric and scalar invariance of the measurement model was established. The structural model was also confirmed, with most of the hypothesized relationships were supported. Multigroup SEM analyses were conducted to compare structural path coefficient differences across different personal moderators (i.e., gender, grade, and experience), environmental moderators (i.e., both parents’ education level), and PE use-relevant moderators (i.e., programming competition, computational thinking, and programming modality). The results revealed significant path differences in six group comparisons, with most of the path differences associated with perceived self-efficacy and perceived ease of use. It should be noted that no significant path differences were identified for the gender and programming competition group comparisons. This work serves as a pioneer study of a comprehensive understanding of the determinants and moderators of children’s PE use intention. The findings offer important theoretical implications through accommodating essential constructs within a PE acceptance framework and recommending effective strategies to improve primary students’ PE acceptance for programming learning in primary education.},
journal = {Education and Information Technologies},
month = {nov},
pages = {939–969},
numpages = {31},
keywords = {Programming education, Programming environment, Technology acceptance, Primary students, Multigroup structural equation modeling}
}

@article{10.1016/j.chb.2019.03.003,
author = {Papavlasopoulou, Sofia and Sharma, Kshitij and Giannakos, Michail N.},
title = {Coding activities for children: Coupling eye-tracking with qualitative data to investigate gender differences},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2019.03.003},
doi = {10.1016/j.chb.2019.03.003},
journal = {Comput. Hum. Behav.},
month = {apr},
numpages = {11},
keywords = {Learning strategies, Gender differences, Eye-tracking, Computational thinking, Coding}
}

@article{10.1145/280495.280502,
author = {Battaglia, David and Burke, Austin and Beidler, John},
title = {An ADA reuse support system for Windows 95/NT},
year = {1998},
issue_date = {Jan./Feb. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {XVIII},
number = {1},
issn = {1094-3641},
url = {https://doi.org/10.1145/280495.280502},
doi = {10.1145/280495.280502},
abstract = {This paper describes a software resource that is being developed as part of the graduation requirement for the Master in Software Engineering degree at the University of Scranton. This project evolved from a series of experiments that were performed in undergraduate and graduate courses at the University. A basic editor was developed as part of an undergraduate course in rapid prototyping. Several students used that project as the basis for undergraduate Senior Projects. All undergraduates are required to complete a project as a degree requirement. This basic editor was handed over to a graduate course in Software Generation and Maintenance and used as the starting point for the construction of various software project management features. The system was constructed to support Ada source code development. However, the system could be readily modified to support source code management in other languages, notably C++. This paper describes the construction of resources that encourage the use of reusable software. Subsequent sections describe the overall framework of the system and selected details that carry out features that make reuse attractive. The system is called ReUSE (the Reuse University of Scranton Environment).ReUSE is an Ada programming environment which facilitates and promotes code reuse by individual developers or teams of developers. It provides centralized storage of project files, a package browser, automatic function and procedure call creation, a compiler interface, interactive error processing, multiple simultaneous editors, standard windows tools (menus, toolbars, etc.), and other features to help the developer write and reuse Ada code efficiently.ReUSE was developed in Microsoft Visual Basic 4.0 (32-bit) for the Windows 95 / NT operating systems.},
journal = {Ada Lett.},
month = {feb},
pages = {78–85},
numpages = {8}
}

@phdthesis{10.5555/909456,
author = {Van Houten, Karen Jane Hamilton},
title = {Kl, a well-structured beginning computer language},
year = {1980},
publisher = {University of Idaho},
address = {USA},
abstract = {One of the most exciting aspects of modern computer science is the transformation of programming from an art to a science. Program reliability, software management, and structured programming are all helping produce quality programs. An important contribution to the ever increasing excellence of software is the improvement of computer science education at the earliest levels. A new programming language, KL, has been designed and developed specifically for use in introductory computer science courses. The developmental goals of KL were: (1) inclusion of structured-programming tools, (2) simplicity for the user, (3) straightforward implementation, and (4) adaptability to small computers.These goals were accomplished in a number of ways. The structure of KL requires that each statement begin with a keyword and end with a semi-colon. This decreases the implementation effort and adds to the simplicity of the language. This form also eliminates the necessity for reserved words which often cause difficulty for the user. The keywords have been chosen to be meaningful and, when possible, short. Restricting the number of different statements available supports all four goals.The tools needed for abstraction are included in KL. User-defined data types are available in KL. Procedures and functions are also available for dividing a computer problem into subtasks.KL is a suitable language for teaching programming for several reasons. It requires structured programming techniques because no other constructs are available. This should start good programming habits. The concepts of structured programming can be learned using KL and later applied to other less structured languages. The concepts of structured programming are language independent and KL enables the learning of these concepts early in a computer scientist's career.},
note = {AAI8019797}
}

@inproceedings{10.1145/2795122.2795126,
author = {Connors, Dan and Dunn, Kyle and Bueter, Ryan},
title = {PyCompArch: python-based modules for exploring computer architecture concepts},
year = {2015},
isbn = {9781450337175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2795122.2795126},
doi = {10.1145/2795122.2795126},
abstract = {As computer architecture integrates multiple concepts such as microarchitecture, design, the hardware-software interface, compilers, and operating systems, there is an always increasing need to develop new methods for learning and exploring the field. Parallelism in computer systems is a key focus in computer architecture and some core parallel concepts include Amdahl's law, efficiency, and overhead. While there are a number of ways to examine these topics in traditional lectures and assignments, a unique way is to leverage Python-based programming environments that allow students to independently explore concepts and their governing parameters.This paper presents the highlights of PyCompArch Python module developed using the IPython Notebook environment to help the study of concepts in computer architecture. Python is a widely used general-purpose, high-level programming language, but traditionally the language does not play a leading role in the education of computer architecture. IPython Notebooks allow developers to interactively run Python code cells and to construct Python codes that execute on remote servers that eliminate any system requirements of the individual. In this way, the environment supports web-based remote "in the cloud" code development that can be modified during lectures or in homework assignments. The PyCompArch module supports a number of ways to help individuals learn concepts of parallelism related to computer architecture as well as explore experiments in computer performance and control. For example, PyCompArch supports the evaluation of performance of real-world benchmarks such as Open Computer Vision (OpenCV) and dynamic frequency scaling (DFS) in Raspberry Pi systems. Overall, the PyCompArch supports student learning and development of experiments in computer architecture.},
booktitle = {Proceedings of the Workshop on Computer Architecture Education},
articleno = {4},
numpages = {6},
location = {Portland, Oregon},
series = {WCAE '15}
}

@inproceedings{10.1109/iTAG.2014.15,
author = {Roscoe, Jonathan Francis and Fearn, Stephen and Posey, Emma},
title = {Teaching Computational Thinking by Playing Games and Building Robots},
year = {2014},
isbn = {9781479967957},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/iTAG.2014.15},
doi = {10.1109/iTAG.2014.15},
abstract = {Computing in schools has gained momentum in the last two years resulting in GCSEs in Computing and teachers looking to up skill from Digital Literacy (ICT). For many students the subject of computer science concerns software code but writing code can be challenging, due to specific requirements on syntax and spelling with new ways of thinking required. Not only do many undergraduate students lack these ways of thinking, but there is a general misrepresentation of computing in education. Were computing taught as a more serious subject like science and mathematics, public understanding of the complexities of computer systems would increase, enabling those not directly involved with IT make better informed decisions and avoid incidents such as over budget and underperforming systems. We present our exploration into teaching a variety of computing skills, most significantly "computational thinking", to secondary-school age children through three very different engagements. First, we discuss Print craft, in which participants learn about computer-aided design and additive manufacturing by designing and building a miniature world from scratch using the popular open-world game Mine craft and 3D printers. Second, we look at how students can get a new perspective on familiar technology with a workshop using App Inventor, a graphical Android programming environment. Finally, we look at an ongoing after school robotics club where participants face a number of challenges of their own making as they design and create a variety of robots using a number of common tools such as Scratch and Arduino.},
booktitle = {Proceedings of the 2014 International Conference on Interactive Technologies and Games},
pages = {9–12},
numpages = {4},
keywords = {technology, games, e ducation, computational thinking, STEM},
series = {ITAG '14}
}

@inproceedings{10.1145/3328778.3372680,
author = {Williams, Renaldo and Garcia, Dan},
title = {CodeKey - An Online Code Editor to Study Code Patterns and Enhance Student Performance in CS Courses},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3372680},
doi = {10.1145/3328778.3372680},
abstract = {In the past several years, there has been an increase in web-based compilers that allow students to learn how to code using a browser. Many Universities use online code editors for their large Computer Science (CS) courses. For example, the CS200 course at UC Berkeley uses Jupyter Notebooks to teach Python for data science to 800+ students. All the students in the course must write and submit their code assignments in the web-browser. These online code editors for large CS courses presents several benefits. One benefit is that it becomes easier to monitor the steps that a student takes to solve a coding problem since keystrokes can be tracked using Javascript. Another benefit is that the code written by students can be stored in one central database, creating less barriers for code analysis. The CodeKey project aims to take advantage of analyzing code patterns of students in a CS course in order to find key insights. CodeKey aims to find these insights by monitoring the interactions (i.e. clicks and keystrokes) of students as each student attempts to solve a coding problem. The goal is to study the code patterns of students in a CS course in order to understand similarities and differences between students who perform well on a problem and students who do not. We also aim to study how revealing these coding patterns to a student can increase his understanding of how to solve a difficult coding problem by showing common mistakes, and by showing simple steps that lead to the correct solution.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1357},
numpages = {1},
keywords = {web-compilers, intelligent tutor systems, computer science education},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1109/ITNG.2015.47,
author = {Bani-Salameh, Hani and Jeffery, Clinton},
title = {Evaluating the Effect of 3D World Integration within a Social Software Environment},
year = {2015},
isbn = {9781479988280},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ITNG.2015.47},
doi = {10.1109/ITNG.2015.47},
abstract = {Several virtual communities have spread during the last decade where hundreds of people interact and socialize. Users use these communities in many fields including education, health, business, and entertainment. They use them to communicate, share information, and collaborate to achieve common goals and finish their tasks. To our knowledge, no existing literature or research studies show the benefit of using virtual world in 1) improving software engineering education, 2) enhancing the collaboration in distributed software development environment, and 3) increasing their effectiveness in the distributed developers' progress. For this purpose we conducted a case study to test effect of integrating a virtual environment called CVE in software development environments. This study presents both qualitative and quantitative analysis of the data collected from the case study surveys and log files. It conducted a survey on the users' preferences a. Also, it collected data about the developer's interactions with the 3D objects, and analyzed the collected results.},
booktitle = {Proceedings of the 2015 12th International Conference on Information Technology - New Generations},
pages = {255–260},
numpages = {6},
keywords = {Virtual Environments, Social Networking, Social Interaction, IDE (Integrated Development Environment), CVEs, 3D World},
series = {ITNG '15}
}

@inproceedings{10.5555/1369599.1369663,
author = {Zong, Nuannuan and Adjouadi, Malek and Ayala, Melvin},
title = {Artificial neural networks approaches for multidimensional classification of acute lymphoblastic leukemia gene expression samples},
year = {2005},
isbn = {9608457297},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Accurate classification of human blood cells plays a decisive role in the diagnosis and treatment of diseases. Artificial Neural Networks (ANNs) have been consistently used as a trusted classification tool for this type of analysis. In the present case study, two approaches are implemented on two different parametric data clusters in a multidimensional space using ANNs trained with cross-validation. Beckman-Coulter Corporation supplied flow cytometry data of numerous patients as training sets for the first approach to exploit the physiological characteristics of the different blood cells provided. The goal was to establish a programming tool for the identification of different white blood cell categories of a given blood sample and provide information to medical doctors in the form of diagnostic references for the specific disease state that is considered for this study, namely Acute Lymphoblastic Leukemia (ALL). Successful initial results of this first approach have been published. The second approach is focusing on the gene expression profiling of ALL to classify its six subtypes. Generated by the oligonucleotide microarrays, this data provides additional insights into the biology underlying the clinical differences between these leukemia subgroups. With the application of the hypothesis space, along with the learning bias, the system is also trained to assess the inherent problem of data overlap and be able to recognize abnormal blood cell patterns. An analysis of the systems regarding computational load and receiver-operating characteristic (ROC) was conducted. The algorithms as proposed provide solutions to data overlap from our initial results. And by applying ANNs, the classification accuracy of the first approach is remarkably improved up to 100% and 92% for the second approach.},
booktitle = {Proceedings of the 9th WSEAS International Conference on Computers},
articleno = {64},
numpages = {6},
keywords = {white blood cell, receiver operating characteristics (ROC) analysis, microarrays, gene expression profiling, cross-validation, artificial neural networks (ANNs), acute lymphoblastic leukemia (ALL)},
location = {Athens, Greece},
series = {ICCOMP'05}
}

@inproceedings{10.1109/FIE.2011.6143100,
author = {Murray, Meg Coffin},
title = {Work in progress  --  Creating a professional software development environment to support capstone programming projects},
year = {2011},
isbn = {9781612844688},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FIE.2011.6143100},
doi = {10.1109/FIE.2011.6143100},
abstract = {Experiential education provides valuable learning opportunities for students in the computing disciplines. Assigning students to work on real-world projects is often seen as a way for students to practice what they have learned in the classroom. While a desirable goal, logistics often make it difficult to provide these types of experiences. However, it is vitally important for students to be exposed to and experiment with tools used in commercial software development environments. This paper provides a descriptive overview of the development and implementation of a professional software development environment used to support computer science capstone programming projects. To date, the environment has been used to support work on a National Science Foundation funded database courseware project that includes over 100 interactive software modules. The environment was built using a collection of open-source applications that provide version control, task assignment and tracking, collaborative team tools, bug tracking, and project documentation management. This project has created a venue for providing consistent high quality real-world types of experiences for students completing their capstone requirement.},
booktitle = {Proceedings of the 2011 Frontiers in Education Conference},
pages = {S4F-1–1-S4F-3},
series = {FIE '11}
}

@inproceedings{10.5555/1778650.1778651,
author = {Berkling, Kay and Geisser, Michael and Hildenbrand, Tobias and Rothlauf, Franz},
title = {Offshore software development: transferring research findings into the classroom},
year = {2007},
isbn = {3540755411},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Distributed software projects are becoming increasingly commonplace in industry. Yet, software engineering education rarely graduates students with the necessary skills and hands-on experience that are particular to off-shore software development projects. Three key areas in successful offshore software development projects are well documented in the literature as communication, knowledge management, as well as project and process management. This paper maps tasks within each of these three areas to functions that have to be provided by remote collaboration platforms and tools that distributed projects rely on. A case-study of an off-shore requirements engineering class experience between a Master course of Polytechnic University of Puerto Rico and a customer in a Swiss financial institution shows a correlation between areas of learning by the students and functionalities covered with the tools used in the classroom. The paper identifies additional tools, developed by the authors, which will provide additional functionalities in the deficient areas to increase the learning and preparation of the students for off-shore software development projects.},
booktitle = {Proceedings of the 1st International Conference on Software Engineering Approaches for Offshore and Outsourced Development},
pages = {1–18},
numpages = {18},
keywords = {traceability, software engineering education, requirements engineering, offshore software development, distributed and global software development, development tools, collaborative software development},
location = {Zurich, Switzerland},
series = {SEAFOOD'07}
}

@book{10.5555/554971,
author = {Deitel, Harvey M. and Deitel, Paul J.},
title = {The  Complete Java 2 Training Course with Book},
year = {1999},
isbn = {0130852473},
publisher = {Prentice Hall PTR},
address = {USA},
edition = {3rd},
abstract = {From the Publisher: Includes the Best Seller Java How to Program 3/e in print and on CD-ROM! 13+ hours of expert audio describing 16,000+ lines of fully-tested sample program code, practice exams with hundreds of questions, full-text searching, hyperlinking, and more! Complete coverage of Java 2, including the new Swing GUI, Graphics, Multimedia, JavaBeans, Networking, Databases, Servlets, RMI, and more. Perfect for C, C++, Visual Basic™, Web, and Internet developers, and non-programmers moving to Java for the first time. Bonus: 2nd CD includes Java 2 SDK, Borland® Jbuilder™ 3 University Edition IDE, and NetBeans Developerx2 IDE The Complete Java™ 2 Training Course, 3/e The world's #1 JAVA Training CD-ROM, FULLY UPDATED FOR JAVA 2! You get the world's #1 Java 2 training CD-ROM plus a worldwide best-selling Java book, JDK 2, two great development environments and more than 16,000 lines of live codetogether, they make you a Java 2 expert faster than you ever imagined. Java 2 Multimedia cyber Classroom CD-ROM 25 practice exams with over 800 questions to test your knowledge! 13+ hours of detailed audio descriptions of hundreds of Java applications and applets-16,000+ lines of live code! Hundreds of tips that show you how to build code that's reusable, portable, bug free, and optimized for performance! Find it fast! Full-text searching, hyperlinking, and more! The World's #1 Java 2 Textbook Start-to-finish, 1400-page guide to Java 2 Friendly, practical, and full of examples Hands-on tutorials with hundreds of tips andtricks Total coverage of Java 2, from the basics to the leading edge Master all this and more: fundamentals of object-oriented development; multithreading, exception handling, Swing classes and AWT graphics, multimedia, Java 2D, even servlets and Java's powerful new enterprise APIs (JavaBeans, JDBC, RMI)! Free Bonus 2ND CD-ROM FULL OF GREAT JAVA DEVELOPMENT RESOURCES! NetBeans™ Developerx2, Borland® Jbuilder™) 3 University Edition, and much more! Run hundreds of sample Java programs from the Cyber Classroom-or any Java 2 code you write! Two powerful Java 2 Integrated Development Environments are included, plus the official Sun Java 2 SDK! They're perfect for learning Java fast. Raves for THE COMPLETE JAVA TRAINING COURSE, SECOND EDITION! I just wanted to say that the Java Multimedia Cyber Classroom Training Course, Second Edition is greatlove it. It's great to see an applet work using the skills I've gained from this course… Joseph Casale, Principal Chemist, Connecticut Department of Health I recently purchased The Java™ Multimedia Cyber Classroom . I found this multimedia disc to be the best investment I ever made. This interactive way of presenting Java is awesome...if you have anything like this for other programs, I want to purchase all of them.Robert O'Meara Specially Designed for: New programmers and students Visual Basic, C, and C++ programmers Web and component Developers Java 1.x programmers: update your skills! Multimedia Cyber Classroom System Requirements* Windows® 95/98, Windows NT™ 4.x or higher 20 MB disk space 32 MB RAM CD-ROM drive and sound card support *Additional disk space required for bonus software on second CD-ROM.}
}

@phdthesis{10.5555/933807,
author = {Liu, Qiong},
advisor = {Levinson, Stephen E. and Huang, Thomas S.},
title = {Interactive and incremental learning via a multisensory mobile robot},
year = {2001},
isbn = {0493274650},
publisher = {University of Illinois at Urbana-Champaign},
address = {USA},
abstract = {As computers are widely used and computer-programming gets increasingly complicated, computer users and programmers demand more convenient human-computer interfaces and programming tools. Motivated by facilitating computer programming and human-computer interaction, this project explores teaching a computer to react properly to external stimuli through natural human-computer interaction. The long-term goal of the project is to program a computer as we teach an infant, and to enable the computer to interact with us like a human and perform jobs accordingly. The project uses a multisensory mobile robot as the interface for natural human-computer interaction. We develop a computational efficient scheme that facilitates the robot to learn spoken language online, and react properly to learned speech commands. Compared to the existing speech recognizers, our system does not use text or other symbolic information to represent speech, thus is not restricted by the limitations that a speech-to-text mechanism may inherently have. Due to this fact, our system can learn speech online in any language. This learning flexibility is not achieved by state-of-the-art speech recognizers. The thesis reports the design and implementation of our scheme for spoken language acquisition. In the thesis, we discuss the available resources and possible pitfalls for the robot project: we discuss the speech communication techniques that can be or cannot be used in our project; we choose the classifiers that are suitable to our project; and we also present a humancomputer interaction model and the robot training strategies. Based on these analyses and discussions, we develop our learning scheme that can learn online from both labeled and unlabeled data. The learning scheme is based on successive refinement of the decision boundary.},
note = {AAI3017152}
}

@inproceedings{10.1007/978-3-642-25283-9_1,
author = {Reiter, Andreas and Neubauer, Georg and Kapfenberger, Michael and Winter, Johannes and Dietrich, Kurt},
title = {Seamless integration of trusted computing into standard cryptographic frameworks},
year = {2010},
isbn = {9783642252822},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25283-9_1},
doi = {10.1007/978-3-642-25283-9_1},
abstract = {Trusted Software Stacks (TSS) are the interfaces between applications and Trusted Platform Modules (TPMs). In order to avoid wrong usage of the stacks which could lead to security holes, they should provide an easy-to-use interface for developers. Moreover, they should be designed in a flexible way to adapt to new requirements resulting from specification or algorithm changes. However, the currently specified TSS interface is highly complex and requires a vast amount of training effort for developers to get familiar with it. Moreover, existing stacks are monolithic blocks of software - they either support the full range of TPM functions which makes them large or they support a customized subset of features which reduces their scope of use. In this paper, we propose a novel design for a Trusted Software Stack (TSS) that can be integrated into existing security frameworks. Instead of designing a new application programming interface (API), our stack uses the APIs from well known and established frameworks, allowing developers that are not familiar with Trusted Computing (TC) to easily adapt to this new technology. Furthermore, our stack supports multiple TPMs, dynamic component loading and Over-The-Air updates that allow the stack to support customized sets of features even after it has been deployed in the field. Moreover, the stack provides built-in support for user authentication and TPM access control. Our prototype stack is developed for the .NET programming environment, thereby eliminating common implementation faults like buffer overflows. Due to the managed nature of the .NET runtime environment, it is portable between different operating systems and can be used on desktop systems as well as on embedded systems without the need for recompiling it for the specific target architecture.},
booktitle = {Proceedings of the Second International Conference on Trusted Systems},
pages = {1–25},
numpages = {25},
keywords = {trusted computing, TSS, TPM, .NET},
location = {Beijing, China},
series = {INTRUST'10}
}

@inproceedings{10.1145/3372923.3404798,
author = {Kitromili, Sofia and Jordan, James and Millard, David E.},
title = {What Authors Think about Hypertext Authoring},
year = {2020},
isbn = {9781450370981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372923.3404798},
doi = {10.1145/3372923.3404798},
abstract = {Despite significant research into authoring tools for interactive narratives and a number of established authoring platforms, there is still a lack of understanding around the authoring process itself, and the challenges that authors face when writing hypertext and other forms of interactive narratives. This has led to a monolithic view of authoring, which has hindered tool design, resulting in tools that can lack focus, or ignore important parts of the creative process. In order to understand how authors practise writing, we conducted semi-structured interviews with 20 interactive narrative authors. Using a qualitative analysis, we coded their comments to identify both processes and challenges, and then mapped these against each other in order to understand where issues occurred during the authoring process. In our previous work we were able to gather together a set of authoring steps that were relevant to interactive narratives through a review of the academic literature. Those steps were: Training/Support, Planning, Visualising/Structuring, Writing, Editing, and Compiling/Testing. In this work we discovered two additional authoring steps, Ideation and Publishing that had not been previously identified in our reviews of the academic literature - as these are practical concerns of authors that are invisible to researchers. For challenges we identified 18 codes under 5 themes, falling into 3 phases of development: Pre-production, where issues fall under User/Tool Misalignment and Documentation; Production, adding issues under Complexity and Programming Environment; and Post-production, replacing previous issues with longer term issues related to the narrative's Lifecycle. Our work shows that the authoring problem goes beyond the technical difficulties of using a system, rather it is rooted in the common misalignment between the authors' expectations and the tools capabilities, the fundamental tension between expressivity and complexity, and the invisibility of the edges of the process to researchers and tool builders. Our work suggests that a less monolithic view of authoring would allow designers to create more focused tools and address issues specifically at the places in which they occur.},
booktitle = {Proceedings of the 31st ACM Conference on Hypertext and Social Media},
pages = {9–16},
numpages = {8},
keywords = {interactive fiction, hypertext fiction, digital interactive storytelling, digital interactive narratives, authors, authoring tools, authoring},
location = {Virtual Event, USA},
series = {HT '20}
}

@inproceedings{10.5555/792762.793279,
author = {Beynon, Michael D. and Andrade, Henrique and Saltz, Joel},
title = {Low-Cost Non-Intrusive Debugging Strategies for Distributed Parallel Programs},
year = {2002},
isbn = {0769517455},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Debugging is an important and challenging component of the software development cycle. The utilization of proper tools that help trace execution, inspect variable values, do postmortem analysis, dynamically attach to running processes, among other tasks can greatly increase programmer productivity by reducing the time to understand incorrect behavior. It is well known that many more hours are spent debugging software than compiling source code [11], therefore having the appropriate tools and making the best use of information provided by them is fundamental. When a programmer deals with parallel programs in shared or distributed memory settings, debugging becomes quite complicated given the various potential interactions that may occur between threads and processes. Data races, deadlocks, synchronization issues, and communication problems must be dealt with in addition to the traditional sequential (single process) problems such as memory leaks, memory overruns, etc. Since debugging is clearly important in both sequential and parallel/distributed environments, much effort has been focused on this task. It ranges from standardization initiatives [6] and list of requirements [3], to specific research prototypes [5, 14, 15], and commercial and open source products [4, 7, 9, 10, 13].GDB [12] is freely available and omnipresent in most research labs and universities, which contributes to its status as a debugging tool of choice for many programmers writing sequential code. On the other hand, it does not target the particular debugging issues presented by parallel/distributed applications as does, for example, TotalView [4]. Nevertheless, GDB's newer versions are well capable of dealing with multi-threaded programs. There are also many capable parallel/cluster programming environments that go beyond the ideas presented in this paper. We present this work as strategies that can be implemented without capital expenditure 1 somewhat easily into existing projects without adopting a programming environment.In this work, we will show how five low-cost and non-intrusive techniques that work using free commodity tools such as GDB can be used to improve the debugging process of multi-threaded and/or distributed parallel programs. These techniques have been used in the development of two major software middlewares DataCutter [2] and MQO [1] and have proven their value by lowering the time necessary to detect and correct bugs.},
booktitle = {Proceedings of the IEEE International Conference on Cluster Computing},
pages = {439},
series = {CLUSTER '02}
}

@inproceedings{10.1145/3576123.3576134,
author = {Finnie-Ansley, James and Denny, Paul and Luxton-Reilly, Andrew and Santos, Eddie Antonio and Prather, James and Becker, Brett A.},
title = {My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises},
year = {2023},
isbn = {9781450399418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576123.3576134},
doi = {10.1145/3576123.3576134},
abstract = {The introduction of OpenAI Codex sparked a surge of interest in the impact of generative AI models on computing education practices. Codex is also the underlying model for GitHub Copilot, a plugin which makes AI-generated code accessible to students through auto-completion in popular code editors. Research in this area, particularly on the educational implications, is nascent and has focused almost exclusively on introductory programming (or CS1) questions. Very recent work has shown that Codex performs considerably better on typical CS1 exam questions than most students. It is not clear, however, what Codex’s limits are with regard to more complex programming assignments and exams. In this paper, we present results detailing how Codex performs on more advanced CS2 (data structures and algorithms) exam questions taken from past exams. We compare these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students. We consider the implications of such tools for the future of undergraduate computing education.},
booktitle = {Proceedings of the 25th Australasian Computing Education Conference},
pages = {97–104},
numpages = {8},
keywords = {AI, AlphaCode, CS1, CS2, Codex, DeepMind, GPT-3, GitHub, OpenAI, academic integrity, algorithms, artificial intelligence, code generation, copilot, data structures, deep learning, introductory programming, machine learning, neural networks, novice programming},
location = {<conf-loc>, <city>Melbourne</city>, <state>VIC</state>, <country>Australia</country>, </conf-loc>},
series = {ACE '23}
}

@inproceedings{10.5555/1149126.1149274,
author = {DiGello, Elizabeth A.},
title = {Understanding the social and instructional meaning of a multimedia, bilingual instructional tool for educators of the deaf},
year = {2004},
publisher = {International Society of the Learning Sciences},
abstract = {The bilingual-bicultural perspective in deaf education advocates the use of American Sign Language (ASL) as the language of classroom instruction and introduces English in print form only (a visually accessible form) to deaf students. Research into the effectiveness of the bilingual-bicultural perspective is now increasing, but teacher preparation programs were not introducing their students to this approach until very recently. Therefore, schools that choose to use this perspective will need to educate their teachers about Deaf culture, ensure that teachers are fluent users of ASL, and demonstrate how ASL and English can be used effectively in the classroom in order to achieve the goal of bilingual-bicultural development in their students. To meet these needs, a multimedia, bilingual professional development tool was created for pre-service and in-service educators of the deaf that explains and demonstrates teaching practices that can be used to promote ASL and English language development in deaf students. The instructional tool utilizes digital video, embedded in a website-like interface, to present information in live motion ASL next to English text that communicates the same information. Preliminary findings with a pilot version of the tool indicated that Deaf teachers and hearing teachers are uniquely impacted by the tool and may use similar technology in a variety of ways in the classroom environment; it was evident from the initial inquiry that further investigation was needed into the role of this and similar instructional tools. Therefore, the research study aims to determine how one bilingual, multimedia instructional tool can impact educators of the deaf differently with respect to its personal, cultural, and instructional value, depending on the educational setting the teachers work in and their personal hearing status. Surveys and focus group discussions were used to collect the data needed to answer the research questions. Final analyses will make connections between the bilingual education literature, the educational technology literature, and historical and current issues in deaf education.},
booktitle = {Proceedings of the 6th International Conference on Learning Sciences},
pages = {650},
numpages = {1},
location = {Santa Monica, California},
series = {ICLS '04}
}

@article{10.1109/MS.2020.3045817,
author = {Albusays, Khaled and Bjorn, Pernille and Dabbish, Laura and Ford, Denae and Murphy-Hill, Emerson and Serebrenik, Alexander and Storey, Margaret-Anne},
title = {The Diversity Crisis in Software Development},
year = {2021},
issue_date = {March-April 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {38},
number = {2},
issn = {0740-7459},
url = {https://doi.org/10.1109/MS.2020.3045817},
doi = {10.1109/MS.2020.3045817},
abstract = {This special issue of IEEE Software focuses on diversity and inclusion in software development, presenting research results and best practices for making the field equitable for all. It is well documented that the industry does not provide evenhanded participation conditions. Research has shown that implicit gender biases significantly impact hiring decisions,&lt;sup&gt;1&lt;/sup&gt; women disengage faster than men,&lt;sup&gt;2&lt;/sup&gt; Palestinian tech entrepreneurs do not have access to Internet-based distribution and payment platforms,&lt;sup&gt;3&lt;/sup&gt; software developers with a visual impairment lack tools to navigate code editors,&lt;sup&gt;4,5&lt;/sup&gt; and women are sometimes less likely to get their code accepted.&lt;sup&gt;6&lt;/sup&gt; Tools, processes, products, and education are not inclusive. Dimensions such as geography, gender, socioeconomic politics, age, ethnicity, and disability shape who can participate in creating technology.},
journal = {IEEE Softw.},
month = {mar},
pages = {19–25},
numpages = {7}
}

@phdthesis{10.5555/AAI27735970,
author = {Liu, Liu and Isaacman, Sibren and Zhang, Desheng and Martin, Richard and Yu, Linbin},
advisor = {Ulrich, Kremer,},
title = {Trading Quality for Resource Consumption Through Approximation Management},
year = {2020},
isbn = {9798662455146},
publisher = {Rutgers The State University of New Jersey, School of Graduate Studies},
abstract = {The goal of traditional optimizations is to map applications onto limited machine resources such that application performance is maximized while application semantics (program correctness), is preserved. Semantics is thought of as a unique mapping from inputs to outcomes. Relaxing application semantics through approximations has the potential of orders of magnitude performance improvements by trading off outcome quality for resource usage. Here, an execution outcome is not only based on its inputs but also resource availability and user quality expectations.Emerging approximation techniques provides various ways to trade-off output quality for lower resource consumption. However, as a developer, the guidance and support on how to utilize the power of approximation in everyday applications are limited and rarely discussed in recent works. The offline training overhead to support approximation is usually huge, but often be treated as "free." Besides, it is surprising that end-users involvement is always overlooked when determining the quality notion, which should be highly subjective. Finally, supporting approximation in a multi-programming environment is crucial to let approximation be widely accepted as a general technique.In this dissertation, I introduce Rapids, Reconfiguration, Approximation, Preferences, Implementation, Dependencies, and Structure, a framework for developing and executing applications suitable for dynamic configuration management for approximate computing. The main contribution of Rapids is its design to address the above concerns through exploiting the different expertise/strengths of the three actors (developers, users, applications) involved. I conduct comprehensive experiments and show that Rapids is adaptive and extendable by providing customizable configuration spaces for developers and the support for customizable quality for end-users. It has low overheads and small cross-platform porting costs. I also introduce an extension of Rapids, Rapids-M (Rapids for Multiprogramming), which is the first system that discusses cross-application approximation management. The target is to understand and overcome the challenges in approximation management fundamentally, then let both developers and end-users benefit from approximation with little extra efforts so that a wider audience can accept the technique.},
note = {AAI27735970}
}

@proceedings{10.1145/1066129,
title = {eclipse '04: Proceedings of the 2004 OOPSLA workshop on eclipse technology eXchange},
year = {2004},
isbn = {9781450377980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This foreword sketches the history of the Eclipse project. It
then presents the context of the present workshop, the eclipse
Technology Exchange (eTX), which was held on October 24, 2004 at
OOPSLA 2004, Vancouver, British Columbia, Canada.&lt;b&gt;The Eclipse Project&lt;/b&gt;The Eclipse Project began in April 1999 at IBM's OTI laboratory.
It was initially conceived as a successor product for the VisualAge
family of software development tools. VisualAge was a commercially
successful IDE, but it was also a closed environment built on
proprietary APIs. It did not integrate well with other vendors'
tools, and only the IBM/OTI team could enhance or extend the
product. Moreover, it was becoming apparent from customer
experience that more was required than a simple re-write of
VisualAge. In fact, there was growing demand for a tool integration
platform --- a programming environment that would provide kernel
IDE functionality, but also allow developers, third party vendors
and users to seamlessly add their own extensions, personalizations,
and enhancements.The Eclipse team set out to identify the essential kernel
concepts underlying the VisualAge product line (or any other IDE
for that matter). In effect, they wanted to strip out all of the
functionality within an IDE that was specific to a particular
programming language, development task, or programming model. The
hope was that there would be substantial residual function left
behind, that could then be restructured to form a content-neutral,
and programming language-neutral, foundation on which IDEs and
similar products could be built from components. It was a bold
venture, since there was no guarantee that anything practically
useful would result.What they discovered was Eclipse: a tool integration platform
together with a set of components (plugins, in the Eclipse
vernacular) that could be seamlessly assembled into a wide variety
of software development products. The Java Development Toolkit
(JDT) --- the Eclipse Java IDE --- became their proof-point. It was
built in parallel by a separate team, which operated independently
from the Eclipse Platform project. The JDT team had no special
privileges; they had to use the same APIs as any third party
product and were allowed no "back door" access to Platform kernel
functionality. The intent was that, despite these constraints, the
finished JDT should be indistinguishable from a purpose-built,
vertically integrated IDE product like VisualAge. This goal was
realized, the Eclipse Project was a success, and the Eclipse
Community was born.In the years since the Eclipse code base was released into open
source by IBM, its growth has been nothing short of spectacular.
Tens of thousands download the Eclipse SDK every week from over
fifty mirror sites around the globe. Thousands of Eclipse plugins
are now available from open source and commercial suppliers.
Software vendors are now shipping several hundred commercial
products based on Eclipse. Approximately 60 companies are members
of the Eclipse Foundation, which hosts Eclipse open source
development. The first Eclipse Developer Conference (EclipseCON
2004) was held in February 2004 in Anaheim, California. Over 220
companies and organizations from nearly 25 countries were
represented.&lt;b&gt;ECLIPSE AND COMPUTER SCIENCE RESEARCH&lt;/b&gt;It has been particularly interesting to see the uptake of
Eclipse within the research community. In retrospect one could
perhaps have anticipated this. Computing is, after all, an
empirical discipline --- ideas must be implemented to be validated.
For software researchers in particular, the computer becomes our
laboratory. We necessarily build on the work of those who have gone
before, and of course as time progresses our technology pyramids
keep getting higher. Complexity is our bane: the low-hanging fruit
were picked long ago, and most interesting problems are just not
simple. Consequently, experimentation usually requires complex
infrastructure, plumbing, as we often call it. Most researchers
spend far too much time building (and rebuilding, and fixing) this
plumbing, and far too little time actually developing new ideas.
Given the nature of research, there are seldom any applicable
standards for such infrastructure (these only come much later when
the research has matured into products). Consequently researchers
up to now have had to live and work in their own vertical towers,
sharing their ideas but only infrequently sharing code. The only
common programming platform among researchers was "emacs", and
while this continues to be very flexible, it lags far behind
industrial-scale IDEs in terms of functionality.But Eclipse changes this context. It provides a means to create
and share that necessary common infrastructure, particularly for
investigators in such areas as programming languages, tools, and
environments. Researchers can focus more of their time on their
real mission of innovation, and much less on the tedious plumbing
tasks. Moreover, Eclipse-based implementations are built from
commercial-quality components, resulting in robust demonstration
systems that make it much easier for researchers to publicize and
promote their work.What specifically does Eclipse offer researchers that makes it
so attractive? First, it is an extensible platform for integrating
components, which comes replete with a large number of commercial
quality components out of the box. It runs on nearly all operating
systems and GUI combinations, and is one of the few Java
implementations that actually realizes that language's "write once
run everywhere" potential (rather than the typical "write once test
everywhere" experience). Perhaps most importantly, it is available
in open source with a generous non-viral license. Finally, it has
tremendous visibility due to broad based industry support, which
includes the backing of such powerhouse firms as IBM, HP, SAP,
Intel, and many more.&lt;b&gt;ECLIPSE AND COMPUTER SCIENCE EDUCATION&lt;/b&gt;There are numerous challenges in education these days such as
distance, limited resources and the recognized need to make
learning a personalized and active experience. Many educators are
consequently looking at how technology can address these challenges
and enhance learning in the classroom and beyond. For computer
science education, Eclipse has already been widely adopted as an
IDE to support programming. The advantages for some are that it is
free, platform independent and industrially relevant. But beyond
these obvious advantages, other researchers have recognized that
Eclipse provides an excellent infrastructure for developing
learning tools. These tools can leverage the wealth of technology
already present in the Eclipse community, as well as benefit from
integration with other learning tools developed by other
researchers and educators. The result of these multiple efforts is
the emergence of Eclipse as an effective and powerful platform to
support research in educational technologies and an improved
learning experience in many settings.&lt;b&gt;THE ECLIPSE TECHNOLOGY EXCHANGE&lt;/b&gt;That idea that Eclipse would provide exactly the rich, open and
robust platform that IT researchers needed was not initially an
obvious one, and so it needed to be promoted within the academic
community. IBM and eclipse.org set out to popularize these ideas by
hosting a series of workshops and birds-of-a-feather events at
various software research conferences. This ad hoc program
gradually evolved into the eclipse Technology Exchange (eTX)
workshops, the most recent of which was held at OOPSLA 2004 in
Vancouver. These events provide a forum for researchers who are
using Eclipse to network and share their experiences and their
code.The foundation for a successful eTX is a set of high quality,
refereed presentations, which serve to illustrate the breadth and
vitality of the Eclipse research and teaching communities. The
papers in this volume are exemplary in this regard. Several
describe plugins that build on the Eclipse Platform to offer new
programming tools, such as for aspect browsing; debugging
distributed applications; profiling and monitoring program
behaviour; visualization of complex data; and feature modeling, a
technique used in product-line development to model similarities
and differences of products. About half of the papers describe the
use of Eclipse for teaching object-oriented programming and
software engineering, in both classroom and distance settings. One
such paper addresses distributed collaborative programming. Others
describe Eclipse courseware plugins for code-based tutorials;
visualization of computer organization concepts; and tracking
student programming projects.Each paper illustrates the advantages that Eclipse offers
researchers and teachers. They describe rich full-featured
implementations, which can be used and modified by other
researchers/teachers in their respective areas, at minimal cost in
terms of incremental programming effort. This works because they
all leverage the rich infrastructure and base of components
provided by Eclipse. And of course by developing their own new
components, each of these projects extends that base and enables
others to build on their work --- a virtuous cycle that is creating
an eco-system around Eclipse and enriching the entire software
research and teaching communities.},
location = {Vancouver, British Columbia, Canada}
}

@techreport{10.5555/871296,
author = {Liu, Qiong},
title = {Interactive and Incremental Learning Via a Multisensory Mobile Robot},
year = {2001},
publisher = {University of Illinois at Urbana-Champaign},
address = {USA},
abstract = {As computers are widely used and computer programming gets increasingly complicated, computer users and programmers demand more convenient human-computer interfaces and programming tools. Motivated by facilitating computer programming and human-computer interaction, our project explores teaching a computer to react properly to external stimuli through natural human-computer interaction. The long-term goal of our project is to program a computer as we teach an infant, and to enable the computer to interact with us like a human and perform jobs accordingly. The project uses a multisensory mobile robot as the interface for natural human-computer interaction. We develop a computational efficient scheme that facilitates the robot to learn spoken language online, and react properly to learned speech commands. Compared to the existing speech recognizers, our system does not use text or other symbolic information to represent speech, thus is not restricted by the limitations that a speech-to-text mechanism may inherently have. Due to this fact, our system can learn speech online in any languages. This learning flexibility is not achieved by state-of-the-art speech recognizers. The thesis reports the design and implementation of our scheme for spoken language acquisition. In the thesis, we discuss the available resources and possible pitfalls for the robot project; we discuss the speech communication techniques that can be or cannot be used in our project; we search the classifiers that are suitable to our project; and we also present a human-computer interaction model and the robot training strategies. Based on these analyses and discussions, we develop our learning scheme that can learn online from both labeled and unlabeled data. The learning scheme is based on successive refinement of the decision boundary. It learns incrementally through its interaction with the teacher. Experimental results of a multilingual short-utterance learning task are given after the presentation of the learning scheme. In the learning experiment, both speech and tactile sense are incorporated. Further extension of this project includes incorporating more versatile modalities into human-computer interaction, and applying our learning scheme to practical systems such as home appliances.}
}

@phdthesis{10.5555/AAI29140217,
author = {da Costa Pinho, C\'{e}sar Alexandre},
advisor = {Miranda, de Sousa, Armando Jorge},
title = {Tactode Programming for Robotics and Other Targets},
year = {2021},
isbn = {9798835558124},
publisher = {Universidade do Porto (Portugal)},
abstract = {The demand for graduates in Science, Technology, Engineering, and Mathematics (STEM) areas is currently quite visible. Thus, educational curricula are being adjusted. In schools, subjects are being introduced to develop more logical techniques and reasoning, the so-called Computational Thinking (CT).The original version of Tactode is a tangible block programming system aimed at children. It focuses on multi-target programming (robots, simulations, or high-level language programs) by creating behaviors/programs with tangible pieces that fit like a puzzle. Then a picture is taken of the set of grouped pieces, and it is introduced into the Tactode system application so that the program can be compiled (transposed) into the target application. If a user desires, the Tactode application also allows the program's creation by assembling the parts in the application but with many limitations.Tactode allows the compilation of the program for some physical platforms, but manufacturers' applications are required to run the program, and no debug information at all in the Tactode app. These additional steps cause a low-quality User Experience (UX) in the child, making learning difficult.O Scratch is a visual and block programming tool, top-rated in education and used by several schools. However, it lacks support to allow multi-target programming, especially of physical robotic agents.Scratch's popularity and previous studies show that the best is incorporating Tactode into the Scratch ecosystem, making it more attractive and promoting better educational goals.This dissertation's broad goal was to obtain a programming language for physical or virtual multi-targets. The solution was to merge Tactode's features with the Scratch tool, replacing Tactode's application with Scratch itself. A Tactode extension was created for Scratch to add the Tactode functionalities to Scratch. Moreover, a Tactode Link application was developed, which establishes Scratch's connection with external platforms and allows the program's execution directly on the chosen platform. The platform of choice was a miniature robot R FL of the PRO (Portuguese Robotics Open) of open software and hardware developed at FEUP.This solution allows children to improve the UX of the Tactode system so that they can complete the whole process from the creation of the program to the execution by the external target. The acquired usability raised children's interest and improved learning in several areas of the STEM spectrum, including robotics and CT. Validation of the system was done by performing a set of test challenges and then by experimentation by students solving some challenges.},
note = {AAI29140217}
}

@book{10.5555/1805911,
author = {Cypher, Allen and Dontcheva, Mira and Lau, Tessa and Nichols, Jeffrey},
title = {No Code Required: Giving Users Tools to Transform the Web},
year = {2010},
isbn = {012381541X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Revolutionary tools are emerging from research labs that enable all computer users to customize and automate their use of the Web without learning how to program. No Code Required takes cutting edge material from academic and industry leaders - the people creating these tools -- and presents the research, development, application, and impact of a variety of new and emerging systems. *The first book since Web 2.0 that covers the latest research, development, and systems emerging from HCI research labs on end user programming tools *Featuring contributions from the creators of Adobe s Zoetrope and Intel s Mash Maker, discussing test results, implementation, feedback, and ways forward in this booming area *Companion Web site features video demonstrations of each system (http://www.elsevierdirect.com/v2/companion.jsp ISBN=9780123815415) Table of Contents Introduction End User Programming on the Web Allen Cypher (IBM) Why We Customize the Web Robert Miller (MIT) I. End User Programming Languages for the Web Sloppy Programming Greg Little (MIT) Mixing the reactive with the personal: Opportunities for end user programming in Personal information management (system) Max Van Kleek (MIT) Going beyond PBD: A Play-by-Play and Mixed-initiative Approach (system) Hyuckchul Jung (Institute for Human and Machine Cognition) Rewriting the Web with Chickenfoot (system) Robert Miller (MIT) A Goal-Oriented Web Browser (system) Alexander Faaborg (Mozilla) II. Systems and Applications Clip, Connect, Clone: Combining Application Elements to Build Custom Interfaces for Information Access (system) Jun Fujima (Hokkaido) Mash Maker (system) Robert Ennals (Intel) Collaborative scripting on the web (system) Tessa Lau (IBM) Programming by a Sample: Rapidly Creating Web Applications with d.mix (system) Bj rn Hartmann (Stanford) Highlight: End User Mobilization of Existing Web Sites (system) Jeffrey Nichols (IBM) Subjunctive Interfaces for the Web Aran Lunzer (University of Copenhagen) From Web Summaries to Search Templates: Automation for Personal Web Content (system) Mira Dontcheva (Adobe Systems) Access to the Temporal Web Through Zoetrope (system) Eytan Adar (University of Washington) Enabling End Users to Independently Build Accessibility into the Web Jeffrey Bigham (University of Washington) Social Accessibility: A Collaborative Approach For Improving Web Accessibility (system) Yevgen Borodin (Stony Brook) III. Data Management and Interoperability A World Wider than the Web: End User Programming Across Multiple Domains (system) Will Haines (SRI) Knowing What You're Talking About: Natural Language Programming of a Multi-Player Online Game (system) Henry Lieberman (MIT) IV. User Studies Mashups for Web-Active End Users Nan Zang (Penn State) Mashed layers and muddled models: debugging mashup applications M. Cameron Jones (Yahoo!) Reuse in the world of end-user programmers Christopher Scaffidi (CMU) Using Web Search to Write Programs Joel Brandt (Stanford)}
}

@inproceedings{10.1145/3568812.3603483,
author = {Brock, Janet and Gransbury, Isabella and Catet\'{e}, Veronica and Barnes, Tiffany and Grover, Shuchi and Ledeczi, Akos},
title = {Student Attitudes During the Pilot of the Computer Science Frontiers Course},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603483},
doi = {10.1145/3568812.3603483},
abstract = {Motivation. We have created a modular project-based learning curriculum, Computer Science Frontiers (CSF) [1, 8], for secondary students in attempts to increase the persistence of computer science (CS) students in higher education. The CSF course is divided into four different modules (Distributed Computing, Internet of Things, Artificial Intelligence, and Software Engineering), each centered around a topic typically introduced to students only in higher education. Using the block-based programming environment NetsBlox [4], students are able to access various Application Programming Interfaces related to their interests [2, 3]. The goal of this course is to increase student interest in CS during high school - when first career choices occur [7] - in hopes they will persist in CS during their undergraduate studies. Research question. The research question for this study was: How does the Computer Science Frontiers course affect student attitudes towards computer science?Research Methods. We conducted over 20 interviews with students throughout a CSF pilot course that took place over the 2022-2023 school year. Interviews were conducted with at least five students at the end of every module. Two researchers have conducted thematic analysis with student responses from the first two modules [5]: Distributed Computing (DC) and Internet of Things (IoT). First, the two researchers developed a norm by tagging one interview together [6]. Next, the researchers independently coded the rest of the interviews for each module. After completing a single module’s interviews, the researchers met to rectify any discrepancies. Finally, the tags were grouped together based on common themes. Through this process, we found a total of seven themes. Results. The themes found through thematic analysis include: computer science, attitudes towards course, student wants, student struggles, attitudes towards projects, collaboration, and student progression. As a result of this study, we have identified different needs for secondary students with varying background in CS when studying more advanced CS topics, such as IoT. For example, a need of students who have less prior CS knowledge than others may be to review programming concepts in order to be successful in the course. We have also identified a positive change in student’s attitudes towards computer science after the first two modules. These insights provide the CS education community with ways to engage students with concepts that they have not been exposed to and how to increase their interest in CS. Implications. The CSF curriculum is currently online, and is available to computer science instructors. Each module is separated into eight to nine units which are accompanied by activities and teaching guides. This curriculum provides educators with materials and activities to introduce students to more advanced CS topics, either through individual modules or as an entire course. In future research, we plan to use CSF in an outreach program and implement the course in two secondary classrooms in the 2023-2024 school year.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {24–25},
numpages = {2},
keywords = {socially relevant examples, secondary, project-based learning, education research, collaborative learning environment, block-based learning},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@book{10.5555/582699,
author = {Keating, Jody and Software, Fig Leaf},
title = {Inside Flash MX},
year = {2002},
isbn = {0735712549},
publisher = {New Riders Publishing},
address = {USA},
edition = {2nd},
abstract = {From the Publisher:  Inside Flash MX  employs a comprehensive and comparatively advanced approach for designers and developers, addressing the emerging fact that Flash is both a designer's tool and a programming environment. Other "large" books are either more introductory or suffer from a lack of consistency and cohesiveness due to being written by several unassociated authors. Working professionals-designers, developers, and those who do both-who need a comprehensive, non-introductory tutorial reference. This is the largest growth area in the Flash market: NOT simply designers, but those familiar with the technology who use it as front end tool and as a programming tool as well. Fig Leaf's Creative Media Department has designed and developed engaging web-based media for some of the most recognized companies and organizations in the country. Author Biography: Fig Leaf's Creative Media Department has designed and developed engaging web-based media for some of the most recognized companies and organizations in the country. Fig Leaf offers creative services from a talented team of award winning graphic artists, programmers, writers, and instructional technologists who consistently build next- generation solutions for our clients. Using a blend of creative talent, experience and technical know-how, we are able to take our client's web media to a new level that balances form, flow and function. Members of the Creative Media team speak regularly at tradeshows, user groups, and design conferences to inform others about the tools, technologies, and processes that they successfully employ.   Jody Keating  is the Assistant Director of Interactive Media at Fig Leaf, where she makes the Interactive Media programmers and designers play nice together. Jody is a Macromedia Certified Professional (MCP) and has taught Macromedia Authorized classes for Flash, Generator, UltraDev, Dreamweaver, Fireworks, and CourseBuilder. In addition to her duties at Fig Leaf, Jody is the Program Coordinator for the Washington Area Macromedia Organization (WAMMO). Jody is also a Cold Fusion developer and is having way too much fun playing with integrating Flash and ColdFusion. She has an undergraduate degree in Anthropology from the George Washington University and has done graduate work in Geobiology. When Jody's not writing or managing, she's racing sailboats, kayaking, or rowing on the Chesapeake Bay.  Tom Pizer  is a partner and the Vice President of Creative Media for Fig Leaf Software. He is responsible for Fig Leaf's creative vision and the success of the client web sites and interactives that Fig Leaf creates. During his career, he has written course material on computer-based design and collaborated on industry-related books. His work has been featured in leading industry magazines and been written up in online trade sites. Tom is the current president of the Washington, DC area Macromedia User's Group (WAMMO). During his tenure as president, he has forged strong ties with Macromedia and their Product Directors, programmers, sales staff, and training managers for the full suite of Macromedia's web tools.When  Branden Hall  is not working with computers (rarely), he enjoys the great outdoors: camping, hiking, biking, and climbing. As an Eagle Scout, these playtime pursuits come as naturally to him as programming, which is what Branden is doing much of the rest of the time. Branden is known for developing cutting edge ActionScript techniques. At Fig Leaf Software, he is the Senior Interactive Developer and a Macromedia Instructor. His work at Fig Leaf, combined with Branden's prominence on his Flash ActionScript mailing list, FlashCoders, and his participation in many other lists, conferences, and user groups have spawned speculation that Branden Hall cannot possibly be just one man. Amusing though the thought of a host of Branden clones running around may be, his wife Patricia Hall guarantees us all that there is, indeed, only one.  Tracey Sheeley  is the Assistant Director of Creative Services at Fig Leaf, and is one of the original "Figs". While creating many online presences for clients, she has eagerly watched Flash evolve into the indispensable program it is today. Tracey has a BA in Studio Art earned at the University of Maryland. Spanning three states throughout her life, she grew up in Maryland, works in Washington, DC, and now lives in Virginia.  Chris Smith  is an instructor and designer at Fig Leaf. He also teaches Flash, Dreamweaver, and Fireworks at the Corcoran School of Fine Arts in Washington, DC. Chris graduated in 1995 with a BFA from the University of Southern California. While earning his degree, he worked at Activision, Inc. and contracted an incurable interest in computer game graphics and animation that has dominated his creative pursuits ever since. Thirty years before the mast, he lives in Alexandria, VA.  Kevin Towes  is a Co-Founder and Chief Technical Officer of Pangaea NewMedia Inc. in Toronto, Canada. His experience ranges from traditional film, photography, and sound to 3D animation and database and software development, culminating in the integration of all of these disciplines. Kevin's rich mix of visual and technical knowledge has been a leading force in Pangaea's success. A leader in the ColdFusion community for the past three years, Kevin is the founder of the Toronto Cold Fusion Users Group, one of the largest CF groups in the world. He is a certified ColdFusion Developer and has led his team at Pangaea to achieve much success with Macromedia Products.   Matthew David  has been developing Internet solutions for over seven years and has serviced many Fortune 500 companies. His books include Flash MX Magic (New Riders) and contributions to Flash 5 Magic, Flash 5: Visual Design, Web Development Bible, Inside Dreamweaver 4, and the Dreamweaver Bible. He is currently working on chapters for three new books. Matthew also contributes regularly to Devx.com, Element K Journal's Macromedia Solution, and Inside Project Management magazines, as well as web sites such as Macromedia.com, SitePoint.com, and UDZone.com. His own web site is http://www.matthewdavid.ws. Guy Rish is an independent consultant. He carries developer and instructor certifications from Macromedia and Rational, and in the last few years he has concentrated on applying object-oriented methodologies to web development. When not consulting or training, he does technical writing and editing for various publishers. In recent years he has taken up a traveling habit with his wife, with the end goal of seeing it all.  Patricia Lee Hall  is both a ColdFusion developer and Allaire Certified Instructor. Patti has degrees in English and French. She has also pursued higher education in Public Relations. She has a cat named Reese.  Doug Clarke  has a background in illustration, 3D motion graphics, and sound design, and he likes to use his multi-disciplinary background to push the envelope of interactive animation. "I'm never ever really satisfied with my work, but I do enjoy the process of creating and combining different mediums within Flash. Drawing is the foundation for all my work, which in turn is based on keen observation." His work has been featured in various publications including USAToday, Richmond Times Dispatch, and the Virginian Pilot. When he's not working in Flash, Doug enjoys illustration and painting. Doug is also an avid surfer and spends his off time with his family and friends in Virginia Beach. He holds a Bachelor of Fine Arts degree, graduating Magna Cum Laude of Virginia Commonwealth University. You can find Doug's current work at http://www.liquidmethod.com.  Christopher Hayes  has been using Flash since Flash 3 and enjoys creating bleeding-edge design. Chris received his Master's degree in Computer Art from Savannah College of Art and Design and his Bachelor's degree in Fine Arts from Xavier University of Louisiana. Chris (aka P the Wicked) is also an emcee/music producer and enjoys the true hip hop culture. Chris believes that his best work ever is the next one.}
}

@inproceedings{10.1109/FIE49875.2021.9637170,
author = {Kastner-Hauler, Oliver and Tengler, Karin and Demarle-Meusel, Heike and Sabitzer, Barbara},
title = {Adapting an OER Textbook for the Inverted Classroom Model — How To Flip the Classroom with BBC micro:bit Example Tasks},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE49875.2021.9637170},
doi = {10.1109/FIE49875.2021.9637170},
abstract = {Full Paper Research-to-Practice The current COVID-19 crisis has created significant challenges for schools. The growing importance of “flipping the classroom” and the needful emphasizing of online-learning were owed to the situation. To meet these requirements, materials and tasks must be adapted. The Open Educational Resource (OER) textbook “Computational Thinking with the BBC micro:bit” was developed for the introduction of Computational Thinking (CT) for 10-14-year-old pupils in Austria's secondary schools. Example tasks in the textbook are designed with an open end and present extensions with ideas for further development instead of ending abruptly. This article provides a guideline for a clear distinction in redesigning existing lessons following the Inverted Classroom Model (ICM) using videos for pre-class work and live task extensions for in-class work. Which parts in the learning design must remain as live lessons and which parts can be adapted for video lessons? The respective research shows that examples that have a makerspace activity as an extension are especially helpful for an efficient determination of the appropriate part in the learning design and particularly suitable for an adaptation with ICM. The central advantage of the ICM is that it responds flexibly to the individual learning needs of each student. It allows students to take their time reviewing the material at their own pace without getting left behind. The textbook used here encourages pupils to find their own solutions by explorative learning using the block-based programming environment MakeCode. Additional information to be uncovered by the learner is provided for every single step in the accompanying online wiki website. Results from observations showed that this uncover-function, being a central element of the online material, encouraged the learners to explore their own way in finding a solution with playful elements and increased motivation. The many haptic elements of a makerspace activity are in particular useful for consolidation of the learned and are predisposed for in-class work and deepening the understanding following the constructionism theory. A Design-Based Research (DBR) approach is used to create and evaluate the redesign of a proven example task in a pilot project. Teachers, who are already familiar with the BBC micro:bit and the OER textbook, were trained on how to use the “flip-version” of an example task in their lessons and asked to develop a lesson plan for implementation. The didactic approach to redesigning the material and teacher training was evaluated during the first cycle of DBR. Results from expert interviews showed that the redesigned material and training deliver a solid ground for rework and further research on a larger scale.},
booktitle = {2021 IEEE Frontiers in Education Conference (FIE)},
pages = {1–8},
numpages = {8},
location = {Lincoln, NE, USA}
}

@inproceedings{10.1145/330908.331912,
author = {K\"{o}lling, Michael and Rosenberg, John},
title = {Objects first with Java and BlueJ (seminar session)},
year = {2000},
isbn = {1581132131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/330908.331912},
doi = {10.1145/330908.331912},
abstract = {Object-oriented languages have been taught for some time at universities. The most common approach has been to teach those constructs required for imperative programming first and to introduce the notion of classes and objects somewhat later in the course. More recently, many educators have been promoting the notion of teaching about classes and objects first. This helps students to adopt the object-oriented paradigm at an early stage and encourages them to focus on the application structure before beginning coding. Most new textbooks have followed such an approach.While this method has clear advantages, it is not easy to realise in practice. This is partly a result of the languages used for teaching. However, we would argue that the major difficulty comes from the lack  of program development environments and tools which themselves fully embrace the object-oriented paradigm.The use of Java as the language for teaching addresses some of the problems. Java with its clean support for the object-oriented paradigm is now widely regarded as a suitable choice for introductory teaching. The choice of environment, however, remains an issue.The view of the development environment as a major difficulty in Java courses is further supported by numerous reports of educators relating their experiences with teaching introductory Java courses. While Java was consistently described as an excellent language for teaching the object-oriented paradigm, the environments available are regularly identified as a significant source of problems. These may be  divided into two areas:
The environments are designed for professional programmers. They are too complex and have a steep learning curve. Thus valuable teaching time is spent teaching the students how to use the environment and this detracts from the principles of programming.Most of the existing environments fail to fully adopt the object-oriented paradigm. Users of the environment must deal with files, lines of code and directory hierarchies rather than classes, objects and relationships.In this seminar we will argue the case that the requirements for teaching the object-oriented paradigm and Java can only be satisfied by the provision of a program development environment specifically designed for teaching.We will introduce BlueJ, a relatively new development environment which addresses all of these issues. We will show how the unique features of this environment can be used to create an introductory Java course that fully embraces the “object first” approach and supports the presentation of a cleaner picture of the paradigm than previously possible.BlueJ is based heavily on earlier work by us on a language and environment called Blue. BlueJ is a complete Java development environment, written entirely in Java. It provides graphical support for object-oriented design, abstracts over files and the operating system and provides fully integrated support for a design, edit, compile and test cycle. In addition, BlueJ supports interactive creation of objects and interactive calling of methods of objects. This provides support for incremental development, one of the major advantages of object-orientation. It includes an easy-to-use debugger and support for applications and applets.One of the main differences between BlueJ and other environments is its distinct focus on a teaching context. It combines powerful tools with an easy-to-use interface, avoiding the complexity that creates so many problems when using existing environments in a classroom.BlueJ has been used very successfully for two semesters as Monash University.The presentation will provide the context in which the BlueJ project has been developed. We will discuss the design principles for BlueJ, the major aims of the project and our experiences with using it in class. A demonstration of the current version of BlueJ will be given. We will also demonstrate a set of examples and problems which can be used in a first Java course and show how the course structure can be improved and support teaching “objects first” with the availability of an environment that fully supports the paradigm.BlueJ is available free of charge and can be used by any interested institution. Details of how to obtain a copy of BlueJ will be provided at the seminar.},
booktitle = {Proceedings of the Thirty-First SIGCSE Technical Symposium on Computer Science Education},
pages = {429},
location = {Austin, Texas, USA},
series = {SIGCSE '00}
}

@inproceedings{10.1145/1384271.1384395,
author = {Kiesmueller, Ulrich and Brinda, Torsten},
title = {How do 7th graders solve algorithmic problems? a tool-based analysis},
year = {2008},
isbn = {9781605580784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1384271.1384395},
doi = {10.1145/1384271.1384395},
abstract = {Informatics education, not only in higher but also in secondary education, is often assisted by special learning software to teach the fundamental ideas of algorithms [2]. In this context pupils also learn the basics of programming using didactically reduced, textbased or visual programming languages. Therefore in Germany, in some federal countries (for example Bavaria), where the basics of algorithms are already taught in the 7th grade (age 12 to 13 years), age-based learning and programming environments, such as Karel, the robot and Kara, the programmable ladybug [1], are used. Although the design of these environments is age-based, working with them to solve algorithmic problems often causes problems in the classroom. These tools give feedback to the learners based on the analysis of a current solution attempt without taking the previous problem solving process into account. The system messages are often rather technical and therefore hardly helpful especially for weaker learners to enable them to correct arisen problems by themselves. In order to give optimal support to pupils in these situations and therefore improve the learning processes, the learner-system interaction of the used educational software environments should be enhanced and better be adapted to the learners? individual problem solving strategies.The main objective of this research project is to find out, to what extent the automated diagnosis of a problem solving strategy of a learner is possible, and to what extent this knowledge can be used to enhance the learner-system interaction. Starting from the advantages and disadvantages of standardized process observation methods, two software-based research instruments for the system supported diagnosis of the individual proceedings, using the learning environment Kara, were designed and implemented. With the first component learner-system interactions are recorded, the second one provides functions to analyse the collected data. Using test-cases gives a first idea of the quality of the solution attempts.The requirements for the software components resulted from several test scenarios with a small number of participants with different qualification in computer science (from novices to graduating computer science students). During these tests each individual was observed by a researcher and additionally interviewed afterwards. A first version of the implemented instruments was tested in case studies with more than 100 participants (12 to 13 years old) from Bavarian grammar schools to evaluate the suitability for daily use. During the studies the learners were asked to solve three given tasks in a session of 45 minutes, provided by the Kara system, individually (one pupil per computer), but communication between the test persons was allowed. The tasks required knowledge of the control structures (sequence, selection, iteration).The results of these studies indicate that it is possible to identify and to evaluate different problem solving patterns with the help of the developed instruments. To identify different types of learners? strategies it is necessary to combine the various kinds of visualizations of the collected data. To support automatic categorization pattern-recognition methods will be used. The collected ordinal (test-case results) and nominal data can be used for analyses of the correlation between different factors (for example number of error messages or program executions compared with the assessment of the solution attempt) with methods of descriptive statistics.},
booktitle = {Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education},
pages = {353},
numpages = {1},
keywords = {tool-based analysis, secondary computer science education, problem solving process, kara, didactics of informatics, algorithms},
location = {Madrid, Spain},
series = {ITiCSE '08}
}

@article{10.1002/spe.2471,
author = {Kim, Neunghoe and Park, Soojin and Jeong, Dongwon and Hwang, Mansoo and Park, Sooyong and In, Hoh Peter},
title = {EURECA: End-user requirements engineering with collaborative animation},
year = {2017},
issue_date = {July 2017},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {47},
number = {7},
issn = {0038-0644},
url = {https://doi.org/10.1002/spe.2471},
doi = {10.1002/spe.2471},
abstract = {In recent years, software development environments have changed from being driven by professional developers to being driven by technical end users. One of the key issues in end-user-driven software development environments is how to guarantee the quality of the software artifacts. Measuring the quality of developed software requires the correct specification of a quality range the software is expected to meet. However, technical end users are non-professionals in engineering techniques for software requirements, and training a developer to be an expert in requirements engineering in a short period of time is difficult. This paper proposes a new software development life cycle based on reutilizing previously evaluated software requirements assets from completed projects. End-User Requirements Engineering with Collaborative Animation, a tool that was developed to support the proposed software development life cycle, is described and demonstrated by application to three projects. The results from real cases are used as the basis for a discussion on the efficiency enhancement of requirements work, an increased rate of reusing legacy software requirements assets, and an improvement in the technical end user's individual competency level using the End-User Requirements Engineering with Collaborative Animation. Copyright © 2017 John Wiley &amp; Sons, Ltd.},
journal = {Softw. Pract. Exper.},
month = {jul},
pages = {1001–1012},
numpages = {12},
keywords = {software reuse, requirements engineering, end-user software engineering}
}

@inproceedings{10.1145/3305275.3305330,
author = {Ou, Xiu-ying},
title = {Research on University Education Management System Based on Big Data},
year = {2018},
isbn = {9781450365703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3305275.3305330},
doi = {10.1145/3305275.3305330},
abstract = {In recent years, information technology has achieved rapid development and has been widely used in many fields. With the continuous improvement of the level of higher education and the scale of education in China, colleges and universities have paid more and more attention to the use of information technology in education management. Many universities have carried out research work on education management systems based on big data. It summarizes the related theories of big data, analyzes the importance of big data, and puts forward effective solutions according to the application status of big data in college education management system. In the overall system architecture, database security, system requirements, After detailed requirements analysis of system functions and other aspects, a system solution combining flexibility, openness and applicability was developed. For the domestic educational management systems, most of them are based on C/S or B/S single mode, they are difficult to meet the system solution requirements proposed in this paper. The system developed in this paper proposes a combination of C/S and B/S modes. Oracle and PL/SQL are used as the back-end database. The front-end development tools use Delphi2009, ASP.net, PL/SQL Developer and auxiliary software Dreamweaver5.0.},
booktitle = {Proceedings of the International Symposium on Big Data and Artificial Intelligence},
pages = {275–280},
numpages = {6},
keywords = {management system, impact countermeasures, college education, C/S and B/S, Big data},
location = {Hong Kong, Hong Kong},
series = {ISBDAI '18}
}

@article{10.5555/3606402.3606429,
author = {Char, Bruce and Earth, Steve and Johnson, Jeremy},
title = {A Web App for Writing with Mathematical Logic},
year = {2023},
issue_date = {April 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {8},
issn = {1937-4771},
abstract = {Logicwriter Actual is a web app (https://www.cs.drexel.edu/~bchar/logicwriter/standardConfig/web/index.html) designed for freeform entry and linear display in Unicode of text combined with symbolic logic characters such as ≡, ∃, ∧, ⇒, λ, and Ω. It is designed for the writing done by students or instructors in foundational-level (second year) courses introducing mathematical reasoning: elementary formal or informal proofs often involving commonplace situations or computer science contexts such as program behavior. Rather than being a scaffolded practice harness [1], or an automated reasoning tool/proof checker [2, 5], the goal of Logicwriter Actual is just to make it easier for students to practice more mathematical writing. The WYSIWYG result can be copy/pasted into most document processors (for submitted or shared work), Discord or Slack channels (for chat conversations), email, code editors, etc. It is designed to be immediately usable by browser- and laptop-savvy students, so more convenient to use in a foundational course than available alternatives (word processors, LaTeX, LyX, keyboard entry of Unicode indices, Math Jax plugins, etc. [3, 4, 6]) It is designed to need minimal computer resources (runs in browser, can be delivered from any web page server), and instructor time (for student training, or tech support). Because it is just a writing tool, it is compatible with most instructional approaches that ask students to write their own proofs and explanations. Assessment is underway through student survey of usage experience and effects, and by instructor survey/interview.to see if there are perceived benefits to its approach to text entry and style of implementation as a web app.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {220–221},
numpages = {2}
}

@inproceedings{10.5555/645979.675853,
author = {Lucero, Don Scott},
title = {Software measurement in the U.S. Army},
year = {1997},
isbn = {0818681055},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many means for organizing software measures have been developed over the years. The Army has found that the larger the organization, the higher in the hierarchy the requirement for software measurement needs to be stated. The reason for this stems from the growing diversity of developers in larger organizations. Small organizations that have a well defined process for developing software, or a common set of software development tools, can require that specific data elements be collected, or specific measurements be taken. If larger organizations attempting to impose such specific requirements on developers are likely to meet some resistance, a tight policy does not allow for variation in development processes and development tools. With software development centers building air defense systems in Huntsville, Alabama; command and control systems in Monmouth, New Jersey; logistics systems in Petersburg, Virginia; personnel systems in Fairfax, Virginia; communications systems in Sierra Vista, Arizona; aviation systems in St. Louis, Missouri, armor systems in Picatinny, New Jersey and Detroit Michigan, and engineering systems at various locations throughout the US, the Army must have a policy for software measurement that puts in place flexible reporting requirements, yet provides training and support tailored to the needs of specific programs.},
booktitle = {Proceedings of the 21st International Computer Software and Applications Conference},
pages = {589–590},
numpages = {2},
keywords = {software measures, software measurement, software development tools, software development centers, small organizations, personnel systems, military computing, logistics systems, larger organizations, engineering systems, command and control systems, aviation systems, armor systems, air defense systems, US Army operational test/evaluation command, US Army},
series = {COMPSAC '97}
}

@phdthesis{10.5555/AAI28649526,
author = {Ross, Anne Spencer and Jennifer, Mankoff, and Meredith, Ringel Morris, and Mark, Harniss,},
advisor = {James, Fogarty, and O, Wobbrock, Jacob},
title = {A Large Scale, Multi Factor Approach to Understanding and Improving Mobile Application Accessibility},
year = {2021},
isbn = {9798480642926},
publisher = {University of Washington},
abstract = {Accessibility failures in mobile applications (apps) create barriers for disabled people and people who use assistive technologies. Given the growing role of apps in everyone's daily life, equitable access is imperative. Toward this goal, I created a conceptual framework for understanding and improving app accessibility at scale inspired by epidemiology. My epidemiology‑inspired framework poses app accessibility failures as "diseases" in a "population" of apps. This perspective forefronts a population-level perspective within an ecosystem of factors that impact app accessibility (e.g., developer tools, guidelines, company culture, and many more). In this dissertation, I demonstrate my thesis that applying my epidemiology‑inspired framework, which emphasizes large‑scale and multi‑factor approaches, (1) can reveal population‑level trends of accessibility failures, (2) can aid in identifying a range of factors that impact app accessibility, and (3) can inform the design of tools for identifying and repairing accessibility failures in apps.To enhance our understanding of the state of app inaccessibility, I performed the first large‑scale analyses of Android app accessibility. My results measured the prevalence of accessibility failures across apps and identified classes of elements that frequently had accessibility failures. Missing labels was one of the most prevalent failures; 23% of the 8,901 apps had more than 90% of their image-based elements missing labels. Reflecting a less frequent but severe accessibility failure, 8% of 9,999 tested apps were completely unusable with many assistive technologies, such as screen readers. Apps with such failures disproportionately came from the Education category.Using a multi‑factor assessment, partially guided by my large‑scale analyses, I identified accessibility shortcomings in environmental factors such as programming tools, developer guidelines, and inter-team dynamics that could contribute to app inaccessibility at scale. For example, I identified a set of game engines and cross-platform tools (e.g., Unity, Adobe Air) that frequently produced apps that were unusable with many assistive technologies. Another factor I assessed was Android's implementation documentation, finding many instances of missing labels in the example code snippets.Toward improving app accessibility, I explored techniques for enhancing developer tools, testing tools, and third‑party repairs. In developer tools, I present novel designs and techniques that aim to improve the efficiency, effectiveness, and education of developers by tightening the runtime-implementation feedback loop, leveraging screen context to provide more specific repairs, and using new visualization and interaction tool interface designs. Within testing tools, I prototyped an extension to Google's Accessibility Scanner to allow human annotation of automated results. Professional accessibility testers found the tool promising and discussed factors beyond tools, such as knowledge gaps and social dynamics with the developer teams, which affected their testing. Addressing repairs after an app is released, I present a proof-of-concept for third-party repair that people who use screen readers found useful while highlighting factors around trust, repair-production infrastructure, and co-creation that would impact real-word deployment.Throughout my dissertation, I leverage my epidemiology‑inspired concepts and language to highlight the value of my research and place it within the larger space of work on app accessibility. Together, this research aims to expand our understanding of app accessibility at scale and inform efforts to improve it.},
note = {AAI28649526}
}

@inproceedings{10.5555/794188.794299,
author = {Zalewski, Janusz},
title = {Automatic Development Tools in Software Engineering Courses},
year = {2000},
isbn = {0769504213},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, the author discusses the role of automatic software development tools in graduate software engineering courses. The basic requirements for such tools, from the industry perspective, are presented, followed by the selection of tools meeting a comprehensive set of criteria in four process-related dimensions: internal, vertical, horizontal, and diagonal. Typical software development projects for student teams used in the Software Engineering Program at the University of Central Florida are presented, involving the following four software tools: SES/workbench, ObjecTime Developer, iLogix Rhapsody, and Gensym G2.},
booktitle = {Proceedings of the 13th Conference on Software Engineering Education &amp; Training},
pages = {200},
series = {CSEET '00}
}

@inproceedings{10.1007/978-3-031-09342-5_45,
author = {Patton, Evan and Van Woensel, William and Seneviratne, Oshani and Loseto, Giuseppe and Scioscia, Floriano and Kagal, Lalana},
title = {Development of AI-Enabled Apps by Patients and Domain Experts Using the Punya Platform: A Case Study for Diabetes},
year = {2022},
isbn = {978-3-031-09341-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-09342-5_45},
doi = {10.1007/978-3-031-09342-5_45},
abstract = {It is challenging for programmers to build a mobile health app that is rich in AI features, and near impossible for non-technical users such as domain experts and patients. However, it is exactly these users that possess the domain knowledge and experience on how to best manage health conditions, and how AI features can help achieve that goal. End-user development environments, such as MIT Punya, can help lay users to better collaborate on mobile health apps; and even open the door for these users, given some training, to prototype their own mobile health apps. As a subfield of AI, Semantic Web technology can help with integrating online data sources with patient health data, and reasoning over the integrated data to issue smart health recommendations.},
booktitle = {Artificial Intelligence in Medicine: 20th International Conference on Artificial Intelligence in Medicine, AIME 2022, Halifax, NS, Canada, June 14–17, 2022, Proceedings},
pages = {431–435},
numpages = {5},
keywords = {Semantic web, Diabetes management, Patient apps},
location = {Halifax, NS, Canada}
}

@inproceedings{10.1145/1066129.1066130,
author = {Meyer, Matthias and Wendehals, Lothar},
title = {Teaching object-oriented concepts with Eclipse},
year = {2004},
isbn = {9781450377980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066129.1066130},
doi = {10.1145/1066129.1066130},
abstract = {Object-oriented software development is a subject area difficult to teach, especially to beginners. They face a lot of abstraction and (from a beginners point of view) isolated topics, such as the syntax and semantics of a programming language, the functionality of a software development environment and basic object-oriented concepts. Although many professionals in education believe in the "object first" approach as the best method of introducing object-oriented concepts, there is no common agreement on how to start such courses. Current study programs often begin by teaching a programming language, instead of focusing on the basics of object-oriented concepts.In the last years a learning environment was developed based on a visual programming language to abstract away from details. It assists teaching step-by-step object-oriented concepts and the syntax and semantics of a programming language in secondary schools and first year university courses. Our goal is to port this learning environment to the widely used IDE Eclipse.},
booktitle = {Proceedings of the 2004 OOPSLA Workshop on Eclipse Technology EXchange},
pages = {1–5},
numpages = {5},
location = {Vancouver, British Columbia, Canada},
series = {eclipse '04}
}

@proceedings{10.1145/1117696,
title = {eclipse '05: Proceedings of the 2005 OOPSLA workshop on Eclipse technology eXchange},
year = {2005},
isbn = {1595933425},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {eTX and The Eclipse PhenomenonThe following sketches the history of the Eclipse project and
the eTX events.The Eclipse ProjectThe Eclipse Project began in April 1999 at IBM's OTI laboratory.
It was initially conceived as a successor product for the VisualAge
family of software development tools. VisualAge was a commercially
successful IDE, but it was also a closed environment built on
proprietary APIs. It did not integrate well with other vendors'
tools, and only the IBM/OTI team could enhance or extend the
product. Moreover, it was becoming apparent from customer
experience that more was required than a simple re-write of
VisualAge. In fact, there was growing demand for a tool integration
platform --- a programming environment that would provide kernel
IDE functionality, but also allow developers, third party vendors
and users to seamlessly add their own extensions, personalizations,
and enhancements.The Eclipse team set out to identify the essential kernel
concepts underlying the VisualAge product line (or any other IDE
for that matter). In effect, they wanted to strip out all of the
functionality within an IDE that was specific to a particular
programming language, development task, or programming model. The
hope was that there would be substantial residual function left
behind, that could then be restructured to form a content-neutral,
and programming language-neutral, foundation on which IDEs and
similar products could be built from components. It was a bold
venture, since there was no guarantee that anything practically
useful would result.What they discovered was Eclipse: a tool integration platform
together with a set of components (plugins, in the Eclipse
vernacular) that could be seamlessly assembled into a wide variety
of software development products. The Java Development Toolkit
(JDT) --- the Eclipse Java IDE --- became their proof-point. It was
built in parallel by a separate team, which operated independently
from the Eclipse Platform project. The JDT team had no special
privileges; they had to use the same APIs as any third party
product and were allowed no "back door" access to Platform kernel
functionality. The intent was that, despite these constraints, the
finished JDT should be indistinguishable from a purpose-built,
vertically integrated IDE product like VisualAge. This goal was
realized, the Eclipse Project was a success, and the Eclipse
Community was born.In the years since the Eclipse code base was released into open
source by IBM, its growth has been nothing short of spectacular.
Tens of thousands download the Eclipse SDK every week from over
fifty mirror sites around the globe. Thousands of Eclipse plugins
are now available from open source and commercial suppliers.
Software vendors are now shipping several hundred commercial
products based on Eclipse. Over 60 companies are members of the
Eclipse Foundation, which hosts Eclipse open source development.
The first Eclipse Developer Conference (EclipseCON 2004) was held
in February 2004 in Anaheim, California. Over 220 companies and
organizations from nearly 25 countries were represented. EclipseCON
2005, another success, was held in February 2005 in Burlingame,
California.Eclipse and Computer Science ResearchIt has been particularly interesting to see the uptake of
Eclipse within the research community. In retrospect one could
perhaps have anticipated this. Computing is, after all, an
empirical discipline --- ideas must be implemented to be validated.
For software researchers in particular, the computer becomes our
laboratory. We necessarily build on the work of those who have gone
before, and of course as time progresses our technology pyramids
keep getting higher. Complexity is our bane: the low-hanging fruit
were picked long ago, and most interesting problems are just not
simple. Consequently, experimentation usually requires complex
infrastructure, plumbing, as we often call it. Most researchers
spend far too much time building (and rebuilding, and fixing) this
plumbing, and far too little time actually developing new ideas.
Given the nature of research, there are seldom any applicable
standards for such infrastructure (these only come much later when
the research has matured into products). Consequently researchers
up to now have had to live and work in their own vertical towers,
sharing their ideas but only infrequently sharing code. The only
common programming platform among researchers was "emacs", and
while this continues to be very flexible, it lags far behind
industrial-scale IDEs in terms of functionality.But Eclipse changes this context. It provides a means to create
and share that necessary common infrastructure, particularly for
investigators in such areas as programming languages, tools, and
environments. Researchers can focus more of their time on their
real mission of innovation, and much less on the tedious plumbing
tasks. Moreover, Eclipse-based implementations are built from
commercial-quality components, resulting in robust demonstration
systems that make it much easier for researchers to publicize and
promote their work.What specifically does Eclipse offer researchers that makes it
so attractive? First, it is an extensible platform for integrating
components, which comes replete with a large number of commercial
quality components out of the box. It runs on nearly all operating
systems and GUI combinations, and is one of the few Java
implementations that actually realizes that language's "write once
run everywhere" potential (rather than the typical "write once test
everywhere" experience). Perhaps most importantly, it is available
in open source with a generous non-viral license. Finally, it has
tremendous visibility due to broad based industry support, which
includes the backing of such powerhouse firms as IBM, HP, SAP,
Intel, and many more.Eclipse and Computer Science EducationThere are numerous challenges in education these days such as
distance, limited resources and the recognized need to make
learning a personalized and active experience. Many educators are
consequently looking at how technology can address these challenges
and enhance learning in the classroom and beyond. For computer
science education, Eclipse has already been widely adopted as an
IDE to support programming. The advantages for some are that it is
free, platform independent and industrially relevant. But beyond
these obvious advantages, other researchers have recognized that
Eclipse provides an excellent infrastructure for developing
learning tools. These tools can leverage the wealth of technology
already present in the Eclipse community, as well as benefit from
integration with other learning tools developed by other
researchers and educators. The result of these multiple efforts is
the emergence of Eclipse as an effective and powerful platform to
support research in educational technologies and an improved
learning experience in many settings.The eclipse Technology eXchangeThat idea that Eclipse would provide exactly the rich, open and
robust platform that IT researchers needed was not initially an
obvious one, and so it needed to be promoted within the academic
community. IBM and eclipse.org set out to popularize these ideas by
hosting a series of workshops and birds-of-a-feather events at
various software research conferences. This ad hoc program
gradually evolved into the eclipse Technology eXchange (eTX)
workshops held in 2003, 2004 and 2005, the most recent of which
being held at OOPSLA 2005 in San Diego. These events provide a
forum for researchers who are using Eclipse to network and share
their experiences and their code. The foundation for a successful
eTX is a set of high quality, refereed presentations, which serve
to illustrate the breadth and vitality of the Eclipse research and
teaching communities. The paper presentations are combined with
lively discussions which will help set the stage for future
research and development using Eclipse.},
location = {San Diego, California}
}

@inproceedings{10.1109/VLHCC.2010.28,
author = {Rosson, Mary Beth and Sinha, Hansa and Edor, Tisha},
title = {Design Planning in End-User Web Development: Gender, Feature Exploration and Feelings of Success},
year = {2010},
isbn = {9780769542065},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/VLHCC.2010.28},
doi = {10.1109/VLHCC.2010.28},
abstract = {We report an empirical study of nonprogrammers who built a database-centered web application using an end-user web development tool. Half of the users spent time planning their project by creating a concept map before starting the programming; across planning conditions, half of the users were males and half female. Participants who did concept mapping or who were male were more attracted to database programming; however planning did not affect feelings of success and in general females felt more successful than males. We discuss the implications of these findings for work on gender and for future EUP tools and training.},
booktitle = {Proceedings of the 2010 IEEE Symposium on Visual Languages and Human-Centric Computing},
pages = {141–148},
numpages = {8},
keywords = {web development, Gender HCI, End-user programming},
series = {VLHCC '10}
}

@inproceedings{10.1145/3549179.3549181,
author = {Liu, Xiaojuan and Diao, Jietao and Li, Nan},
title = {FPGA hardware implementation of Q-learning algorithm with low resource consumption},
year = {2022},
isbn = {9781450396080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549179.3549181},
doi = {10.1145/3549179.3549181},
abstract = {Q-learning is a kind of reinforcement learning, having a wide range of applications varying in different fields. However, in some circumstances like robot control which has shorter training time requirement, Q-learning algorithm implemented on GPU or CPU may not meet the requirement. In this paper, we proposed a novel serial acceleration architecture for Q-learning algorithm and implemented the architecture on xczu7ev-ffvc1156 FPGA using Vivado 2019.1 development environment. As a result, the resource consumption is reduced by about 50% compared with the architecture proposed in [1],and the update cycle of Q-learning algorithm is fixed to 4 clock cycles.},
booktitle = {Proceedings of the 2022 International Conference on Pattern Recognition and Intelligent Systems},
pages = {8–13},
numpages = {6},
keywords = {acceleration, Q-learning, FPGA},
location = {Wuhan, China},
series = {PRIS '22}
}

@phdthesis{10.5555/AAI28778306,
author = {Liu, Xiao and Beth, Rosson, Mary},
advisor = {Dinghao, Wu,},
title = {Neural Program Synthesis for Compiler Fuzzing},
year = {2020},
isbn = {9798535592121},
publisher = {The Pennsylvania State University},
abstract = {Compilers are among the most fundamental programming tools for building software. However, production compilers remain buggy. GNU compiler collection (GCC), as a long-lasting software released in 1987, provided as a standard compiler for most Unix-like operating systems, has caught over 3,410 bugs from the day they were created. Fuzzing is often leveraged for stress testing purposes with newly-generated, or mutated inputs to find new security vulnerabilities. In our study, we propose a grammar-based compiler fuzzing framework called DᴇᴇᴘFᴜᴢᴢ that continuously synthesizes well-formed C programs to trigger internal compiler errors or "bugs", as they are commonly called. In this framework, we are interested in how to apply generative deep neural networks (DNNs), such as the sequence-to-sequence model, to synthesize well-formed C programs based on training through syntax-correct programs. We are also interested in how to synthesize programs using a novel form of reinforcement learning, where the model becomes its teacher to start with a random neural network with no training data and trains itself through self-play. We will use a synthesized set of new C programs to fuzz off-the-shelf C compilers, e.g., GCC and Clang/LLVM. This thesis describes our analysis of neural program synthesis for compiler fuzzing in three steps.First, we conduct a first-step study by implementing DᴇᴇᴘFᴜᴢᴢ that deploys a sequence-to-sequence model to synthesize C programs. We have performed a detailed case study on analyzing the pass rate of generating well-formed programs and achieving the goal of fuzz testing, which requires a certain degree of variation. In general, DᴇᴇᴘFᴜᴢᴢ generated 82.63% syntax valid programs and improved the testing efficacy with regards to line, function, and branch coverage. It identified previously unknown bugs, and 8 of them were confirmed by the GCC developers.Second, for the cases when we could not get any or enough data to train a model for representing the grammar, we build a reinforcement learning framework for program synthesis and apply it to the BF programming language. With no training data set required, the model is initialized with random weights at the very beginning, and it evolves with environment rewards provided by the target compiler being tested. During the performance of the learning iterations, the neural network model gradually learns how to construct valid and diverse programs to improve testing efficacies under four different reward functions that we defined. We implemented the proposed method into a prototyping tool called AʟᴘʜᴀPʀᴏɢ. We performed an in-depth diversity analysis of the generated programs that explains the improved testing coverage of a target compiler being tested. We reported two important bugs for this production compiler and they were confirmed and addressed by the project owner.Third, we extend the framework to synthesize C programs, which is more challenging in terms of state space. We propose an automatic code mutation framework called FᴜᴢᴢBᴏᴏsᴛ that is based on deep reinforcement learning. By adopting testing coverage information collected at runtime as the reward, the fuzzing agent learns to fuzz a seed program that achieves an overall goal of testing coverage improvement. We implemented this new approach, and preliminary evidence showed that reinforcement fuzzing can outperform baseline random fuzzing on production compilers. It also showed that a pre-trained model can boost the fuzzing process for seed programs with similar patterns.This thesis solves the problem of using the DNN to synthesize new programs for compiler fuzz testing. Specifically, the proposed framework is able to handle compilers of different programming languages. Accordingly, DᴇᴇᴘFᴜᴢᴢ and FᴜᴢᴢBᴏᴏsᴛ are designed for the C compiler testing, and AʟᴘʜᴀPʀᴏɢ is designed for the BF language compiler testing. Additionally, the generative neural networks for program synthesis can be trained with or without training data. Moreover, the model in DᴇᴇᴘFᴜᴢᴢ is trained based on training data but AʟᴘʜᴀPʀᴏɢ and FᴜᴢᴢBᴏᴏsᴛ rely on reinforcement learning, which requires no training samples. We built prototyping tools for each study and applied them for practical use. Their effectiveness was evaluated, and they caught real bugs in off-the-shelf compilers.},
note = {AAI28778306}
}

@techreport{10.5555/896825,
author = {Ramamritham, Krithi and Stankovic, John A.},
title = {Overview of the Spring Project},
year = {1989},
publisher = {University of Massachusetts},
address = {USA},
abstract = {THE SPRING PROJECT AT THE UNIVERSITY OF MASSACHUSETTS IS A RESEARCH AND DEVELOPMENT EFFORT AIMED AT STUDYING `NEXT GENERATION'' TIME-CRITICAL SYS- TEMS. IN ADDITION TO BEING FAST AND PREDICTABLE, THESE SYSTEMS WILL HAVE TO BE FLEXIBLE, ADAPTIVE, AND RELIABLE. THESE REQUIREMENTS ARISE FROM THE FACT THAT FUTURE SYSTEMS WILL BE LARGE AND COMPLEX AND WILL OPERATE IN ENVIRONMENTS THAT ARE DYNAMIC, DISTRIBUTED, AND FAULT-INDUCING. IN ORDER TO ACHIEVE ITS GOALS, THE SPRING PROJECT TAKES A SYNERGISTIC APPROACH INVOLVING THE DEVELOPMENT OF SCHEDULING ALGORITHMS, OPERATING SYSTEM SUPPORT, DISTRIBUTED SYSTEM ARCHITECTURE, AND APPLICATION DEVELOPMENT TOOLS. IN ADDITION, THE FOLLOWING TOPICS ARE ALSO BEING INVESTIGATED AS PART OF THE PROJECT: TRANSACTION MANAGEMENT IN REAL-TIME DATABASES, SUPPORT FOR REAL-TIME APPLICATIONS INVOLVING ARTIFICIAL INTELLIGENCE, AND FORMAL APPROACHES TO THE SPECIFICATION AND VERIFICATION OF REAL-TIME SYSTEMS. THIS REPORT SUMMARIZES THE CURRENT STATUS OF THE PROJECT.}
}

@article{10.4018/joeuc.2011100105,
author = {Kelly, Diane},
title = {An Analysis of Process Characteristics for Developing Scientific Software},
year = {2011},
issue_date = {October 2011},
publisher = {IGI Global},
address = {USA},
volume = {23},
number = {4},
issn = {1546-2234},
url = {https://doi.org/10.4018/joeuc.2011100105},
doi = {10.4018/joeuc.2011100105},
abstract = {The development of scientific software is usually carried out by a scientist who has little professional training as a software developer. Concerns exist that such development produces low-quality products, leading to low-quality science. These concerns have led to recommendations and the imposition of software engineering development processes and standards on the scientists. This paper utilizes different frameworks to investigate and map characteristics of the scientific software development environment to the assumptions made in plan-driven software development methods and agile software development methods. This mapping exposes a mismatch between the needs and goals of scientific software development and the assumptions and goals of well-known software engineering development processes.},
journal = {J. Organ. End User Comput.},
month = {oct},
pages = {64–79},
numpages = {16},
keywords = {Software Development Process, Scientific Software Development, Scientific Software, Professional End-User Developer, Extreme Programming}
}

@inbook{10.5555/770484.770501,
author = {Le Blanc, Louis A. and Rucks, Conway T. and Murray, W. Scott},
title = {A decision support system for prescriptive academic advising},
year = {2002},
isbn = {1930708424},
publisher = {IGI Global},
address = {USA},
abstract = {A decision support system (DSS) was constructed to assist the academic advising staff of a college of business. The microcomputer-based system identifies any remaining unsatisfied degree program requirements, selects courses in which the student can enroll and then prioritizes them. Advisors are then able to spend time on more substantive or developmental advising issues, such as choice of electives, career options and life career goals. Using this system, a student with a minimum of computer knowledge can obtain an optimized course listing without the assistance of a human advisor in less than five minutes.A high-end spreadsheet (i.e., DSS generator) permits a workable and effective academic advising DSS. The database is the most significant part of this DSS. And, since the modeling component is difficult to separate from the structure of the data itself, a database management system might be a better choice as the DSS generator. This platform would provide a more flexible user interface as well as superior data handling capability but at some sacrifice in cost and implementation time.A recent development in the management of university and college organizations is the integrated software system, known also as enterprise resource planning (ERP) software. This integrated administrative software for higher education (e.g., CMDS [Computer Management Development Services]), operating on various hardware platforms, provides student advising data for a variety of prototyping and application development tools, such as Powersoft's Infomaker and Microsoft's Access.},
booktitle = {Advanced Topics in End User Computing},
pages = {263–284},
numpages = {22}
}

@article{10.1109/43.31543,
author = {McMacken, J. R.F. and Chamberlain, S. G.},
title = {CHORD: a modular semiconductor device simulation development tool incorporating external network models},
year = {2006},
issue_date = {November 2006},
publisher = {IEEE Press},
volume = {8},
number = {8},
issn = {0278-0070},
url = {https://doi.org/10.1109/43.31543},
doi = {10.1109/43.31543},
abstract = {A description is given of CHORD, a semiconductor device simulation development tool created at the University of Waterloo. CHORD is written as several small independent programs which use a centralized database facility and is specifically designed to allow the simple creation, development, and testing of new simulation models and algorithms. Communication between programs is implemented by using a byte-stream process. This allows transfer of information between individual programs running on different machines. CHORD is a general environment in that its models can incorporate any set of user-defined variables of equations. This is illustrated by the development of contact boundary conditions incorporating external circuit elements and circuit-level device models using the modified nodal formulation. Several examples of both steady-state and transient simulations are presented},
journal = {Trans. Comp.-Aided Des. Integ. Cir. Sys.},
month = {nov},
pages = {826–836},
numpages = {11}
}

@article{10.1109/MS.2011.72,
author = {Spinellis, Diomidis},
title = {Agility Drivers},
year = {2011},
issue_date = {July 2011},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {28},
number = {4},
issn = {0740-7459},
url = {https://doi.org/10.1109/MS.2011.72},
doi = {10.1109/MS.2011.72},
abstract = {Our growing ability to swiftly put together sophisticated software affords us the luxury to listen to our customers, to try out new things, to make mistakes, to redesign as we move along—in short to be agile. On the technological front, the main driving forces are powerful operating systems, the widespread availability database management systems, a wide selection of libraries, interoperability standards, versatile programming languages, ample processing power, and sophisticated development tools. On the environment front, agility is driven by specialized education, informal management structures, Web access, open source software, shifting user expectations, and the ubiquitous availability of IT infrastructures. Where agility drivers are present, we must adjust our development processes, demand more from our software suppliers, and develop in-house capacity to organically grow applications and services that will delight and even captivate our users and customers.},
journal = {IEEE Softw.},
month = {jul},
pages = {96},
numpages = {1},
keywords = {software management practices, agility}
}

@article{10.1145/190650.190655,
author = {Stewart, Carolee},
title = {Distributed systems in the undergraduate curriculum},
year = {1994},
issue_date = {Dec. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {0097-8418},
url = {https://doi.org/10.1145/190650.190655},
doi = {10.1145/190650.190655},
abstract = {Much of the design and development for new computing systems in the 1990's is being done in a networked computing environment with distributed goals. So why do so many 4-year college computer science departments still not teach "Distributed computing systems" in their undergraduate curriculum? The reasons are varied, but one main one is the belief that such a course requires expensive hardware and the very latest software development tools. This article demonstrates how a course for undergraduates in distributed computing can be successful at giving the students the concepts and principles, while enabling them to create such an application to experience the distributed environment, and do it all on a limited budget. The principles are highlighted along with a practical design and development component, which can give seniors a way to tie together many of the principles and applications of previous courses.},
journal = {SIGCSE Bull.},
month = {dec},
pages = {17–20},
numpages = {4}
}

@inproceedings{10.1109/FIE43999.2019.9028606,
author = {Font\~{a}o, Awdren and Gadelha, Bruno and J\'{u}nior, Alberto Castro},
title = {Balancing Theory and Practice in Software Engineering Education – A PBL, toolset based approach},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE43999.2019.9028606},
doi = {10.1109/FIE43999.2019.9028606},
abstract = {This paper presents an interdisciplinary approach designed to join theoretical foundations required in software engineering with practice required for professional performance in software industry. Although this is a recurring issue on software engineering education, students still have difficulties in dealing with software development environments that are usually more complex than any software projects they have worked at university courses. We developed an approach aligned with market demands and theoretical requirements in software engineering education that incorporates principles of Project Based Learning (PBL) and Agile Methods. It aims to support development of student technical and soft skills in three different, but related, disciplines: Software Engineering Practice, Database Practice and Software Project Management. We describe a set of seven recommendations on how we used largely adopted tools in software development industry in order to develop student technical and soft skills. We describe two case studies: (1) an exploratory case study with three professors and 24 students involving multiple projects; and (2) a confirmatory case study with three professors and 15 students involving a large-scale project. As a result, we point seven recommendations for tool adoption on capstone courses based on lessons learned.},
booktitle = {2019 IEEE Frontiers in Education Conference (FIE)},
pages = {1–8},
numpages = {8},
location = {Covington, KY, USA}
}

@inproceedings{10.1145/3334480.3382879,
author = {Schoop, Eldon and Huang, Forrest and Hartmann, Bj\"{o}rn},
title = {SCRAM: Simple Checks for Realtime Analysis of Model Training for Non-Expert ML Programmers},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382879},
doi = {10.1145/3334480.3382879},
abstract = {Many non-expert Machine Learning users wish to apply powerful deep learning models to their own domains but encounter hurdles in the opaque model tuning process. We introduce SCRAM, a tool which uses heuristics to detect potential error conditions in model output and suggests actionable steps and best practices to help such users tune their models. Inspired by metaphors from software engineering, SCRAM extends high-level deep learning development tools to interpret model metrics during training and produce human-readable error messages. We validate SCRAM through three author-created example scenarios with image and text datasets, and by collecting informal feedback from ML researchers with teaching experience. We finally reflect upon our feedback for the design of future ML debugging tools.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {debugging, interactive visualization, machine learning, tutorial systems},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI EA '20}
}

@inproceedings{10.1145/1315580.1315604,
author = {Grosman, Tom},
title = {Hibachi: the eclipse ada development toolset},
year = {2007},
isbn = {9781595938763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1315580.1315604},
doi = {10.1145/1315580.1315604},
abstract = {Hibachi is an open source (EPL), standard, extensible, vendor-neutral Eclipse Ada development environment. Hibachi is currently in the project proposal phase (see http://www.eclipse.org/projects/dev_process/development_process.php, which involves gathering a viable developer/tester/user community around the project and IP rights to any code contributions. When these pieces are in place, the project can be approved by the Eclipse Management Organization (EMO) and begin producing high quality releases available to end users, as well as third party integrators.As Hibachi project lead, Mr. Grosman has been in touch with the major active Ada development environment vendors (ACT, Aonix, DDC-I, GHS) and they have all expressed interest and a desire to commit resources to making Hibachi a success. In addition, gnuada development team members have also expressed support for Hibachi and a willingness to participate. Mr. Grosman is also in contact with several Universities who will be contributing engineering effort as part of their curricula (master theses), as well as some major industrial partners who are large scale users of Ada and who have expressed interest in contributing development and/or financial resources. Aonix has offered to contribute the source for its Eclipse plugin, AonixADT as a code base for Hibachi. AonixADT currently supports ObjectAda as well as GNAT/gnuada development. Contributions of existing code from other sources are also a possibility depending on the willingness of other potential contributors. Because of these factors, and the encouragement of the Hibachi project mentor Doug Schaefer, CDT project lead, Mr. Grosman has every expectation that the Hibachi proposal will be accepted and that we will be able to produce a plugin with features as outlined in the Hibachi Proposal-http://www.eclipse.org/proposals/adt/. The development plan outlined in the Hibachi proposal indicates that a first useable release of Hibachi will be available in time for SIGAda in November.},
booktitle = {Proceedings of the 2007 ACM International Conference on SIGAda Annual International Conference},
pages = {99},
numpages = {1},
keywords = {software, safety, reliability, languages, high integrity, eclipse, development environment, ada},
location = {Fairfax, Virginia, USA},
series = {SIGAda '07}
}

@inproceedings{10.5555/942796.943259,
author = {Auer, M. and Tschurtschenthaler, T. and Biffl, S.},
title = {A Flyweight UML Modelling Tool for Software Development in Heterogeneous Environments},
year = {2003},
isbn = {0769519962},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A large and growing variety of tools can support allkinds of UML modeling aspects: from model creation toadvanced round-trip engineering of UML models and code.However, such tools aim at supporting specific life-cyclephases, but they often do not meet basic requirements arisingin heterogeneous environments, UML education, earlylife-cycle phases, or agile processes: hassle-free tool deployment,support for fast model sketching, and flexiblegraphic export features.This paper presents the freely available modeling toolUMLet we designed to specifically address these basic issues.It is a flyweight Java application that can easily bedeployed in various development environments; it featuresan intuitive and pop-up-free user interface, while still providingoutput to common high-quality publishing formats.Thus, the tool UMLet provides an effective way to teachUML and to create and share UML sketches, especiallyin agile environments and during early life-cycle phases.Its user interface supports intuitive and exploratory modeling,its architecture makes distribution and deploymentcost-efficient in heterogeneous environments.},
booktitle = {Proceedings of the 29th Conference on EUROMICRO},
pages = {267},
series = {EUROMICRO '03}
}

@book{10.5555/1076970,
author = {Tate, Kevin},
title = {Sustainable Software Development: An Agile Perspective},
year = {2005},
isbn = {0321286081},
publisher = {Addison-Wesley Professional},
abstract = {"Over the years I have seen the software development pendulum swing from one extreme to the other, as deficiencies in 'best practices' at one end of the spectrum spawned a new set of 'best practices' at the opposite end. Kevin Tate's book has finally brought the pendulum to a screeching halt, right about dead center. This book provides a balanced and practical guide to what's important if your goal is to develop software that lasts."-Mary Poppendieck, Poppendieck.LLC. Author of "Lean Software Development""1) In this very practical and accessible book interspersed with real-world examples and personal opinions, Kevin has distilled his years of developing quality software into a set of principles and practices that have been proven to work. If you are thinking of introducing an agile development environment (ADE) into your organization or of improving the one you already have, this book will help you clearly understand the benefits of a sustainable ADE, establish the practices to make it happen and coach you through the follow-up required to change the culture of your organization to make sure the changes take hold.I am currently faced with exactly this challenge and this book has already given me several ideas I am looking forward to trying out.2) In an industry plagued with missed deadlines despite long overtime hours, this book offers a refreshing alternative: a set of guiding principles and simple practices to follow that allow you to get the job done by working smarter, not harder. Drawing on the author's extensive experience developing quality software, the book clearly explains the principles behind a sustainable agile development environment, why it works, the practices to make it happen and the follow through required to turn these practices into habits."-Peter Schoeler, Technical Director, Artificial Mind &amp; Movement"It's a familiar scene-the schedule's tight, people are putting in heroic efforts to get everything done, then at the last minute a change request comes in that wipes out the gains you had finally managed to make in meeting your ship date. Looks like it's pizza at your desk for the weekend again! An unfortunate situation to be in but a pattern that repeats itself all too often. "Sustainable Software Development" offers hope to break this cycle. It shows how a change in mindset can free you from the tyranny of unrealistic expectations and brings development realities out onto the table for everyone to see. By following these techniques you will be able to define and manage a software development environment that will work for the long haul."-Kevin PicottSoftware development for immediate success and long-term sustainabilitySustainable Software Development brings together principles and practices for building software that is technically superior, delivers exceptional business value, and can evolve rapidly to reflect any change to your business or technical environment.Kevin Tate shows how to eliminate practices that make development unsustainable and replaces these practices with a sustainable approach that draws on the best ideas from both agile and conventional development. Tate demonstrates how to balance rapid releases and long-term sustainability, achieving both rich functionality and superior quality. You'll learn how to build a development organization that is more productive and can continually improve its capability to handle complexity and change.Writing for developers, architects, project leaders, and other software team members, Tate shows how to: Take control of your development environment, so you can outship your competitors, leveraging new technologies and responding to new business opportunities Maintain a consistent pace that optimally balances short- versus long-term requirements Keep your code base in a "near-shippable" state between releases Prevent defects, rather than just recognizing and fixing them Invest continually and cost-effectively in software design improvements Leverage the fundamentals of the craft of software development Integrate sustainable processes with Agile and traditional methodologies© Copyright Pearson Education. All rights reserved.}
}

@inproceedings{10.1145/98894.98888,
author = {Kerner, Janet T. and Freedman, Roy S.},
title = {Developing intelligent tutoring systems with a Hypermedia Object-Based Intelligent Educator (HOBIE)},
year = {1990},
isbn = {0897913728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/98894.98888},
doi = {10.1145/98894.98888},
abstract = {There is a great potential for exploiting computer assisted tutoring in industrial training situations and in formal educational environments. This paper discusses our research in developing the concept of an Intelligent Authoring/Instructional System(IA/IS) that can be used as an intelligent courseware development environment as well as an intelligent tutoring system. The goal of our research is to integrate the contributions of the content expert, the instructional design expert, and the course-ware developer into one system that will be used by both authors and students. We discuss how artificial intelligence techniques associated with case-based planning can improve the management of the instructional interaction between the Instructional System and the student.},
booktitle = {Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2},
pages = {890–897},
numpages = {8},
location = {Charleston, South Carolina, USA},
series = {IEA/AIE '90}
}

@inproceedings{10.1109/DEXA.2008.89,
author = {Stoyanov, S. and Mitev, D. and Minov, I. and Glushkova, T.},
title = {eLearning Development Environment for Software Engineering Selbo 2},
year = {2008},
isbn = {9780769532998},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DEXA.2008.89},
doi = {10.1109/DEXA.2008.89},
abstract = {This paper gives brief overview of Selbo 2 – development environment for creating SCORM compatible electronic content. Selbo 2 uses intelligent editors (combination of component and agent) to manipulate learning content and aid content developer during content creation. Ontologies provide developers with predefined resources covering specific domain that can be used directly in the content. Selbo 2 also utilizes education templates that define pedagogical goals and agents to govern them. Furthermore, the environment employs schemes for adapting itself to its user and for collaborating with the learning management system (LMS).Although Selbo 2 is designed primarily for electronic education, it can also be used in Virtual Engineering. Different teams, working on the same project, can prepare e-lectures that give overview of their current progress, using terms and concepts from the same ontology. These lectures can be shared among teams and used as initial teaching materials for new project members.},
booktitle = {Proceedings of the 2008 19th International Conference on Database and Expert Systems Application},
pages = {100–104},
numpages = {5},
keywords = {ontology, e-lesson, agent},
series = {DEXA '08}
}

@inproceedings{10.5555/1565321.1565322,
title = {Front Matter},
year = {2006},
isbn = {1586036734},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for the new economy and science. It creates new markets and new directions for a more reliable, flexible, and robust society. It empowers the exploration of our world in ever more depth. However, software often falls short behind our expectations. Current software methodologies, tools, and techniques remain expensive and not yet reliable for a highly changeable and evolutionary market. Many approaches have been proven only as case-by-case oriented methods.This book presents a number of new trends and theories in the direction in which we believe software science and engineering may develop to transform the role of software and science in tomorrow's information society.This book is an attempt to capture the essence of a new state of art in software science and its supporting technology. The book also aims at identifying the challenges such a technology has to master. It contains papers accepted at the Fifth International Conference on New Trends in software Methodology Tools, and Techniques, V, (SoMeT_06) held in Quebec, Canada, from 25th to 27th October 2006, (url: www.somet.soft.iwate-pu.ac.jp/somet_06). This workshop had brought together researchers and practitioners to share their original research results and practical development experiences in software science, and its related new challenging technology.One of the important issues addressed in this book is software security tools and techniques. Another example we challenge in this conference is, Lyee methodology as a new Japanese emerged software methodology that has been patented in several countries in Europe, Asia, and America. But it is still in its early stage of emerging as a new software style. This book and the series it continues will also contribute to elaborate on such new trends and related academic research studies and development.A major goal of this book was to gather scholars from the international research community to discuss and share research experiences on new software methodologies, and formal techniques. The book also investigated other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. These are essential for developing a variety of information systems research projects and to assess the practical impact on real-world software problems.For an outline of the past series of related events that contributed to this publication are SoMeT_02 that was held October 3--5, 2002, in Sorbonne, Paris, France. SoMeT_03 was held in Stockholm, Sweden, SoMeT_04 held in Leipzig, Germany, SoMeT_05 held in Tokyo, Japan and this publication covers SoMeT_06; held in Quebec, Canada. These events, also initiate the forthcoming SoMeT_07, to be organized in Rome, Italy in November 2007, (url: www.somet.soft.iwate-pu.ac.jp/somet_07/).This book participates to provide an opportunity for exchanging ideas and experiences in the field of software technology opening up new avenues for software development, methodologies, tools, and techniques, especially, software security and program coding diagnosis and related software maintenance techniques aspects.The Lyee framework for example, captures the essence of the innovations, controversies, challenges and possible solutions of the software industry. This world wide patented software approach was born and enriched from experience, and it is time, and again through SoMeT_06 to try to let it stimulate the academic research on software engineering attempting to close the gap so far existed between theory and practice. We believe that this book creates an opportunity for us in the software science community to think about where we are today and where we are going.The book is a collection of 28 carefully reviewed best-selected papers by the reviewing committee.This book covers the following areas:• Software engineering aspects on software security, programs diagnosis and maintenance• Static and dynamic analysis on Lyee-oriented software performance model• Software security aspects on Java mobile code, and networking• Practical artefact on software security, software validation and diagnosis• Software optimization and formal methods• Requirement engineering and requirement elicitation• Software methodologies and Lyee oriented software techniques• Automatic software generation, reuse, and legacy systems• Software quality and process assessment• Intelligent software systems and evolution• End-user requirement engineering and programming environment• Ontology and philosophical aspects on software engineering• Business software models and other kinds of software application models, based on Lyee theoryAll papers published in this book are carefully reviewed selected by the SOMET international reviewing committee. Each paper has been reviewed by three and up to four reviewers and has been revised based on the review reports. The papers were reviewed on the basis of technical soundness, relevance, originality, significance, and clarity.This book outcome is also a collective effort from all the Lyee International project collaborators and industrial supporters. We also, gratefully thank Iwate Prefectural University, University of Laval, Catena Co., ISD, Ltd, SANGIKYO co., and others for their overwhelming support, on this work. We specially, are thankful to the review committee and others who participated in the review of all submitted papers and thanks also for the hot discussion we have had on the reviews evaluation meetings that selected those contributed in book.This outcome is another milestone in mastering new challenges on software and its new promising technology, within SoMeT's consecutive events. Also, it gives the reader new insights, inspiration and concrete material to elaborate and study this new technology.Also, at last and not least, we would like to thank and acknowledge the support of the University of Leipzig, Telematik and e-Business group for allowing us to use the Paperdyne System as a conference-supporting tool during all the phases on review transactions.The Editors},
booktitle = {Proceedings of the 2006 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Fifth SoMeT_06},
pages = {i–xiv}
}

@techreport{10.5555/867967,
author = {Makedon, Fillia and Matthews, James and Owen, Charles B. and Rebelsky, Samuel A.},
title = {Multimedia authoring, development environments, and digital video editing.},
year = {2001},
publisher = {Dartmouth College},
address = {USA},
abstract = {Multimedia systems integrate text, audio, video, graphics, and other media and allow them to be utilized in a combined and interactive manner. Using this exciting and rapidly developing technology, multimedia applications can provide extensive benefits in a variety of arenas, including research, education, medicine, and commerce. While there are many commercial multimedia development packages, the easy and fast creation of a useful, full-featured multimedia document is not yet a straightforward task. This paper addresses issues in the development of multimedia documents, ranging from user-interface tools that manipulate multimedia documents to multimedia communication technologies such as compression, digital video editing and information retrieval. It outlines the basic steps in the multimedia authoring process and some of the requirements that need to be met by multimedia development environments. It also presents the role of video, an essential component of multimedia systems and the role of programming in digital video editing. A model is described for remote access of distributed video. The paper concludes with a discussion of future research directions and new uses of multimedia documents.}
}

@inproceedings{10.5555/1659308.1659309,
title = {Front Matter},
year = {2009},
isbn = {9781607500490},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for science and the new economy. It creates new markets and new directions for a more reliable, flexible and robust society. It empowers the exploration of our world in ever more depth. However, software often falls short of our expectations. Current software methodologies, tools, and techniques remain expensive and are not yet sufficiently reliable for a constantly changing and evolving market, and many promising approaches have proved to be no more than case-by-case oriented methods.This book explores new trends and theories which illuminate the direction of developments in this field, developments which we believe will lead to a transformation of the role of software and science integration in tomorrow's global information society. By discussing issues ranging from research practices and techniques and methodologies, to proposing and reporting solutions needed for global world business, it offers an opportunity for the software science community to think about where we are today and where we are going.The book aims to capture the essence of a new state of the art in software science and its supporting technology, and to identify the challenges that such a technology will have to master. It contains extensively reviewed papers presented at the eighth International Conference on New Trends in software Methodology Tools, and Techniques, (SoMeT_09) held in Prague, Czech Republic, with the collaboration of Czech Technical University, from September 23rd to 25th 2009. (http://www.action-m.com/ somet_2009/).This conference brought together researchers and practitioners to share their original research results and practical development experience in software science and related new technologies.This volume participates in the conference the SoMeT series Previous related events that contributed to this publication are: SoMeT_02 (the Sorbonne, Paris, October 3rd to 5th 2002); SoMeT_03 (Stockholm, Sweden); SoMeT_04 (Leipzig, Germany); SoMeT_05 (Tokyo, Japan); SoMeT_06 (Quebec, Canada); SoMeT_07 (Rome, Italy); SoMeT_08 (Sharjah, UAE) and SoMeT_09 (Prague, Czech Republic). This series of conferences will be continued with SoMeT 10 in Japan from September 29th to October 1st 2010, of which it forms a part, by providing an opportunity for exchanging ideas and experiences in the field of software technology; opening up new avenues for software development, methodologies, tools, and techniques, especially with regard to software security and programme coding diagnosis and aspects of related software maintenance techniques. The emphasis has been placed on human-centric software methodologies, end-user development techniques, and emotional reasoning, for an optimally harmonised performance between the design tool and the user.This book, and the series it forms part of, will continue to contribute to and elaborate on new trends and related academic research studies and developments in SoMeT_2010 in Japan.A major goal of this work was to assemble the work of scholars from the international research community to discuss and share research experiences of new software methodologies and techniques. One of the important issues addressed is the handling of cognitive issues in software development to adapt it to the user's mental state. Tools and techniques related to this aspect form part of the contribution to this book. Another subject raised at the conference was intelligent software design in software security and programme conversions. The book also investigates other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. This is essential for a comprehensive overview of information systems and research projects, and to assess their practical impact on real-world software problems. This represents another milestone in mastering the new challenges of software and its promising technology, addressed by the SoMeT conferences, and provides the reader with new insights, inspiration and concrete material to further the study of this new technology.The book is a collection of 41 carefully refereed papers selected by the reviewing committee and covering:• Software engineering aspects of software security programmes, diagnosis and maintenance• Static and dynamic analysis of software performance models• Software security aspects and networking• Practical artefacts of software security, software validation and diagnosis• Software optimization and formal methods• Requirement engineering and requirement elicitation• Software methodologies and related techniques• Automatic software generation, re-coding and legacy systems• Software quality and process assessment• Intelligent software systems and evolution• End-user requirement engineering, programming environment for Web applications• Ontology and philosophical aspects on software engineering• Cognitive Software and human behavioural analysis in software design.All papers published in this book have been carefully reviewed, on the basis of technical soundness, relevance, originality, significance, and clarity, by up to four reviewers. They were then revised on the of the review reports before being selected by the SoMeT international reviewing committee.This book is the result of a collective effort from many industrial partners and colleagues throughout the world. We would like to thank Iwate Prefectural University, in particular the President, Prof. Yoshihisa Nakamura; SANGIKYO Co., especially the president Mr. M. Sengoku; the Czech Technical University of Prague (Czech Republic); ARISES of Iwate Prefectural University and all the others who have contributed their invaluable support to this work. Most especially, we thank the reviewing committee and all those who participated in the rigorous reviewing process and the lively discussion and evaluation meetings which led to the selected papers which appear in this book. Last and not least, we would also like to thank the Microsoft Conference Management Tool team for their expert guidance on the use of the Microsoft CMT System as a conference-support tool during all the phases of SoMeT.The Editors},
booktitle = {Proceedings of the 2009 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Eighth SoMeT_09},
pages = {i–xiv}
}

@inproceedings{10.5555/1562524.1562569,
author = {Johnson, W. Lewis and Vilhjalmsson, Hannes and Marsella, Stacy},
title = {Serious Games for Language Learning: How Much Game, How Much AI?},
year = {2005},
isbn = {1586035304},
publisher = {IOS Press},
address = {NLD},
abstract = {Modern computer games show potential not just for engaging and entertaining users, but also in promoting learning. Game designers employ a range of techniques to promote long-term user engagement and motivation. These techniques are increasingly being employed in so-called serious games, games that have non-entertainment purposes such as education or training. Although such games share the goal of AIED of promoting deep learner engagement with subject matter, the techniques employed are very different. Can AIED technologies complement and enhance serious game design techniques, or does good serious game design render AIED techniques superfluous? This paper explores these questions in the context of the Tactical Language Training System (TLTS), a program that supports rapid acquisition of foreign language and cultural skills. The TLTS combines game design principles and game development tools with learner modelling, pedagogical agents, and pedagogical dramas. Learners carry out missions in a simulated game world, interacting with non-player characters. A virtual aide assists the learners if they run into difficulties, and gives performance feedback in the context of preparatory exercises. Artificial intelligence plays a key role in controlling the behaviour of the non-player characters in the game; intelligent tutoring provides supplementary scaffolding.},
booktitle = {Proceedings of the 2005 Conference on Artificial Intelligence in Education: Supporting Learning through Intelligent and Socially Informed Technology},
pages = {306–313},
numpages = {8}
}

@inproceedings{10.1109/AHS.2008.53,
author = {Sartain, P. and Hopkins, A. B. T. and McDonald-Mair, K. D. and Howells, W. G. J.},
title = {A Framework for Self-Diagnosis and Condition Monitoring for Embedded Systems Using a SOM-Based Classifier},
year = {2008},
isbn = {9780769531663},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/AHS.2008.53},
doi = {10.1109/AHS.2008.53},
abstract = {This paper presents a system level framework for System-on-Chip (SoC) based embedded devices that may include adaptive and reconfigurable elements. Current development support and debugging solutions are highly dependant on off-line post-mortem style inspection, and even those that utilise tracing for real-time and schedule-critical systems rely on external development tools and environments. This new framework introduces an AI-lead infrastructure that has the potential to reduce much of the development effort while complementing existing debugging circuits. Specifically this paper investigates how to use a Kohonen self-organising map (SOM) as a classifier, and shows a preliminary investigation into how to determine the quality of a map after training. This classifier is a first step in diagnosing failure, degradation and anomalies (i.e. provides condition monitoring) in an embedded system from a system level point of view, and in the larger task of self-diagnosis of an embedded system.},
booktitle = {Proceedings of the 2008 NASA/ESA Conference on Adaptive Hardware and Systems},
pages = {417–423},
numpages = {7},
keywords = {System-on-Chip, Kohonen self-organising map, system level debugging, SOM classifier, novelty filter},
series = {AHS '08}
}

@techreport{10.5555/897034,
author = {Wolf, Alexander L.},
title = {An Overview of Arcadia},
year = {1986},
publisher = {University of Massachusetts},
address = {USA},
abstract = {ARCADIA IS A RESEARCH PROJECT AIMED AT THE DISCOVERY AND DEVELOPMENT OF BOTH ARCHITECTURAL PRINCIPLES AND SOFTWARE TOOLS FOR SOFTWARE DEVELOPMENT ENVIRONMENTS. THE PRINCIPLES ARE INTENDED TO ENABLE THE CONSTRUCTION OF ENVIRONMENTS THAT ARE BOTH INTEGRATED AND EXTENSIBLE, WHILE THE TOOLS ARE INTENDED TO SUPPORT THE DESCRIPTION AND ANALYSIS OF SOFTWARE SYSTEMS THROUGHOUT THEIR LIFETIMES, FROM INITIAL CONCEPTION THROUGH MAINTENANCE. A MAJOR GOAL OF THE ARCADIA PROJECT IS TO CREATE A RESEARCH PLATFORM THAT CAN BE USED TO BUILD ARCADIA-1, A FIRST PROTOTYPE OF THE SORT OF NEXT-GENERA- TION ENVIRONMENT THAT WE BELIEVE CAN BETTER SUPPORT THE DEVELOPMENT AND MAINTENANCE OF LARGE, COMPLEX SOFTWARE SYSTEMS. THE ARCADIA PROJECT IS ORGANIZED AS A CONSORTIUM OF ACADEMIC AND INDUS- TRIAL RESEARCHERS. THE PRINCIPAL MEMBERS ARE FROM THE UNIVERSITY OF CALI- FORNIA AT IRVINE, THE UNIVERSITY OF COLORADO AT BOULDER, THE UNIVERSITY OF MASSACHUSETTS AT AMHERST, STANFORD UNIVERSITY, INCREMENTAL SYSTEMS CORPOR- ATION, TRW, AND THE AEROSPACE CORPORATION. IN ADDITION TO THEIR RESEARCH CONTRIBUTION, THE INDUSTRIAL MEMBERS ARE EXPECTED TO ACT AS CONDUITS OF THE TECHNOLOGY THAT EMERGES FROM THE ARCADIA PROJECT. THIS PAPER PROVIDES A BRIEF OVERVIEW OF THE RESEARCH DIRECTIONS BEING EXPLORED BY MEMBERS OF THE ARCADIA PROJECT IN THE AREAS OF ARCHITECTURAL PRINCIPLES AND SOFTWARE TOOLS, AND DESCRIBES OUR PLANS FOR ARCADIA-1.}
}

@inproceedings{10.5555/3241067.3241072,
author = {Ingegneri, David and Timoteo, Dominic and Hyle, Patrick and Parraga, Fidel and Reyes, Alex},
title = {A cybersecurity test and evaluation facility for the next generation air transportation system (nextgen)},
year = {2016},
publisher = {USENIX Association},
address = {USA},
abstract = {The Federal Aviation Administration (FAA) is developing the Cybersecurity Test and Evaluation Facility (CyTF) for the FAA Air Transportation System as it transitions to the Next Generation Air Transportation System (NextGen). This paper describes the goals, capabilities, architecture, current implementation, initial experience, lessons learned and future implementation of the CyTF. The FAA Air Transportation System is an attractive cybersecurity threat target and the FAA must proactively and continually adjust its cybersecurity capabilities to match the changing cybersecurity threat landscape. The CyTF is providing an adaptable cybersecurity research and development environment independent of the operational system to satisfy research, test and evaluation needs. The CyTF has a number of complex requirements: testing cybersecurity tools and technologies prior to their integration into the Air Transportation System, the evaluation of individual FAA Air Transportation subsystems security, security of end-to-end services involving multiple subsystems, procedures to respond and recover from a cybersecurity event and cybersecurity training of the FAA workforce. One of the major lessons learned, described in the paper, has been how to address some aspects of the CyTF's complex requirements},
booktitle = {Proceedings of the 9th USENIX Conference on Cyber Security Experimentation and Test},
pages = {5},
numpages = {1},
location = {Austin, TX},
series = {CSET'16}
}

@inproceedings{10.1109/IROS40897.2019.8967798,
author = {Lin, Ni-Ching and Benjamin, Michael and Chen, Chi-Fang and Wang, Hsueh-Cheng and Hsiao, Yu-Chieh and Huang, Yi-Wei and Hung, Ching-Tung and Chuang, Tzu-Kuan and Chen, Pin-Wei and Huang, Jui-Te and Hsu, Chao-Chun and Censi, Andrea},
title = {Duckiepond: An Open Education and Research Environment for a Fleet of Autonomous Maritime Vehicles},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS40897.2019.8967798},
doi = {10.1109/IROS40897.2019.8967798},
abstract = {Duckiepond is an education and research development environment that includes software systems, educational materials, and of a fleet of autonomous surface vehicles Duckieboat. Duckieboats are designed to be easily reproducible with parts from a 3D printer and other commercially available parts, with flexible software that leverages several open source packages. The Duckiepond environment is modeled after Duckietown and AI Driving Olympics environments: Duckieboats rely only on one monocular camera, IMU, and GPS, and perform all ML processing using onboard embedded computers. Duckiepond coordinates commonly used middlewares (ROS and MOOS) and containerized software packages in Docker, making it easy to deploy. The combination of learning-based methods together with classic methods enables important maritime missions: track and trail, navigation, and coordinate among Duckieboats to avoid collisions. Duckieboats have been operating in a man-made lake, reservoir and river environments. All software, hardware, and educational materials are openly available (https://robotx-nctu.github.io/duckiepond), with the goal of supporting research and education communities across related domains.},
booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {7219–7226},
numpages = {8},
location = {Macau, China}
}

@article{10.1155/2022/3414935,
author = {Feng, Peilu and Wu, Qi and Zhu, Fusheng},
title = {Digital Teaching Management System Based on Deep Learning of Internet of Things},
year = {2022},
issue_date = {2022},
publisher = {IOS Press},
address = {NLD},
volume = {2022},
issn = {1574-017X},
url = {https://doi.org/10.1155/2022/3414935},
doi = {10.1155/2022/3414935},
abstract = {In order to solve a series of problems similar to the repetitive construction of resources and low degree of resource sharing in cruciform teaching, this paper studies the digital disarming management. Based on the in-depth analysis of the actual needs of the digital teaching resource service system and the key problems in the system, taking the resources and business process as the starting point, based on Java Web related technology, combined with the related processes of resources and business processing, this paper designs the digital teaching resource system of colleges and universities. The system has the functions of resource digitization, process management, and interaction with other platforms. The system database platform and operation platform are built; the development environment is configured; and the user login service function, data conversion service function, process management service function, and system management service function are completed. The function is tested by unit test, and the performance is tested and analyzed according to user requirements and design objectives. The system adopts Java web development technology, uses S2SH framework as the development basis, and takes Oracle database as the data storage platform through Tomcat6.0 web server for program publishing. Through system testing and analysis, the software can operate normally and achieve the expected functions and can be put into use.},
journal = {Mob. Inf. Syst.},
month = {jan},
numpages = {11}
}

@inproceedings{10.1145/3524383.3533247,
author = {Zhao, Huimin},
title = {Design of English Teaching Interactive System Based on Artificial Intelligence},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3533247},
doi = {10.1145/3524383.3533247},
abstract = {By exploring the characteristic of interaction of network teaching, we proposed to develop an English teaching interaction system based on artificial intelligence, which can solve the problems in current English teaching class. We analyzed the functional requirements of the system corresponding to different user. By buiding Ubuntu operating system as the computing platform and exploring the object-oriented dynamic teaching environment provided by artificial intelligence platform, we build the LAMP development environment with MySQL database and PHP script language. We constructed a three-tier separated architecture system by using B/S mode, and refined the design and implementation of each module function through the secondary development of platform module plug-ins. To exhibit the advantages of the artificial intelligence system, we take college English teaching as an example to illustrate its functions. The functions of curriculum creation, teaching resources and activity design can be realized in the system, which verifies the effectiveness of the artificial intelligence system in interactive teaching and learning.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {451–454},
numpages = {4},
keywords = {LAMP development environment, English teaching, B/S architecture, Artificial intelligence},
location = {Shanghai, China},
series = {ICBDE '22}
}

@article{10.5555/614651.614726,
author = {M\"{u}hlh\"{a}user, Max and Gecsei, Jan},
title = {Services, Frameworks, and Paradigms for Distributed Multimedia Applications},
year = {1996},
issue_date = {September 1996},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {3},
number = {3},
issn = {1070-986X},
abstract = {The development of distributed multimedia applications is supported by an increasing number of services. While such services pave the way toward sophisticated multimedia support even in distributed systems, using them still makes the task of developers quite tedious. This is because several inconsistent services have to be interfaced in order to reflect different aspects. As a way to alleviate this problem, we make the case for an encompassing framework in which all services would be offered under a unifying paradigm. First we give an overview of existing multimedia services with a focus on distribution, extracting the requirements imposed on multimedia extensions to general frameworks as a set of so-called abstractions. Known development environments for distributed applications are obvious candidates for such encompassing frameworks. We review these based on four popular paradigms: client-server/remote procedure call, object-orientation, hypermedia, and open documents. We also investigate possible multimedia extensions and discuss the "expressive power" of the paradigms. In conclusion, we propose steps towards an encompassing framework based on a hybrid object/hypermedia paradigm. Readers may contact M\"{u}hlh\"{a}user at the University of Linz, Altenbergerstrasse 69, A-4040 Linz, Austria, e-mail: max@tk.uni-linz.ac.at},
journal = {IEEE MultiMedia},
month = {sep},
pages = {48–61},
numpages = {14}
}

@inproceedings{10.1145/3407982.3408003,
author = {Petkov, Emiliyan and Angelov, Vladislav},
title = {Virtual Reality Training System for Specialists Who Operate on High-Voltage Switchgears in an Oil Plant in Russia},
year = {2020},
isbn = {9781450377683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407982.3408003},
doi = {10.1145/3407982.3408003},
abstract = {Operating on high voltage switchgears in oil plants in Russia as well as anywhere else in the world requires well-trained operative personnel. To improve the training of the workers in such plants computer-based logical simulators are developed usually. However, they are not sufficient enough in training because the trainees learn theoretically and not practically. Training specialists from different areas of industry using virtual reality environments where participants learn practically is up-to-date topic and a sphere for top-of-the-art research and development. This article examines the creation of a virtual reality training system for personnel from oil plants and represents the requirements for the training environment, the model of the system and the selection of a specific virtual reality technology and development environment. The definitions of the problem, the research and the development of the system have been entrusted to the authors of this paper by CS Construction Solutions Ltd., UK.},
booktitle = {Proceedings of the 21st International Conference on Computer Systems and Technologies},
pages = {266–269},
numpages = {4},
keywords = {training, switchgears, plant, oil, Virtual reality, Industry 4.0},
location = {Ruse, Bulgaria},
series = {CompSysTech '20}
}

@inproceedings{10.5555/1253522.1253586,
author = {Gustafson, D. and Hankley, B. and Wallentine, V.},
title = {The Master of Software Engineering degree: an integrative engineering discipline},
year = {1995},
isbn = {0780330226},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software based products are an ever increasing portion of industrial output. However few engineers who produce software have had formal training in the specification, design, implementation, documentation, and maintenance of large software systems. In addition, few have had training in software engineering management and development tools and processes. KSU has implemented a Master of Software Engineering degree program which provides an opportunity for new engineering graduates and practicing engineers to learn state of the art software engineering analysis and processes. The purpose is to provide students a foundation in the software engineering "life cycle", software measurement, software management, software specification, and software validation and verification. In addition to the core courses on software methodology, students are required to take courses in an application area which is reliant on software development. This involves faculty from many engineering and science disciplines who teach these courses and supervise the development of the student's software "portfolio" in the designated engineering or science area. The goal is to integrate the software engineering process into traditional engineering processes.},
booktitle = {Proceedings of the Frontiers in Education Conference on 1995. Proceedings., 1995 Vol 1. - Volume 01},
pages = {2b1.5–2b1.8vol.1},
series = {FIE '95}
}

@inproceedings{10.1145/1275604.1275610,
author = {Franklin, Diana and Seng, John},
title = {Experiences with the Blackfin architecture for embedded systems education},
year = {2005},
isbn = {9781450347341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1275604.1275610},
doi = {10.1145/1275604.1275610},
abstract = {In the course of a major curriculum change at California Polytechnic State University, the embedded processing course was redesigned. During this process, the course had the opportunity to purchase new hardware. Analog Device's Black-fin processor was chosen based mostly on cost, but also on performance, development environment, and documentation.We first present our goals in the class. We then give an overview of the Blackfin architecture and how the Blackfin fits in with many of our goals. We then present the implementation of an expansion board developed to interface with Blackfin's EZ-KIT Lite board.We present our experiences with this setup in the hopes that others who might be thinking of a similar curricular change can learn from our successes and failures. We outline the strengths and weaknesses of the Blackfin architecture as an educational platform, followed by a discussion of our experiences and a presentation of the support materials we developed to accompany the course, including lecture material and laboratories. Finally, we discuss our future directions for our uses with the board.},
booktitle = {Proceedings of the 2005 Workshop on Computer Architecture Education: Held in Conjunction with the 32nd International Symposium on Computer Architecture},
pages = {3–es},
location = {Madison, Wisconsin},
series = {WCAE '05}
}

@inproceedings{10.1145/2591062.2591165,
author = {Hudepohl, John and Dubey, Alpana and Moisy, Sylvie and Thompson, Jessica and Niederer, Hans-Martin},
title = {Deploying an online software engineering education program in a globally distributed organization},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591165},
doi = {10.1145/2591062.2591165},
abstract = {A well-trained software engineering workforce is a key to success in a highly competitive environment. Changing tools and technologies, along with a rapidly changing development environment, make it incumbent on organizations to invest in training. In this paper, we describe our experience in deploying an online training program in a globally distributed organization. We write about the reasons behind ABB’s Software Development Improvement Program (SDIP), the requirements we established upfront, the people, processes and technologies we used, the promotion of SDIP, and metrics for measuring success. Finally, we share and describe results and lessons learned that could be applied to many organizations with similar issues. The goal of this paper is to provide a set of replicable best practices for initiating a software training program in a multi-national organization. The first SDIP online course was offered in June 2012. Since then, we have had more than 10,000 enrollments from employees in 54 countries. Today, our training library contains 89 e-learning, 17 webinar, video and virtual lab courses, and we have delivered more than 180 hosted webinars. Following each class, we ask students to evaluate the class. Ninety-eight percent are satisfied with the classes.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {301–310},
numpages = {10},
keywords = {webinars, web-based training platform, virtual labs, e-learnings, Training in distributed organizations},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.5555/1323159.1323205,
author = {Derrick, E. Joseph and Htay, Maung and Vaccare, Carmel},
title = {EVA (extended video application): developing tablet PC software for multi-disciplinary mobile e-learning},
year = {2007},
publisher = {ACTA Press},
address = {USA},
abstract = {The use of video within clinical courses at Radford University (RU) is currently limited by labor-intensive, tape-based equipment for organizing and accessing specific scenes and events. Our faculty has successfully applied Tablet PCs and Microsoft OneNote® in courses to capture and analyze instructional sessions, but these existing systems have limited capabilities. For example, OneNote currently only supports synchronous live feeds of web cameras with moderate video fidelity. However, prerecorded asynchronous sessions fed from DVD players or saved digital movie files of sessions at remote locations are all primary sources of observation data. The overall goal of this project is to design, implement, and evaluate a Tablet PC-based software application called EVA (Extended Video Application) to overcome these limitations and substantially improve the effectiveness of specific clinical learning experiences in education, counseling, and social work courses. This paper gives the background, motivation, and an overview of the project, reports on project status, describes the software development environment, and discusses the technical difficulties and issues surrounding the design and implementation components. The paper also covers future work (which includes a rigorous evaluation component), expected outcomes, and conclusions.},
booktitle = {Proceedings of the Sixth Conference on IASTED International Conference Web-Based Education - Volume 2},
pages = {258–263},
numpages = {6},
keywords = {tablet, software development, mobile e-learning, assessment},
location = {Chamonix, France},
series = {WBED'07}
}

@inproceedings{10.5555/1566971.1566972,
title = {Front Matter},
year = {2007},
isbn = {9781586037949},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for the new economy and science. It creates new markets and new directions for a more reliable, flexible, and robust society. It empowers the exploration of our world in ever more depth. However, software often falls short of our expectations. Current software methodologies, tools, and techniques remain expensive and not yet reliable enough for a highly changeable and evolutionary market. Many approaches have been proven only as case-by-case oriented methods.This bookas part of SOMET seriescontributes on new trendstheories in the direction in which we believe software science and engineering may develop to transform the role of software and science integration in tomorrow's global information society.This book is an attempt to capture the essence on a new state of art in software science and its supporting technology. The book also aims at identifying the challenges such a technology has to master. It contains highly extensively reviewed papers lectured at the Sixth International Conference on New Trends in Software Methodology Tools, and Techniques, V, (SoMeT_07) held in Rome (CNR), Italy, from 7 to 9 November 2007, (http://www.somet.soft.iwate-pu.ac.jp/somet_07). This conference brought together researchers and practitioners to share their original research results and practical development experiences in software science, and its related new challenging technology.One of the important issues addressed by this book is software development security tools and techniques. Another example we challenge in this conference is intelligent software design from the human aspect and the technology aspect. This book and the series it continues will also elaborate on such new trends and related academic research studies and development.A major goal was to gather scholars from the international research community to discuss and share research experiences on new software methodologies, and formal techniques. The book also investigates other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. These are essential for developing a variety of information systems research projects and to assess the practical impact on real-world software problems.For an outline of the past series of related events that contributed to this publication, SoMeT_02 was held on October 3--5, 2002, in Sorbonne, Paris, France.SoMeT_03 was held in Stockholm, Sweden, SoMeT_04 in Leipzig, Germany, SoMeT_05 in Tokyo, Japan, SoMeT_06 in Quebec, Canada, and most recently SoMeT_07 held in Rome, Italy. These events also initiate a future event to be organized in October 2008, (http://www.somet.soft.iwate-pu.ac.jp/somet_08/).This book provides an opportunity for exchanging ideas and experiences in the field of software technology, opening up new avenues for software development, methodologies, tools, and techniques, especially, software security and program coding diagnosis and related software maintenance techniques aspects. Also, we have emphasized human centric software methodologies, end-user development techniques, and human emotional reasoning for best performance harmony between the design tool and the user.Issues discussed are research practices, techniques and methodologies proposing and reporting solutions needed for global world business. We believe that this book creates an opportunity for us in the software science community to think about where we are today and where we are going.The book is a collection of 33 carefully reviewed best-selected papers by the reviewing committee.The areas covered are:• Software engineering aspects on software security, programs diagnosis and maintenance• Static and dynamic analysis on Lyee-oriented software performance model• Software security aspects on Java mobile code, and networking• Practical artefact on software security, software validation and diagnosis• Software optimization and formal methods• Requirement engineering and requirement elicitation• Software methodologies and Lyee oriented software techniques• Automatic software generation, reuse, and legacy systems• Software quality and process assessment• Intelligent software systems and evolution• End-user requirement engineering and programming environment• Ontology and philosophical aspects on software engineering• Business software models and other kinds of software application models, based on Lyee theoryAll papers published in this book have been carefully reviewed and selected by the SOMET international reviewing committee. Each paper has been reviewed by between three and six reviewers and has been revised based on the review reports. The papers were reviewed on the basis of technical soundness, relevance, originality, significance, and clarity.This book outcome is also a collective effort from many industrial partners and colleagues from around the world. We also gratefully thank Iwate Prefectural University, especially its President Prof. Makoto Taniguchi, CNR Rome, Italy, Catena Co., SANGIKYO Co., ARISES and others for their overwhelming support of this work. We especially are thankful to the reviewing committee and others who participated in the hard effective review of all submitted papers and thanks also for the hot discussions we have had at the review evaluation meetings which selected the contributions in this book.This outcome is another milestone in mastering new challenges in software and its new promising technology, within SoMeT's consecutive events. It also gives the reader new insights, inspiration and concrete material to elaborate and study this new technology.Last but not least, we would like to thank and acknowledge the Microsoft Conference Management Tool team for the support it has provided on the use of Microsoft CMT System as a conference-supporting tool during all the phases of the SOMET transactions.The Editors},
booktitle = {Proceedings of the 2007 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Sixth SoMeT_07},
pages = {i–xiii}
}

@inproceedings{10.1145/1878537.1878588,
author = {Kuskuntla, Radha and Imsand, Eric S. and Hamilton, J. A. "Drew"},
title = {Enhanced expert field medical training simulations and their effect on the modern combat life saver training procedures},
year = {2010},
isbn = {9781450300698},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
url = {https://doi.org/10.1145/1878537.1878588},
doi = {10.1145/1878537.1878588},
abstract = {The increase in fidelity found in the latest generation of video game platforms has garnered interest from many different arenas. Up to now, the primary objective of these games has been to simulate the real world within the gaming environment. Thus it was only natural that some would begin using video games in place of specially developed simulators as a means of providing training. Some argue that the advancement of training programs based on video game platforms has not advanced as much as hoped for. The prospect of utilizing video game platforms as the basis for training systems remains a tempting one, though, in light of today's financial climate.To attain these goals, advanced game development tools should enhance already existing functionality within existing game engines, aiming for high quality sound and visual effects, realistic dynamics, appropriate responses and well maintained timing. In this effort we propose modifications to already existing game functionalities of a training module to bring about effective and dynamic simulations involving the use of unused functionalities of the existing game engine. We emphasize how the promoted changes mold the existing training module effectively to create highly realistic and interactive training environments for the Combat Life Savers of the US Army.},
booktitle = {Proceedings of the 2010 Spring Simulation Multiconference},
articleno = {49},
numpages = {6},
keywords = {training, combat medic, Unreal Engine 3, America's Army},
location = {Orlando, Florida},
series = {SpringSim '10}
}

@inproceedings{10.5555/1860875.1860876,
title = {Front Matter},
year = {2010},
isbn = {9781607506287},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for science and the new economy. It creates new markets and new directions for a more reliable, flexible and robust society. It empowers the exploration of our world in ever more depth. However, software often falls short of our expectations. Current software methodologies, tools, and techniques remain expensive and are not yet sufficiently reliable for a constantly changing and evolving market, and many promising approaches have proved to be no more than case-by-case oriented methods.This book explores new trends and theories which illuminate the direction of developments in this field, developments which we believe will lead to a transformation of the role of software and science integration in tomorrow's global information society. By discussing issues ranging from research practices and techniques and methodologies, to proposing and reporting solutions needed for global world business, it offers an opportunity for the software science community to think about where we are today and where we are going.The book aims to capture the essence of a new state of the art in software science and its supporting technology, and to identify the challenges that such a technology will have to master. It contains extensively reviewed papers presented at the ninth International Conference on New Trends in software Methodology Tools, and Techniques, (SoMeT_10) held in Yokohama, Japan, with the collaboration of SANGIKYO Co., from September 29th to October 1st 2009. (http://www.somet.somet.iwate-pu.ac.jp/somet_10/).This conference brought together researchers and practitioners to share their original research results and practical development experience in software science and related new technologies.This volume participates in the conference and the SoMeT series Previous related events that contributed to this publication are: SoMeT_02 (the Sorbonne, Paris, 2002); SoMeT_03 (Stockholm, Sweden, 2003); SoMeT_04 (Leipzig, Germany, 2004); SoMeT_05 (Tokyo, Japan, 2005); SoMeT_06 (Quebec, Canada, 2006); SoMeT_07 (Rome, Italy, 2007); SoMeT_08 (Sharjah, UAE, 2008); SoMeT_09 (Prague, Czech Republic, 2009) and SoMeT_10 (Yokohama, Japan, 2010). of which it forms a part, by providing an opportunity for exchanging ideas and experiences in the field of software technology; opening up new avenues for software development, methodologies, tools, and techniques, especially with regard to software security and programme coding diagnosis and aspects of related software maintenance techniques. The emphasis has been placed on human-centric software methodologies, end-user development techniques, and emotional reasoning, for an optimally harmonised performance between the design tool and the user.This book, and the series it forms part of, will continue to contribute to and elaborate on new trends and related academic research studies and developments in SoMeT_2011 in Germany.A major goal of this work was to assemble the work of scholars from the international research community to discuss and share research experiences of new software methodologies and techniques. One of the important issues addressed is the handling of cognitive issues in software development to adapt it to the user's mental state. Tools and techniques related to this aspect form part of the contribution to this book. Another subject raised at the conference was intelligent software design in software security and programme conversions. The book also investigates other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. This is essential for a comprehensive overview of information systems and research projects, and to assess their practical impact on real-world software problems. This represents another milestone in mastering the new challenges of software and its promising technology, addressed by the SoMeT conferences, and provides the reader with new insights, inspiration and concrete material to further the study of this new technology.The book is a collection of 30 carefully refereed papers selected by the reviewing committee and covering:• Software engineering aspects of software security programmes, diagnosis and maintenance• Static and dynamic analysis of software performance models• Software security aspects and networking• Agile software and lean methods• Practical artefacts of software security, software validation and diagnosis• Software optimization and formal methods• Requirement engineering and requirement elicitation• Software methodologies and related techniques• Automatic software generation, re-coding and legacy systems• Software quality and process assessment• Intelligent software systems design and evolution• Artificial Intelligence Techniques on Software Engineering, and Requirement Engineering• End-user requirement engineering, programming environment for Web applications• Ontology, cognitive models and philosophical aspects on software design• Business oriented software application models• Model Driven Development (DVD), code centric to model centric software engineering• Cognitive Software and human behavioural analysis in software design.All papers published in this book have been carefully reviewed, on the basis of technical soundness, relevance, originality, significance, and clarity, by up to four reviewers. They were then revised on the basis of the review reports before being selected by the SoMeT_10 international reviewing committee.This book is the result of a collective effort from many industrial partners and colleagues throughout the world. I would like acknowledge my gratitude to JSPS (Japan Society for the Promotion of Science) and SANGIKYO Co., for the sponsorship and support. Also, my thanks go to Iwate Prefectural University, ARISES, and all the others who have contributed their invaluable support to this work. Most especially, I thank the reviewing committee and all those who participated in the rigorous reviewing process and the lively discussion and evaluation meetings which led to the selected papers which appear in this book. Last and not least, I would also like to thank the Microsoft Conference Management Tool team for their expert guidance on the use of the Microsoft CMT System as a conference-support tool during all the phases of SoMeT_10.The editor},
booktitle = {Proceedings of the 2010 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the 9th SoMeT_10},
pages = {i–xiii}
}

@inproceedings{10.5555/555920.830376,
author = {Thorn, LucinCia Heloisa and Scheidt, Neiva and Molz, Kurt Werner},
title = {A First Report On a New Technique to Model Workflow Systems},
year = {2000},
isbn = {0769509037},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Workflow can be understood as the automation of a business process in the total or partial meaning, when documents or activities are passed from one participant to another. The goal is that actions start according to a set of rules and behaviors. The objective of this article isto introduce the workflow technology and to present a new technique to model workflow systems. This technique is defined as an improvement of the Triggers Model, validated by Stef Joosten. It's formed by a method to specify activities and by a set of graphic elementsinherent to the main concepts of workflow. As validation, this technique is used to model the Case Study to Approve Research Projects at the University at Santa Cruz do SUB. This modeling applies to the implementation of a prototype on Lotus Notes 4.6, The purpose of the improvement is to introduce aspects of the specification of the project that were not contemplated, through a sequence of stages aimed at the creation of a more efficient model, improving the implementation of a workflow system regardless of the development tool usedand making it easier.},
booktitle = {Proceedings of the International Conference on Software Methods and Tools (SMT'00)},
pages = {223},
keywords = {modeling, concepts, business process, Workflow},
series = {SMT '00}
}

@inproceedings{10.1145/3442536.3442546,
author = {Weng, Ting-Sheng and Li, Chien-Kuo and Hsu, Meng-Hui},
title = {Development of Robotic Quiz Games for Self-Regulated Learning of Primary School Children},
year = {2021},
isbn = {9781450388832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442536.3442546},
doi = {10.1145/3442536.3442546},
abstract = {The progressive development of information technology has provided multiple learning modes. The rich content and innovative applications available allow pupils to improve their skills through self-regulated learning (SRL), which has become an important education goal. Intelligent robots can be used in a wide range of applications, from programmed movements for learning activities, to the combination of artificial intelligence and sensor technology for human life and education. A robot's dynamic and interesting interface is more suitable for children's self-regulated learning. This study used a Zenbo robot as the development tool and Zenbo Scratch platform programming to develop an AI robot math quiz game for primary school students. Two elementary school math teachers, and a parent and a 5th grade primary school student were involved in the development of the game. This study used the parent's and student's continuous interaction with the robot to adjust the code and achieve the best human-computer interaction in robotic mathematics problem solving. Moreover, this study developed a companion robot for a math quiz game, which can be used for reviewing what has been learned in class. The robot can be used for self-regulated learning by young children to increase student learning outcome.},
booktitle = {Proceedings of the 2020 3rd Artificial Intelligence and Cloud Computing Conference},
pages = {58–62},
numpages = {5},
keywords = {Self-regulated learning, Robotic quiz games, Perceived usefulness, Perceived enjoyment, AI robot},
location = {Kyoto, Japan},
series = {AICCC '20}
}

@inproceedings{10.1007/978-3-031-20102-8_38,
author = {Li, Kangshun and Wang, Yi and Feng, Tian and Jalil, Hassan and Nie, Huabei},
title = {Morphology-Based Soft Label Smoothing Strategy for Fine-Grained Domain Adaptationming},
year = {2023},
isbn = {978-3-031-20101-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-20102-8_38},
doi = {10.1007/978-3-031-20102-8_38},
abstract = {Semantic segmentation algorithms are the cornerstone of the autonomous driving algorithm. The implementation of these algorithms requires a lot of data. However, due to the complex and changeable production environment, the differences in image style, illumination and weather conditions not only significantly degrade the performance of network models under development environment data, but also cause costly data annotation. This paper proposes a dilation-corrosion soft label smoothing strategy. This strategy utilizes the existing self-supervised learning soft label generation strategy and overcomes the premise assumption defect of the fine-grained adversarial model on the target domain model. Our strategy evaluates its effectiveness on the domain adaptation task of GTA5 → Cityscapes. At the same time, we reduce the amount of source domain data by half and the number of domain classification training iterations by half, the resulting in the comprehensive evaluation index is 1.5% higher, and the individual category is 10 percentage points higher. Our strategy shortens the time and improves the efficiency of segmentation.},
booktitle = {Machine Learning for Cyber Security: 4th International Conference, ML4CS 2022, Guangzhou, China, December 2–4, 2022, Proceedings, Part III},
pages = {499–506},
numpages = {8},
keywords = {Soft label smoothing, Self-supervised learning, Domain adaptive learning, Semantic segmentation},
location = {Guangzhou, China}
}

@article{10.1002/aaai.12145,
author = {Ju, Peizhong and Li, Chengzhang and Liang, Yingbin and Shroff, Ness},
title = {AI‐EDGE: An NSF AI institute for future edge networks and distributed intelligence},
year = {2024},
issue_date = {Spring 2024},
publisher = {American Association for Artificial Intelligence},
address = {USA},
volume = {45},
number = {1},
issn = {0738-4602},
url = {https://doi.org/10.1002/aaai.12145},
doi = {10.1002/aaai.12145},
abstract = {This paper highlights the overall endeavors of the NSF AI Institute for Future Edge Networks and Distributed Intelligence (AI‐EDGE) to create a research, education, knowledge transfer, and workforce development environment for developing technological leadership in next‐generation edge networks (6G and beyond) and artificial intelligence (AI). The research objectives of AI‐EDGE are twofold: “AI for Networks” and “Networks for AI.” The former develops new foundational AI techniques to revolutionize technologies for next‐generation edge networks, while the latter develops advanced networking techniques to enhance distributed and interconnected AI capabilities at edge devices. These research investigations are conducted across eight symbiotic thrust areas that work together to address the main challenges towards those goals. Such a synergistic approach ensures a virtuous research cycle so that advances in one area will accelerate advances in the other, thereby paving the way for a new generation of networks that are not only intelligent but also efficient, secure, self‐healing, and capable of solving large‐scale distributed AI challenges. This paper also outlines the institute's endeavors in education and workforce development, as well as broadening participation and enforcing&nbsp;collaboration.},
journal = {AI Mag.},
month = {feb},
pages = {29–34},
numpages = {6}
}

@inproceedings{10.5555/1563296.1563297,
title = {Front Matter},
year = {2005},
isbn = {1586035568},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for the new economy and science. It creates new markets and new directions for a more reliable, flexible, and robust society. It empowers the exploration of our world in ever more depth.However, software often falls short of our expectations. Current software methodologies, tools, and techniques remain expensive and not yet reliable for a highly changeable and evolutionary market. Many approaches have been proven only as case-by-case oriented methods.This book presents a number of new trends and theories in the direction in which we believe software science and engineering may develop to transform the role of software and science in tomorrow's information society.This book is an attempt to capture the essence of a new state of art in software science and its supporting technology. The book also aims at identifying the challenges such a technology has to master. It contains papers accepted at the fourth International Conference on New Trends in Software Methodologies Tools and Techniques, IV, (SoMeT_05) held in Tokyo, Japan, from 28th to 30th September 2005, (http://www.somet.soft.iwate-pu.ac.jp/somet_05). This workshop brought together researchers and practitioners to share their original research results and practical development experiences in software science, and its related new challenging technology.One example we challenge in this conference is Lyee methodology --a newly emerged Japanese software methodology that has been patented in several countries in Europe, Asia, and America, but which is still at an early stage of emerging as a new software style. This conference and the series it continues will also contribute to elaborate on such new trends and related academic research studies and development.A major goal of this international conference was to gather scholars from the international research community to discuss and share research experiences on new software methodologies, and formal techniques. The conference also investigated other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. These are essential for developing a variety of information systems research projects and to assess the practical impact on real-world software problems.SoMeT_02 was held on October 3--5, 2002, in Sorbonne, Paris, France, SoMeT_03 in Stockholm, Sweden, SoMeT_04 in Leipzig, Germany and the conference that these proceedings cover, SoMeT_05, was held in Tokyo, Japan. These events initiate a forthcoming series that will include the 5th conference, SoMeT_W06, to be organized in Quebec, Canada on September 2006 (http://www.somet.soft.iwate-pu.ac.jp/somet_06/).This book is also in part a means for presenting few selected parts of the results of the Lyee International research project (http://www.lyee-project.soft.iwate-pu.ac.jp), which aims at the exploration and development of novel software engineering methods and software generation tools based on the Lyee framework. This project was sponsored by Catena and other major Japanese enterprises in the area of software methodologies and technologies.This book participates to provide an opportunity for exchanging ideas and experiences in the field of software technology, opening up new avenues for software development, methodologies, tools, and techniques.The Lyee framework for example, captures the essence of the innovations, controversies, challenges and possible solutions of the software industry. This world wide patented software approach was born and enriched from experience, and it is time, and again through SoMeT_W05 to try to let it stimulate the academic research on software engineering, attempting to close the gap that has so far existed between theory and practice. We believe that this book creates an opportunity for us in the software science community to think about where we are today and where we are going.The book is a collection of 26 carefully reviewed best-selected papers by the reviewing committee.The areas covered in the book are: --Requirement engineering and requirement elicitation, and its tools; --Software methodologies and tools for robust, reliable, non-fragile software design; --Lyee oriented software techniques, and its legacy systems; --Automatic software generation versus reuse, and legacy systems, source code analysis and manipulation; --Software quality and process assessment; --Intelligent software systems design, and software evolution techniques; --Software optimization and formal methods; --Static and dynamic analysis on software performance model, and software maintenance; --End-user programming environment, User-centered Adoption-Centric Reengineering techniques; --Ontology, cognitive models and philosophical aspects of software design; --Software design through interaction, and precognitive software techniques for interactive software entertainment applications; --Business oriented software application models; --Software Engineering models, and formal techniques for software representation, software testing and validation; --Aspect oriented programming; --Other software engineering disciplines.All papers published in this book are carefully reviewed and selected by SOMET international program committee. Each paper has been reviewed by three or four reviewers and has been revised based on the review reports. The papers were reviewed on the basis of technical soundness, relevance, originality, significance, and clarity. The acceptance rate for the papers listed in this book is 48% in this year SoMeT.This book was made possible by the collective efforts of all the Lyee International project collaborators and other people and supporters. We gratefully thank Iwate Prefectural University, University of Laval, Catena Co., ISD, Ltd, SANGIKYO co., and others for their overwhelming support. We are especially thankful to the program committee and others who participated in the review of all submitted papers and thanks also for the hot discussion we have had on the PC meetings to select the papers listed in this book.This book is another milestone in mastering new challenges on software and its new promising technology, within the SoMeT framework and others. Also, it gives the reader new insights, inspiration and concrete material to elaborate and study this new technology.We would also like to thank and acknowledge the support of the University of Leipzig, Telematik and e-Business group for allowing us to use the Paperdyne System as a conference-supporting tool during all the phases on this transaction.The Editors},
booktitle = {Proceedings of the 2005 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Fourth SoMeT_W05},
pages = {i–xiii}
}

@article{10.5555/2010532.2010538,
author = {Economou, Daphne and Gavalas, Damianos and Kenteris, Michael and Micha, Katy},
title = {Multimedia applications for handheld devices: analysis of requirements for development platforms and application authoring tools},
year = {2007},
issue_date = {March 2007},
publisher = {Rinton Press, Incorporated},
address = {Paramus, NJ},
volume = {3},
number = {1},
issn = {1550-4646},
abstract = {This paper explores requirements application authoring tools should satisfy for the development of cultural applications tailored for deployment on Personal Digital Assistants (PDAs) and mobile phones. The paper reviews the use of mobile technologies in the context of cultural organizations and tourism. It identifies and evaluates the development and design facilities provided by state-of-the-art multimedia application development tools for PDAs and mobile phones: Macromedia Flash Lite, Navipocket and Java 2 Micro Edition. It describes the way these tools have been used in the implementation phase of two projects that have been developed at the Cultural Heritage Management Lab (CHMLab), at the Department of Cultural Technology and Communication, University of the Aegean. These projects focus on the use of PDAs and mobile phones for providing cultural and tourist information, keeping the visitors' interest and attention, as well as promoting various cultural organizations' and tourist facilities. Based on these two case studies the paper extracts a set of PDA and mobile phone application requirements. The paper concludes with a set of suggestions related to the way application authoring tools should be exploited in order to gratify application and designer needs for developing operational and profitable cultural and tourist applications.},
journal = {J. Mob. Multimed.},
month = {mar},
pages = {65–87},
numpages = {23},
keywords = {requirements, development platforms, cultural and tourist multimedia applications, application authoring tools, Navipocket, J2ME}
}

@inproceedings{10.1109/ITSC.2019.8917398,
author = {Elmadawi, Khaled and Abdelrazek, Moemen and Elsobky, Mohamed and Eraqi, Hesham M. and Zahran, Mohamed},
title = {End-to-end sensor modeling for LiDAR Point Cloud},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC.2019.8917398},
doi = {10.1109/ITSC.2019.8917398},
abstract = {Advanced sensors are a key to enable self-driving cars technology. Laser scanner sensors (LiDAR, Light Detection And Ranging) became a fundamental choice due to its long-range and robustness to low light driving conditions. The problem of designing a control software for self-driving cars is a complex task to explicitly formulate in rule-based systems, thus recent approaches rely on machine learning that can learn those rules from data. The major problem with such approaches is that the amount of training data required for generalizing a machine learning model is big, and on the other hand LiDAR data annotation is very costly compared to other car sensors. An accurate LiDAR sensor model can cope with such problem. Moreover, its value goes beyond this because existing LiDAR development, validation, and evaluation platforms and processes are very costly, and virtual testing and development environments are still immature in terms of physical properties representation.In this work we propose a novel Deep Learning-based LiDAR sensor model. This method models the sensor echos, using a Deep Neural Network to model echo pulse widths learned from real data using Polar Grid Maps (PGM). We benchmark our model performance against comprehensive real sensor data and very promising results are achieved that sets a baseline for future works.},
booktitle = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
pages = {1619–1624},
numpages = {6},
location = {Auckland, New Zealand}
}

@article{10.1155/2021/8357488,
author = {Ma, Jing and Feng, Bo and Zhang, Yuanpeng},
title = {Integrated Design of Graduate Education Information System of Universities in Digital Campus Environment},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/8357488},
doi = {10.1155/2021/8357488},
abstract = {This study takes the digital campus construction planning of the high school as an example and determines the requirements of the postgraduate management information system under the digital campus environment through the analysis of the overall framework and technology of the digital campus. Combining the current situation of computer technology, network technology, and the actual situation of our university, the current mainstream B/S three-layer architecture is adopted, the web adopts the current popular Java Server Pages technology, and the struts framework connects to the Oracle backend database through the Java Database Connectivity interface to design the browser-side and server-side programs. The struts framework connects to the Oracle backend database through the Java Database Connectivity interface to design browser-side and server-side programs. The functional model and data flow model of the system were established through a detailed and effective analysis of the entire workflow of postgraduate students’ training management during their school years. Then, the system analysis, design, and drawing of the swim lane diagram and data business flow diagram were carried out. The system was designed in detail in terms of system architecture, development tools, functional modules, and database design, and the core module of training program making in postgraduate training management was highlighted as an example to discuss the principles and methods in the construction of departmental business systems and informatization under the digital campus environment, and a flexible and efficient postgraduate management information system was realized. It standardizes the construction of data standardization in universities; does a good job of standardizing and normalizing information; improves the accuracy, validity, and real-time production of data collection and the real and safe unified management of historical data; and provides scientific and reasonable data support for the leadership to make relevant decisions.},
journal = {Wirel. Commun. Mob. Comput.},
month = {jan},
numpages = {12}
}

@inproceedings{10.5555/2486788.2487077,
author = {Carver, Jeffrey C. and Epperly, Tom and Hochstein, Lorin and Maxville, Valerie and Pfahl, Dietmar and Sillito, Jonathan},
title = {5th international workshop on software engineering for computational science and engineering (SE-CSE 2013)},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Computational Science and Engineering (CSE) software supports a wide variety of domains including nuclear physics, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. The increases importance of CSE software motivates the need to identify and understand appropriate software engineering (SE) practices for CSE. Because of the uniqueness of the CSE domain, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the CSE development environment. SE community members must interact with CSE community members to understand this domain and to identify effective SE practices tailored to CSEs needs. This workshop facilitates that collaboration by bringing together members of the CSE and SE communities to share perspectives and present findings from research and practice relevant to CSE software and CSE SE education. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods for CSE software engineering.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1547–1548},
numpages = {2},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1109/TE.2005.850709,
author = {Del Corso, D. and Ovcin, E. and Morrone, G.},
title = {A teacher friendly environment to foster learner-centered customization in the development of interactive educational packages},
year = {2005},
issue_date = {November 2005},
publisher = {IEEE Press},
volume = {48},
number = {4},
issn = {0018-9359},
url = {https://doi.org/10.1109/TE.2005.850709},
doi = {10.1109/TE.2005.850709},
abstract = {A good teacher is able to customize the lesson to fit the requirements and needs of the learners he or she has in the classroom. This process becomes difficult and expensive in open and distance education, where customization means availability of similar contents, presented in diversified styles. A methodology and the tools to tackle the problem by using automated course compilation have been developed in the 3DE project (Design, Development, and Delivery-Electronic Environment for Educational Multimedia). The work on course customization showed the key role of the authoring process and the related problems. This paper summarizes the methodology and describes the development environment designed to assist authors in the creation of customized educational material. The environment seeks to help teachers/authors understand the relations among pedagogical and technical aspects and provides instructions, guidelines, and assistance for the development of learning-styles-aware material. The paper focuses on the author interface of the environment with details on the pedagogical framework, the authors' guide, the classification guide, and the metadata tool.},
journal = {IEEE Trans. on Educ.},
month = {nov},
pages = {574–579},
numpages = {6},
keywords = {reuse, personalized learning, learning styles (LSs), interoperability, educational effectiveness, automated course construction, Authoring guide}
}

@inproceedings{10.1007/11499053_40,
author = {Holcombe, Mike and Kalra, Bhavnidhi},
title = {Agile development environment for programming and testing (ADEPT) – eclipse makes project management extreme},
year = {2005},
isbn = {3540262776},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11499053_40},
doi = {10.1007/11499053_40},
abstract = {Genesys Solutions is a bespoke IT company, first of its kind, run by MSc and fourth year students of Department of Computer Science, University of Sheffield under the supervision of Prof. Mike Holcombe and Dr. Marian Gheorghe. Genesys follows the eXtreme Programming (XP) methodology for software development based on client requirements. The commitment towards XP and its ‘good software practices' can be considered as the greatest strength of Genesys.Agile Development Environment for Programming and Testing (ADEPT) is our contribution towards supporting the XP methodology by adopting the Eclipse platform along with its associated tools and frameworks within Genesys Solutions. It aimed to teach good software practices in Genesys to support XP by providing a software development life cycle management tool that will encompass the best practices of XP. It comprises of tools based on the principles of XP such as story cards, system metaphor, estimations, testing and quality assurance. ADEPT was the result of the IBM Eclipse Innovation 2004 awarded to the University of Sheffield. Also, based on the previous year's performance and more innovative ideas to implement more principles of XP we have been awarded another grant under the IBM Eclipse Innovation 2005 programme.},
booktitle = {Proceedings of the 6th International Conference on Extreme Programming and Agile Processes in Software Engineering},
pages = {255–258},
numpages = {4},
keywords = {software life cycle, project management, extreme programming, eclipse},
location = {Sheffield, UK},
series = {XP'05}
}

@inproceedings{10.1145/504450.504460,
author = {Marzullo, Keith and Ogg, Michael and Ricciardi, Aleta and Amoroso, Alessandro and Calkins, F. Andrew and Rothfus, Eric},
title = {NILE: wide-area computing for high energy physics},
year = {1996},
isbn = {9781450373395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/504450.504460},
doi = {10.1145/504450.504460},
abstract = {The CLEO project [2], centered at Cornell University, is a large-scale high energy physics project. The goals of the project arise from an esoteric question---why is there apparently so little antimatter in the universe?---and the computational problems that arise in trying to answer this question are quite challenging.To answer this question, the CESR storage ring at Cornell is used to generate a beam of electrons directed at an equally strong beam of positrons. These two beams meet inside a detector that is embedded in a magnetic field and is equipped with sensors. The collisions of electrons and positrons generate several secondary subatomic particles. Each collision is called an event and is sensed by detecting charged particles (via the ionization they produce in a drift chamber) and neutral particles (in the case of photons, via their deposition of energy in a crystal calorimeter), as well as by other specialized detector elements. Most events are ignored, but some are recorded in what is called raw data (typically 8Kbytes per event). Offline, a second program called pass2 computes, for each event, the physical properties of the particles, such as their momenta, masses, and charges. This compute-bound program produces a new set of records describing the events (now typically 20Kbytes per event). Finally, a third program reads these events, and produces a lossily-compressed version of only certain frequently-accessed fields, written in what is called roar format (typically 2Kbytes per event).The physicists analyze this data with programs that are, for the most part, embarrassingly parallel and I/O limited. Such programs typically compute a result based on a projection of a selection of a large number of events, where the result is insensitive to the order in which the events are processed. For example, a program may construct histograms, or compute statistics, or cull the raw data for physical inspection. The projection is either the complete pass2 record or (much more often) the smaller roar record, and the selection is done in an ad-hoc manner by the program itself.Other programs are run as well. For example, a Monte Carlo simulation of the experiment is also run (called monte carlo) in order to correct the data for detector acceptance and inefficiencies, as well as testing aspects of the model used to interpret the data. This program is compute bound. Another important example is called recompress. Roughly every two years, improvements in detector calibration and reconstruction algorithms make it worthwhile to recompute more accurate pass2 data (and hence, more accurate roar data) from all of the raw data. This program is compute-bound (it currently requires 24 200-MIP workstations running flat out for three months) and so must be carefully worked into the schedule so that it does not seriously impact the ongoing operations.Making this more concrete, the current experiment generates approximately 1 terabyte of event data a year. Only recent roar data can be kept on disk; all other data must reside on tape. The data processing demands consume approximately 12,000 SPECint92 cycles a year. Improvements in the performance of CESR and the sensitivity of the detector will cause both of these values to go up by a factor of ten in the next few years, which will correspondingly increase the storage and computational needs by a factor of ten.The CLEO project prides itself on being able to do big science on a tight budget, and so the programming environment that the CLEO project provides for researchers is innovative but somewhat primitive. Jobs that access the entire data set can take days to complete. To circumvent limited access to tape, the network, or compute resources close to the central disk, physicists often do preliminary selections and projections (called skims) to create private disk data sets of events for further local analysis. Limited resources usually exact a high human price for resource and job management and ironically, can sometimes lead to inefficiencies. Given the increase in data storage, data retrieval, and computational needs, it has become clear that the CLEO physicists require a better distributed environment in which to do their work.Hence, an NSF-funded National Challenge project was started with participants from both high energy physics, distributed computing, and data storage, in order to provide a better environment for the CLEO experiment. The goals of this project, called NILE [7], are:Finally, the CLEO necessity of building on a budget carries over to NILE. There are some more expensive resources, such as ATM switches and tape silos, that it will be necessary to use. However, as far as possible we are using commodity equipment, and free or inexpensive software whenever possible. For example, one of our principal development platforms is Pentium-based PCs, interconnected with 100 Mbps Ethernet, running Linux and the GNU suite of tools.},
booktitle = {Proceedings of the 7th Workshop on ACM SIGOPS European Workshop: Systems Support for Worldwide Applications},
pages = {49–54},
numpages = {6},
location = {Connemara, Ireland},
series = {EW 7}
}

@phdthesis{10.5555/143498,
author = {Ryder, Robert Michael},
title = {An object-oriented, knowledge-based system for cardiovascular rehabilitation},
year = {1992},
publisher = {Clemson University},
address = {USA},
abstract = {Cardiovascular rehabilitation is an interdisciplinary medical care program which includes patient specific exercise therapy, stress management, psychological counseling, aggressive dietary lipid management and extensive patient education to modify lifestyle factors which place the heart at risk. This research investigates the development of an intelligent clinical database/knowledge base system to assist in the management of the cardiovascular rehabilitation patient. Utilizing development tools for the database and knowledge base implementation, the prototype system maintains a rehabilitation clinical database and provides a rule-based system for automated therapy planning and risk monitoring.The extensive database contains over 400 variables in over 20 clinical specialties, fulfilling daily rehabilitation requirements in addition to supporting long-term, epidemiological studies. The database is designed utilizing an object-oriented paradigm; the software consists of new and modified classes added to a standard Smalltalk environment. The knowledge base retrieves parameter values from the database into a forward-chaining rule structure. Additional Smalltalk methods were added to accomplish an interface between the database and knowledge base.The system was evaluated by analyzing patient records provided by the rehabilitation clinic, completing risk analysis and reviewing the therapy plan for each patient. Risk categorizations and therapy plan deficiencies produced by the system agreed with the staff's recommendations.},
note = {UMI Order No. GAX92-33366}
}

@inproceedings{10.1145/800225.806822,
author = {Purtilo, James},
title = {Polylith: An environment to support management of tool interfaces},
year = {1985},
isbn = {0897911652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800225.806822},
doi = {10.1145/800225.806822},
abstract = {Polylith is the name of a set of enhanced execution time system services along with development tools and an interfacing methodology.1 As a system, Polylith supports the reliable union of many component tools, addressing the problems of data interchange and synchronization between these tools. It facilitates reuse of code, and promotes the notion that construction of large programs should be viewed instead as orchestration of services. The Polylith is visible as a grammar in which instances of environments2 are precisely and rapidly specified; it is, through compilation and execution of assertions in that language, a medium through which many programs and tools can be united with impunity.This paper presents an overview of the Polylith architecture, along with some brief remarks on the requirements analysis leading to Project Polylith at the University of Illinois. Section 2 presents this architecture, summarizing language and data transformation issues. Simple examples are included. Section 3 introduces one particular instance of an environment specified within Polylith called Minion. It is presented as an extended example, showing how the Polylith is utilized to construct an enthusiastic assistant for mathematical problem solving. The closing section contains some evaluation of how Polylith affects the task of environment development.},
booktitle = {Proceedings of the ACM SIGPLAN 85 Symposium on Language Issues in Programming Environments},
pages = {12–18},
numpages = {7},
location = {Seattle, Washington, USA},
series = {SLIPE '85}
}

@article{10.1134/S1054661821030020,
author = {Andriyanov, N. A.},
title = {Application of Computer Vision Systems for Monitoring the Condition of Drivers Based on Facial Image Analysis},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {3},
issn = {1054-6618},
url = {https://doi.org/10.1134/S1054661821030020},
doi = {10.1134/S1054661821030020},
journal = {Pattern Recognit. Image Anal.},
month = {jul},
pages = {489–495},
numpages = {7},
keywords = {driver monitoring system, pattern recognition, human condition monitoring, convolutional neural networks, Open Source Computer Vision Library, Haar cascades, eye extraction}
}

@phdthesis{10.5555/979046,
author = {Cox, Stephen Michael and Mcgrath, Diane},
title = {Web-based individual education plan software: research and development},
year = {2003},
publisher = {Kansas State University},
address = {USA},
abstract = {The purpose of this Research and Development study was to develop and evaluate Web-IEP, a Web-based Individual Education Plan (IEP) development tool, which uses artificial intelligence for decision-making based on the theories of Piaget, Vygotsky, Kohlberg, Bloom, and Erikson; and to generate Mager and Gronlund style objectives for students in special education. The software provides support to multidisciplinary teams whose charge it is to develop IEPs, which will be accessible on the Web, so that students, parents, teachers, and advocacy groups will have access to the software. The software provides assistance to novice teachers developing IEPs. The system is consistent with special education regulations, ensures IEP compliance and helps teachers develop IEP objectives. The research and development study followed a procedure based on the Systems Approach Model of Educational Research and Development (R&amp;D) method. The participants in the formative evaluations provided comments, suggestions, and criticisms, which led to the development of Web-IEP. The results of the evaluations indicated the Web-IEP fulfilled its development objectives. The results of the evaluation indicated that several schools in a regional education center could use Web-IEP to produce an IEP, IEP data would be easy to transfer from school to school, and could be shared.**This dissertation is compound (contains both a paper copy and a CD as part of the dissertation). The CD requires the following system requirements: Microsoft Office; Windows MediaPlayer or RealPlayer.},
note = {AAI3096691}
}

@book{10.5555/1841541,
author = {OECD Organisation for Economic Co-operation and Development},
title = {The Development Dimension ICTs for Development: Improving Policy Coherence},
year = {2010},
isbn = {9264077391},
publisher = {OECD},
abstract = {Information communication technologies (ICTs) are crucial to reducing poverty, improving access to health and education services and creating new sources of income and employment for the poor. Being able to access and use ICTs has become a major factor in driving competitiveness, economic growth and social development. In the last decade, ICTs, particularly mobile phones, have also opened up new channels for the free flow of ideas and opinions, thereby promoting democracy and human rights. The OECD and infoDev joined forces at a workshop on 10-11 September 2009 to examine some of the main challenges in reducing the discrepancies in access to ICTs and use of ICTs between developing countries. The workshop discussed best practices for more coherent and collaborative approaches in support of poverty reduction and meeting the Millennium Development Goals. There is much work to be done on improving policy coherence and there is a need to engage more actively with partner countries. Making the most of ICTs requires that they are seen as part of innovation for development, rather than just another development tool. This publication examines access to ICTs, as a precondition to their use; broadband Internet access and governments' role in making it available; developments in mobile payments; ICT security issues; ICTs for improving environmental performance; and the relative priority of ICTs in education. For more information The OECD/infoDev workshop on ICTs for Development: www.oecd.org/ICT/4D OECD work on Policy Coherence for Development: www.oecd.org/development/policycoherence infoDev: www.infoDev.org Table of Content : Acronyms and Abbreviations Executive Summary Chapter 1. Why ICTS Matter for Development Chapter 2. Where Next for ICTs and International Development? Chapter 3. How the Developing World may Participate in the Global Internet Economy - Innovation Driven by Competition Chapter 4. What Role Should Governments Play in Broadband Development? Chapter 5. Regulatory Issues around Mobile Banking Chapter 6. ICTs and the Environment in Developing Countries - Opportunities and Developments Chapter 7. Policy Coherence in ICTs for Education - Examples from South Asia}
}

@inproceedings{10.5555/2819009.2819251,
author = {Carver, Jeffrey C. and Hong, Neil Chue and Ciancarini, Paolo},
title = {SE4HPCS'15: the 2015 international workshop on software engineering for high performance computing in science},
year = {2015},
publisher = {IEEE Press},
abstract = {HPC software is developed and used in a wide variety of scientific domains including nuclear physics, computational chemistry, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. The increase in the importance of this software motivates the need to identify and understand appropriate software engineering (SE) practices for HPC architectures. Because of the variety of the scientific domains addressed using HPC, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the HPC, research-oriented development environment. This situation creates a need for members of the SE community to interact with members of the scientific and HPC communities to address this need. This workshop facilitates that collaboration by bringing together members of the SE, the scientific, and the HPC communities to share perspectives and present findings relevant to research, practice, and education. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods regarding SE for HPC science.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {1003–1004},
numpages = {2},
keywords = {software engineering, high performance computing, computational science, computational engineering},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1002/cpe.3154,
author = {Kersten, Rody W.J. and Gastel, Bernard E. and Shkaravska, Olha and Montenegro, Manuel and Eekelen, Marko C.J.D.},
title = {ResAna: a resource analysis toolset for real-time JAVA},
year = {2014},
issue_date = {September 2014},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {26},
number = {14},
issn = {1532-0626},
url = {https://doi.org/10.1002/cpe.3154},
doi = {10.1002/cpe.3154},
abstract = {For real-time and embedded systems, limiting the consumption of time and memory resources is often an important part of the requirements. Being able to predict bounds on the consumption of such resources during the development process of the code can be of great value. In this paper, we focus mainly on memory-related bounds. Recent research results have advanced the state of the art of resource consumption analysis. In this paper, we present a toolset that makes it possible to apply these research results in practice for real-time systems enabling JAVA developers to analyse symbolic loop bounds, symbolic bounds on heap size and both symbolic and numeric bounds on stack size. We describe which theoretical additions were needed in order to achieve this. We give an overview of the capabilities of the RESANA Radboud University Nijmegen, The Netherlands toolset that is the result of this effort. The toolset can not only perform generally applicable analyses, but it also contains a part of the analysis that is dedicated to the developers' real-time virtual machine, such that the results apply directly to the actual development environment that is used in practice. Copyright © 2013 John Wiley &amp; Sons, Ltd.},
journal = {Concurr. Comput. : Pract. Exper.},
month = {sep},
pages = {2432–2455},
numpages = {24},
keywords = {stack bounds, resource analysis, ranking function, polynomial interpolation, loop bounds, heap bounds}
}

@inproceedings{10.1109/ESEM.2017.37,
author = {Fronza, Ilenia and Wang, Xiaofeng},
title = {Towards an approach to prevent social loafing in software development teams},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.37},
doi = {10.1109/ESEM.2017.37},
abstract = {[Background] A high-functioning team is a decisive factor for a successful software development project. However building such a team is not easy. Among many issues and obstacles encountered by teams, social loafing is a common but difficult one to tackle.[Aim] We intend to construct an approach to effectively prevent social loafing behaviors in software development teams.[Method] We built one social loafing prevention approach based on existing literature and survey instruments. It has been applied in an educational context with 2nd-year computer science students working on software development projects in teams.[Results] The approach starts with increasing team members' awareness of social loafing. Team Expectations Agreement (TEA) is then used to help the team to write down the terms that explicitly prevent social loafing. During the project, a small survey instrument is used to track regularly if the specified terms are followed by the team members. At the end of a period, the presence/absence of social loafing is assessed by the team using another short survey. How to interpret the results of the surveys is explained as part of the presented approach.[Conclusions] This approach has potential to improve teamwork skills of students, which is not adequately addressed in higher education programs. Meanwhile it can be adapted in professional software development environments to prevent social loafing and improve teamwork. The next step of our study will be using the collected data to evaluate the proposed approach, and formulating a set of recommendations to use the approach in the professional software development context.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {241–246},
numpages = {6},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@book{10.5555/3385370,
author = {Ramachandran, Muthu and Mahmood, Zaigham},
title = {Software Engineering in the Era of Cloud Computing},
year = {2020},
isbn = {3030336239},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This book focuses on the development and implementation of cloud-based, complex software that allows parallelism, fast processing, and real-time connectivity. Software engineering (SE) is the design, development, testing, and implementation of software applications, and this discipline is as well developed as the practice is well established whereas the Cloud Software Engineering (CSE) is the design, development, testing, and continuous delivery of service-oriented software systems and applications (Software as a Service Paradigm). However, with the emergence of the highly attractive cloud computing (CC) paradigm, the tools and techniques for SE are changing. CC provides the latest software development environments and the necessary platforms relatively easily and inexpensively. It also allows the provision of software applications equally easily and on a pay-as-you-go basis. Business requirements for the use of software are also changing and there is a need for applications in big data analytics, parallel computing, AI, natural language processing, and biometrics, etc. These require huge amounts of computing power and sophisticated data management mechanisms, as well as device connectivity for Internet of Things (IoT) environments. In terms of hardware, software, communication, and storage, CC is highly attractive for developing complex software that is rapidly becoming essential for all sectors of life, including commerce, health, education, and transportation. The book fills a gap in the SE literature by providing scientific contributions from researchers and practitioners, focusing on frameworks, methodologies, applications, benefits and inherent challenges/barriers to engineering software using the CC paradigm.}
}

@book{10.5555/3031651,
author = {Koelsch, George},
title = {Requirements Writing for System Engineering},
year = {2016},
isbn = {1484220986},
publisher = {Apress},
address = {USA},
edition = {1st},
abstract = {Learn how to create good requirements when designing hardware and software systems. While this book emphasizes writing traditional shall statements, it also provides guidance on use case design and creating user stories in support of agile methodologies. The book surveys modeling techniques and various tools that support requirements collection and analysis. Youll learn to manage requirements, including discussions of document types and digital approaches using spreadsheets, generic databases, and dedicated requirements tools. Good, clear examples are presented, many related to real-world work the author has done during his career. Requirements Writing for System Engineeringantages of different requirements approaches and implement them correctly as your needs evolve. Unlike most requirements books,Requirements Writing for System Engineeringteaches writing both hardware and software requirements because many projects include both areas. To exemplify this approach, two example projects are developed throughout the book, one focusing on hardware and the other on software. This book Presents many techniques for capturing requirements. Demonstrates gap analysis to find missing requirements. Shows how to address both software and hardware, as most projects involve both. Provides extensive examples of shall statements, user stories, and use cases. Explains how to supplement or replace traditional requirement statements with user stories and use cases that work well in agile development environments What You Will LearnUnderstand the 14 techniques for capturing all requirements.Address software and hardware needs; because most projects involve both.Ensure all statements meet the 16 attributes of a good requirement.Differentiate the 19 different functional types of requirement, and the 31 non-functional types.Write requirements properly based on extensive examples of good shall statements, user stories, and use cases.Employ modeling techniques to mitigate the imprecision of words.AudienceWriting Requirements teaches you to write requirements the correct way. It is targeted at the requirements engineer who wants to improve and master his craft. This is also an excellent book from which to teach requirements engineering at the university level. Government organizations at all levels, from Federal to local levels, can use this book to ensure they begin all development projects correctly. As well, contractor companies supporting government development are also excellent audiences for this book.}
}

@inproceedings{10.1145/3540250.3558965,
author = {Shen, Sijie and Zhu, Xiang and Dong, Yihong and Guo, Qizhi and Zhen, Yankun and Li, Ge},
title = {Incorporating domain knowledge through task augmentation for front-end JavaScript code generation},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558965},
doi = {10.1145/3540250.3558965},
abstract = {Code generation aims to generate a code snippet automatically from natural language descriptions. Generally, the mainstream code generation methods rely on a large amount of paired training data, including both the natural language description and the code. However, in some domain-specific scenarios, building such a large paired corpus for code generation is difficult because there is no directly available pairing data, and a lot of effort is required to manually write the code descriptions to construct a high-quality training dataset. Due to the limited training data, the generation model cannot be well trained and is likely to be overfitting, making the model's performance unsatisfactory for real-world use. To this end, in this paper, we propose a task augmentation method that incorporates domain knowledge into code generation models through auxiliary tasks and a Subtoken-TranX model by extending the original TranX model to support subtoken-level code generation. To verify our proposed approach, we collect a real-world code generation dataset and conduct experiments on it. Our experimental results demonstrate that the subtoken-level TranX model outperforms the original TranX model and the Transformer model on our dataset, and the exact match accuracy of Subtoken-TranX improves significantly by 12.75% with the help of our task augmentation method. The model performance on several code categories has satisfied the requirements for application in industrial systems. Our proposed approach has been adopted by Alibaba's BizCook platform. To the best of our knowledge, this is the first domain code generation system adopted in industrial development environments.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1533–1543},
numpages = {11},
keywords = {Task Augmentation, Domain Knowledge, Code Generation},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {ESEC/FSE 2022}
}

@phdthesis{10.5555/927446,
author = {Hung, Shih-Hao and Davidson, Edward S.},
title = {Optimizing parallel applications},
year = {1998},
isbn = {0591944391},
publisher = {University of Michigan},
address = {USA},
abstract = {While parallel computing offers an attractive perspective for the future, developing efficient parallel applications today is a labor-intensive process that requires an intimate knowledge of the machines, the applications, and many subtle machine-application interactions. Optimizing applications so that they can achieve their full potential on parallel machines is often beyond the programmer's or the compiler's ability; furthermore its complexity will not be reduced with the increasingly complex computer architectures of the foreseeable future. In this dissertation, we discuss how application performance can be optimized systematically. We show how insights regarding machine-application pairs and the weaknesses in their delivered performance can be derived by characterizing the machine, the application, and the machine-application interactions. We describe a general performance tuning scheme that can be used for selecting and applying a broad range of performance tuning actions to solve major performance problems in a structured sequence of steps, and discuss the interrelationship among and between performance problems and performance tuning actions. To guide programmers in performance tuning, we developed a goal-directed performance tuning methodology that employs hierarchical performance bounds to characterize the delivered performance quantitatively and explain where potential performance is lost. To reduce the complexity of performance tuning, we developed an innovative performance modeling scheme to quickly derive machine-application interactions from abstract representations of the machine and application of interest.Collectively, this dissertation unifies a range of research work done within the Parallel Performance Project at the University of Michigan over the past seven years and significantly improves the state-of-the-art in parallel application development environments.},
note = {AAI9840559}
}

@book{10.5555/1076985,
author = {Berg, Clifford},
title = {High-Assurance Design: Architecting Secure and Reliable Enterprise Applications},
year = {2005},
isbn = {0321375777},
publisher = {Addison-Wesley Professional},
abstract = {How to Design for Software Reliability, Security, and MaintainabilityMany enterprises unfortunately depend on software that is insecure, unreliable, and fragile. They compensate by investing heavily in workarounds and maintenance, and by employing hordes of "gurus" to manage their systems' flaws. This must change. And it can. In this book, respected software architect Clifford J. Berg shows how to design high-assurance applications-applications with proven, built-in reliability, security, manageability, and maintainability.High-Assurance Design presents basic design principles and patterns that can be used in any contemporary development environment and satisfy the business demand for agility, responsiveness, and low cost. Berg draws on real-world experience, focusing heavily on the activities and relationships associated with building superior software in a mainstream business environment. Practicing architects, lead designers, and technical managers will benefit from the coverage of the entire software lifecycle, showing how to: Understand and avoid the problems that lead to unreliable, insecure software Refocus design and development resources to improve software Identify project risks and plan for assurable designs Obtain the requirements needed to deliver high assurance Design application systems that meet the identified requirements Verify that the design satisfies these requirements Plan and design tests for reliability and security Integrate security design, reliability design, and application design into one coherent set of processes Incorporate these concerns into any software development methodology© Copyright Pearson Education. All rights reserved.}
}

@inproceedings{10.1145/3606150.3606158,
author = {Weeldenburg, Gwen and Kromkamp, Len and Borghouts, Lars and Verburg, Pepijn and Hansen, Nicolai Brodersen and Vos, Steven},
title = {TARGET-tool: Participatory Design of an Interactive Professional Development Tool for Secondary School Physical Education Teachers},
year = {2023},
isbn = {9798400707353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606150.3606158},
doi = {10.1145/3606150.3606158},
abstract = {Secondary school physical education (PE) teachers are continuously challenged to find ways to support students learning and motivate them for an active and healthy lifestyle. To address this complexity, continuing teacher professional development (TPD) is key. Technological tools can facilitate the effective delivery of TPD in this context. Successful implementation of this technology, however, is not self-evident. Based on the general aim of effectively integrating technologies in the educational process and focusing on the needs of educators, this study examines how the evidence-based theoretical TARGET framework for creating a motivating PE learning climate might be embedded into a digital professional development tool for PE teachers, useful in everyday practice. It presents a case study in which a multidisciplinary team of researchers, designers, and end-users iteratively went through several phases of need identification, idea generation, designing, development, and testing. By using a participatory approach, we were able to collect contextualized data and gain insights into users’ preferences, requirements, and ideas for designing and engaging with the tool. Based on these insights the TPD TARGET-tool for PE teachers was ultimately developed. The most prominent characteristics of this tool are (1) the combination of an evaluative function with teaching strategy support, (2) the strong emphasis on ease of use due to the complex PE teaching context, (3) the avoidance of social comparison, and suggestions of normative judgment, and (4) the allowance for a high level of customization and teacher autonomy.},
booktitle = {Proceedings of the 2023 9th International Conference on Frontiers of Educational Technologies},
pages = {40–51},
numpages = {12},
keywords = {Teacher Professional Development, Physical Education, Participatory Design, Human-computer Interaction, Educational Technology},
location = {<conf-loc>, <city>Bali</city>, <country>Indonesia</country>, </conf-loc>},
series = {ICFET '23}
}

@phdthesis{10.5555/1970710,
author = {Sayda, Atalla F.},
title = {Intelligent control and asset management of oil and gas production facilities},
year = {2008},
isbn = {9780494637883},
publisher = {University of New Brunswick},
address = {CAN},
abstract = {Driven by the technical demand of the offshore oil and gas industry in Atlantic Canada, a joint venture among several Atlantic Canadian universities, and local and national companies was established in order to advance wireless systems technology in the oil and gas industry, and to assess the feasibility of an intelligent control and asset management system built on a wireless sensor network. As part of this research project, our team at the University of New Brunswick (UNB) is developing an  intelligent control and asset management system  (ICAM system) to manage the massive information flow from offshore oil rigs. The objective of this PhD thesis is to design the ICAM system architecture, to analyze its multi-faceted requirements, and to verify and validate its performance and logical behavior in normal and abnormal process situations. The conceptual model of the ICAM system was defined along with its architecture, functional description and general logical behavior. A development plan to design such a complex system and the appropriate development tools were decided. The artificial intelligence (AI) requirements of the system were analyzed in terms of knowledge representation and processing, and the appropriate AI paradigm. The communication requirements were also analyzed after conducting a thorough review of middleware (i.e., communications) technologies. The structure, implementation and deployment of the system agents were defined based on the suggested system requirements.A simple prototype of the ICAM system was designed in terms of the middleware layer, the intelligent supervisory agent of the system, and the reactive agents of the system prototype. The verification and validation of the system were demonstrated, where several scenarios were applied to the system to analyze its performance in real time and its logical behavior. The oil production facility simulation model, upon which the system's verification and validation have been demonstrated, was developed. A system performance analysis was conducted to detect any computational bottlenecks. Although the system prototype design has limitations, simulation results have demonstrated an effective system logical behavior and performance in real time.},
note = {AAINR63788}
}

@book{10.5555/1611277,
author = {Jelen, Bill},
title = {Excel VBA and Macros with MrExcel},
year = {2009},
isbn = {0789739380},
publisher = {Que Publishing Company},
edition = {1st},
abstract = {DVD includes: 7+ HOURS OF VIDEO INSTRUCTION 50 TIPS &amp; TECHNIQUES SKILLS YOU CAN LEARN IN Fifteen MINUTES OR LESS In Excel VBA and Macros with MrExcel, renowned Excel instructor and author Bill Jelen (MrExcel) teaches all the skills youll need to automate virtually any routine task with Excel and build powerful Excel macros! This package brings together nearly eight hours hours of personalized, expert video training: 50 quick, practical video lessons that demonstrate all the skills youll need to successfully use both the Excel macro recorder and the Visual Basic for Applications development environment. Youll learn one step at a time, at your own paceusing hands-on examples that reflect realistic challenges and showcase Excels remarkable capabilities. Along the way, Jelen will take you from the absolute basics through PivotTables and data filtering. Excel VBA and Macros with MrExcel delivers the power of the best classroom training at a small fraction of the cost. If you dont have time to read a huge book on Excel macros and scripting, this is exactly what youve been searching for! For all serious Excel users: managers, financial pros, entrepreneurs, marketers, analysts, and more. Looking for a better way to master todays rapidly changing technologies? Want expert help, but dont have the time or energy to read a book? Cant find classroom training worth the money? Discover LiveLessons: self-paced, personal video instruction from the worlds leading technology experts. LiveLessons are video courses, on DVD with a book supplement, that are organized into bite-sized, self-contained sessionsyoull learn key skills in as little as fifteen minutes! Each lesson begins with well-defined learning objectives and ends with comprehensive summaries, which help you track your progress. Follow along as your trainer shows you how to make the most of Excels macro recorder and its powerful VBA development environment! Bill Jelen is called MrExcel for a reason! Nobody knows more about Excel macros and scriptingand nobody knows more about teaching these skills to working professionals! Thought youd never write your own Excel macros and programs? Think again! You willand you can start in just minutes! Bill Jelen, known worldwide as MrExcel, presents live Excel seminars across the United States and appears in over 800 podcast episodes. His 20 Excel books include Special Edition Using Excel 2007 and VBA and Macros for Microsoft Excel. His website, MrExcel.com, answers 30,000 Excel questions per year. System Requirements OS: Windows 98, 2000, XP, and Windows Vista; Mac OS X; versions of Linux with the Flash 8 Player or later. Multimedia: DVD drive, an 1024x768 or higher display, and a sound card with speakers. Computer: 500MHz or higher, 128MB RAM or more Microsoft Office Spreadsheets/Desktop Applications $49.99 USA / $59.99 CAN / 31.99 U.K. mylivelessons.com informit.com/que}
}

@book{10.5555/1611309,
author = {Browne, Paul},
title = {JBoss Drools Business Rules},
year = {2009},
isbn = {1847196063},
publisher = {Packt Publishing},
abstract = {Capture, automate, and reuse your business processes in a clear English language that your computer can understand. An easy-to-understand JBoss Drools business rules tutorial for non-programmers Automate your business processes such as order processing, supply management, staff activity, and more Prototype, test, and implement workflows by themselves using business rules that are simple statements written in an English-like language Discover advanced features of Drools to write clear business rules that execute quickly For confident users of Excel or other business software, this book is everything you need to learn JBoss Drools business rules and successfully automate your business. In Detail In business, a lot of actions are trigged by rules: "Order more ice cream when the stock is below 100 units and temperature is above 25 C", "Approve credit card application when the credit background check is OK, past relationship with the customer is profitable, and identity is confirmed", and so on. Traditional computer programming languages make it difficult to translate this "natural language" into a software program. But JBoss Rules (also known as Drools) enables anybody with basic IT skills and an understanding of the business to turn statements such as these into running computer code. This book will teach you to specify business rules using JBoss Drools, and then put them into action in your business. You will be able to create rules that trigger actions and decisions, based on data that comes from a variety of sources and departments right across your business. Regardless of the size of your business, you can make your processes more effective and manageable by adopting JBoss Rules. Banks use business rules to process your mortgage (home loan) application, and to manage the process through each step (initial indication of amount available, actual application, approval of the total according to strict rules regarding the amount of income, house value, previous repayment record, swapping title deeds, and so on). Countries such as Australia apply business rules to visa applications (when you want to go and live there)--you get points for your age, whether you have a degree or masters, your occupation, any family members in the country, and a variety of other factors. Supermarkets apply business rules to what stock they should have on their shelves and where--this depends upon analyzing factors such as how much shelf space there is, what location the supermarket is in, what people have bought the week before, the weather forecast for next week (for example, ice cream in hot weather), and what discounts the manufacturers are giving. This book shows how you can use similar rules and processes in your business or organization. It begins with a detailed, clear explanation of business rules and how JBoss Rules supports them. You will then see how to install and get to grips with the essential software required to use JBoss Rules. Once you have mastered the basic tools, you will learn how to build practical and effective of the business rule systems. The book provides clear explanations of business rule jargon. You will learn how to work with Decision Tables, Domain-Specifi c Languages (DSL)s, the Guvnor and JBoss Integrated Development Environment (IDE), workflow and much more. By the end of the book you will know exactly how to harness the power of JBoss Rules in your business. What you will learn from this book? Understand the basics of business rules and JBoss rules with minimal effortInstall the required software easily and learn to use the Guvnor, which is a user-friendly web editor that's also powerful enough to test our rules as we write themLearn to write sophisticated rules and import the fact model into the Guvnor and then build a guided rule around it, which makes your web pages a lot clearerGain complete knowledge of what we can do with the Guvnor rule editor, and then use the JBoss IDE as an even more powerful way of writing rules, and automate processes for discounts, orders, sales, and moreKnow the structure of the rule file through the example of a shipping schedule, which will help you with your own shipping scheduleTest your rules not only in the Guvnor, but also using FIT for rule testing against requirements documents; run unit tests using JUnit for error-free rules and interruption-free servicesSpecifically, non-developers can work with Excel spreadsheets as a fact model to develop business processes without learning any other new technologyWork with DSLs (Domain-Specific Languages) and rule flow to make writing rules easy; which makes staff training quicker and your working life easierDeploy your business rules to the real world, which completes your project successfully, and combine this into a web project using the framework of your choice to provide better servicesBenefit from concepts such as truth maintenance, conflict resolution, pattern matching rules agenda, and the Rete algorithm to provide advanced and faster business systems so that staff efficiency is maximized Approach This book takes a practical approach, with step-by-step instructions. It doesn't hesitate to talk about the technologies, but takes time to explain them (to an Excel power-user level). There is a good use of graphics and code where necessary. Who this book is written for? If you are a business analyst - somebody involved with enterprise IT but at a high level, understanding problems and planning solutions, rather than coding in-depth implementations - then this book is for you. If you are a business user who needs to write rules, or a technical person who needs to support rules, this book is for you. If you are looking for an introduction to rule engine technology, this book will satisfy your needs. If you are a business user and want to write rules using Guvnor/JBoss IDE, this book will be suitable for you. This book will also suit your need if you are a business user and want to understand what Drools can do and how it works, but would rather leave the implementation to a developer.}
}

@inproceedings{10.1145/186281.186296,
author = {Gallivan, Michael J.},
title = {Changes in the management of the information systems organization: an exploratory study},
year = {1994},
isbn = {0897916522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/186281.186296},
doi = {10.1145/186281.186296},
abstract = {The present study was conducted as exploratory research to understand the activities and beliefs of IS and line managers, with regard to the management of information technology (IT). Semi-structured interviews were conducted with 25 managers in seven firms to understand their current initiatives, future vision, and the factors driving change. Managers from three different positions from each company were interviewed—a senior IS manager, an IS application development manager, and a line manager. The results showed that there were a variety of different initiatives underway—with the most common ones being rapid prototyping, an emphasis on purchasing packages, business reengineering, and building IT infrastructure. Beyond these few commonalities, different firms were  adopting a variety of changes to their IS organization structure, working relationships with users and outside vendors, system development tools and methodologies, and their training and other human resource policies.Similarly, a broad range of factors were cited as driving changes in IT management practice—with these clustering into four major sets of drivers: business cost pressures, business service pressures, IS service pressures, and technology-push factors. Few respondents were able to articulate a vision for the IS organization of the future, beyond describing their expectations for the initiatives currently underway. Of those respondents who provided such a vision, few described the steps required to achieve the transition. These findings are analyzed in terms of  a management framework derived from Harold Leavitt and discussed in light of other recent research on IS management. Questions for follow-up research are suggested.},
booktitle = {Proceedings of the 1994 Computer Personnel Research Conference on Reinventing IS : Managing Information Technology in Changing Organizations: Managing Information Technology in Changing Organizations},
pages = {65–77},
numpages = {13},
location = {Alexandria, Virginia, USA},
series = {SIGCPR '94}
}

@book{10.5555/519071,
author = {Corporation, Microsoft},
title = {Web Applications with Microsoft Visual Interdev 6.0 MCSD Training Kit: For Exam 70-152 with Cdrom},
year = {2000},
isbn = {0735609675},
publisher = {Microsoft Press},
address = {USA},
abstract = {From the Publisher: This official Microsoft training kit delivers comprehensive preparation for MCP Exam 70-152--an elective exam on the Microsoft Certified Solution Developer (MCSD) track. Through a self-paced system of lessons and hands-on labs, students learn how to analyze, design, build, and implement Web-based solutions using Microsoft Visual InterDev version 6.0. The training helps build competency in 12 critical skill areas defined by the MCSD program: analyzing business requirements, defining technical architecture, developing the conceptual and logical design, designing a user interface and user services, deriving the physical design, establishing the development environment, creating user services, creating data services, testing, deployment, and Web site management. Self-assessment questions and complete model application on CD-ROM supplement and extend the learning experience. By the end of the course, students have applied real-world development skills to the creation of a full-featured data-driven Web application--and they're ready for the MCP exam! Self-paced study and preparation for Exam 70-152--an elective credit for MCSD certification Provides proven, self-paced training for developing Web solutions using Microsoft Visual InterDev 6.0 Supports skills transfer and certification exam preparation in a single kit DV-DLT Fundamentals Includes sample questions to help assess learner progress Provides a self-study, learner-driven alternative to expensive classroom training CD-ROM contains hands-on lab exercises, demos, and complete model application for a total learning solution}
}

@article{10.5555/1241721.1241728,
author = {Paluri, Srinivas and Gershenson, John K.},
title = {Attribute-Based Design Description System In Design For Manufacturability And Assembly},
year = {2001},
issue_date = {April 2001},
publisher = {IOS Press},
address = {NLD},
volume = {5},
number = {2},
issn = {1092-0617},
abstract = {Present computer-aided design (CAD) systems, intentionally developed as detail oriented designing tools, do not fully support the activities at the early stage of product development. CAD systems, which require a detailed level of design, prohibit the creative and free expression of a design idea. The solution to the limitations of present CAD systems is to fully utilize the graphical ability of current computer systems to represent a design with an easily understood design description in the conceptual design stage. We have developed a computerized product development tool to support designing activities in the conceptual design phase. The attribute-based design description system (ADDS) is a feature-based system that incorporates life-cycle engineering analysis and solid modeling to form an integrated CAD system. It provides a simple design representation interface and assembly modeling, evaluates the design for life-cycle engineering issues, and exports the design to AutoCAD as a solid model with flexible information input requirements. The research thus provides a starting point to the development of CAD systems that support productivity in the conceptual design stage. ADDS has been validated by describing three different design examples of power transmission systems in ADDS and exporting them to AutoCAD. This paper examines the benefits of applying a specification driven approach and presents a framework for environments that can support the related design activities. The Design Analysis and Simulation Environment (DASE) based upon this framework has been successfully implemented through a joint initiative between Bell Canada and McGill University.},
journal = {J. Integr. Des. Process Sci.},
month = {apr},
pages = {83–94},
numpages = {12}
}

@inproceedings{10.1145/186281.186299,
author = {Dospisil, Jana and Polgar, Tony},
title = {Conceptual modelling in the hypermedia development process},
year = {1994},
isbn = {0897916522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/186281.186299},
doi = {10.1145/186281.186299},
abstract = {This paper explores the potential of using multiple software requirements specification methods for elicitation of concepts in the development of university courseware based on the multimedia and hypermedia technology. At universities, the use of hypermedia and multimedia technology is perceived as one of the most efficient ways to enhance student access to information and improve learning interaction in the undergraduate computer science course. Such an approach will guarantee high level quality teaching whilst permitting significant reduction in lecturer-student contact hours. The process which plays the critical role in such a development environment is the concepts acquisition and requirements specification, collectively called requirements engineering.The hypermedia  design and programming development process shows some peculiarities which make them different from the “classic” development activities. The main areas of interest include the user requirements engineering and removing ambiguity of natural language from software requirements specifications. There is a need to devise an integrated set of specification notations which would cater for the irregularities introduced by the differences in media while supporting commonly used development activities.In this case study, we assume a typical interactive multimedia development process in a teaching environment. The objective is to assess the role of prototyping and to establish the main characteristics of the formal requirements specification notation(s) as a means for   achieving a more efficient utilisation of complex computing technologies. The horizontal design partitioning concept is used to establish clear demarcation lines between the development process phases. The vertical design partitioning is used to reduce complexity of the design by enforcing predictable structural patterns and presentation logic components. This approach allows deployment of the multiple specification notations in the design and implementation process.The paper is organized as follows: Firstly, we discuss the key characteristics of the hypermedia design process. In section two, the role of prototyping is critically evaluated. Section three provides our view of concepts acquisition and requirements management process. Remaining sections of this case study depict  our proposed conceptual model for hypermedia based courseware development process.},
booktitle = {Proceedings of the 1994 Computer Personnel Research Conference on Reinventing IS : Managing Information Technology in Changing Organizations: Managing Information Technology in Changing Organizations},
pages = {97–104},
numpages = {8},
location = {Alexandria, Virginia, USA},
series = {SIGCPR '94}
}

@inproceedings{10.1145/2961111.2962615,
author = {Hira, Anandi and Boehm, Barry},
title = {Using Software Non-Functional Assessment Process to Complement Function Points for Software Maintenance},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962615},
doi = {10.1145/2961111.2962615},
abstract = {Context: Most widely used cost models use source lines of code (SLOC) as the software size input measure, due to its quantifiability and high correlation with effort. Estimating the SLOC of a project is very difficult in early stages of the software lifecycle, especially for software maintenance tasks. Depending on the reuse model being used, one would need to size the existing code that needs modifications and the size of the changes being made in SLOC. Functional size measures, such as Function Points (FPs) and the Software Non-functional Assessment Process (SNAP), have been developed to improve the ability to estimate project size early in the lifecycle for both development and maintenance projects. While FPs represent software size by functions; SNAP complements FPs by sizing non-functional requirements, such as data operations and interface design. Goal: SNAP complements Function Points by sizing non-functional requirements, such as data operations and interface design. Through an empirical analysis, the authors want to determine whether SNAP might be an effective software size measure individually or in conjunction with FPs to improve effort estimation accuracy. Method: The empirical analysis will be run on Unified Code Count (UCC)'s dataset, a software tool maintained by University of Southern California (USC). Results: The analyses found that separating projects adding new functions from those modifying existing functions resulted in improved estimation models using SNAP. The effort estimation model for projects modifying functions in UCC had high prediction accuracy statistics, but less impressive results for projects adding existing functions to UCC. The effort estimation accuracy were satisfactory when using SNAP in conjunction with FPs for both groups of projects. Conclusions: SNAP, indeed, complements FPs in terms of the requirements that are considered and sized. Both size metrics should be treated as individual metrics, but can be used together for acceptably accurate cost models in UCC's development environment.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {50},
numpages = {6},
keywords = {Software Non-Functional Assessment Process, Software Maintenance, SNAP, Project Management, Local Calibration, Function Point Analysis, Effort Estimation, Cost Estimation},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@article{10.1155/2022/5248308,
author = {Lin, Shiu-Shin and Zhu, Kai-Yang and Wang, Jun-Yuan and Liao, Ying-Po and Hrovat, Andrej},
title = {Integrating ANFIS and Qt Framework to Develop a Mobile-Based Typhoon Rainfall Forecasting System},
year = {2022},
issue_date = {2022},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2022},
issn = {1530-8669},
url = {https://doi.org/10.1155/2022/5248308},
doi = {10.1155/2022/5248308},
abstract = {Machine learning methods such as Adaptive Network-Based Fuzzy Inference System (ANFIS) have been widely employed in intelligent urban storm water disaster warning for the purpose of smart city. However, there exists lack of research proposed for applying ANFIS and mobile application (App) to reach the purpose of smart city. In order to accomplish the goal, the study integrates ANFIS and Qt Framework to develop a Typhoon Rainfall Forecasting System to real-time typhoon rainfall forecast via a mobile device. The Service is first built by applying cluster analysis to typhoon data (Tamsui Weather Station of Taiwan) during June 1967 and November 2020 to classify the data into four groups and then applying the ANFIS to construct the Service with data in each group. The fuzzy rule of ANFIS is established by grid partition method. Both the Service and App employ Qt Framework as the cross-operating development tool, and the App is transformed to a smart mobile device App of different platforms. The simulated results show the following: (1) Taking the example of typhoon Nakri in group 1, the lowest root mean square error (7.898 mm) and the lowest computation time (178 sec) were obtained for training with 1000 steps and three membership functions. (2) Using the optimal parameters of the typhoon belonging to that group can obtain better prediction results. The developed typhoon rainfall forecasting system App in the supplementary information demonstrates that the user can use the smart mobile device for real-time typhoon forecasting at the most three hours ahead easily.},
journal = {Wirel. Commun. Mob. Comput.},
month = {jan},
numpages = {11}
}

@techreport{10.5555/903388,
author = {Hartson, H. R and Hix, Deborah and Kraly, Thomas M.},
title = {Developing Human-Computer Interface Models and Representation Techniques(Dialogue Management as an Integral Part of Software Engineering)},
year = {1987},
publisher = {Virginia Polytechnic Institute &amp; State University},
address = {USA},
abstract = {The Dialogue Management Project at Virginia Tech is studying the poorly understood problem of human-computer dialogue development. This problem often leads to low usability in human-computer dialogues. The Dialogue Management Project approaches solutions to low usability in interfaces by addressing human-computer dialogue development as an integral and equal part of the total system development process. This project consists of two rather distinct, but dependent, parts. One is development of concepts for dialogue management, and the other is implementation of a dialogue management system (DMS) to evaluate these concepts. The goal of this paper is to describe our approach to the development of two of these conceptual aspects and how we oriented those toward the needs of practical implementation. The two conceptual aspects are (a) a structural, descriptive model of human-computer interaction, and (b) Techniques for representing both the behavioral (end-user''s) view and the constructional (developer''s) view of dialogue. The approach to their development was a technology transfer process that was part of a two-year university/industry research liaison between the Dialogue Management Project and IBM Federal Systems Division (FSD), now called Systems Integration Division. Part of this liaison was aimed at moving our research ideas and results into a real-world dialogue development environment. Following presentation of the technical problems and solutions, the paper concludes with a discussion of results of our liaison and by raising and addressing some questions of mutual interest that arose during our cooperative interaction.}
}

@proceedings{10.5555/3105494,
title = {SE4Science '17: Proceedings of the 12th International Workshop on Software Engineering for Science},
year = {2017},
isbn = {9781538627914},
publisher = {IEEE Press},
abstract = {This is a time of great growth at the intersection of Software Engineering (SE) and scientific software, in academia, industry, and research labs. Software is developed and used in a wide variety of scientific domains including nuclear physics, computational chemistry, satellite data processing, fluid dynamics, climate modeling, bioinformatics, vehicle development, population modeling and social simulation, sensor networks, drug discovery, and digital humanities. Despite its importance, the development of scientific software historically has attracted less attention from the software engineering community than other subdomains have. The increase in the importance of this software motivates the need to identify and understand appropriate SE practices for its development. Because of the variety and complexity of the scientific domains addressed software, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the research-oriented development environment. This situation creates a need for members of the SE community to interact with members of the scientific software community to address this need. This workshop facilitates that collaboration by bringing together members of the SE and scientific software communities to share perspectives and present findings relevant to research, practice, and education. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods regarding SE for science.},
location = {Buenos Aires, Argentina}
}

@inproceedings{10.1145/3304221.3319751,
author = {Kennedy, Cazembe and Kraemer, Eileen T.},
title = {Qualitative Observations of Student Reasoning: Coding in the Wild},
year = {2019},
isbn = {9781450368957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3304221.3319751},
doi = {10.1145/3304221.3319751},
abstract = {Understanding student thinking and identifying student misconceptions are important precursors to developing high quality pedagogical materials and approaches. Prior work has used conceptual assessment surveys and task-based interviews to pursue this knowledge, typically having students evaluate existing code or predict or explain code behavior. However, features of student thinking captured under these conditions may differ from features of student thinking that occurs "in the wild." We present the results of a study conducted with 10 introductory CS students at a large, engineering-focused US university. In this work, students were asked to "Please, think aloud" as they interacted in a development environment, attempting to reverse engineer a solution to a defined task based on a provided executable. We captured and analyzed video recordings of the screen and audio recordings of their utterances to characterize their actions, current task, the relevant CS concept involved, current problem-solving phase, and expressed level of certainty. We also took note of the nature of their uncertainties and if and how these uncertainties were resolved. We found that students showed uncertainty regardless of success at task completion. Students who were successful at the task engaged in live experimentation to address these uncertainties in their thinking and coding. However, the ability to produce a program that behaved correctly did not guarantee that the students fully grasped the underlying concepts: some students submitted erroneous code that coincidentally worked, while expressing doubts about the correctness of their implementation.},
booktitle = {Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {224–230},
numpages = {7},
keywords = {qualitative analysis, pedagogical content knowledge, misconceptions, introductory programming, cs2, cs1, computing education},
location = {Aberdeen, Scotland Uk},
series = {ITiCSE '19}
}

@inproceedings{10.1145/3287324.3293750,
author = {Leelanupab, Teerapong and Meephruek, Tiwipab},
title = {CodeBuddy (Collaborative Software Development Environment): In- and Out-Class Practice for Remote Pair-Programming with Monitoring Coding Students' Progress},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293750},
doi = {10.1145/3287324.3293750},
abstract = {Pair-programming is an Agile technique in Extreme Programming (XP) where traditionally two programmers need to be collocated and work together at one workstation. Previous research has shown that pair-programming is very beneficial in software engineering education. However, learning and practicing pair-programming are mostly limited in a class where students can only learn to collaboratively program with another student in controlled or laboratory settings. Although nowadays there exist some collaborative tools, such as CodePilot, Google Colaboratory and Git, they are not specifically pair-programming-oriented. This impedes a pairing's ability to discuss effective strategies in problem solving, to form productive or mutually learning pairs, and to predict pair compatibility. To encourage students in out-class practice of pair-programming, we present a demonstration of a novel web-based software development environment, called CodeBuddy, for remote pair-programming. CodeBuddy provides instructors and students with several features for managing laboratory classes and practicing pair-programming. Examples of CodeBuddy's features include: coding screen mirroring between a pair, output terminal to show compiled results, face-to-face like communication channels (i.e., video calling and instant text messaging), automatic and manual role switching, code quality analysis for monitoring coding students' progress and recommending a pair with targeted pairing goals, implicit code reviews using face detection for tracking a reviewer's engagement, line-by-line code commenting, etc. The demonstration consists of a walkthrough of two use-case scenarios: an instructor assigns a problem-solving task and two students remotely work together in a pair using CodeBuddy on two different workstations to solve it.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1290},
numpages = {1},
keywords = {software engineering, remote collaboration and learning, pair programming, agile software development},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.5555/1416502.1416527,
author = {Hashiura, Hiroaki and Yamashita, Kotaro and Ishikawa, Tatsuya and Isozaki, Yuka and Komiya, Seiichi},
title = {A software development group exercise support environment, EtUDE: the system overview and the system evaluation through applying to classes},
year = {2008},
isbn = {9789606766428},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {In the recent years, software development has become larger in scale and more complicated. Furthermore, development with faster delivery and lower cost is required, thus the software development environment is becoming more and more complex. Accordingly, companies seek high potential students in universities who have a practical sense. The information engineering department of Shibaura Institute of Technology provides classes that adopt a more practical approach to software development so that students can obtain knowledge and skills necessary for software development. However, as class hours assigned for learning software engineering are not sufficient, a support system that enables students to practice outside the classroom or school is required. This support system should have work such that it enables each member belonging to the same group to collectively perform tasks just like in the classroom. In order to solve this problem, the authors developed EtUDE [1][2], the group exercise support environment for software development. Although group exercise helps to reduce the burden of instructing compared to individual exercise, it is difficult to offer instructions that meet the individual's needs. However, it is the goal of the exercise-based classes to accomplish the obtainment of knowledge and skills for software development according to each student's level. Therefore, the authors developed the group exercise support environment for software development, EtUDE which features various functions necessary for group exercise support as well as the function that detects learners who do not benefit from the group exercise and need individual instruction. With this, the software development exercise in more practical form will be available, and at the same time, it is made possible to acquire the knowledge and skills necessary for software development according to each student's level. This essay presents the overview of EtUDE system and the outcome of the application of the system.},
booktitle = {Proceedings of the 7th WSEAS International Conference on Software Engineering, Parallel and Distributed Systems},
pages = {124–131},
numpages = {8},
keywords = {software development group exercises, software development environment},
location = {Cambridge, UK},
series = {SEPADS'08}
}

@phdthesis{10.5555/930480,
author = {Khatri, Anil and Rine, David},
title = {Validation of patient headache care education systems developed from a software reuse reference model},
year = {2000},
isbn = {0599608951},
publisher = {George Mason University},
address = {USA},
abstract = {Educational medical information system development tools have emerged as one of the most important computer applications for health care providers. The research goal of this study of educational medical information system tools is to improve the performance of health care decision making by integrating professional medical care and computer information systems. Health care organizations are concerned with both quality products and cost-effective development processes. Our reference model approach allows us to apply software reuse technology to decrease development time in bringing new software products to the market. Software engineering processes may include the process of reusing common requirements. At the conceptual level a software reuse reference model is itself a domain independent reusable structure comprised of one or more generic, generalized activities that serve to represent a generic, generalized software engineering process. Our reuse reference model derived medical information system evaluation focused on education for headache patients. This particular medical information system was effectively and efficiently derived using a software reuse reference model to generate a domain model of common requirements in headache patient education. Then, from this general domain model, a specific migraine headache domain model was derived. This in turn showed how a full line of Patient Headache Care Education System (PHCES) products could be developed. The resulting domain model derived products should significantly improve the effectiveness and efficiency of the PHCES information systems by leveraging software development reuse. This dissertation has two major steps. The first step is to develop an effective patient care education system (PCES) for headaches. The second step is to expand the PHCES customer base by increasing efficiency in the process of developing effective PHCES with variations. Use of the software reuse reference model promises to: (1)&nbsp;increase software product quality, (2)&nbsp;decrease software production effort and time, (3)&nbsp;decrease time to bring products to market, and (4)&nbsp;reduce initial up front investment by the developer. The approach to our research had three parts: (1)&nbsp;a literature survey of the headache domain, the patient care education systems (PCES) and the domain software engineering concepts. (2)&nbsp;research and development of a reusable PCES domain model with expansion capabilities, using a software reuse reference model approach. (3)&nbsp;research and development of a prototype patient headache education information system derived from the domain model, demonstrating the reusability features of our PCES object framework in the reference model. Contributions from this research include: (i)&nbsp;Reference Model of PCESs, (ii)&nbsp;PCES domain model, (iii)&nbsp;Prototype derivation from the PCES domain model for different types of headaches with variations and, (iv)&nbsp;Validation of the prototype.},
note = {AAI9957629}
}

@article{10.1145/291712.295792,
author = {Hendrix, T. Dean and Cross, James H. and Teate, Joe C. and Barowski, Larry A. and Mathias, Karl S.},
title = {Assessing GRASP utilization through instrumentation},
year = {1998},
issue_date = {Sept./Oct. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {XVIII},
number = {5},
issn = {1094-3641},
url = {https://doi.org/10.1145/291712.295792},
doi = {10.1145/291712.295792},
abstract = {The idea that representing something visually can help us understand it has long been promoted in common practice and in the literature [1,2,3,4,5,6]. Indeed, "a picture is worth a thousand words" has become a standard cliché in our culture. In the case of software, however, one must take great care that it is the correct thousand words that are being conveyed [5]. Nonetheless, appropriate visualizations of software can be quite beneficial to programmers, especially when faced with program comprehension tasks. Such tasks exist throughout the software life cycle (e.g., formal technical reviews, debugging, verification, reverse engineering) and in the classroom (e.g., students reading examples from the text or examples from the professor).The GRASP (Graphical Representations of Algorithms, Structures, and Processes) research project at Auburn University seeks to develop tools and techniques for the effective use of graphical representations and visualizations of software. The overall goal of this research is to increase the efficiency of programmer comprehension and understanding of source code, and thereby decrease overall software cost. As an integral part of the research project, the GRASP software engineering tool has been developed as a continuously evolving prototype. The emphasis of the tool to this point has been on visualizing program structure and complexity via the automatic generation of Control Structure Diagrams (CSDs) and Complexity Profile Graphs (CPGs) from Ada source code [7]. The current release of GRASP provides generation of CSDs and CPGs together with other program comprehension aids such as syntax coloring, typographical enhancements, and source code folding [8]. When coupled with an appropriate compilation system such as GNAT, GRASP becomes an integrated graphical development environment for Ada 95, allowing users to edit, visualize, pretty-print, compile, link, execute and debug Ada software.The GRASP prototype for Ada was first made available to the public in January 1996. Since that time, thousands of copies of GRASP have been downloaded via anonymous file transfer protocol (FTP) and the World Wide Web (WWW) from educational, government, military and commercial sites, both in the United States and abroad. When it was released to the public, GRASP was also made available to users of the Auburn University College of Engineering computer network. GRASP is now used extensively throughout the computer science and engineering curriculum at Auburn University, in approximately three to five courses per quarter.},
journal = {Ada Lett.},
month = {sep},
pages = {51–56},
numpages = {6}
}

@inproceedings{10.5555/1864181.1864187,
author = {Mammino, Liliana},
title = {Plenary lecture 6: language aspects in science and technology education: novel approaches for new technologies},
year = {2010},
isbn = {9789604742028},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Language is the fundamental tool for the development of thought. It is therefore an essential tool for all the inquiry aspects in the sciences (identifying relationships between pieces of information, identifying investigation questions, formulating and verifying hypotheses, making inferences) and in the trains of thoughts leading from information to interpretation and ultimately to theory. It is thus extremely important that science students acquire sufficiently sophisticated levels of language-mastering to be able to use it for a real familiarization with the main aspects of doing science.In recent years, there is a growing concern about fast deterioration of the quality of language-mastering among the young generation, mostly as a result of the dominant use of communication technologies for which short, grammatically and logically unconnected sentences are viewed as the most suitable options. Such deterioration poses a threat to the development of science thoughts in future years, because of the risk of inadequacies in the ability to utilise the essential thought-development tool to its full power.The current presentation suggests that the development of language-mastering abilities up to the sophistication levels that are needed for the generation and communication of scientific information needs to become a relevant component of science and technology education. This requires the design of novel approaches, integrating the increasing utilization of new, computer-based, educational technologies with the development of language-mastering abilities. The design is challenging, because of the complexity of the language skills that are relevant within the sciences ? skills concerning the identification and expression of individual logical or method-related relationships (e.g., cause-effect, hypothesis-thesis, condition-consequence) and of comprehensive logical and interpretation frameworks. The presentation proposes and discusses some options, considering implementation pathways, feasibility assessments and expected impacts, on the basis of long experience with the analysis of language-related difficulties encountered by science students and of the interplays between language communication and other communications forms, like visualization.},
booktitle = {Proceedings of the 7th WSEAS International Conference on Engineering Education},
pages = {20},
numpages = {1},
location = {Corfu Island, Greece},
series = {EDUCATION'10}
}

@book{10.5555/559355,
author = {Butow, Eric and Ryan, Tommy},
title = {C#: Your Visual Blueprint for Building .Net Applications},
year = {2001},
isbn = {076453601X},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
abstract = {From the Publisher: Welcome to the only guidebook series that takes a visual approach to professional-level computer topics. Open the book and you'll discover step-by-step screen shots that demonstrate over 100 key C# programming tasks, including: Employing class inheritance Including event-handling methods Authoring a component Working with strings Declaring abstract properties Adding Web forms and controls Adding multidimensional arrays Processing XML comments Creating workgroup-enabled applications Managing the integrated debugger Extra Apply It Apply It and Extra sidebars highlight useful tips High-resolution screen shots demonstrate each task Succinct explanations walk you through step by step Two-page lessons break big topics into bite-sized modules .NET development tools on CD-ROM! TextPad and Antechinus C# Editor shareware Trial versions of VMWare Workstation and MineC#sweeper Plus all sample code and an e-version of the book System requirements: PC running Windows 98 or later. See the What's on the CD-ROM appendix for details and complete system requirements. Author Biography: Eric Butow has used Intel-compatible computers since 1984 and is currently training for his A+ certification in computer systems and an MCSE (Microsoft Certified Systems Engineer) in Windows NT 4.02000. Eric has used Windows NT since version 3.1 was introduced in 1994 through the current release candidate of Windows 2000. Eric is a technical editor for IDG Books, and has edited seven books covering Windows 98 and several Linux distributions. In real life, Eric is a contract technical writer. He was the Editor-in-Chief for Sacramento PC Users Group Newletter Sacra Blue for two years, and remains a contributing editor. Tommy Ryan has his MCT, MCSE, and MCP + Internet and is a technical project consultant. He has worked on projects for Ryder, the Salvation Army, Ceridian, Ernst &amp; Young, and Dow Chemical. Tommy is an author for ASPToday.com. He is also the co-author of C#: Your visual blueprint for building .NET applications.}
}

@phdthesis{10.5555/AAI29324138,
author = {Mahaju, Sweta and Jeff, Gray, and Chris, Crawford, and Randy, Smith, and Gary, Bradshaw,},
advisor = {Jeffrey, Carver,},
title = {Application of Human Error Theories in Managing Human Errors in Software Engineering},
year = {2022},
isbn = {9798368462530},
publisher = {The University of Alabama},
abstract = {Context: Software development, especially in its initial requirements phase, is a human-centric activity and hence vulnerable to human error. Human errors are flaws in the human thought process. To ensure software quality, it is essential for practitioners to understand how to manage these human errors. Organizations often introduce changes into the requirements engineering process to either prevent human errors from occurring or to mitigate the harm caused when those errors do occur. While there are studies on human error management in other disciplines, research studies on the prevention and mitigation of human errors in software engineering and requirements engineering specifically are scarce. The current studies in software engineering do not provide strong results about the types of changes most effective in requirements engineering. Objective: The goal of this dissertation research is to structure and organize the findings on human error prevention and mitigation approaches and provide an initial evaluation of their effectiveness. To that end, I developed a taxonomy of human error prevention and mitigation strategies based on data gathered from requirements engineering professionals. Furthermore, I validated its feasibility to be broadly representative and useful in real software development processes. Method: I performed a qualitative analysis of data from two practitioner surveys on requirements engineering practices to identify and classify strategies for preventing and mitigating human errors. Then, I attempted to fit human error prevention and mitigation strategies identified in software engineering and cognitive psychology domains into the taxonomy to enhance and broaden it. Finally, I evaluated the feasibility and usefulness of the taxonomy by training senior-level undergraduate students to use the error management strategies organized in the taxonomy to handle their software development problems. Results: I organized the human error management strategies into a formal taxonomy based on whether the changes primarily affect People, Processes, or the Environment. I further organized the strategies into low-level classes inside each of these high-level categories. I found that error management strategies focused on changes in Process are more frequently used and, hence, more effective than those focused on changes in People. Conclusions: The Human Error Management taxonomy (HEMT) provides a systematic classification and organization of strategies for the prevention and mitigation of human errors in software engineering. This systematic organization provides a foundation upon which future research can build. This dissertation research provides an initial structure to the scattered error management approaches and an initial evaluation of the feasibility and usefulness of that structure. Further empirical studies are needed in more real software development environments and settings to validate and generalize the findings reported in this dissertation.},
note = {AAI29324138}
}

@phdthesis{10.5555/921884,
author = {Bevill, Douglas A.},
advisor = {Childs, John},
title = {An investigation into the efficacy of transaction shells as a computer-based instructional design and delivery tool for an instructional designer, instructor, and end user},
year = {1995},
publisher = {Wayne State University},
address = {USA},
abstract = {The development of computer based instruction (CBI) is a costly labor intensive endeavor which is considered a serious obstacle to the widespread use of CBI. Transaction shells, a CBI authoring and delivery tool, represents one attempt at finding a solution to this problem. The specific purposes of this study were to investigate: (a) The efficacy of transaction shells as an authoring environment and instructional paradigm for an instructional developer, (b) the efficacy of a transaction shell based instructional product for an instructor as a user of the development tool or owner of the instructional lesson, and (c) the efficacy of a transaction shell based instructional lesson for the student or user of that lesson. The samples for this study involved a graduate student in Instructional Technology as an instructional developer, an instructor in the School of Engineering, and users of CBI drawn from the graduate population in Instructional Technology all at Wayne State University in Detroit, Michigan.The results of the study include: (1) Transaction shells as represented in this version of ID Expert contain enactment problems that need to be addressed. (2) The transaction shell authoring approach is different from other authoring programs and the commercial version of this software should be provided with clear and concise directions. (3) The educational environment will be slow in feeling the impact of this authoring tool due to hardware requirements. (4) Transaction shells provide an effective and efficient instructional development review process. (5) The transaction shell automated instructional delivery paradigm allows the developer to concentrate on the instructional content and not the instruction. (6) Transaction shells can be used as a presentation program. (7) Transaction shells appear to be efficient and effective for instructional delivery. (8) The use of multimedia resources within transaction shells should be determined by motivational concerns identified with specific audiences or where the instructional outcomes are not being accomplished.Recommendations were made for additional research that would build on the findings of this study and implications were made for the field of instructional design and automated authoring tools.},
note = {AAI9530525}
}

@inproceedings{10.1145/3352740.3352755,
author = {Duan, Wei and Chen, Haiyan},
title = {Development and Application of Online Tourism English Teaching Platform},
year = {2019},
isbn = {9781450372053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352740.3352755},
doi = {10.1145/3352740.3352755},
abstract = {In the age of Internet+, the integration of information technology and vocational English course teaching is bound to bring about the reformation of ESP teaching. On the basis of introducing the main idea of "flipped classroom" teaching model reform in tourism English course, this paper mainly discusses the development and application of online tourism English teaching platform including the designing goals, framework, modules, and database design of the platform. It has been found that the tourism English teaching platform designed in this paper can offer effective learning scaffolding to college students majoring in tourism and tourism practitioners, meeting their needs of learning and practicing at anytime and anywhere. At the same time, teachers can handle the learning progress and quality of students so as to realize the "flipped classroom" teaching model of tourism English course.Under the background of the development of digital technology, CG (Compute Graphics) digital painting is an artistic form of expression that closely combines painting art with digital technology. CG painting digitally transforms two-dimensional or three-dimensional graphics into digital graphics by means of mathematical algorithms. Painting works can be widely used in animation, film and television, games, visual communication design and other fields. The main purpose of the design and development of teaching system is to construct a teaching support system that can meet the requirements. This research takes Python as the development environment, adopts the four-tier software framework of object-oriented program design, including application-level knowledge point modeling, communication-level curriculum application management, resource-level management design, user-level UI design and other modules.The core issues to be solved include the establishment of teaching environment, the presentation and editing of knowledge point framework modeling, the establishment of pre-basic skills and learning objectives, and then the establishment of the teaching department. The application of the system has been tentatively studied. Finally, five indicators, including student experience, learning process satisfaction, teaching effect evaluation, teaching time-consuming comparison and teaching effect, are selected to verify the effectiveness of teaching supported by the experimental teaching system of CG digital painting course. The experimental results show the superiority of the experimental teaching system proposed in this paper.},
booktitle = {Proceedings of the 2019 3rd International Workshop on Education, Big Data and Information Technology},
pages = {87–92},
numpages = {6},
keywords = {Tourism English, The development of online teaching platform, "Flipped classroom" teaching model},
location = {Guilin, China},
series = {EBDIT 2019}
}

@inproceedings{10.5555/1659364.1659365,
title = {Front Matter},
year = {2009},
isbn = {9781607500520},
publisher = {IOS Press},
address = {NLD},
abstract = {Mobile Commerce (M-Commerce) comprises applications and services that are accessible from Internet-enabled mobile devices. It involves new technologies, services, and business models. Whilst it is different from traditional e-Commerce it can also be considered as an extension of the same since, among other reasons, it makes e-Commerce in a modern way available to new application areas and to new customers. Mobile devices, such as phones or PDAs open the door to a great assortment of innovative applications and services. They go wherever you go, enable users to access the Internet at any moment under any circumstances, allowing access to several different services should they be necessary (e.g. looking for a nearby restaurant or gas station while walking down the street with friends and family). These emerging situations also imply avant-garde engineering requirements, as well as fresh development tools and methodologies.  The Internet has become an essential component in all aspects of everyday life regardless of location, the time of the day or environment. Along the same lines, even today, mobile phones and PDAs have become an indispensable part of daily routine as sources of all kinds of information and services and, especially, as a permanently available interface to our surroundings. Under this scenario, the number of mobile services available is constantly increasing and evolving towards manageable and practical applications. Tomorrow, they may very well turn into intelligent assistants capable of anticipating many of our wishes and needs; but, for all these changes to happen, key issues of interoperability, usability, security and privacy still need to be addressed under the special scope of mobile services and commerce.  This book comprises the papers presented at TAMoCo 2009. It covers such diverse areas of mobile commerce as context-aware mobile applications, Web services for mobile applications, mobile technologies in urban systems, mobile technologies for education, and autonomic computing and mobile commerce.  Acknowledgements  The organizers would like to express their gratitude to the authors for submitting their work to TAMoCo 2009 and to the program committee for providing such useful evaluations of the submitted papers. We would also like to thank the invited speakers and all the participants for their presentations and further discussion. We would especially like to thank all those people who collaborated in the event organization. Finally, we also want to thank IOS Press for their understanding and their valuable support in producing these proceedings.  TAMoCo 2009 Editors  Juan Enrique Agudo (University of Extremadura, Spain)  Cherif Branki (University of the West of Scotland, UK)  Brian Cross (University of the West of Scotland, UK)  Gregorio D\'{\i}az (University of Castilla La Mancha, Spain)  Frank-Dieter Dorloff (University of Duisburg-Essen, Germany)  Guadalupe Ortiz (University of Extremadura, Spain)  Key Pousttchi (Universit\"{a}t Augsburg, Germany)  Martin Randles (Liverpool John Moores University, UK)  Mercedes Rico (University of Extremadura, Spain)  H\'{e}ctor S\'{a}nchez (University of Extremadura, Spain)  A. Taleb-Bendiab (Liverpool John Moores University, UK)  Frank Teuteberg (University of Osnabr\"{u}ck, Germany)  Rainer Unland (University of Duisburg-Essen, Germany)},
booktitle = {Proceedings of the 2009 Conference on Techniques and Applications for Mobile Commerce: Proceedings of TAMoCo 2009},
pages = {i–viii}
}

@book{10.5555/517603,
author = {Dahnoun, Naim},
title = {Digital Signal Processing Implementation Using the TMS320C6000 DSP Platform},
year = {2000},
isbn = {0201619164},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
edition = {1st},
abstract = {From the Book: PREFACE: Preface Digital signal processing techniques are now so powerful that sometimes it is extremely difficult, if not impossible, for analogue signal processing to achieve the same or closer performance. Added to this, digital signal processors are very affordable and include good development tools and support. This is sufficient to explain the growing number of areas of application for DSP, including motor drives, communications, biomedical instrumentation and automotive applications. Having dealt for some time with undergraduate and postgraduate students, researchers and digital signal processor users in general, I have found that first-time users of DSP find a barrier obstructing them in progressing from theory to the full implementation of algorithms. When it comes to implementing an algorithm many questions arise, questions such as: Which processor to use - fixed or floating point Which manufacturer to choose Which application hardware to use How many I/O interfaces are needed and how fast should they be When these questions are answered, more questions arise regarding the implementation on the specific processor and hardware selected. In this book, use of the TMS320C6000 will be justified, and the hardware and complete implementation of selected algorithms will be dealt with in detail. Material used for the teaching of undergraduate and postgraduate students, along with laboratory experiments, are used to demonstrate and simplify the transition from theory to the full implementation on the TMS320C6201 processor. This book is divided into nine chapters. Chapters2and 3 are very important and it is advisable that they are well understood before progressing onto subsequent chapters. Chapter 1 Introduction This introductory chapter provides the reader with general knowledge on general-purpose DSP processors and also provides an up-to-date TMS320 roadmap showing the evolution of Texas Instruments' DSP chips in terms of processing power. Chapter 2 The TMS320C62xxlC67xx architecture The objective of this chapter is to provide a comprehensive description of the 'C6x architecture. This includes a detailed description of the Central Processing Unit (CPU) and program control along with an overview of the memory organisation, serial ports, boot function and internal timer. Chapter 3 Software development tools and TMS32OC6201 EVM overview This chapter is divided into three main parts. The first part describes the software development tools, the second part describes the Evaluation Module (EVM) and, finally, the third part describes the codec, and use of interrupts along with some useful programs for testing the TMS320C6201 EVM. Chapter 4 Software optimisation To introduce the need for code optimisation, this chapter starts by developing the concept of pipelining. Since the TMS320C62xx and the TMS320C67xx each have eight units, which are dedicated to different operations, and since different instructions can have different latencies, the programmer or the tools are left with the burden of scheduling the code. Backed by examples, this chapter explains the different techniques used to optimise DSP code on these processors. Chapter 5 Finite Impulse Response (FIR) filter implementation The purpose of this chapter is twofold. Primarily, it shows how to design an FIR filter and implement it on the TMS320C62xx processor, and secondly, it shows how to optimise the code as discussed in Chapter 4. This chapter discusses the interface between C and assembly, how to use intrinsics, and how to put into practice material that has been covered in the previous chapters. Chapter 6 Infinite Impulse Response (IIR) filter implementation This chapter introduces the IIR filters and describes two popular design methods, that is the bilinear and the impulse invariant methods. Step by step, this chapter shows the procedures necessary to implement typical IIR filters specified by their transfer functions. Finally, this chapter provides complete implementation of an IIR filter in C language, assembly and linear assembly, and shows how to interface C with linear assembly. Chapter 7 Adaptive filter implementation This chapter starts by introducing the need for an adaptive filter in communications. It then shows how to calculate the filter coefficients using the Mean Square Error (MSE) criterion, exposes the Least Mean Square (LMS) algorithm and, finally, shows how the LMS algorithm is implemented in both C and assembly. Chapter 8 Goertzel algorithm implementation This chapter deals with Dual Tone Multi-Frequency (DTMF) detection and provides a practical example of the Goertzel algorithm. This chapter also shows how to produce optimised code by the pen and paper method, describes linear assembly and demonstrates how to program the Direct Memory Access (DMA). Chapter 9 Implementation of the Discrete Cosine Transform This chapter starts by introducing the need for video compression to reduce the channel bandwidth requirement, then explains the Joint Photographic Experts Group (JPEG) image codec. This includes a detailed discussion and the implementation of the Discrete Cosine Transform (DCT) and Inverse Discrete Cosine Transform (IDCT) and concentrates on their optimisation. An explanation of the PC-DSP communication via the PCI bus is also provided. Software The accompanying CD includes all the programs used in this book. To help the reader in locating or viewing the files, an Index.htm file has been included. The files are in separate directories corresponding to each chapter. Some directories are further divided in sub-directories to separate different implementations. Batch files for compiling, assembling and linking these programs are included. All the files have been tested (the environment may need to be modified: see env.bat file). Software using Code Composer Studio environment is also provided. Software updates including code running on the TMS320C6211 DSK can be obtained from the Publisher. Acknowledgements As you can imagine, it is hard to produce any textbook on state-of-the-art technology, especially when the time factor is playing against you. However, with the first very encouraging comments from the five anonymous reviewers, my motivation for writing this book surged, and therefore I would like to thank them for their constructive comments. Due to the unfamiliarity with this processor, it was difficult to share ideas with other users. But with Tuan-Kiang Chiew, Kwee-Tong Heng and Michael Hart many problems were solved and many grey areas were clarified; I extend to them special thanks. I am indebted to Robert Owen, Hans Peter Blaettel, Gene Frantz, Neville Bulsara, Greg Peake, Helga and Graham Stevenson and Maria Ho of Texas Instruments for their encouragement, continuous help and support. I owe my thanks to Professor Barrie Jones, Professor David Evans, Dr. John Fothergill and Fernando Schlindwein from Leicester University for their encouragement, and Dr Anthony Brooms from Oxford University and Dr Mark Yoder from the Rose-Hulman Institute of Technology, USA, for reviewing the material. My thanks to all of my colleagues at the Department of Engineering at Bristol University and to all of our students, in particular Khaled, Fernando, Mohamed, Samir, Chris, Shirley and Julian. Also I would like to thank Cornelius Kellerhoff, European DSP Business Development Consultant, Paul Coulton, Communications Research Centre, Lancaster University, and Mariusz Jankowski, University of Southern Maine, for their valuable technical reviews. I thank my parents, family and friends for their support and encouragement. Finally, many thanks to Karen Sutherland, Julie Knight and all of the Pearson Education and Prentice Hall team who were very kind, supportive and encouraging. N. Dahnoun Naim.Dahnoun@Bristol.ac.uk}
}

@book{10.5555/560371,
author = {Wahli, Ueli and Matthews, Alex and Lapido, Paula and Norguet, Jean-Pierre},
title = {Websphere Version 4 Application Development Handbook},
year = {2002},
isbn = {0130092258},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Book: Preface This IBM Redbook provides detailed information on how to develop Webapplications for IBM WebSphere Application Server Version 4 using a variety ofapplication development tools. The target audience for this book includes team leaders and developers who aresetting up a new J2EE development project using WebSphere Application Serverand related tools. It also includes developers with experience of earlier versionsof the WebSphere product, who are looking to migrate to the Version 4environment. This book is split into four parts, starting with an introduction, which is followed byparts presenting topics relating to the high-level development activities ofanalysis and design, code, and unit test. A common theme running through allparts of the book is the use of tooling and automation to improve productivity andstreamline the development process. In Part 1 we introduce the WebSphere programming model, the application development tools, and the example application we use in our discussions. In Part 2 we cover the analysis and design process, from requirements modeling through object modeling and code generation to the usage of frameworks. In Part 3 we cover coding and building an application using the Java 2 Software Development Kit, WebSphere Studio Version 4, and VisualAge for Java Version 4. We touch on Software Configuration Management using Rational ClearCase and provide coding guidelines for WebSphere applications. We also cover coding using frameworks, such as Jakarta Struts and WebSphere Business Components. In Part 4 we cover application testing from simple unit testing through application assemand deployment to debugging and tracing. We also investigate how unit testing can be automated using JUnit. In our examples we often refer to the PiggyBank application. This is a verysimple J2EE application we created to help illustrate the use of the tools,concepts and principles we describe throughout the book. The team that wrote this redbook This redbook was produced by a team of specialists from around the worldworking at the International Technical Support Organization, San Jose Center. Ueli Wahli is a Consultant IT Specialist at the IBM International TechnicalSupport Organization in San Jose, California. Before joining the ITSO 17 yearsago, Ueli worked in technical support at IBM Switzerland. He writes extensivelyand teaches IBM classes worldwide on application development, objecttechnology, VisualAge products, data dictionaries, and library management. Ueliholds a degree in Mathematics from the Swiss Federal Institute of Technology. Alex Matthews is a Consulting IT Specialist in the IBM Software Business,based in London, United Kingdom (UK). He has spent the last two and a halfyears providing post-sales services to customers who have purchasedWebSphere products and related tools. Alex has seven years experience buildingdistributed systems using a variety of middleware products. He holds a degree inComputing Science from Aston University, Birmingham, UK. Paula Coll Lapido works as an IT Specialist in the e-business Innovation Centerat Madrid, Spain. Her current area of expertise focuses on developing e-businessapplications using the WebSphere platform. She has been working at IBM forone year and a half. She holds a degree in Physics from the ComplutenseUniversity of Madrid. Jean-Pierre Norguet is an IT Specialist, Team Leader and Coach in the IBMe-business department in Belgium. He has been working at IBM for three years.His areas of expertise include the entire application development life cycle. Heholds a 5-year Engineering degree in Computer Science from the UniversiteLibre de Bruxelles and a Socrates European master's degree from the EcoleCentrale Paris. Special notice This publication is intended to help application analysts and developers to createWeb applications for WebSphere Application Server using a variety of applicationdevelopment and test tools. The information in this publication is not intended asthe specification of any programming interfaces that are provided by WebSphereApplication Server. See the PUBLICATIONS section of the IBM ProgrammingAnnouncement for WebSphere Application Server for more information aboutwhat publications are considered to be product documentation. Comments welcome Your comments are important to us! We want our IBM Redbooks to be as helpful as possible. Send us yourcomments about this or other Redbooks in one of the following ways: Use the online Contact us review redbook form found at:ibm.com/redbooks Send your comments in an Internet note to:redbook@us.ibm.com Mail your comments to the address on page ii.}
}

@book{10.5555/515908,
author = {Clark, Jeffrey E.},
title = {VBA for AutoCAD 2002: Writing AutoCAD Macros},
year = {2001},
isbn = {0130652016},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Book: Preface Data management has been a central focus of mine throughout most of my 30-plus-year career. As an architect engaged mostly in working with business managers, helping them define their project needs (Should I build, lease, expand, or what ... And how much do I need ), I needed a computer early on. I began using BASIC in 1970, on a timesharing network, to manage the data I collected and to produce reports. Thus began my involvement with the BASIC language. I have used many dialects, from the line-numbered, Beginners All-purpose Symbolic Instruction Code that came out of Dartmouth University around 1960, to the modern versions of VB and VBA. These are now among the favored languages for communicating with relational databases such as Oracle, SQL Server, and the like, and for creating content for the Internet. BASIC was so-named because, as an interpreted language, it was easy to learn by incrementally developing a program, entering a few lines of code at a time, and then testing the result. At the time, however, many professional programmers thought it to be something of a toy. Mr. Gates, on the other hand, was a strong supporter of the language, introducing three levels of BASIC with the first IBM PC in 1981. Cassette BASIC was hardwired into the machines' ROM, and you could only save files to a cassette tape. (I never actually did that!) The disk and advanced levels were built into Microsoft's first version of its operating system: MS-DOS 1.0. A BASIC compiler (BASCOM) was introduced shortly thereafter, which produced executable programs in .EXE format. Late in the 1980s, QuickBasic wasintroduced with the first Interactive Development Environment (IDE) for DOS. This was followed in 1991 by Visual Basic 1.0, in both DOS and Windows versions. Visual Basic (VB) continues its growth in popularity. Microsoft Word and Excel were the first components of Microsoft Office, and each had its own internal functionality for creating macros, essentially scripts that allowed certain procedures to be automated within the application. Visual Basic for Applications (VBA) was first introduced in Excel in 1993. With the addition of Access, PowerPoint, and other products to the mix, VBA now provides an object-oriented programming environment common to all the Office components. This not only allows the creation of macros within each of the applications, but macros that allow Access, for example, to actually start up and communicate with Excel. The AutoCAD World AutoCAD is and always has been a database program. Underlying its graphic interface are lists within lists that define the entities that make up your drawing. During its evolution into a fully Windows-compatible product, many new data-oriented features were added. Initially conceived as a drafting package and written by engineers, AutoCAD was designed with an accessible data structure that allows both the manipulation of its graphic entities as well as the attachment and extraction of textual and numeric data. Attribute extraction using AutoLISP was at first the only means of manipulating the data within the drawing, and this method was widely used in the DOS days of the mid-1980s, when I began using the application. The AutoCAD SQL Extension (ASE), introduced with Release 12 in the early 90s, provided a rudimentary interface to databases such as DBASE and PARADOX. AutoCAD was recast in object-oriented C++ with Release 13 and the introduction of the AutoCAD Runtime Extension (ARX). Using ARX requires a programmer capable of creating DLLs using C++, which the average AutoCAD user is not. About the same time, however, AutoCAD users began experimenting with programs written in Visual Basic 3.0 using Dynamic Data Exchange (DDE). Curiously, support for a more robust connection between VB and AutoCAD had been provided in Release 12, but was dropped in Release 13. Autodesk added support for Microsoft's ActiveX Automation interface standard to Release 14, first allowing VBA access to AutoCAD objects. Autodesk initially released AutoCAD 2000 in the spring of 1999, fully incorporating VBA. Further enhancements focusing on Internet functionality were a feature of the 2000i release in the summer of 2000. The development environment has a look and feel identical to that of any Microsoft Office component. With this convergence, the ability to write AutoCAD macros and communicate with other VBA-enabled applications is now fully accessible to the user. Now, with the release of AutoCAD 2002, Autodesk has adopted a policy of regular releases, incorporating "technology extensions" in the form of incremental upgrades. The extensions added over each period of time will be incorporated into the latest modular release. Who This Book Is For AutoCAD 2002 is the flagship product of Autodesk, which, according to the company's web page "is the world's leading supplier of PC design software and digital content creation." Since its initial release in 1982, well over two million copies of AutoCAD have been shipped. There are nearly 1000 Autodesk Training Centers worldwide, and over one million students are trained on Autodesk products each year. Eighty five percent of the companies in the Fortune 500 are Autodesk customers, and the firm's products are available in 19 languages. There are, according to Autodesk, over 200 user groups worldwide, along with almost 3000 registered developers. There are also countless unregistered developers who have no formal relationship with Autodesk. They produce add-ons and customizations both for sale and for their own and their companies' use. The CAD Manager who needs to convert the layers of several hundred drawings received from another consultant is a potential AutoCAD developer. The Facility Manager who needs to link drawing attributes representing occupied and available areas to an Excel spreadsheet can now use VBA to accomplish this. Automation using VBA allows the user to work on the AutoCAD side, or the Excel side, or in some other application, whichever he or she is more comfortable with. I have routinely used VBA to write SQL scripts to populate database tables, as well accessing and modifying drawings in AutoCAD. I used to do many of these things in AutoLISP, which was the only act in town for 15 years. This book is for the AutoCAD and Office user who has problems to solve. Users who are already programmers can use it to familiarize themselves with the AutoCAD object model. Users who are not programmers will be able to get started by studying the book's examples. My paramount goal, more than just presenting the components of VBA, is to tie together the disparate elements of AutoCAD with which you must be reasonably comfortable in order to use VBA effectively. The AutoCAD documentation treats, in unrelated discussions, many of those features that need to work in concert to get a job done. Understanding the DXF file representation, for example, together with some of the basic syntax of AutoLISP as it relates to the underlying database structure, provides the keys to unlocking the AutoCAD drawing and manipulating its data. It is the purpose of this book to provide those necessary links. What's in the Book VBA for AutoCAD 2002: Writing AutoCAD Macros is divided into three major sections: The AutoCAD VBA Environment (Chapters 1-4) Using the AutoCAD Object Model (Chapters 5-17) Communicating with Other Applications and the Internet (Chapters 18-20) There are four appendices containing additional reference material as well as some supplemental utilities and examples. Part Two, Using the AutoCAD Object Model, is by far the longest section, in which the application's numerous collections and objects will be discussed in detail. The following outline gives you an overview of what you can expect to see in each chapter. Part One: The AutoCAD VBA Environment Chapter 1, Taking Control of AutoCAD, introduces you to some of the concepts of Automation and Microsoft's Component Object Model (COM). In it we will talk about what we mean by object-oriented programming; its tripartite foundation of encapsulation, inheritance, and polymorphism; and define such terms as class, interface, and binding. We will look at a programming example that reads data from an Excel worksheet and creates an AutoCAD drawing with it, without either application being visible, in 30 lines of code. Finally, we will introduce a system that we will use to chart all of AutoCAD's methods, properties, and events, categorizing them in a concise reference format that will be used throughout the book. Chapter 2, The VBA Environment, shows you how to use the Interactive Development Environment (IDE); how to create, edit, and save VBA projects; and differentiates between global and embedded projects. Chapter 3, DXF: Key to the Drawing Structure, goes into some necessary detail about DXF that will add to your understanding of the drawing database. Subclass markers, which are the reflection of AutoCAD's internal object structure, are introduced. We will develop two small VBA projects. One lets you look at selected DXF entity data within an AutoCAD drawing (DWG) file, and the other searches for specified entities in a DXF file. At the end of the chapters we will look at two tiny procedures that do the same thing, one in VBA and one in AutoLISP. You can decide for yourself which is the more accessible of the two languages. Chapter 4, Elements of the Object Model, introduces the AutoCAD object model. We will talk a little more about Automation interfaces, but will concentrate on taking a high-level view of the collections and objects that VBA offers within AutoCAD. Part Two: Using the AutoCAD Object Model Chapter 5, Documents and the User Interface, begins our top-down examination of the object model in detail. In the first part of the chapter we will talk about file management: creating, opening, saving, closing, importing, and exporting drawings. Then we will turn our attention to the user interface, discussing how to control your display and how to handle views and viewports. Chapter 6, Collections and Objects, continues our traversal of the object model with a discussion of the Application and Document objects. We will discuss such document-related functions as layer management and then look at how to manage Collections and access the data within the Objects they contain. Chapter 7, Utility Objects, concludes our introduction of the AutoCAD object model. In this chapter we will concentrate on functions related to creating and editing drawing data, such as Selection Sets: How to select entities in the drawing in order to do something to them. The utility object includes methods for acquiring and converting the formats of data and accessing the Internet. Chapter 8, Blocks and External References, prefaces Chapters 9-11, which deal with the AutoCAD graphic objects, or entities, both two- and three-dimensional. Blocks are complete AutoCAD drawings that have been inserted into other drawings, as symbols in many instances. We discuss Attributes in this chapter, which provide one of the means by which alphanumeric data can be stored and accessed in AutoCAD drawings. External References, which are similar to blocks, exist when other drawings are not actually inserted, but references to them are created so that changes appear automatically when the referenced drawings are updated. In Chapter 8 we will create procedures to rename and redefine XRefs, as they are called, while preserving their insertion instances. Chapter 9, Entities, covers all 23 basic AutoCAD Entities exclusive of the modeled solids and dimensions. We will dwell at some length upon some of the more interesting ones such as the Multiline, seeing how it is constructed and how to create multiline styles using DXF representation. Both kinds of meshes are treated in detail. We will create a Polyface Mesh from a data table stored in a text file and a Polygon Mesh using an Excel VBA macro and a point matrix stored in a worksheet. Chapter 10, Solids, covers the 11 three-dimensional entities in the domain of AutoCAD's solid modeler. In addition to constructing them, we will concentrate on the methods for editing them, creating compound objects using Boolean functions, making sections, and slicing. Finally, we will discuss their mass properties, such as moment of inertia, and what these properties mean. Chapter 11, Dimensions, covers the seven basic dimension types together with the Leader and Tolerance objects. We begin this chapter with a discussion of dimension styles and how to manage them. Creating and using special symbols with the tolerance object for annotation purposes is also covered. The remainder of the chapter is devoted to the dimensioning properties, which are categorized according to AutoCAD's dimension style manager. Each property is defined, along with its corresponding system variable. Chapter 12, Editing, extends our vocabulary of methods for changing and manipulating AutoCAD objects, such as Copy, Move, and the like. We will spend some time on a subject that is often glossed over, the use of geometric transformation matrices. We will develop a procedure that uses the TransformBy method to dynamically zoom and scale a three-dimensional object. Chapter 13, Events, deals with AutoCAD's "inflection points," which occur whenever there is a change of state in the application itself, the drawing you are working on, or an entity. You can write VBA subroutines called event handlers that automatically execute whenever the event to which they are connected occurs. Chapter 14, Forms and Controls, covers the principal user interface with VBA programs. Forms are containers for dialog box controls such as command buttons, list boxes, and the like, through which the user directs the program. After introducing the standard toolbox and some tips for using it, we will spend the balance of the chapter developing a utility application to discover how to integrate the form interface with many of the entity creation and editing functions we have been discussing. The utility application, called Relative, copies or moves selected entities relative to an existing location, using a dialog box. Relative also draws lines and polylines relative to a specified start point. Chapter 15, PaperSpace and Plotting, will discuss Viewports along with the Plot Configuration and Layout objects, two alternative means of formatting drawings. PaperSpace is AutoCAD's environment intended for setting up drawings to print. We will look at the methods for detecting plotter and media characteristics in order to produce the desired output. We will develop a plotting application called BatchPlot, which will format (in paper space) and plot multiple drawings with your desired layer settings. You specify the drawings to be printed using either Visual Basic's common dialog box or a list contained in a text file. Chapter 16, Preferences, deals with parts of the AutoCAD object model that control characteristics of the application itself, its environment, and the means of manipulating it, rather than the drawing itself. These are of two types, preferences stored in the registry, corresponding to the tabs on the AutoCAD's dialog box for user options, plus user options stored in the drawing that can be accessed from other applications without the use of AutoCAD itself. Chapter 17, Menus, gives you the means of altering AutoCAD's main menu, toolbars, and other menus using VBA. Part Three: Communicating with Other Applications and the Internet Chapter 18, Extensibility, will demonstrate how you can use VBA to communicate with other applications. It covers extended entity data in detail, introducing some concepts and test data that will be used more extensively in Chapter 19's project. This chapter also covers working in a zero document state, the VBA interface itself, and a brief section on ARX application handling. Chapter 19, The Facility Project, ties together much of the material presented throughout the book into an application that links an AutoCAD drawing to a Microsoft Access database. The facility project (FP) uses Microsoft's Data Access Objects (DAO) object model to integrate AutoCAD with Access, in order to track area allocation in an office layout. Chapter 20, The DWF Object Model, will go beyond AutoCAD, using and give you an understanding of how to use this additional functionality in querying and displaying drawings in DWF format over the Internet. AutoCAD's Whip! viewer, which is freely downloadable for displaying drawings in a browser, also has a VBA programming interface. We will develop an Excel-based procedure that locates a specified floor drawing and room based on a row selection in a worksheet. Appendices Appendix A, System Variables: A dictionary of the AutoCAD system variables, other than those pertaining to dimensions covered in Chapter 11. Appendix B, Enums: A listing of AutoCAD's enumerated variables, defined integer constants used in developing VBA macros. Appendix C, Object Inheritance: A chart of AutoCAD's COM interface hierarchy. Appendix D, DXF Reference: A selection of the basic graphical entities as they are represented in DXF format. An Excel VBA application is included that will read your DXF files and list the various sections in a conveniently readable format.}
}

@phdthesis{10.5555/2049172,
author = {Amar, Amichi},
advisor = {Krintz, Chandra},
title = {Support for resource constrained microcontroller programming by a broad developer community},
year = {2010},
isbn = {9781124445656},
publisher = {University of California at Santa Barbara},
address = {USA},
abstract = {Resource constrained microcontrollers with as little as several hundred bytes of RAM and a few dozen megahertz of processing power are the most prevalent computing devices on earth. Microcontrollers and the many application components that interface to them, such as sensors, actuators, transceivers and displays are now cheap and readily available. Once costly development tools are now downloadable from the Internet and usable for free. Interest in application development using resource constrained microcontrollers has expanded beyond embedded system engineers to a broad audience of those that include artists, designers, students and product developers in all sectors of industry and fields of research. Developing application software for resource constrained systems is a complex process. The lack of microcontroller resources preclude the use of modern high-level programming languages and operating systems. Modern development practices that support uniform software development across hardware platforms are virtually nonexistent. Additionally, device manufacturers adhere to customer lock-in business practices making compatibility between vendor tools hard to come by and transitions between vendor technologies costly and time consuming. The focus of this dissertation is to support the development of software for resource constrained microcontroller-based systems by an audience with a broad range of technical skills. Our goal is to support uniform development for a diversity of application categories on heterogeneous hardware. Specifically, we design, implement and evaluate a new high-level programming language called Em with constructs and support for modularity, abstraction, software reuse, portability and reconfigurability for differing application requirements, hardware configurations, and quantities of runtime resources. For additional application development support we design, implement in Em, and evaluate a hardware abstraction layer and model for runtime concurrency.Our empirical results indicate that high-level language constructs can effectively be used in a resource constrained environment and achieve at least equivalent resource utilization compared to C and other related systems. We show through a demonstration and evaluation of real applications how we can support modern software development practices for authoring reusable, configurable, portable software for a diversity of hardware platforms. A hardware abstraction layer and a runtime model for concurrency provide additional support for development by, respectively, providing uniform interfaces to hardware functionality and relieving developers from individually implementing concurrency mechanisms. Finally, we conduct a user study with university students showing how non-embedded systems experts and people with generally less technical expertise can successfully learn and develop non-trivial applications with Em.},
note = {AAI3439403}
}

@inproceedings{10.1109/HPCMP-UGC.2010.31,
author = {Morris, Gerald R. and Abed, Khalid H.},
title = {Mapping Hierarchical Multiple File VHDL Kernels onto an SRC-7 High Performance Reconfigurable Computer},
year = {2010},
isbn = {9780769543925},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HPCMP-UGC.2010.31},
doi = {10.1109/HPCMP-UGC.2010.31},
abstract = {The increasing computational requirements of today's software systems have led researchers to investigate ways of accelerating military and scientific computing applications. Contemporary field programmable gate arrays (FPGAs) are now equipped with multimillion gate logic fabrics, faster clock rates, reasonably large on-chip memory, and fast I/O resources for off-chip communication. The use of FPGAs as reconfigurable computational units complementing a fixed computational device such as a general-purpose processor (GPP) is the basic idea behind what are known as high performance reconfigurable computers (HPRCs). These exciting architectures allow development of reconfigurable processors that target the computationally intensive parts of a given application. Ideally, one should use a high-level language (HLL) rather than a hardware description language (HDL) to implement HPRC-based applications. However, in order to accelerate some applications, an HDL must be used to design computational kernels. The HPRC used in the joint research project between the U.S. Army Engineer Research and Development Center {DoD Supercomputing} Resource Center (ERDC DSRC) and Jackson State University (JSU) employs the SRC Computers' Carte development environment. Carte allows application development using a conventional HLL, an HLL-to-HDL compiler, and custom-built VHDL-based kernels ("user macros" in SRC parlance). Currently, the off-the-shelf Carte mechanism for incorporating user macros does not directly support the common case of a multiple file VHDL hierarchy. This research explores a novel approach that allows multiple file VHDL kernels to be mapped onto the SRC-7 HPRC. The approach facilitates the development of FPGA-based elements via a hybrid technique that uses the Carte HLL-to-HDL compiler in conjunction with multiple file VHDL-based user macros. This paper describes the use of this novel approach to map a parameterized, parallelized, and pipelined FPGA-based sparse matrix vector multiply kernel onto an SRC-7 HPRC. The HPRC-based version runs nearly four times faster than the software-only version.},
booktitle = {Proceedings of the 2010 DoD High Performance Computing Modernization Program Users Group Conference},
pages = {524–533},
numpages = {10},
keywords = {sparse matrix, reconfigurable computer, VHDL, FPGA},
series = {HPCMP-UGC '10}
}

@article{10.1155/2021/3256924,
author = {Zhang, Yibo and Tang, Jianjun and Huang, Hui and Tsai, Sang-Bing},
title = {Motion Capture and Intelligent Correction Method of Badminton Movement Based on Machine Vision},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/3256924},
doi = {10.1155/2021/3256924},
abstract = {In recent years, badminton has become more and more popular in national fitness programs. Amateur badminton clubs have been established all over the country, and amateur badminton events at all levels have increased significantly. Due to the lack of correct medical supervision and health guidance, many people have varying degrees of injury during sports. Therefore, it is very important to study the method of badminton movement capture and intelligent correction based on machine vision to provide safe and effective exercise plan for amateur badminton enthusiasts. This article aims to study the methods of motion capture and intelligent correction of badminton. Aiming at the shortcoming of the mean shift algorithm that it is easy to lose the target when the target is occluded or the background is disturbed, this paper combines the mean shift algorithm with the Kalman filter algorithm and proposes an improvement to the combined algorithm. The improved algorithm is added to the calculation of the average speed of the target, which can be used as the target speed when the target is occluded to predict the area where the target may appear at the next moment, and it can also be used as a judgment condition for whether the target is interfered by the background. The improved algorithm combines the macroscopic motion information of the target, can overcome the problem of target loss when the target is occluded and background interference, and improves the robustness of target tracking. Using LabVIEW development environment to write the system software of the Japanese standard tracking robot, the experiment verified the rationality and correctness of the improved target tracking algorithm and motion control method, which can meet the real-time performance of moving target tracking. Experimental results show that 83% of amateur badminton players have problems with asymmetric functions and weak links. Based on machine vision technology, it can provide reliable bottom line reference for making training plans, effectively improve the quality of action, improve the efficiency of action, and promote the development of sports competitive level.},
journal = {Mob. Inf. Syst.},
month = {jan},
numpages = {10}
}

@book{10.5555/1524104,
author = {Balter, Alison},
title = {Application Development with Microsoft Access 2007},
year = {2008},
isbn = {0672330210},
publisher = {SAMS},
address = {USA},
edition = {1st},
abstract = {Application Development with Microsoft Access 2007 Live Lessons Microsoft Access/Databases For Access power users, programmers, and anyone who wants to master Access 2007 development fast. In Application Development with Access 2007, renowned Access developer, trainer, and author Alison Balter teaches all the skills you need to build professional-quality Access 2007 applications. This package brings together more than twelve hours of personalized, expert video training: 100+ quick, practical video lessons that demonstrate all the skills you need to build virtually any Access application. Youll learn one step at a time, at your own paceusing hands-on examples that reflect realistic development challenges and showcase Access 2007s remarkable capabilities. Application Development with Access 2007 delivers the power of the best classroom training at a small fraction of the cost. If you dont have time to read a huge book on Access development, this is exactly what youve been searching for! Looking for a better way to master todays rapidly changing programming technologies? Want expert help, but dont have the time or energy to read a book? Cant find classroom training worth the money? Discover LiveLessons: self-paced, personal video instruction from the worlds leading technology experts. LiveLessons are video courses, on DVD with a book supplement, that are organized into bite-sized, self-contained sessionsyoull learn key skills in as little as fifteen minutes! Each lesson begins with well-defined learning objectives and ends with comprehensive summaries, which help you track your progress. Follow along as your instructor shows how to get results in todays top development environments: including Microsofts Visual Studio and eclipse.orgs Eclipse. We are reaching more and more for video alternatives because they make sense! The imprints of Pearson Technology Group are the most trusted source of quality technology books, and they will be the brands we turn to for visual learning. Leo J. Hauguel, Information Security Analyst 5 Wells Fargo Services Company Alison Balter, one of the worlds most experienced Access developers and trainers, has created Access applications ranging from small end-user projects to enterprise-wide systems. Her thirteen books on Access include Alison Balters Mastering Microsoft Office Access 2007 Development. System Requirements Operating System: Windows 98, 2000, XP, or Vista; Mac OS X; versions of Linux with the Flash 8 Player or later Multimedia: DVD drive, an 800x600 or higher display, and sound card with speakers Computer: 500MHz or higher, 128MB RAM or more $69.99 U.S. / $76.99 CANADA / 44.99 Net UK}
}

@inproceedings{10.5555/520047.854883,
author = {Ushakov, I. and Coleman, D. and Costa, L. da},
title = {Object Technology},
year = {1999},
isbn = {0769500684},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This panel discusses the impact OT has made and is making on traditional software development methodologies. The scope of the panel is defined by, but not limited to, the following areas: - Object-Oriented (OO) Modeling and Design; - OO Architectures and Frameworks; - Distributed objects and web technology; - Roadmap to OT adoption, - Transition to OT; - OT and Standardization.The goal of this panel session is to study the aforementioned topics, define research areas and state-of-the-art in more detail, and to identify open problems.1. OT and software development. OO Analysis and UML are currently combined with various requirements analysis techniques, e.g., goal-oriented analysis, user centered work modeling, user interaction scenarios and business modeling. While a domain model and use-case model describe a system as a black box, a logical architecture (analysis model) describes what the system's structure [2, 3]. Layered object architecture facilitates better design of software components.2. OO Architectures and Frameworks. Some major impacts of OT on software development practice are definition and usage of OO frameworks and design patterns to facilitate reuse of code and design. Software architecture is a conceptual model defining system components, their interfaces and how they interact between each other. Frameworks provide services and mechanisms for a set of components and can define structural relationships between components. Components vary in granularity and have value within specified environments. OMG Business Object Component Architecture serves as an architectural template for defining domain- and technology-specific architectures and frameworks.3. OT and Web Technology. Internet middleware architecture adopted a concept of "object Web", i.e., universal connectivity, heterogeneous distributed environments, and cross platform interoperation of both infrastructure and applications. Three major drivers: CORBA, DCOM and Enterprise JavaBeans.4. OT adoption. A successful OT adoption is normally based on a plan that includes: resources, scope, management, process, technology and training.5. OT and Standardization. UML is a standard modeling language for OO Analysis and Design. It has been widely recognized and is currently used in many areas. An OO software development process (SDP) is harder to standardize. A Unified Rational Approach which can serve as a framework for definition of OO SDP specifics to a particular project or software development environment. CORBA standards specify many services that can be used in a consistent manner across platforms. More work has to be done to standardize components.},
booktitle = {Proceedings of the 4th IEEE International Symposium and Forum on Software Engineering Standards},
pages = {245},
series = {ISESS '99}
}

@phdthesis{10.5555/1368745,
author = {Pyla, Pardha S.},
advisor = {Hartson, H. Rex},
title = {Connecting the usability and software engineering life cycles through a communication-fostering software development framework and cross-pollinated computer science courses},
year = {2007},
isbn = {9780549322313},
publisher = {Virginia Polytechnic Institute &amp; State University},
address = {USA},
abstract = {Interactive software systems have both functional and user interface components. User interface design and development requires specialized usability engineering (UE) knowledge, training, and experience in topics such as psychology, cognition, specialized design guidelines, and task analysis. The design and development of a functional core requires specialized software engineering (SE) knowledge, training, and experience in topics such as algorithms, data structures, software architectures, calling structures, and database management. Given that the user interface and the functional core are two closely coupled components of an interactive software system, with each constraining the design of the other, there is a need for the SE and UE life cycles to be connected to support communication among roles between the two development life cycles. Additionally, there is a corresponding need for appropriate computer science curricula to train the SE and UE roles about the connections between the two processes.In this dissertation, we connected the SE and UE life cycles by creating the Ripple project development environment which fosters communication between the SE and UE roles and by creating a graduate-level cross-pollinated SE-UE joint course offering, with student teams spanning the two classes, to educate students about the intricacies of interactive-software development. Using this joint course we simulated different conditions of interactive-software development (i.e. with different types of project constraints and role playing) and assigned different teams to these conditions. As part of semester-long class projects these teams developed prototype systems for a real client using their assigned development condition. Two of the total of eight teams in this study used the Ripple framework.As part of this experimental course offering, various instruments were employed throughout the semester to assess the effectiveness of a framework like Ripple and to investigate candidate factors that impact the quality of product and process of interactive-software systems. The study highlighted the importance of communication among the SE and UE roles and exemplified the need for the two roles to respect each other and to have the willingness to work with one another. Also, there appears to exist an inherent conflict of interest when the same people play both UE and SE roles as they seem to choose user interface features that are easy to implement and not necessarily easy to use by system’s target users. Regarding pedagogy, students in this study indicated that this joint SE-UE course was more useful in learning about interactive-software development and that it provided a better learning experience than traditional SE-only or UE-only courses.},
note = {AAI3288673}
}

@inproceedings{10.1145/1509239.1509241,
author = {Daugherty, Paul R.},
title = {The future of software architectures for large-scalebusiness solutions: modularity, scalability, andseparation of concerns},
year = {2009},
isbn = {9781605584423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1509239.1509241},
doi = {10.1145/1509239.1509241},
abstract = {Modern software projects are of large scale, often involving years of development, tens of thousands of days of work effort, and millions of lines of code. This complexity is aggravated by the fact that development is often distributed over several geographic locations, as dictated by cost considerations, the availability of domain specialists, legal requirements, and other factors. Despite advances in development tools and techniques, software initiatives have lagged behind in utilizing novel software engineering methods and techniques effectively to reduce the complexity of large-scale software. The results can be seen in Corporate and Government IT budgets - based on Accenture and Industry research, IT cost overruns are still commonplace, and the cost to "keep the lights on" for fragile legacy applications typically consumes up to 60% IT budgets.Increased adoption of advanced software engineering techniques holds great promise for solving these key business challenges. For example, modularization holds a promise in reducing complexity of software design by hiding low-level implementations in well-defined units of deployment. Specifically, vendors build platforms that allow architects to design large-scale systems that can be composed out of services on the fly. For example, JBoss built a platform that allows architects to seamlessly integrate Service-Oriented Architecture (SOA) with application and business-process management (BPM) in enterprise distributions. According to research with Accenture's CIO Council, at least 58% of global organizations are implementing or piloting SOA.Proper separation of concerns is a key to effective modularization. While separation of concerns is more of art than science in the work of software architects, novel technologies that enable effective separation of concerns are gaining traction. Most recently Accenture used AOP on a government project to do audit tracking. It was very successful and the techniques are currently being incorporated in Accenture Delivery Architecture (ADA), which is a standards-based architecture used for very large scale software development. A large focus of our efforts is in making advanced software engineering techniques more "consumable" by across our network of developers - this is done through standardized architectures, reference applications, and training.The talk will focus on progress that has been made, and challenges ahead in driving further business value through use of these types of techniques. In addition to the areas mentioned, we will focus on related issues such as: requirements traceability, automated software quality assurance, role of DSLs andMDA, and Agile techniques.},
booktitle = {Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development},
pages = {1–2},
numpages = {2},
keywords = {soa, aop},
location = {Charlottesville, Virginia, USA},
series = {AOSD '09}
}

@article{10.1155/2021/7106104,
author = {Wu, Shumei and Lv, Zhihan},
title = {Intelligent Communication Management Terminal in the Construction of Human Resource Management Mode},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/7106104},
doi = {10.1155/2021/7106104},
abstract = {With the rapid development of the economy, the integration of corporate strategic management and human resource management has become an issue of concern. This research mainly discusses the role of intelligent communication management terminal in the construction of human resource management mode. In this research, the system development process of this research mainly uses the class library in the software architecture layer to support the software development process. The main development language of Android, JAVA, is to install the Android Develop Tools plug-in on eclipse and install the Android SDK in the computer operating system to build the Android development environment. The development and application of the system not only make the enterprise managers more convenient and efficient in the process of managing the enterprise but also smooth the operation of the enterprise while reducing the human resource investment and also gives the employees more right to know and the right to participate in the enterprise construction. By creating more value while reducing human resource input, enterprises will enable it to obtain more benefits, and thus enter a cycle of good development and contribute to society. The system has the functions of personnel management, recruitment management, attendance management, training management, work management, and salary management. The recruitment management function of the system is mainly composed of recruitment plan management, recruitment information management, and talent pool. In the system’s recruitment plan management function, important information such as the recruitment part, the number of recruits, personnel requirements, and the specific arrival time of the personnel must be clarified. The personnel in charge of the enterprise personnel department shall conduct corresponding regulations according to the specific needs of the enterprise and shall be experienced by the personnel department. The review is carried out, and all parts of the enterprise are coordinated and completed at the same time. In the platform performance test, when the number of concurrent users reaches 50000, the request time is about 6 seconds, which meets the requirement that the response time of 10000 people per second is less than 10 seconds. This research puts forward suggestions and countermeasures for the optimization of human resource management, which can not only improve the efficiency of Y company’s human resource management but also provide useful reference and reference for other enterprises facing the same problem.},
journal = {Wirel. Commun. Mob. Comput.},
month = {jan},
numpages = {14}
}

@phdthesis{10.5555/AAI29137972,
author = {Pe\~{n}aherrera, Esteban Eduardo Cando},
advisor = {Carlos, Grilo, and Jose, Ribeiro, and Maria, Lapina,},
title = {A Conversational Agent to Assist Users in Public Institutions of Ecuador},
year = {2020},
isbn = {9798835533800},
publisher = {Instituto Politecnico de Leiria (Portugal)},
abstract = {This project described in this dissertation is being carried out for the Ministry of Social and Economic Inclusion (MIES) of Ecuador based on its current problems. The de ciencies that government entities have to help their citizens with information and paperwork has exceeded their operational capacity. The MIES currently has a Marketing and Information department that is saturated with questions through channels such as call centers, information centers in their of- ces and social networks outside their o ce hours.In recent years there has been an increase in requests for information through digital channels such as its Facebook page, since internet access to places far from the big cities has increased. This has been seen as an opportunity for improvement by creating an chatbot agent that helps citizens 24 hours a day, 7 days a week. Most of the queries can be made through a query to their internal data and this allows the operational burden of the operators to be released to carry out management processes and not just information.Many public and private companies have resorted to using chatbots to help their users with simple information tasks. They have also relied on these technological tools to create registration and management functions. Today we can use chatbots development tools and platforms such as Dialog ow that allow to create chatbots in a manageable and scalable way according to needs. Facebook has multiple connection tools that allow it to integrate safely and e ciently with its platforms such as Facebook Messenger, allowing customers to create applications that will reach the majority of citizens who have internet access.Developing a chatbot agent can be a simple and straightforward task, but in which the remaining time must be invested in training and helping the chatbot agent to understand words or idioms of language used in di erent regions of the same country. In the development of a chatbot, the training cycle is constant and allows the chatbot to increase its ability to understand the users who use them.In the tests carried out on this chatbot, it was concluded that although the use of these tools allows to release the operational load, it does not avoid the need for users to interact with natural persons, but the requirement is drastically reduced. In the tests it was also obtained that many problems can be obtained by nding a wide range of synonyms and local expressions, but correct training and constant review of the answers provided helps the chatbot to self-train and evolve its answers over time interactions.},
note = {AAI29137972}
}

@inproceedings{10.1145/3568812.3603473,
author = {Chen, Melissa and O'Rourke, Eleanor},
title = {Designing a Real-Time Intervention to Address Negative Self-Assessments While Programming},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603473},
doi = {10.1145/3568812.3603473},
abstract = {Enrollments in university-level introductory computing courses are skyrocketing [3], but many students struggle in these courses [2]. Recent research suggests that student perceptions of the programming process may contribute to this problem. Students often have inaccurate expectations of programming that may lead them to negatively assess their abilities in response to natural programming moments [6]. For example, many students believe they are doing poorly when they use resources to look up syntax, even though this is considered good practice [7]. This is important because negative self-assessments correlate with lower self-efficacy [6], or one’s belief that they can achieve a goal [1], and students with lower self-efficacy tend to exhibit lower persistence in undergraduate computing programs [9]. In this poster, we present an initial design and evaluation of an intervention that aims to reduce overly negative self-assessments and improve self-efficacy by providing real-time feedback as students program. We created an extension to the jGRASP development environment [4] that delivers feedback messages in response to eight self-assessment moments that can be automatically detected by an expert system developed in prior work [5] (see Figure 1). Informed by recommendations from the feedback literature [8, 10], we developed six messages for each moment that aim to help students develop more accurate expectations by normalizing the moment or highlighting how it could support future growth (see Figure 2). By delivering this feedback automatically, in real-time, and in the context of the task, this intervention aims to address negative self-assessments as they occur. This approach has been successful in other domains [8, 10] and allows us to provide individual feedback at scale, which is particularly challenging as course enrollments grow [3, 11].  We conducted a formative user study with 10 CS1 and 11 CS2 students to understand how they perceived the intervention and which feedback messages they preferred, with the goal of informing future design iterations. First, participants completed a modified version of the survey from [6] to measure their self-efficacy and self-assessments; this served as a pretest. Then, they worked on a programming problem with the intervention for twenty minutes. Finally, participants completed the same survey as a posttest and we interviewed students about their reactions to the intervention as they watched a video of their session. The pretest results showed that many participants do not negatively self-assess in response to these eight programming moments, which is surprising since previous research with other populations has found that negative self-assessments are common [6]. Our preliminary analysis indicates that some participants found the messages reassuring and timely while others found them unhelpful. Participants expressed preferences for some message designs over others, and overall the feedback resonated most with participants who had more negative self-assessments or struggled more on the programming problem. Based on this feedback, we are refining the intervention and collecting data with other populations ahead of a summative evaluation to measure the intervention’s impact on self-assessments and self-efficacy.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {9–10},
numpages = {2},
keywords = {self-efficacy, self-assessments, CS1},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/100348.100412,
author = {Lewis, John A.},
title = {An experiment to determine software reusability factors (abstract)},
year = {1990},
isbn = {0897913485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/100348.100412},
doi = {10.1145/100348.100412},
abstract = {Software reusability has been proclaimed as the common sense solution to many software development problems. The concept of reusability promotes productivity because it avoids “reinventing the wheel.” Using existing components which are similar to the current needs can be much faster than creating components from scratch. Reusability can also be viewed as promoting reliability since reused components have the benefit of both experimental and field testing.However, reusability has not fulfilled its potential for revolutionizing the software development industry. Identifying the factors which cause current reuse efforts to fail is essential to its later success. Likewise, identifying the factors that seem to promote successful reusability is equally important. Furthermore, practical ways to eliminate the detrimental factors must be developed.An experiment designed to ferret out the causes of software reuse success and failure must consider several important issues: (1) The experiment must consist of actual development and reuse. Questionnaires and subjective measurements about whether to reuse, etc. are necessary but not sufficient. (2) The experiment must be greatly controlled to avoid extraneous factors from skewing the results. Factors which might influence the outcome must be deliberately tested for, or controlled such that they do not bias the experimental data. (3) The components to be reused must be determined. Reusing requirements and designs has been suggested, but with little success. On the other hand, reusing test cases has been greatly successful. In between is code. Current experiments should still concentrate on the ability to reuse source code. You must walk before you run. (4) Finally, the factors being tested must be established and they must consider two main tangents. First, specific factors concerning the code characteristics, the organization of components, and the development environment must be considered. Other concerns deal with the human factors. Predisposition, ego, training and skill must be taken into account for an accurate study of reusability.A current reusability experiment concentrates on the use of an object-oriented organization scheme, reusable code characteristics, and several human factors. The experimental subjects actually design and implement code under varying conditions. Subjects are divided into groups that must reuse whenever possible, may reuse if desired, and cannot reuse at all. Comparing the results of the various groups will lead to a better understanding of the problems faced in software reusability.},
booktitle = {Proceedings of the 1990 ACM Annual Conference on Cooperation},
pages = {405},
location = {Washington, D.C., USA},
series = {CSC '90}
}

@book{10.5555/1405668,
author = {Baker, Art and Lozano, Jerry},
title = {Windows® 2000 device driver book: a guide for programmers, second edition, the},
year = {2000},
isbn = {0130204315},
publisher = {Prentice Hall Press},
address = {USA},
edition = {Second},
abstract = {The #1 Windows device driver book-fully updated for Windows 2000! Step-by-step planning, implementation, testing, debugging, installation, and distribution Complete coverage of the new Windows Driver Model (WDM) Practical debugging and interactive troubleshooting CD-ROM: Exclusive tools for streamlining driver development, plus extensive C/C++ sample driver library! Windows Driver Model (WDM) for Windows 2000 and 98-in depth! Building drivers that support Plug-and-Play and Power Management Windows Management Instrumentation: logging device errors and events-and interpreting them Constructing safe reentrant driver code Handling time-out conditions safely and effectively Advanced techniques: kernel-mode threads, layered drivers, and more Start-to-finish debugging and troubleshooting techniquesForeword by Andrew Scoppa, UCI CorporationThe #1 book on Windows driver development-totally updated for Windows 2000!With The Windows 2000 Device Driver Book, any experienced Windows programmer can master driver development start to finish: planning, implementation, testing, debugging, installation, and distribution. Totally updated to reflect Microsoft's Windows Driver Model (WDM) for Windows 2000 and 98, this programming bestseller covers everything from architecture to tools, and includes a powerhouse library of exclusive tools and code for streamlining any driver development project.You'll start with a high-level overview of WDM components and then move quickly into the details of the development environment and driver installation. Next, master the Windows 2000 I/O Manager, its data structures, and its interaction with drivers. Using extensive practical examples, you'll implement Plug-and-Play and Power Management; construct safe reentrant driver code; use Windows Management Instrumentation to log errors and events, and more.The book covers today's most advanced Windows driver development techniques and provides extensive debugging guidance, including crash dump analysis using WinDbg; lists of common bugcheck codes, meanings, and probable causes; and much more.About the CD-ROMBonus CD-ROM contains powerful resources for streamlining device driver development! An exclusive Device Driver AppWizard that works with Visual Studio to instantly create your driver's framework A library of complete sample drivers C++ classes to jumpstart any project-including a Unicode string handling class that eliminates tedious, repetitive code An exclusive Driver Installation Utility to simplify initial testingUCIUCI Software Training Centers specializes in high-end developer, systems, and Internet Training on Microsoft products and technologies. For more information about training in this topic and others, UCI can be reached at 800-884-1772, or on the Web at www.ucitraining.com}
}

@book{10.5555/2597859,
author = {Friedenthal, Sanford and Moore, Alan and Steiner, Rick},
title = {A Practical Guide to SysML: The Systems Modeling Language},
year = {2011},
isbn = {9780123852076},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {A general purpose graphical modeling language used to specify, analyze, and design systems that may include hardware, software, and personnel, SysML is now being adopted by companies across a broad range of industries, including aerospace and defense, automotive, and IT system developers. This book is the bestselling, authoritative guide to SysML for systems and software engineers, providing a comprehensive and practical resource for modeling systems with SysML. Fully updated to cover newly released version 1.3, it includes a full description of the modeling language along with a quick reference guide, and shows how an organization or project can transition to model-based systems engineering using SysML, with considerations for processes, methods, tools, and training. Numerous examples to help readers understand how SysML can be used in practice, while reference material facilitates studying for the OMG Systems Modeling Professional (OCSMP) Certification Program, designed to test candidates knowledge of SysML and their ability to use models to represent real-world systems. Authoritative and comprehensive guide to understanding and implementing SysML A quick reference guide, including language descriptions and practical examples Application of model-based methodologies to solve complex system problems Guidance on transitioning to model-based systems engineering using SysML Preparation guide for OMG Certified Systems Modeling Professional (OCSMP) Table of Contents Part I Introduction Systems Engineering Overview Model-Based Systems Engineering3 SysML Language Overview SysML Language Overview Part II Language Description SysML Language Architecture Organizing the Model with Packages Modeling Structure with Blocks Modeling Constraints with Parametrics Modeling Flow-Based Behavior with Activities Modeling Message-Based Behavior with Interactions Modeling Event-Based Behavior with State Machines Modeling Functionality with Use Cases Modeling Text-Based Requirements and their Relationship to Design Modeling Cross-Cutting Relationships with Allocations Customizing SysML for Specific Domains Part III Modeling Examples Water Distiller Example Using Functional Analysis Residential Security System Example Using the Object-Oriented Systems Engineering Method Part IV Transitioning to Model-Based Systems Engineering Integrating SysML into a Systems Development Environment Deploying SysML into an Organization APPENDIXES A-1 SysML Reference Guide A-2 Cross Reference Guide to the OMG Systems Modeling Professional Certification Program (OCSMP) - NEW}
}

@book{10.5555/535835,
author = {Miller, Nancy E.},
title = {File Structures Using Pascal},
year = {1987},
isbn = {0805370838},
publisher = {Benjamin-Cummings Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: Preface: Motivation After teaching file processing courses for years using COBOL as the vehicle language, I concluded that the students do learn to use COBOL for a variety of file organizations (sequential, indexed sequential, and relative) but do not gain an understanding of the data structures involved in implementing the more complex file structures such as direct files and indexed sequential files. A programming language with less support for file organizations than COBOL allows the students to gain greater in-depth knowledge about the implementation of routines that access data structures on external files. Pascal, with its support for relative files, fills this need. Goal This textbook meets the requirements for The Association for Computing Machinery (ACM) course CS5 as defined in the ACM curriculum guidelines. The goal of the book is to study the external data structures necessary for implementing different file organizations. Most texts currently available present file processing by using languages such as COBOL or PL/1, which have built-in support for direct access and indexed sequential access. Pascal does not have this built-in support. Instead, this language can be used in a more practical, prdagogical way by allowing students to gain more in-depth file implementation experience as they analyze data structures for efficiency and write their own access routines. The algorithms in this book are presented in a Pascal-like pseudocode, which provides students with a familiar environment in which to study the key concepts and structures necessary to implement a variety of file organizations.Datastructures such as trees, linked lists, stacks and queues are studied and analyzed for efficient use in the implementation of various file organizations. By using a superior pedagogical language such as Pascal and analyzing key data structures, students will gain a better understanding of design analysis and the implementation of file organization. Level/Audience/Prerequisites The prerequisites for the course addressed by this book, CS5, are two semesters of Pascal; in other words students should already have taken ACM CS1 and CS2 courses. Third-semester Computer Science majors constitute the primary audience for this book. Organization and Coverage Chapter 1 presents a conceptual overview of the file-processing environment, including discussions of common file organizations, file types and characteristics, and different ways of manipulating files as factors that affect file design. Several of the examples of file applications in this chapter are referenced in later chapters. Chapter 2 reviews the syntax for declaring and using records and for declaring and accessing files in Pascal. This chapter may be omitted for those students with a good understanding of Pascal records and files. Chapter 3 deals with the topics of blocking and buffering of records in a file. The central theme of the chapter is the fewer I/O operations required for a program that accesses a blocked file and the reduction of the time that the CPU waits for an I/O operation to be completed when using a buffered file. Interfacing algorithms for record blocking and deblocking are also presented. Quantitative measures of the effects of blocking and buffering effects are given solely in terms of the number of I/O accesses. Chapter 4 describes external storage devices as a background for understanding the impact of storage devices on file design and manipulation. Quantitative measures of the effects of blocking and buffering (similar to those in Chapter 3) are repeated in terms of physical access time of the devices using various blocking factors and different numbers of buffers. Chapter 5 deals with the design and maintenance of sequential files on both sequential and random-access storage devices. Algorithms for maintenance of sequential files stored on sequential devices are contrasted with algorithms for maintenance of sequential files stored on random-access devices. Sample data for a car-rental agency are used for implementating a sequential file. Quantitative measures of access times are given. Chapter 6 describes external sort/merge techniques, which are necessary for sorting very large sequential files. Sorting is a common file-processing task, especially for manipulating sequential files. Sorting methods discussed at length are the two-way merge, the balanced k-way merge, and the polyphase merge. These methods are compared in terms of the number of merge cycles and external storage devices needed for several example data sets. Chapter 7 begins with a discussion of the basic structures of direct files. A variety of techniques are presented for obtaining random access to data files, including the use of hashing. Examples are used to illustrate several methods for handling hashing collisions. Also included are some algorithms for creating and maintaining random-access files in versions of Pascal with the random-access files extension. In order to compare random and sequential access, the car-rental agency data used in Chapter 5 are stored in a random-access file and quantitative measures of access times are computed. Chapter 8 describes several types of tree structures that are useed to accessing random-access files sequentially. The most important tree structure is the B-Tree, and ways of representing and manipulating the B-tree are discussed along with accompanying algorithms. The chapter also discusses the application of trees that allow sequential and random access to the car-rental agency data file created in Chapter 7. Chapter 9 describes common implementations of indexed sequential organization, including implementations that use a tree structure, such as a B+-Tree, for the indexes. The chapter studies Scope Indexed Sequential files used on CDC computers, cylinder-and-surface-indexed sequential files (ISAM) used on IBM computers, and VSAM file organization used on IBM computers. Also included are algorithms for implementing indexed sequential files using a variety of data structures. Applications include the car- rental agency data, and access times for sequential, random, and indexed sequential files are compared. Chapter 10 investigates other types of file organization that use linked lists or tree structures to provide multiple-key access to random-access data files. Included in this chapter is a discussion of inverted files and multilist files along with creation and manipulation algorithms. The car-rental agency data are implemented as an inverted file and multilist files to provide access by several keys. Quantitative measures of access times are given by comparing these file organizations with others discussed previously. Outstanding Features Pedagogy Case Studies: Chapter 5 introduces a case study based on an actual car-rental agency, and this case study is used throughout the book as an on-going example illustrating practical file concepts and issues. Additional practical, real-world case studies are presented in Chapter 2. They include discussions of an inventory of products, student class schedules, and the assignment of course grades for a class. Examples/Illustrations: Throughout the book algorithms are presented in Pascal-like pseudocode. Students learn best by working with files of varying organizations rather than just reading about them. A variety of exercises and programming projects have been provided that illustrate the creation and manipulation of files for each type of organization. Students can implement the algorithms from the book in a hands-on, file organization programming environment, thus gaining experience and greater knowledge of all key concepts. The program solutions for these exercises are available from the author. Solutions to odd- numbered exercises are provided at the back of the book while the solutions to even-numbered exercises are provided in the Instructor's Guide. Glossary/Key Terms: Key terms are highlighted and defined as they occur in the test, and are also included in the glossary in the end of the book. Class Tested: This book was thoroughly class tested for six semesters in a sophomore-level file structures course. The readability of the book was greatly enhanced because of student and reviewer feedback over the course of several drafts. The Use of Pascal Chapter 2 reviews the Pascal syntax for declaring, using, and accessing Pascal records and files. This chapter can be omitted for those students with a good understanding of Pascal. The Pascal syntax for records, linked lists and trees is included in corresponding sections. The first two sections of the book (I and II) reference the ISO standard Pascal, while sections III and IV require random-access files, which are not included in the ISO standard. Most versions of Pascal have been extended to allow random access (OMSI Pascal, TURBO Pascal, UCSD P-System Pascal, and VAX-11 Pascal, to name a few). Appendix A includes a sequential simulation of random access using arrays in internal memory as an easy alternative to those versions of Pascal without random access. The syntax for random access for various versions of Pascal is included in Appendix B. Instructor's Guide The accompanying Instructor's Guide includes: guidelines for presenting the material in each chapter additional examples for classroom use, including points to be emphasized solutions to all even-numbered exercises transparency masters of various illustrations and tables from the book quizzes for each chapter Software: A disk of solutions to all programming problems is available from the author for a nomimal fee to all instructors. Acknowledgements I am grateful to the numerous individuals that have helped me in preparing this book. I am indebted to the faculty of the Computer Science Department of Mississippi State University for providing equipment and an environment conducive to writing a book. My thanks also go to the reviewers: James D. Schoeffler, Cleveland State University; Rayno D. Niemi, Rochester Institute of Technology; James Blahnik, St. Norbert's College; Medhi Owrand, University of Oklahoma; Robert Uzgalis, University of California at Los Angeles; Walter Scacchi, University of Southern California. Special thanks to the students in my classes who corrected typing errors in earlier versions of the book. I express my appreciation to my Editor Alan Apt and all those individuals at Benjamin/Cummings who have organized the reviewing and production of this book. Finally, I thank my husband for his continued support, encouragement, and understanding. Nancy E. Miller}
}

@book{10.5555/558603,
author = {Deitel, Harvey M. and Deitel, Paul and Nieto, Tem},
title = {The  Complete XML Programming Training Course},
year = {2001},
isbn = {0130895571},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Publisher: Includes the #1 XML Interactive Training Courseware: XML Programming Multimedia Cyber Classroom! Quickly master XML markup and programming with 10+ hours of detailed audio explanation of 10,000+ lines of sample XML document and program code Learn to program XML applications using Java, VBScript, ASP, and Perl Powerful programs show you how to manipulate XML documents using XSL, XSLT, and XPath Coverage includes schemas, DTDs, DOM, SAX, SOAP protocol, VoiceXML, XML Query, SMIL and more Includes the worlds #1 XML textbook: XML How to Program in print and in fully-searchable electronic format on CD-ROM. Master XML development - hands-on! Youre holding the fastest, most effective way to master real-world XML development! This start-to-finish multimedia training course and book package covers key XML technologies and skills. You wont just learn basic XML markup: youll discover how to build effective DTDs and schemas, write powerful Java applications using DOM and SAX, and work with the powerful technologies that build on XML - including XPath, XSL, XSLT, VoiceXML, and much more. Its everything you need, in one box! XML programming multimedia cyber classroom 10+ hours of detailed audio explanations step you through 10,000+ lines of fully tested program code from 250+ complete XMLdocuments and programs Hundreds of interactive self-review questions and programming exercises to test your knowledge CD-ROM includes fully searchable electronic copy of the book XML How to Program Hundreds of tips to avoid and troubleshoot problems - and maximize performance, interoperability, and reusability Includes Copy of XML How to Program The worlds #1 XML programming developers guide Start-to-finish, 900-page guide to XML Friendly, practical, and full of examples Hands-on tutorials with tips and tricks for troubleshooting and optimization Coverage includes: XML, DTDs, schemas, DOM, SAX, XSLXSLT, XLink, XML Query, SMIL, VoiceXML, and more! Real-world insight into leading XML applications and scripting tools: SOAP, BizTalk, CDF, RDF, Perl, VBScript, Java, ASP, and more Bonus 2nd CD-ROM! Packed with live XML source code, links to hundreds of the Webs best XML resources, and all these great XML development tools: Amaya, Cocoon, Jakarta Tomcat, Xalan, FOP (XSLT), Xerces, Crimson, and more! Raves for Deitel training courses! Im an adult student currently in an OOC++ class (earning my bachelors in Computer Info. Systems). Our instructor is great in C and Java but has never taught OOC++. But, today, Ive been blessed. I found your Complete C++ Training Course. I also plan on learning Java as well, and now, thanks to your Java Course, I can fulfill my dream. - Broxi Thomas, University student I wanted to offer kudos for your product. If every training CD utilized the same format, then learning new products would be easy. The format, resources and layout of the classroom is refreshing and useful. It is clean, quick, and effective. I began with Chapter One and took notes and tried every exercise; reviewed and listened to every example. I understand the material so much better now that I am amazed. PLEASE produce cyber classrooms for EVERY software package and technology that is in existence today. You would be doing a beyond the call of duty service to the computer industry. Of course, you would be a little busy for the next millennium.... :) - Jonathan Gravois SPECIALLY DESIGNED FOR All developers seeking to master XMLWeb professionals building on their existing HTML skillsContent developersEnterprise application integrators System Requirements Windows 9598NT 4.x2000Up To 120MB disk space64MB RAM8x CD-ROM drive &amp; sound card support}
}

@book{10.5555/993932,
author = {Anderson, George W.},
title = {mySAP Tool Bag for Performance Tuning and Stress Testing},
year = {2004},
isbn = {0131448528},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {“A good book for people who deal with SAP systems for a living. I haven't read another book like this. It's technical but it's also an entertaining read. A pleasant departure from the norm.” -David C. Gilliland, Senior Consultant, SAP America, Inc. “Clearly, this book could be used as an excellent development tool and could help a company like mine perfect performance tuning steps and standards.” -Dennis Prince, SAP Development/Support Specialist, Hewlett-Packard “SAP optimization is one of those subjects that developers struggle to find time for and managers don't know is necessary. Presenting the value of the process up front serves to give the developer ammunition to win time for optimization and the manager an education in the value and necessity of optimization. Even if the manager types don't read past chapter 3, George's job is done. They should be convinced that someone technical in their IT department needs to be reading this book. Anderson explains the value, then the core technology, and then when we're all on the same page, the process. To me, that was very helpful.” -Crew Reynolds, Software Development Manager, Daydots “This book features good discussion on performance tuning the mySAP suite that no other books have so far. This is the perfect book for SAP Stress Test Project Managers, SAP Stress Test Project Teams, SAP Basis Administrators, Oracle DBAs, Unix Administrators managing SAP systems, and project implementation teams. Those who stress test their systems well with the help of this book will have significant returns.” -Sanjoy Rath, SAP ConsultantDrive maximum performance and value from your SAP investment!In this book, a leading expert on SAP performance walks through every facet of tuning and optimizing mySAP Solutions, and the technology layers underpinning these solutions, to maximize performance and value. George W. Anderson covers the entire testing and tuning process: planning, staffing, developing, testing, executing, validating, evaluating...and acting on what you've learned.Anderson offers unparalleled guidance with regard to predicting the impact of system changes-from new hardware to updated NetWeaver-enabled business processes. Along the way, he shows how to make the most of countless optimization and monitoring tools-from free and low-cost technology stack-based utilities to comprehensive, automated SAP testing suites. His vendor-neutral, unbiased coverage includes: Quantifying concrete performance requirements-even for complex, cross-application business processes Testing and monitoring daily system loads, month-end or seasonal business peaks, key transactions, and complex multi-system business processes Conducting comprehensive server, SAN/disk subsystem, and database testing Managing the testing process, leveraging proven best practices and techniques Analyzing, verifying, and quantifying SAP availability, scalability, and TCORegardless of the technology infrastructure underpinning your SAP solutions, if you're responsible for deploying, managing, maintaining, refreshing, upgrading, or supporting SAP technologies, you need this book-now.}
}

@proceedings{10.5555/962367,
title = {CASCON '93: Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: distributed computing - Volume 2},
year = {1993},
publisher = {IBM Press},
abstract = {Welcome to CASCON '93 -- Working Together. The theme of this year's conference is very appropriate, especially given today's business and economic environment. As an increasing number of businesses and organizations are faced with tough cost-cutting decisions, it is more important than ever to look for ways to utilize our resources more effectively -- leveraging our skills, knowledge and expertise by working together.The IBM Centre for Advanced Studies (CAS)is based on this strategy, successfully bringing together research experts from around the world -- more than 200 to date -- to collectively focus on key issues facing the software industry today ... and into the future.CASCON '93 is a demonstration of this strategy in action -- visible in the nearly 70 technology demos and the numerous research presentations and workshops you will have a chance to participate in throughout the conference.CASCON '93 -- Working Together marks three years of cooperative research activities between the Toronto Lab's Centre for Advance Studies and researchers from leading Canadian, U. S. and European universities. I am proud to say that CAS has lived up to its expectations, taking on a leadership role in the software industry -- building an extensive network of international experts applying their knowledge and experience to key issues facing the industry today.To date, three projects have moved from CAS back into our product development area, eight joint patents have been filed and resultant products are expected to be delivered to the marketplace in the near future.Throughout the next few days, you will hear from a number of our research partners and you will have the opportunity to view the nearly 70 technology demos that represent the results of their collective efforts. I urge you to spend time viewing the demos, talking to the experts and attending as many of the presentations as you can. I would also encourage you to participate in the technology workshops that have been scheduled for Wednesday and Thursday at the Radisson Hotel. This is your opportunity to discuss, in an open forum, key issues and challenges facing the information technology industry.I would also like to welcome the National Research Council of Canada as the co-sponsor of this year's conference. By working together we were able to expand the scope of CASCON '93 to include the participation of leading Canadian and U. S. software organizations. Their technologies are also represented in a number of the demos here today.Welcome to CASCON '93 ! This conference, our third, is co-sponsored with NRC and emphasizes "Working Together, " successfully bridging the gap between the software industry and the research community.In response to the Call for Papers, we received about 126 research papers. The program committee members considered all the papers carefully and each paper was reviewed by at least three reviewers. The review criteria were: technical quality, originality, clarity of presentation, and relevance to CASCON.Our program at the Ontario Science Centre is by design a one-track program where all participants can hear from the many distinguished speakers about their views on the software industry. The main component of the technical program for these two days is the presentation of 17 carefully selected contributed papers of high quality.The main component of Days Four and Five are 16 workshops on selected topics in Quality Engineering, Testing, Broadband Services, Software Integration, Challenges in Deploying Distributed Systems, Parallel Databases, Languages and Compiler Issues for Distributed Memory Machines, New Business Opportunities, Transfer of Technology, Software Evolution, C++ Compiler-Based Development Environments, Software Architecture, Data Integration and Multidatabase Systems, Commercializing Software, Documentation and Interfaces, and Software Development Processes.},
location = {Toronto, Ontario, Canada}
}

@proceedings{10.5555/962289,
title = {CASCON '93: Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: software engineering - Volume 1},
year = {1993},
publisher = {IBM Press},
abstract = {Welcome to CASCON '93 -- Working Together. The theme of this year's conference is very appropriate, especially given today's business and economic environment. As an increasing number of businesses and organizations are faced with tough cost-cutting decisions, it is more important than ever to look for ways to utilize our resources more effectively -- leveraging our skills, knowledge and expertise by working together.The IBM Centre for Advanced Studies (CAS)is based on this strategy, successfully bringing together research experts from around the world -- more than 200 to date -- to collectively focus on key issues facing the software industry today ... and into the future.CASCON '93 is a demonstration of this strategy in action -- visible in the nearly 70 technology demos and the numerous research presentations and workshops you will have a chance to participate in throughout the conference.CASCON '93 -- Working Together marks three years of cooperative research activities between the Toronto Lab's Centre for Advance Studies and researchers from leading Canadian, U. S. and European universities. I am proud to say that CAS has lived up to its expectations, taking on a leadership role in the software industry -- building an extensive network of international experts applying their knowledge and experience to key issues facing the industry today.To date, three projects have moved from CAS back into our product development area, eight joint patents have been filed and resultant products are expected to be delivered to the marketplace in the near future.Throughout the next few days, you will hear from a number of our research partners and you will have the opportunity to view the nearly 70 technology demos that represent the results of their collective efforts. I urge you to spend time viewing the demos, talking to the experts and attending as many of the presentations as you can. I would also encourage you to participate in the technology workshops that have been scheduled for Wednesday and Thursday at the Radisson Hotel. This is your opportunity to discuss, in an open forum, key issues and challenges facing the information technology industry.I would also like to welcome the National Research Council of Canada as the co-sponsor of this year's conference. By working together we were able to expand the scope of CASCON '93 to include the participation of leading Canadian and U. S. software organizations. Their technologies are also represented in a number of the demos here today.Welcome to CASCON '93 ! This conference, our third, is co-sponsored with NRC and emphasizes "Working Together, " successfully bridging the gap between the software industry and the research community.In response to the Call for Papers, we received about 126 research papers. The program committee members considered all the papers carefully and each paper was reviewed by at least three reviewers. The review criteria were: technical quality, originality, clarity of presentation, and relevance to CASCON.Our program at the Ontario Science Centre is by design a one-track program where all participants can hear from the many distinguished speakers about their views on the software industry. The main component of the technical program for these two days is the presentation of 17 carefully selected contributed papers of high quality.The main component of Days Four and Five are 16 workshops on selected topics in Quality Engineering, Testing, Broadband Services, Software Integration, Challenges in Deploying Distributed Systems, Parallel Databases, Languages and Compiler Issues for Distributed Memory Machines, New Business Opportunities, Transfer of Technology, Software Evolution, C++ Compiler-Based Development Environments, Software Architecture, Data Integration and Multidatabase Systems, Commercializing Software, Documentation and Interfaces, and Software Development Processes.},
location = {Toronto, Ontario, Canada}
}

@inproceedings{10.1109/FIE.2018.8658513,
author = {Plaza, Pedro and Sancristobal, Elio and Carro, German and Castro, Manuel and Blazquez, Manuel and Garc\'{\i}a-Loro, F\'{e}lix},
title = {Multiplatform Educational Robotics Course to Introduce Children in Robotics},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE.2018.8658513},
doi = {10.1109/FIE.2018.8658513},
abstract = {Robotics and computational thought are ideal tools for developing science, technology, engineering and mathematics (STEM) pedagogy. Throughout this paper a modular and adaptive course is presented, the main objective of which is to make known simple and economic tools of educational robotics. This course is aimed at those who want to discover the possibilities of educational robotics in the context of the introduction to robotics. Today, robotics training tools are raised with the aim of promoting innovation and motivation of students during the learning process. Robots are becoming more and more common in our daily lives; therefore, it is important to integrate robots into all levels of our society. The course is designed to work with the Scratch, Crumble and Arduino tools as STEM enhancers. Using Scratch, interactive stories, games and animations can be programmed. Scratch helps young people to acquire and improve skills such as think creatively, think systematically, and work collaboratively. Scratch is a project of MIT Media Lab's Lifelong Kindergarten Group. It is offered free of charge. On the other hand, Crumble is an easy-to-use programmable controller. Its programming interface uses a block programming language based on Scratch that makes it easy for children from 10 years old to use it. In addition, the hardware elements associated with Crumble are very intuitive and easy to connect. Last, but not least, Arduino is an open source electronic platform based on hardware and software that is easy to use. It is a platform that incorporates a simple microcontroller and an interface development environment to create the applications to be downloaded on the board. The course offers a three-tiered journey through three levels with each of the three tools. It consists of a total of 9 modules. This course has a very practical approach. A project-based pedagogical methodology is used. Experiments are promoted, where trial and error are part of learning and self-discovery. The student learns to have more autonomy and responsibility. Knowledge is acquired in different disciplines. It develops: motor skills (scale mobility in the hands), group skills, allowing people to socialize, creative abilities, and learning in a fun way. The operational details, materials used and examples of activities for some modules are also presented with the expectation that all teachers will be able to adapt these activities in their class. In addition, results are included from several groups of students who have already completed some modules. Despite not having a large number of students, the experience provided results that may be useful for other teachers to promote a course with similar or equal content for more results. The results of this work show that it is important to combine theory and practice to include fun tasks intertwined with the challenges of applying theory to problem solving.},
booktitle = {2018 IEEE Frontiers in Education Conference (FIE)},
pages = {1–9},
numpages = {9},
location = {San Jose, CA, USA}
}

@book{10.5555/1572526,
author = {Douglass, Bruce Powel},
title = {Real-Time Agility: The Harmony/ESW Method for Real-Time and Embedded Systems Development},
year = {2009},
isbn = {0321545494},
publisher = {Addison-Wesley Professional},
edition = {1st},
abstract = {Regardless of your perceptions of Agile, this is a must read! Douglasss book is a powerful and practical guide to a well-defined process that will enable engineers to confidently navigate the complexity, risk, and variability of real-time and embedded systemsincluding CMMI compliance. From requirements specification to product delivery, whatever your modeling and development environment, this is the instruction manual. Mark Scoville, software architect This book will provide you with the framework of agile development for real-time projects ranging from embedded systems to web-based, data collection applications. I wish I had this book three years ago when we began a real-time, embedded drilling control system project, but all my engineers will be getting copies now that it is available. And, for my academic colleagues, this is the perfect book for graduate seminars in applied software development techniques. Don Shafer, chief technology officer, Athens Group; adjunct professor, Cockrell School of Engineering, The University of Texas at Austin We have used Dr. Douglasss books on real-time (Doing Hard Time, Real-Time UML, and Real-Time Design Patterns) for years. His books are always informative, accessible, and entertaining. Real-Time Agility continues that tradition, and I cant wait to introduce it to my colleagues. Chris Talbott, principal software designer Until now, agile software development has been mostly applied within the IT domain. This book breaks new ground by showing how to successfully traverse the perceived chasm between agility and real-time development. Although embedded systems impose challenging constraints on development teams, you can always benefit from increasing your agility. Scott W. Ambler, chief methodologist/Agile, IBM Rational; author of Agile Modeling Real-time and embedded systems face the same development challenges as traditional software: shrinking budgets and shorter timeframes. However, these systems can be even more difficult to successfully develop due to additional requirements for timeliness, safety, reliability, minimal resource use, and, in some cases, the need to support rigorous industry standards. In Real-Time Agility, leading embedded-systems consultant Bruce Powel Douglass reveals how to leverage the best practices of agile development to address all these challenges. Bruce introduces the Harmony/ESW process: a proven, start-to-finish approach to software development that can reduce costs, save time, and eliminate potential defects. Replete with examples, this book provides an ideal tutorial in agile methods for real-time and embedded-systems developers. It also serves as an invaluable in the heat of battle reference guide for developers working to advance projects, both large and small. Coverage includes How Model-Driven Development (MDD) and agile methods work synergistically The Harmony/ESW process, including roles, workflows, tasks, and work products Phases in the Harmony/ESW microcycle and their implementation Initiating a real-time agile project, including the artifacts you may (or may not) need Agile analysis, including the iteration plan, clarifying requirements, and validation The three levels of agile design: architectural, mechanistic, and detailed Continuous integration strategies and end-of-the-microcycle validation testing How Harmony/ESWs agile process self-optimizes by identifying and managing issues related to schedule, architecture, risks, workflows, and the process itself}
}

@phdthesis{10.5555/2522400,
author = {Luo, Juan},
advisor = {Brodsky, Alexander},
title = {Regression learning in decision guidance systems: models, languages, and algorithms},
year = {2012},
isbn = {9781267316615},
publisher = {George Mason University},
address = {USA},
abstract = {The goal of state-of-the-art research in decision guidance applications is to build complex systems with predicting capability. Systems can make decisions intelligently rewarded by more desirable outcomes. Predictions are actually made based on a dynamically collected amount of information. Some models with unknown parameters in the application can be learned from the collected information and domain knowledge as well [1]. A development tool called Decision Guidance Management System (DGMS) is proposed to develop decision guidance applications. Four phases are involved in the decision guidance applications and will be repeated as needed. They are data collection, model learning, prediction, and optimization phases for decision-making [1]. This dissertation focuses on a framework, models, languages, and algorithms to integrate the machine learning functionality (regression learning) into DGMS applications as their first class citizen. A framework CoReJava (Constraint Optimization Regression in Java), which extends the Java programming language with regression learning or the ability of parameter estimation for a function, is proposed and developed. CoReJava is unique in that functional forms for regression analysis are expressed as first class citizens, that is, as Java programs, in which some parameters are not given in advance, but will be learned from learning data sets provided as input. To implement regression learning, the CoReJava compiler (a) analyzes the structure of the parameterized Java program that represent a functional form; (b) automatically generates a constraint optimization problem, in which constraint variables are the unknown parameters, and the objective function to be minimized is the sum of squares of errors with regarding to the learning set; and (c) solves the optimization problem using an external nonlinear optimization solver. The parameterized Java programs are executed as a regular Java program, with the initially unknown parameters substituted by the found optimal values. CoReJava syntax and semantics are formally defined and exemplified using a simple supply chain example. The if-then-else decision structures of the Java language are naturally adopted to represent piecewise functional forms of regression. Thus, minimization of the sum of squared errors involves an optimization problem with a search space that is exponential to the size of the learning set. A combinatorial restructuring algorithm is proposed to guarantee learning optimality and reduce the search space to be polynomial in the size of the learning set, but exponential to the number of piecewise bounds. A Heaviside restructuring algorithm, which expresses the piecewise linear regression function using a unified functional format [2], instead of multiple pieces, is proposed to decrease the searching complexity further to be polynomial in both the size of the learning set and the number of piecewise bounds, while the learning outcome will be an approximation of the optimality. An Expectation Maximization-based (EM-based) Multi-step Piecewise Surface Regression Algorithm (EMMPSR) is proposed to solve piecewise surface regression problem. The multiple steps involved are local regression on each data point of the training data set and a small set of its closest neighbors, clustering on the feature vector space formed from the local regression, regression learning for each individual surface, and classification to determine the boundaries for each individual surface [3]. An EM-based iteration process is introduced in the regression learning phase to improve the learning outcome [4]. The reassignment of a cluster identifier for every data point in the training set is determined by predictive performance of each submodel [5]. Clustering quality validity indices are applied to the scenario in which the number of piecewise surfaces is not given in advance. The Relational Database Management System (RDBMS) is extended with the piecewise regression learning capability as well. The functional forms are represented as database tables. The EMMPSR algorithm is implemented as stored procedures. A case study is undertaken to describe the decision optimization process based on the learning outcome of the EMMPSR algorithm. Evaluation of the resulting research is established by experiments and empirical analysis in comparison with those of related regression learning packages.},
note = {AAI3506265}
}

@inproceedings{10.1145/322917.322980,
author = {Halkias, Gail F. and Reilly, Kevin D.},
title = {Human factors considerations in the design of a multiple source expert system for military applications (abstract only)},
year = {1987},
isbn = {0897912187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/322917.322980},
doi = {10.1145/322917.322980},
abstract = {In this paper we present a prototype expert system characterized by two major premises:
there are multiple sources of knowledge within the knowledge base;human factors considerations must receive paramount attention.The domain of the prototype is AirLand combat planning. (Airland combat is a new Army warfare doctrine developed in the early 1980's. Its emphases include active defense, interdiction of the second echelon, early counterattack, and other tactical principles.) The potential users are Army division commanders and staff who are engaged in or are training for combat in Central Europe.Our prototype's knowledge sources are representative of those encountered in a wider class of applications, through its use of sources which may be incomplete, ambiguous and conflicting, such as:
doctrinal knowledge found in policy statements, Army regulations, and professional manuals (FM 100-5). Doctrine can be viewed as a constraint on tactics, combat organization, fire and maneuver schemes, and command and control systems;knowledge from an expert from the domain environment (provided by the historian, Colonel Trevor N. Dupuy);distilled wisdom gleaned from the historical writings of great military thinkers, theorists, and commanders. The first sources to be included are the Maxims of Napoleon and material from von Clausewitz;previous experience with analysis of outcome of appropriate historical cases;results from a computer simulation (operations research) model, the Quantified Judgment Model (QJM). The QJM takes information about a combat situation and generates a prediction (based on many variables) of the victor. It also predicts advance rates, casualty rates, equipment loss and recovery rates, plus many other factors.[1] It is being used as a preprocessor for the expert system.Another important feature concerns the status of each of the knowledge components, i.e., whether the overall system is best envisioned as a collection of more or less autonomous expert systems governed by a controlling expert system, or whether the knowledge collection can be organized in some principled way to allow the multiple sources to be handled within a more homogeneous setting, the limit being a single expert system with accessibility to a (relational) database and other resources, especially simulation results. (A simulation capability is considered by some researchers [2,3] to be an important part of an expert system designed to provide task planning.) Consideration is being given to the implementation of the simulation portion on a parallel processing machine, specifically the Sequent 21000.Important human factors exist for many kinds of expert systems and, especially so, for the application study of the paper. [4] Included among these factors are:
mode of the system, defining the attitude of the system with regard to the user, e.g., consultation, critique, advisory, alert, advocacy; [5]conversational style;architecture of the system and its relation to user stress;causal assessment and support of good human decision making in expert systems;avoidance of consequence buffering or transfer of responsibility from the user to the expert system.Constrained for a number of reasons to a microcomputer environment for the expert system portion of the prototype, we have chosen to utilize the Texas Instruments Personal Consultant Plus development tool to achieve such goals as: rapid development time, ease of explanation generation, availability of both forward- and backward-chaining control mechanisms, built-in functions for online help and other explanatory features, convenient knowledge base segmentation through use of frames, consistent user interface between development and user environments.},
booktitle = {Proceedings of the 15th Annual Conference on Computer Science},
pages = {356},
location = {St. Louis, Missouri, USA},
series = {CSC '87}
}

@inproceedings{10.1145/3544548.3580683,
author = {Mcnutt, Andrew M and Outkine, Anton and Chugh, Ravi},
title = {A Study of Editor Features in a Creative Coding Classroom},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580683},
doi = {10.1145/3544548.3580683},
abstract = {Creative coding is a rapidly expanding domain for both artistic expression and computational education. Numerous libraries and IDEs support creative coding, however there has been little consideration of how the environments themselves might be designed to serve these twin goals. To investigate this gap, we implemented and used an experimental editor to teach a sequence of college and high-school creative coding courses. In the first year, we conducted a log analysis of student work (n=39) and surveys regarding prospective features (n=25). These guided our implementation of common enhancements (e.g. color pickers) as well as uncommon ones (e.g. bidirectional shape editing). In the second year, we studied the effects of these features through logging (n=39+) and survey (n=23) studies. Reflecting on the results, we identify opportunities to improve creativity- and novice-focused IDEs and highlight tensions in their design—as in tools that augment artistry or efficiency but may be perceived as hindering learning.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {121},
numpages = {15},
keywords = {Code editors, Creative coding, Introductory programming, p5},
location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
series = {CHI '23}
}

@book{10.5555/515475,
author = {Tamres, Louise},
title = {Introducing Software Testing: A Practical Guide to Getting Started},
year = {2002},
isbn = {0201719746},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: The chaotic testing environment A project is in panic mode and the deadline is rapidly approaching. Management starts to think about the need to test this product, having already missed some prime opportunities for improving software quality. One unfortunate programmer is assigned the task of software testing, which is often viewed as being transferred to purgatory. Needless to say, this poor hapless soul is given no guidance, and nobody in the organization is capable of providing any help. Despite the poor condition of requirements and other product documentation, the product is being built and it will be shipped. The task given to the tester is to minimize the surprises that could manifest themselves after the product is installed at customer sites. Under extreme pressure, this untrained tester is very inefficient and is at a loss how to begin. A clueless manager may even purchase testing tools, despite there being no useful tests to automate. This is the scenario that gives software testing a bad name. Software testing is a specialized discipline requiring unique skills. Software testing is not intuitive; one must learn how to do it. Na\"{\i}ve managers erroneously think that any programmer can test software if you can program, then you can test. This is the motivation behind this book: to provide a step-by-step approach for getting started with the testing effort. Many fine books on software testing are available today. Those that do address test case design describe proven methods such as boundary value analysis, equivalence class partitioning, decision tables, syntax testing, cause-effect diagrams, data-flow methods, and other suchconcepts. Some novice testers wonder how to weed though poor specifications, before even being able to apply these methods. Many texts state that good requirements are necessary for the test effort assuming that requirements exist yet I have not seen any that explain the transition from requirements to test cases. In the chaotic software development environment, adequate requirements are rarely provided, and if they are, their completeness and correctness are questionable. In a situation when no one has analyzed the requirements adequately, the burden falls on the tester to pursue requirements issues prior to defining any tests. It is often impossible to perform thorough testing, given the tight schedules and limited resources. It is possible, however, to make intelligent choices and maximize the effectiveness of the testing effort. The goal is to learn how best to approach the testing tasks and eventually produce a workable test process for future projects. The author's philosophy To introduce the ideas on how to begin testing, I will work through several detailed examples, each containing defective "requirements". By definition, good requirements are testable and unambiguous. The fact that the sample requirements are deficient does not prevent useful test activities from occurring. I use the word "requirements" loosely and equate it with some sort of product description. While the sample test scenarios would not be permissible in a mature software organization, the work described will help the lone tester jumpstart the testing process under duress. The goal is to show that some product information, however deficient, can be used to start the testing effort. I do not advocate working from poor requirements. Properly analyzing requirements corrects many deficiencies. Reviews and inspections have been proven to provide the most cost effective method for finding problems early in the development cycle. Many times, I have had to bite my tongue to avoid blurting out to project managers, "The requirements are absolute garbage and there's no way that we can begin a productive testing effort until you clean up your act." Actually, this phrase would contain unprintable language and be uttered under one's breath. We have undoubtedly all shared this fantasy, and the ugly truth is that despite this valid complaint, the product delivery deadline is fast approaching. Although I do not advocate cutting corners, there are some shortcuts that will help document the testing activities. A crude list of tests is better than no list. The minimum you will have is a documented trail, though rudimentary, that records your testing effort should you need to prove or demonstrate what you did. Subsequent testing efforts will improve on this initial work, producing test documents and developing a test process that is more in line with accepted practices. Incremental changes lead to successful process improvements. Just knowing how to get started with testing is a feat in itself. The tester must understand how to transform product information into test cases; this is the book's chief goal. Many existing books do an outstanding job of explaining software testing concepts and methods. Rather than reiterate what others have written, I make many references to their work. This book is a primer on getting started. It supplements currently available literature on software testing by providing an introduction to known software testing techniques. Intended audience This book is aimed at several types of readers: persons new to software testing who have no guidance or training; managers or mentors, who may themselves be experienced testers, seeking ideas on how to provide guidance to novice testers; experienced programmers who have been assigned testing tasks; knowledgeable testers looking for new ideas. While readers are not assumed to be knowledgeable about software testing concepts, they should be computer literate and able to use a word processor and spreadsheet. Job descriptions The general job description terms used throughout the book are as follows: Tester: The person who defines and executes tests. Developer: The person who produces the application, including the design, source code, and final product integration. Project manager: The person with authority regarding schedules and staffing. Project authority: The domain expert with authority to define and clarify the requirements. I refer to these descriptive titles without implying an organization structure or employee reporting chain. Project staffing decisions and job responsibilities vary across organizations. Depending on how the project is staffed, the tester could be either in the same or in a separate group from the developer. Other projects could require that the same person performs both development and t changing mindset in mid-project. Ideally, a trained software test engineer performs the testing activities. However, some projects simply assign the testing role to whichever person is available. The project authority can be the marketing manager, company executive, or customer support liaison, provided that this person has full authority to define the project contents. This role is necessary to prevent further chaos. Someone must be in charge of deciding which features to incorporate into a product; lack of such control is a well-known cause of problems when trying to get bad requirement definitions sorted out. Your organization may use different job titles than those listed above. The key point is to assign people to perform the necessary tasks each of which requires specialized skills. Mature and immature software development environments The examples cited in this book, with their incomplete product information, are what one could expect to find in an immature software organization. I will refrain from critiquing the work environment and from preaching about software process improvement. The reality is that many companies operate under less than ideal conditions. Despite the lack of suitable software processes, products are still being developed and shipped to customers. Testing, however minimal, can still be done. With poor requirements, the tester spends more time identifying product definition deficiencies rather than proceeding with testing-related tasks. A mature software organization displays the characteristics listed below. An immature organization often does not understand how the following points can improve product quality: provide useful requirements and product descriptions; conduct reviews and inspections; have signoffs or checkpoints before proceeding to the next step; mentor and train personnel; schedule adequate time and resources for testing; overlap testing and development activities; provide defined software development and software testing processes; enforce configuration management. Testing is a responsibility shared with the rest of the development team. The old view of testing as an afterthought design, code, and then you test has never produced good testing results. The adversarial and destructive "developer vs tester" mentality has often resulted from the developers' ignorance about software testing activities more proof that testing is a unique discipline. It is often the case that a tester often knows more about programming than a developer knows about software testing. A collaborative approach between testers and developers fosters goodwill and good communication. By working closely with the testers, many developers learn more about software testing, even if all the developers see is how their knowledge about the product filters into the test documentation. Effective software testing requires co-operation among all the members of a project. Book overview Chapter 1 deals with the unfortunate "you're new to testing, have no idea where to start, and the product ships Friday" nightmare. Hoping that your next project gives you more time to carry out testing activities, Chapter 2 illustrates the use of outlines, which is al analyzing requirements if no one else has done this task. Chapter 3 transforms the outline contents into test cases. Tables and spreadsheets are an integral tool used by software test engineers, and Chapter 4 shows several table formats and shortcuts for documenting test cases. Chapter 5 shows additional usages of tables. Applications built using object-oriented methods can use many of the same test design techniques outlined in the previous chapters. However, Chapter 6 describes some issues particular to testing object-oriented systems. Chapter 7 lists the challenges faced when testing web applications, although many of the strategies presented apply equally to client-server environments. No uniform software testing method exists. Each example uses a different approach for producing tests. You may wonder why one method was used in one example instead of another. The answer is simple: I selected a method based on my experience. You may very well try a different approach that will be just as successful in your testing effort. Although the examples cover different types of applications, the core software testing themes apply equally to all examples, and some concepts are reiterated among all chapters. I recommend that you read through each scenario and not dismiss the subject simply because the example does not reflect your type of application. By following the ideas and methods presented, you will have defined and documented many test cases. Chapter 8 will help you identify the most pertinent tests and thus reduce the necessary number of test cases to execute. Producing a set of test cases to execute is only part of the overall software testing picture. Chapter 9 lists other testing and quality related tasks that are necessary for producing quality software. If this is the organization's first venture into methodical software testing, you will have established a good baseline. Although the work produced will be a vast improvement over prior chaotic efforts, it will fall short of satisfying, and conforming to, industry standards. Chapter 10 briefly describes some of the more common software engineering standards and how each affects the test case examples. Consider this a launching point for improving the testing effort.}
}

@book{10.5555/552037,
author = {Kain, Eugene and Wingo, Scot},
title = {The  MFC Answer Book: Solutions for Effective Visual C++ Applications},
year = {1998},
isbn = {0201185377},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: PREFACE: Why Another Book on Microsoft Foundation Classes Programming To answer this question, let us look at a typical MFC programming scenario. First, you attend an MFC training session or read some introductory books on MFC programming. You quickly become able to write and customize small tutorial applications. AppWizard and ClassWizard allow you reach an unprecedented level of productivity. Your applications support the multiple document interface (MDI) and have a professional-looking user interface with a floating toolbar, a status bar, printing and print preview, and so on. You then go back to work and start using MFC to produce great-looking applications. Code flows freely from your keyboard, the wizards work hard at your side, and life looks great under the MFC sun. One day, you start wondering about how to implement new features that were not explicitly covered in the training session. For example: Make your application remember the last active document and automatically reopen it. Support multiple kinds of views on the same document and allow the user to explicitly open any kind of view. Add ToolTips to the controls in a form view. Dynamically switch the view displayed in a window to replace it with another kind of view. Implement an expanding dialog box. Embed a property sheet (tabbed dialog box) inside another window, such as a form view, a dialog box, or a mini frame window. Display a progress indicator in a status bar pane. Have a menu pop up when the user clicks a button on a toolbar or in a dialog box. Support headers and footers in your print and print preview. Displaya custom Printing . . . dialog box with a progress indicator. You feel that implementing these features cannot be that difficult: after all, you have already seen them in other Windows applications. But where do you start looking for an answer The solution may be as easy as knowing the specific MFC virtual functions that you must override to produce the desired effect or knowing the Windows messages you should trap and handle appropriately. For some features, however, more involved techniques may be neededeven to the point of tracing into MFC's source code to understand just where and how you can act to modify your application's default behavior. One infuriating fact of life is that the answer to your particular question may be lying around somewhere: buried in some MFC programming book or magazine article, on the Microsoft Developer's Network CD-ROM, in the Microsoft Knowledge Base, in the various threads and mailing lists maintained on the Internet, or even in the online books or samples contained on the Visual C++ CD-ROM. The problem is this: How are you going to locate the most relevant and reliable source of information among all these resources How are you going to find the solution you need right now Introducing The MFC Answer Book This book is intended to provide ready-to-use techniques that answer the most common real-world questions that typically confront MFC developers. The structure of this book is specifically designed to help you quickly locate the answers youire looking for and integrate the relevant solutions into your own programs. The FAQ format of this book makes it ideally suited to the needs of the developer looking for a quick answer to a pressing question. At the same time, you will find that many techniques will give you a better understanding of the inner workings of MFC applications and more generally help you improve your MFC programming skills. In particular, the Explanations and Additional Comments sections often delve into the MFC source code or undocumented functions to explain how the techniques discussed work and how they differ from or integrate with MFC's default behavior. Key Features of This Book Although most books about Visual C++ and MFC programming answer valid questions about MFC programming and provide useful tips if you read them from cover to cover, most of them are not structured in a way that allows you to quickly find an answer to a given problem. Moreover, even if you find the answer, it is likely to be buried inside a larger discussion and not readily available as a step-by-step technique that you can simply incorporate into your current project to add a required feature. In contrast, The MFC Answer Book is specifically designed to help MFC developers solve their programming problems in the most efficient way: This book is organized so that the table of contents will help you to quickly zoom in on the FAQs that answer your questions. I have made every effort to build a convenient and comprehensive index that will direct you to all the pages relating to any keyword or function referenced in this book. Each FAQ is written in a concise way that first gives you the step-by-step answer you need. Explanations and additional comments are deferred to later sections so that they do not get in the way of the solution but are readily available for those who want to go further than the cookbook recipe and wish to understand what goes on under the hood. Each explanation comes with tested and reusable sample code that you can plug into your MFC application in a few minutes to integrate the required functionality immediately. To summarize: The goal of this book is to offer you the shortest way from a problem to the corresponding step-by-step solution that you can integrate immediately into your current project. Who Should Read This Book This book is written for all MFC developers who wish to solve their MFC-related problems and at the same time learn advanced MFC techniques that will allow them to add a range of sophisticated features to their applications. This book assumes a basic proficiency both in the C++ language and in MFC programming as well as a knowledge of how to use the Visual C++ integrated development environment and tools such as AppWizard and ClassWizard. The Visual C++ wizards are discussed only when used in nonstandard ways to achieve a specific result. To benefit fully from this book, you should already understand the basic MFC concepts presented in the Scribble tutorial described in the Visual C++ documentation: the document/view architecture, message maps, the UPDATE_COMMAND_UI mechanism, dialog data exchange (DDX), and so on. Typically, you will either have followed the Scribble tutorial, attended a training session in MFC programming, or read one of the many introductory books on this topic. Of course, having a more extensive background in MFC programming will not hurt! Quite to the contrary. Based on feedback from reviewers and colleagues, I know that this book will also appeal to experienced MFC developers, who will find many useful techniques to add to their bag of MFC programming tricks. Finally, reading this book will allow all MFC developers to improve their understanding of fundamental MFC concepts and sharpen their MFC programming skills. How To Use This Book This book focuses on the 32-bit MFC version 4.x for Windows 95 and Windows NT. However, most techniques and concepts discussed here also apply to older versions of MFC. They should also remain valid for future MFC versions, because they rely on core MFC classes and behaviors that are not likely to evolve in a way that breaks existing code. I tried to write this book so that it will become a flexible tool that you can use as you want to. This means that you can either read this book from cover to coverI would certainly appreciate it if you door use it as a reference to look up only the specific topics that interest you. Most FAQs are cross-referenced to help you locate all the relevant information you might need even if you jump into the middle of the book. However, before you start hunting for answers to your MFC questions, I suggest that you take a few minutes to read Chapter 0 (Terminology and Conventions) and Chapter 1 (Document/View Architecture Backgrounder) to make sure that we start on the same ground with respect to fundamental document/view architecture concepts. What Is on the CD-ROM The companion CD-ROM contains source code and executables for all of the book's sample programs. The folder hierarchy is organized first by chapter number and then by project name. Thus, the AutoSaveDoc project for Chapter 2 is located in the d:Chap02AutoSaveDoc folder, where "d:" is your CD-ROM drive's letter. All the executables are located under their respective chapter folders. For example, all the executable sample programs for Chapter 2 are located in the d:Chap02 folder. The EkUtil.h and EkUtil.cpp files located at the root of the hierarchy contain the various helper Ek . . . . . . functions and classes that are presented throughout the book. You can choose to copy the whole folder hierarchy from the CD-ROM to your hard disk, copy only the examples that are of interest to you, or access the files directly from the CD-ROM. If you copy files from the CD-ROM to your hard disk, remember to remove the read-only attribute from the files on your hard disk. All sample programs have been compiled and tested under both Visual C++ 5.0 and Visual C++ 6.0. They will also work properly with Visual C++ 4.x, but you will have to manually create the appropriate .mdp project file. Note, however, that the .dsp project files on the CD-ROM have the Visual C++ 5.0 format: if you open them with Visual C++ 6.0, simply answer Yes to the dialog box asking whether you want to convert these files to the new format. Your Feedback Is Welcome I have done my best to accurately present topics that I feel should be of interest to most MFC developers. However, if you think that a topic should be covered differently or should use another technique, don't hesitate to send me e-mail at ekain@awl.com. Also, e-mail me if you want to submit a topic idea or a technique of your own that solves a problem you have encountered, if you find an error or have any problem with this book, or if you have suggestions or want to discuss anything with me. I can promise that I will read all e-mail messages, take them into account, and try to respond to each of them as soon as possible. Note, however, that I may not have the time to answer specific MFC programming questions. You can also visit my Web site at ...}
}

@inproceedings{10.1145/800214.806560,
author = {Cheriton, David R. and Malcolm, Michael A. and Melen, Lawrence S. and Sager, Gary R.},
title = {Thoth, a portable real-time operating system (Extended Abstract)},
year = {1977},
isbn = {9781450378673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800214.806560},
doi = {10.1145/800214.806560},
abstract = {Thoth is a portable real-time operating system which has been developed at the University of Waterloo. Various configurations of Thoth have been running since May 1976; it is currently running on two minicomputers with quite different architectures (Texas Instruments 990 and Data General NOVA).This research is motivated by the difficulties encountered when moving application programs from one system to another; these difficulties arise when interfacing with the hardware and support software of the target machine. The problems encountered interfacing with the new software support are usually more difficult than those of interfacing with new hardware because of the wide variety of abstract machines presented by the compilers, assemblers, loaders, file systems and operating systems of the various target machines. We have taken the approach of developing portable system software and porting it to “bare” hardware. Because the same system software is used on different hardware, the same abstract machine is available to application programs. Thus most application programs which use Thoth can be portable, if not machine independent.Most previous work on software portability has focused on problems of porting programs over different operating systems as well as hardware. To our knowledge, this is the first time an entire system has been ported. Our experience indicates that this approach is practical both in the cost of porting the system and its time and space performance.The design of Thoth strives for more than portability. A second design goal is to provide a system in which programs can be structured using many small concurrent processes. Thus we have aimed for efficient interprocess communication to make this structuring technique attractive. We have also provided safe dynamic process creation and destruction.A third design goal is that the system meet the demands of real-time applications. To meet this goal, the system guarantees that the worst-case time for response to certain external events (interrupt requests) is bounded by a small constant.A fourth design goal is that the system be adaptable to a wide range of real-time applications. A range of system configurations is possible: A stand-alone application program can use a stripped version of the Thoth kernel which supports dynamic memory allocation and interprocess communication. Such a configuration requires less than 2000 16-bit words of memory. Larger configurations can support process destruction, a device-independent input-output system, a tree-structured file system, and multiple teams of processes. (A team is a set of processes which share the same logical address space and therefore can share data.)Thoth is implemented in a high-level language called Eh (a descendant of BCPL) and a small amount of assembly language. The major job in porting the system seems to be in redesigning the code generation parts of the compiler.Since it appears impractical to design system software to be portable over all computers (even over all existing machines), we have aimed at making Thoth portable over a subset of machines. Machines in the set can be characterized by a set of properties such as: a word must be at least 16 bits in length, a pointer to a word must fit into a word, etc. Roughly, this set of machines includes most modern minicomputers. It is important that many machines which do not yet exist will be included in it.A number of application programs have been written using Thoth. In addition to software development tools, communications and real-time control programs have been written. All of these programs require few if any changes when ported to new hardware. Some of these programs have been developed by inexperienced programmers who were not planning on porting their program. Hence, it seems to take less skill to write portable software in this system than using conventional techniques. However, existing software written for other systems is incompatible with Thoth and usually difficult to port to the Thoth system.Although, at the time of this writing, we have limited experience with porting the system to new hardware, we feel that Thoth has been highly successful in terms of our original objectives. Among other things, it has partially demonstrated the feasibility of building a portable operating system for a specified class of machines.},
booktitle = {Proceedings of the Sixth ACM Symposium on Operating Systems Principles},
pages = {171},
location = {West Lafayette, Indiana, USA},
series = {SOSP '77}
}

@phdthesis{10.5555/925703,
author = {Avila, Roberto Antonio and Moore, James A.},
title = {Methodology and design of a decision support system to predict tree growth response from forest fertilization},
year = {1997},
isbn = {059164634X},
publisher = {University of Idaho},
address = {USA},
abstract = {Decision support system (DSS) technologies are becoming very important supporting tools for helping people in the decision-making process. DSS have been used in forestry and are evolving very rapidly as foresters are demanding more system functionality to improve forestry management operations along with development of dynamic and user-friendly software to cope with the increased demand for information. The Intermountain Forest Tree Nutrition Cooperative (IFTNC) at the University of Idaho initiated a DSS project including, as its primary functional part, a real-time expert system prototype for Central Washington that focuses on tree nutrition management. The project's main objective was to design and develop a system methodology that involves a microcomputer program to predict Douglas-fir growth response based on fertilization treatments of 200 or 400 lb of nitrogen per acre during a six-year period. The system methodology began with the definition of the problem and ended with a preliminary design and operation of an integrated microcomputer application prototype.Nine experts on forest fertilization management issues were interviewed with the purpose to acquire heuristic knowledge they use to conduct fertilization operations and also to obtain their input on how a fertilization-supporting tool could strengthen the decision-making process. In order to reinforce the qualitative information collected from the interviews, quantitative data from different sources were also gathered i.e., data mining. For instance, the IFTNC database was a source for individual tree measurements and site characteristics. Geographic Information Systems (GIS) database related to soil parent materials and potassium level were generalized to include these parameters as input attributes in the data set. Global Positional System (GPS) was used to locate stands and input site-specific conditions.Interpretation and analysis included the interpretation of qualitative and quantitative information to look for system component functioning and then an analysis of how different components would operate under an integrated environment. To facilitate system understanding, a theoretical fertilization control system input was designed. This framework made it easy to design and operate the logic for various system module components. Also, preliminary definitions of the modeling and system architecture were studied.Modeling consisted of searching for an appropriate mathematical technique for system prediction. The system mathematical module uses a neural network approach where input/output data pairs on individual tree measurements and physical site characteristics are trained to predict Douglas-fir growth response based on 200 or 400 lb of nitrogen per acre during a six-year span. This component proved to be a quick and robust prediction technique for the prototype under development.Finally system development primarily dealt with the design and operation of the prototype itself. This system uses distinct software packages within an integrated personal computer environment using Microsoft Visual Basic as the integration development language. Besides the mathematical module, the prototype includes a visualization module implemented with MapObjects that produces maps of geographic stand location and also serves for data interpretation and analysis.The research project involved designing process that involves a feasible and rapid way of using different tools to deal with forest fertilization management operations. It uses different software components within an integrated computer development environment, resulting in a state-of-the-art fertilization prediction tool. The system supports management decisions, and helps people involved in forest fertilization design and administer fertilization prescriptions in the Intermountain Northwest.},
note = {AAI9813712}
}

@inproceedings{10.1145/1805986.1806014,
author = {Vanderheiden, Gregg},
title = {Building national public infrastructures on our way to a global inclusive infrastructure},
year = {2010},
isbn = {9781450300452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1805986.1806014},
doi = {10.1145/1805986.1806014},
abstract = {Broadband technologies are rapidly becoming integral to education, commerce, employment, community participation, health and safety Yet there remain multiple barriers to effective and affordable access by people with disabilities, elder, or those with low literacy creating an increasing digital divide. There are assistive technologies that can provide access for some. However it is not available for all disabilities, not affordable by many, and lags mainstream developments and deployments. Even when the latest AT is close to the latest IT, few people have the latest version. The cost of keeping up with mainstream technologies reduces resources available for innovation in assistive technologies and new directions in broadband technologies will require an already strapped AT industry to retool and re-architect their products. We are moving to an ICT environment with a profusion of hardware models (desktop, laptop, netbook, smartphone, tablet, set top box, game systems, players), multiple operating systems (Windows, Mac, Linux, Chrome OS, iPhone, Android, Windows Mobile, Symbian, Maemo (Nokia), Bada (Samsung), WebOS, etc.), hundreds of software applications that embed another universe of widgets, plug-ins, and players, and a networked information environment that adheres to no standard and mutates far beyond the initial conception of the Web. Our current access technologies and infrastructure cannot handle this; the assistive technologies that now exist do not address all disabilities well, particularly cognitive, language, and learning disabilities, deaf-blindness and the mixed problems faced by elders; current assistive technologies often add, rather than reduce, complexity; finally, but importantly, people are not aware of what is possible, see it as complicated, and do not have any easy way to determine that there is something that can help themA coalition of academic, industry and non-governmental organizations and individuals are coming together to promote the creation of a National Public Inclusive Infrastructure (NPII) to address these problems. The purpose is to ensure that everyone who faces accessibility barriers due to disability, literacy or aging, regardless of economic status, can access and use the Internet and all its information, communities, and services for education, employment, daily living, civic participation, health and safety.An NPII would provide key software enhancements to the physical infrastructure to allow lower cost accessibility that could be invoked on any computer, anywhere. Its key components would be a cloud based delivery system that would allow anywhere, any computer access, a personal preference system to allow systems to automatically configure themselves to users, a system of wizards to make creation of a preference profile simple even when a professional is not available, a metadata server to allow users to find accessible media or captions or descriptions for inaccessible media, a trusted source for malware free solutions, a rich development environment with common building blocks, and an awareness program to make more people aware of what is possible for them. All of the NPII components are being designed to support both commercial assistive technologies and free, built-in access features (universal design). The NPII will include a delivery system, personalization profiles and a rich development system and common modules. In addition to lowering development costs and increasing the number of solutions for different disabilities, the NPII can also enable new types of assistive technologies and services, including assistance-on-demand services that allow consumers to invoke computer or human assistance whenever and wherever they need it. The goal is a richer set of access options that it is less expensive to create and distribute and that can address the needs of a wider range of disabilities than is possible today. And a model infrastructure that can be replicated internationally and bring this wide variety of access options and the lower cost delivery system for both commercial and free access features to countries world-wide.},
booktitle = {Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A)},
articleno = {19},
numpages = {1},
location = {Raleigh, North Carolina},
series = {W4A '10}
}

@phdthesis{10.5555/2519614,
author = {Harris, Jeremy W.},
advisor = {Terrell, Stephen},
title = {Towards an internet-based distance education (ide) framework for religious-based higher education organizations: a case of the alliance for assemblies of god higher education},
year = {2012},
isbn = {9781267380456},
publisher = {Nova Southeastern University},
abstract = {Internet-based distance education (IDE) continues to grow in popularity and ubiquity. Acceptance of IDE among Christian higher education institutions has also increased. However, these institutions seek assistance. Such was the case with the nineteen institutions endorsed by the Assemblies of God (AG). The AG's oversight organization (The Alliance for Assemblies of God Higher Education, Alliance) was asked by member institutions for IDE aid, resources, and direction. To understand the current environment of IDE within AG higher education, an organizational discovery case study reviewed the historical IDE trends within AG higher education, surveyed institutional faculty members and administrators as to their IDE beliefs and situations, and analyzed the data collected. From the research findings, the Alliance gained a better understanding of the needs and intentions of its member institutions. It also realized the aid and resources to offer its endorsed institutions, what endorsement requirements were needed for spiritual development in an online distance education setting, and an overall IDE direction that the organization could provide or facilitate. To aid the organizational discovery, a research framework was created that the Alliance could reuse and share with similar organizations for their own internal discovery.},
note = {AAI3510437}
}

@book{10.5555/1869711,
author = {Aiello, Robert and Sachs, Leslie},
title = {Configuration Management Best Practices: Practical Methods that Work in the Real World},
year = {2010},
isbn = {0321685865},
publisher = {Addison-Wesley Professional},
edition = {1st},
abstract = {Successfully Implement High-Value Configuration Management Processes in Any Development Environment As IT systems have grown increasingly complex and mission-critical, effective configuration management (CM) has become critical to an organizations success. Using CM best practices, IT professionals can systematically manage change, avoiding unexpected problems introduced by changes to hardware, software, or networks. Now, todays best CM practices have been gathered in one indispensable resource showing you how to implement them throughout any agile or traditional development organization. Configuration Management Best Practices is practical, easy to understand and apply, and fully reflects the day-to-day realities faced by practitioners. Bob Aiello and Leslie Sachs thoroughly address all six pillars of CM: source code management, build engineering, environment configuration, change control, release engineering, and deployment. They demonstrate how to implement CM in ways that support software and systems development, meet compliance rules such as SOX and SAS-70, anticipate emerging standards such as IEEE/ISO 12207, and integrate with modern frameworks such as ITIL, COBIT, and CMMI. Coverage includes Using CM to meet business objectives, contractual requirements, and compliance rules Enhancing quality and productivity through lean processes and just-in-time process improvement Getting off to a good start in organizations without effective CM Implementing a Core CM Best Practices Framework that supports the entire development lifecycle Mastering the people side of CM: rightsizing processes, overcoming resistance, and understanding workplace psychology Architecting applications to take full advantage of CM best practices Establishing effective IT controls and compliance Managing tradeoffs and costs and avoiding expensive pitfalls Configuration Management Best Practices is the essential resource for everyone concerned with CM: from CTOs and CIOs to development, QA, and project managers and software engineers to analysts, testers, and compliance professionals. Praise for Configuration Management Best Practices Understanding change is critical to any attempt to manage change. Bob Aiello and Leslie Sachss Configuration Management Best Practices presents fundamental definitions and explanations to help practitioners understand change and its potential impact. Mary Lou A. Hines Fritts, CIO and Vice Provost Academic Programs, University of Missouri-Kansas City Few books on software configuration management emphasize the role of people and organizational context in defining and executing an effective SCM process. Bob Aiello and Leslie Sachss book will give you the information you need not only to manage change effectively but also to manage the transition to a better SCM process. Steve Berczuk, Agile Software Developer, and author of Software Configuration Management Patterns: Effective Teamwork, Practical Integration Bob Aiello and Leslie Sachs succeed handsomely in producing an important book, at a practical and balanced level of detail, for this topic that often goes without saying (and hence gets many projects into deep trouble). Their passion for the topic shows as they cover a wonderful range of topicseven culture, personality, and dealing with resistance to changein an accessible form that can be applied to any project. The software industry has needed a book like this for a long time! Jim Brosseau, Clarrus Consulting Group, and author of Software Teamwork: Taking Ownership for Success A must read for anyone developing or managing software or hardware projects. Bob Aiello and Leslie Sachs are able to bridge the language gap between the myriad of communities involved with successful Configuration Management implementations. They describe practical, real world practices that can be implemented by developers, managers, standard makers, and even Classical CM Folk. Bob Ventimiglia, Bobev Consulting A fresh and smart review of todays key concepts of SCM, build management, and related key practices on day-to-day software engineering. From the voice of an expert, Bob Aiello and Leslie Sachs offer an invaluable resource to success in SCM. Pablo Santos Luaces, CEO of Codice Software Bob Aiello and Leslie Sachs have a gift for stimulating the types of conversation and thought that necessarily precede needed organizational change. What they have to say is always interesting and often important. Marianne Bays, Business Consultant, Manager and Educator}
}

@inproceedings{10.1145/3279720.3279731,
author = {Richards, Brad and Hunt, Ayse},
title = {Investigating the Applicability of the Normalized Programming State Model to BlueJ Programmers},
year = {2018},
isbn = {9781450365352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279720.3279731},
doi = {10.1145/3279720.3279731},
abstract = {It has long been a goal of educators to accurately identify at-risk students early enough in the term to intervene, and the increasing availability of programming process data and learning analytics tools has brought education researchers closer to realizing that objective. The Normalized Programming State Model (NPSM), for example, a recent approach that considers students' editing and testing behaviors in addition to compilations, has been shown to predict a student's overall course grade with 36-67% accuracy [14] depending on the number of assignments analyzed.To date, NPSM has not been evaluated outside of its original context: a group of CS2 students writing C++ programs in Visual Studio. We re-implemented NPSM to analyze the program-development processes of Java programmers using the BlueJ IDE, and applied the model to data collected from 9,338 subjects working to solve a specific programming exercise from the BlueJ textbook. While our study does not yet establish the predictive abilities of NPSM in this new domain, our contributions include the reimplementation of an existing model --- something the education research community has established as a priority --- and a preliminary exploration of the kinds of programming activity an NPSM implementation will encounter when working with BlueJ users. We identify differences between IDEs that necessarily lead to variations in their NPSM models, and highlight aspects of NPSM and this new context that suggest NPSM's predictive power is likely to decrease for BlueJ programmers.},
booktitle = {Proceedings of the 18th Koli Calling International Conference on Computing Education Research},
articleno = {10},
numpages = {10},
keywords = {Predictive measures of student performance and achievement, Normalized Programming State Model, Learning analytics, Educational data mining, BlueJ, Blackbox},
location = {Koli, Finland},
series = {Koli Calling '18}
}

@inproceedings{10.5555/3370272.3370324,
author = {Ahmed, Imtihan and House, Rachael and Deilma, Neil and Luo, Li},
title = {Custom visual recognition model with Watson studio},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {The explosive growth of cameras, image sensors, and computer vision as a discipline of Artificial Intelligence (AI) has garnered strong interest from researchers, developers, businesses and consumers. Image classification refers to a process to classify an image according to a model and match it to a set of classes or categories. Object detection is similar to image classification, which is a process to classify, locate and count multiple objects in an image and their respective locations within the image. Object tracking involves using object detection in each frame of a video to track the desired object through a series of image frames or video [1]. There are a number of use cases for computer vision including face recognition for application or device security, automatically counting and classifying items on a production line, and monitoring and responding to traffic conditions on busy road sections.Computer vision seeks to understand information in digital images through processing and analyzing digital images. This understanding is achieved through extracting high dimension data from images and processing them to produce usable information. Practical applications of computer vision in the context of machine learning include classification, segmentation, and tracking [2].IBM Watson Studio (https://www.ibm.com/watson) is IBM's suite of enterprise-ready AI services, applications and tooling. As a service on IBM Cloud, IBM Watson Visual Recognition uses deep learning algorithms to analyze images for scenes, faces, and objects. This service provides built-in models and can also be used to create and train custom models for specific needs. Watson Studio provides a collaborative platform on top of IBM Cloud's cloud computing capabilities to use existing models or train and deploy new models with minimal coding. Watson Studio has the added capability of setting up custom environments and Notebooks, allowing quick, cloud-enabled development machines that can scale as your projects scale.IBM PowerAI Vision (https://www.ibm.com/caen/marketplace/ibm-powerai-vision) is a Graphics Processing Unit (GPU) accelerated visual recognition solution running on IBM Power Systems. PowerAI Vision (https://www.ibm.com/caen/marketplace/ibm-powerai-vision) puts data science in the hands of subject matter experts. This tool simplifies building machine learning models with IBM Power Systems. As a result, users can build models and deploy them to the web without coding. The models can be accessed through an Application Program Interface (API). On the other hand, users can call the API from their own applications with a few lines of code.IBM provides developers free, open source, state-of-the-art assets for deep learning through the Model Asset Exchange (MAX) (https://developer.ibm.com/exchanges/models/) on IBM Developer. In the repository developers can find both assets for training deep learning models and pre-trained models to use in their projects.The first half of this workshop will focus on exploring the Watson Visual Recognition and Watson Machine Learning Services in IBM Cloud. We will begin by building and deploying a model on Watson Visual Recognition. We will focus on the key benefits of the service, including the ability of anyone with minimal coding experience to be able to train and deploy a computer vision model to the cloud. We will then demonstrate how easy it can be to integrate the model in any web-enabled application through a demo web application. Once this has been completed, we will give a soft introduction to Watson Machine Learning, including how to choose development environments, setting up a Jupyter notebook (https://jupyter.org/), and go over some prepared code snippets to train and analyze a model fully on the cloud. [We will then demonstrate how we can export the model and use it in our applications.In the second half of the workshop, we will demonstrate detecting and labeling objects within an image using PowerAI Vision object detection (https://github.com/IBM/powerai-vision-object-detection), based on customized training. Instead of writing code to train, deploy, and test the new model, we will only need to upload the images, and label the objects in the provided application. Once the model is deployed, we will use the PowerAI Vision user interface (UI) to test it. We will also use our application as a Representational State Transfer (REST) client to locate and count objects in an image using the provided REST API endpoint. At the end of the workshop we will briefly introduce Model Asset Exchange, we will demonstrate how to find a visual recognition model on MAX, deploy it as a microservice and test it.In summary, we will introduce some visual recognition services provided by IBM in this workshop. We together will develop an image classification model using Watson Visual Recognition service with Watson Studio. We also consume a visual recognition service from a client side. Then we discuss the features of PowerAI Vision and demonstrate object detection in PowerAI Vision. Finally, we introduce Model Asset Exchange.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {376–377},
numpages = {2},
keywords = {watson studio, visual recognition, powerai vision, model asset exchange, machine learning, deep learning, artificial intelligence, IBM cloud, IBM Watson visual recognition},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@book{10.5555/524209,
author = {Baker, Art},
title = {Windows NT Device Driver Book: A Guide for Programmers, with Disk with Cdrom},
year = {1996},
isbn = {0131844741},
publisher = {Prentice Hall PTR},
address = {USA},
edition = {2nd},
abstract = {From the Book: PREFACE: In case you havent guessed, this book explains how to write, install, and debug kernel-mode device drivers for Windows NT. If youre in the process of designing or coding an NT driver, or if youre porting an existing driver from some other operating system, this book is a valuable companion to the Microsoft DDK documentation. This book might also have something to say to you if you just need a little more insight into the workings of Windows NT, particularly the IO subsystem. Perhaps youre trying to decide if NT is a reasonable platform for some specific purpose. Or you may be studying operating systems, and you want to see how theory gets applied in the real world. And of course, we mustnt discount the power of morbid curiosity. The same fascination that forces us to slow down as we drive past a car accident can also motivate us to pull a volume off the bookstore shelf. What You Should Already Know Throughout this book, I make several assumptions about what you already know. First of all, you need to have all the basic Windows NT user skills such as logging in and running various utilities. Since driver installation requires you to have administrator-level privileges, you can trash things pretty badly if you dont know how to use the system. Second, youll need decent C-language programming skills. Ive tried to avoid the use of “cleverness” in my code examples, but you still have to be able to read them. Next, some experience with Win32 user-mode programming is helpful, but it isnt really required. If you havent worked with the Win32 API, you might want to browse throughvolumetwo of the Win32 Programmers Reference. This is the one that describes system services. Take a look at the chapters on the IO primitives (CreateFile, ReadFile, WriteFile, and DeviceIoControl) and the thread-model. See the bibliography for other books on Win32 programming. Finally, you need to understand something about hardware in order to write drivers. It would be helpful if you already had some experience working with hardware, but if not, Chapter 2 will give you a basic introduction. Again, the bibliography will point you toward other, more-detailed sources for this kind of information. What Youll Find Here One of the most difficult choices any author has to make is deciding what to write about and what to leave out. In general, Ive attempted to focus on core issues that are crucial to kernel-mode driver development. Ive also tried to provide enough background information so that youll be able to read the sample code supplied with the NT DDK, and make intelligent design choices for your own drivers. The overall flow of the book goes from the theoretical to the practical, with earlier chapters providing the underpinnings for later topics. Heres whats covered: Chapters 1-5: The first part of this book provides the basic foundation youll need if you plan to write drivers. This includes a general examination of the Windows NT driver architecture, a little bit about hardware, and a rather detailed look at the NT IO Manager and its data structures. This group of topics ends with some general kernel-mode coding guidelines and techniques. Chapters 6-13: These eight chapters form the nucleus of the book and present all the details of writing kernel-mode NT device drivers. Youll also find discussions here of full-duplex driver architectures, handling timeout conditions, and logging device errors. Unless youre already familiar with NTs driver architecture, you should probably read these chapters in order. Chapters 14 and 15: The next two chapters deal with alternative driver architectures supported by Windows NT. This includes the use of kernel-mode threads in drivers and higher-level drivers. Chapters 16-18: The final part of the book deals with various practical details of writing NT drivers. Chapter 16 takes a look at all the things your mother never told you about the BUILD utility. Chapter 17 covers various aspects of testing and debugging drivers, including how to analyze crash dumps and how to really get WINDBG to work. If youre actually writing a driver while you read this book, you may want to read these chapters out of order. Chapter 18 examines the crucial issue of driver performance and how to tie your driver into NTs performance monitoring mechanisms. Appendices: The appendices cover various topics that people in my classes have asked about. The first one deals with the mechanics of setting up a driver development environment. The second appendix contains a list of the bugcheck codes youre most likely to encounter, along with descriptions of their various parameters. Used in conjunction with the material in Chapter 17, this may help you track down the cause of a blue screen or two. What You Wont Find I excluded topics from this book for several reasons. Some subjects were just too large to cover. Others addressed the needs of too small a segment of the driver-writing community. Finally, some areas of driver-development are simply unsupported by Microsoft. Specifically, you wont find anything here about the following items: File system drivers: At the time this book went to press, Microsoft still hadnt released any kind of developers kit for NT file system drivers. In fact, there seemed to be a great deal of resistance to the idea within Microsoft. Until this situation changes, theres not much point in talking about the architecture of file system drivers. Net-card and network protocol drivers: NDIS and TDI drivers are both very large topics—large enough to fill a book of their own. Unfortunately, there just wasnt enough room for all of it here. I can offer one bit of consolation: The material in this book will give you much of the background you need in order to understand whats happening inside the NDISTDI framework. SCSI miniport and class drivers: Although SCSI HBA miniport drivers are vital system components, the number of people actually writing them is (I suspect) rather small. Consequently, the only reference to SCSI miniports is the overview material in Chapter 1. I would have liked to include a discussion of SCSI class drivers in this book, but unfortunately there just wasnt any time to write it. The material on developing intermediate drivers in Chapter 15 will give you much of the necessary background. Fro m there, take a look at the sample SCSI class driver for CD-ROMs that comes with the NT DDK. Video, display, and printer drivers: This is another area where I had to make a tradeoff between the number of people writing these kinds of drivers and the time available to finish the book. Unfortunately, graphics drivers for video and hardcopy de vices didnt make the cut this time. Perhaps in a later, expanded version of the book. Virtual DOS device drivers: In my opinion, the best way to run 16-bit MS-DOS and Windows applications under Windows NT is to port the source code to Win32. In any event, the Microsoft documentation does a decent job describing the mechanics of writing VDDs so I havent included anything about them here. About the Sample Code Theres a great deal of sample driver code scattered throughout this book. Youll find all of it on the accompanying floppy disk. Ive created separate directories on the floppy for each chapter, and where appropriate, subdirectories for each component or driver in the chapter. Coding style: Since the purpose of this book is instruction, Ive done a couple things to improve the clarity of the samples. First, Ive adopted a coding style that avoids smart tricks. Some of the examples could probably have been written in fewer lines of code, but I dont think they would have been as easy to understand. Also in the name of clarity, Ive eliminated everything except the bare essentials from each sample. For example, most of the drivers dont contain any error-logging or debugging code, although a real driver ought to include these things. These topics have their own chapters, and you shouldnt have too much trouble back-fitting the code into other sample drivers. Naming conventions: Youll notice that almost all the sample drivers appearing in this book are called “XXDRIVER.” (The only exception is the higher-level driver Chapter 15. Its name is “YYDRIVER.”) This makes it somewhat easier to interchange the parts of different samples. It also reduces the amount of clutter that youll be adding to the Registry while youre playing with these drivers. Within any particular driver, Ive also adopted the convention of adding the prefix, Xx to the names of any driver-defined functions. Similarly, device registers, driver structures, and constants are also prefixed with XX_. This makes it easy to see which things you have to write and which ones come from the folks at Microsoft. Platform dependencies: Its worth mentioning that these samples have been targeted to run on Intel 80x86 platforms. In particular, the drivers all assume that device registers live in IO space rather than being memory-mapped. This is relatively easy to fix with a little bit of coding and some modifications to each drivers hardware-specific header file. To build and run the examples: Youll need several tools if you plan to do any driver development for Windows NT. First, get yourself a Level II subscription to the Microsoft Developer Network CDs. This is the only source for the NT DDK and the Win3 2 SDK. Youll also need a C compiler. Ive chosen to use the Microsoft compiler for developing and testing all the code in this book. Your mileage may vary if youre using some other vendors tools. See Appendix A for more information on setting up your driver development environment. Training and Consulting Services The material in this book is based on classes that Ive been delivering for several years through Cydonix Corporation—a training and consulting firm whose goal is to help its clients develop device drivers and other high-performance Windows NT soft ware. Cydonix offers services that range from formal classroom training to direct participation in software design and coding. For the past three years, Cydonix has been helping companies like Adaptec, AT&amp;T, Compaq Computers, Hewlett-Packard, and Intel to learn more about the workings of Windows NT. We have training available in a number of areas including: Windows NT device driver programming Win32 system service programming Advanced server development techniques Cydonix offers both onsite training at customer facilities and open enrollment classes that are available to the general public. The public classes are hosted by training vendors in several geographic areas. For more information about training and consulting from Cydonix Corporation, visit our Web site at ...}
}

@article{10.14778/3554821.3554836,
author = {Mishchenko, Andrey and Danco, Dominique and Jindal, Abhilash and Blue, Adrian},
title = {Blueprint: a constraint-solving approach for document extraction},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554836},
doi = {10.14778/3554821.3554836},
abstract = {Blueprint is a declarative domain-specific language for document extraction. Users describe document layout using spatial, textual, semantic, and numerical fuzzy constraints, and the language runtime extracts the field-value mappings that best satisfy the constraints in a given document.We used Blueprint to develop several document extraction solutions in a commercial setting. This approach to the extraction problem proved powerful. Concise Blueprint programs were able to generate good accuracy on a broad set of use cases. However, a major goal of our work was to build a system that non-experts, and in particular non-engineers, could use effectively, and we found that writing declarative fuzzy constraint-based extraction programs was not intuitive for many users: a large up-front learning investment was required to be effective, and debugging was often challenging.To address these issues, we developed a no-code IDE for Blueprint, called Studio, as well as program synthesis functionality for automatically generating Blueprint programs from training data, which could be created by labeling document samples in our IDE. Overall, the IDE significantly improved the Blueprint development experience and the results users were able to achieve.In this paper, we discuss the design, implementation, and deployment of Blueprint and Studio. We compare our system with a state-of-the-art deep-learning based extraction tool and show that our system can achieve comparable accuracy results, with comparable development time, for appropriately-chosen use cases, while providing better interpretability and debuggability.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3459–3471},
numpages = {13}
}

@inproceedings{10.1145/2984043.2998388,
author = {Pierce, Benjamin C.},
title = {The science of deep specification (keynote)},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984043.2998388},
doi = {10.1145/2984043.2998388},
abstract = {Abstraction and modularity underlie all successful hardware and software systems: We build complex artifacts by decomposing them into parts that can be understood separately. Modular decomposition depends crucially on the artful choice of interfaces between pieces. As these interfaces become more expressive, we think of them as specifications of components or layers. Rich specifications based on formal logic are little used in industry today, but a practical platform for working with them could signicantly reduce the costs of system implementation and evolution by identifying vulnerabilities, helping programmers understand the behavior of new components, facilitating rigorous change-impact analysis, and supporting maintainable machine-checked verication that components are correct and fit together correctly. Recently, research in the area has begun to focus on a particularly rich class of specifications, which might be called deep specifications. Deep specifications are rich (describing complex component behaviors in detail); two-sided (connected to both implementations and clients); formal (written in a mathematical notation with clear semantics to support tools such as type checkers, analysis and testing tools, automated or machine-assisted provers, and advanced IDEs); and live (connected directly to the source code of implementations via machine-checkable proofs or property-based random testing). These requirements impose strong functional correctness conditions on individual components and permit them to be connected together with rigorous composition theorems. This talk presents the key features of deep specifications, surveys recent achievements and ongoing efforts in the research community (in particular, work at Penn, Princeton, Yale, and MIT on formalizing a rich interconnected collection of deep specifications for critical system software components), and argues that the time is ripe for an intensive effort in this area, involving both academia and industry and integrating research, education, and community building. The ultimate goal is to provide rigorously checked proofs about much larger artifacts than are feasible today, based on decomposition of proof effort across components with deep specifications.},
booktitle = {Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
pages = {1},
numpages = {1},
keywords = {Verified Systems Software, Verified Software Toolchain, Vellvm, QuickChick, Property-Based Testing, Kami, Coq, CompCert, CertiKOS},
location = {Amsterdam, Netherlands},
series = {SPLASH Companion 2016}
}

@book{10.5555/559265,
author = {Girdley, Michael and Emerson, Sandra L. and Woollen, Rob},
title = {J2EE Applications and BEA WebLogic Servers},
year = {2001},
isbn = {0130911119},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Book: Introducing BEA WebLogic Application Servers J2EE Applications and BEA WebLogic Server addresses the need for a practical, state-of-the art book on developing enterprise applications with the market-leading BEA WebLogic Java application servers. The BEA WebLogic family of application servers includes BEA WebLogic Server, BEA WebLogic Enterprise, BEA WebLogic Commerce Server, and BEA WebLogic Personalization Server. This book focuses on BEA WebLogic Server. BEA WebLogic Server (WebLogic Server) is a widely used Java application server for constructing multi-tier, secure, large-scale, distributed Web applications for e-commerce and other high-volume applications. Distributed applications require sophisticated, fast, fault-tolerant networked communication among application tiers and components. In a client-server application, client programs send requests and receive responses from a server system. With the advent of middleware and the Web revolution, many enterprise sites have moved from client-server application environments to n-tierusually, 3- or 4-tierarchitectures. In multi-level architectures, efficient network connectivity is paramount. In a multi-tier application, WebLogic Server provides the framework for developing and deploying server-side business logic, and supports a distributed programming model that hides the complexity of distributed programming from the application writer. The programming model provided by J2EE and the WebLogic Server extensions provides some level of transparency, so that writing a distributed application is similar to writing a local application. Although the programmer must still be concerned about error handling and efficiency, WebLogic Server's implementation of the J2EE services provides an excellent development and execution environment for an enterprise-level distributed application. An application server such as BEA WebLogic Server handles server-side business logic and the administration of a multi-client, distributed application that uses a variety of clients and servers. Giving the responsibility for business logic and traffic control to an application server has the following benefits: Efficiency: Web browser and application clients can share the same business logic, rather than having to deploy business rules with each instance of a client. Performance: Locating server-side business logic with or near resource-intensive modules such as data stores can improve performance. Manageability: System administration and security issues are easier to address when business logic is centralized in an application server. History of BEA's WebLogic Server Division WebLogic, Inc. was founded in 1995--when Java, still a "think tank" project of Sun Microsystems, was code-named "Oak." In 1998, WebLogic merged with BEA Systems, Inc., a major vendor of transaction monitors and other tools for creating and managing enterprise-scale distributed systems. The BEA WebLogic Server product is a part of the BEA E-Business Platform. From the beginning, the WebLogic Server developers determined to use only Java, and to focus on server-side technologies: server support and middleware management of multi-tier applications. Using off-the-shelf Java development tools (and general-purpose text-editing tools such as emacs), the WebLogic Server developers implemented APIs for each new Java standard feature that Sun specified. As a result, WebLogic Server has not only kept current with Java standards development but has also had the capability to influence emerging Java standards. BEA WebLogic Server was an early implementer of each emerging Java Enterprise standard, including Enterprise JavaBeans (EJB), Remote Method Invocation (RMI), servlets, the Java Naming and Directory Interface (JNDI), and Java Database Connectivity (JDBC) for Oracle, Informix, Sybase, and Microsoft SQL Server. Each of these technologies is explained and illustrated in the chapters that follow. In July 2000, the BEA Systems family of application servers successfully completed Sun Microsystems Java 2 Enterprise Edition (J2EE) certification, becoming the first independent company to achieve official J2EE certification. BEA WebLogic Server has won several industry awards, including: Best Java Application Server ( JavaPro , June 2000) Product Excellence and Productivity Award ( Software Development Magazine , March 2000) Java World Reader's Choice Award for best e-commerce application server, 1999 Infoworld 's Product of the Year, 1999 Java Developers Journal Editor's Choice Award in 1998 and 1999 Why We Wrote This Book BEA WebLogic Server has a growing installed base that has been supported by training classes and extensive documentation, but there has been no comprehensive, practical coverage of full-scale application development on the WebLogic Server platform. This step-by-step book explains where to start, and how to put all the pieces together. Planning for deployment and selecting the technologies that you'll use for each tier of the application is as important as laying down code. Target Audience J2EE Applications and BEA WebLogic Server is targeted at intermediate to professional-level Java programmers developing applications for the BEA WebLogic Server platform, the market leader among application servers. This book focuses on best practices for developing enterprise applications using the WebLogic Server APIs. The WebAuction application, a complete sample e-commerce application, is explained and developed as an example in Chapter 14. An accompanying CD-ROM includes all software and code needed to implement the sample application in your own environment. After reading this book, Java developers will possess the skills and knowledge required to develop scalable and robust applications on the WebLogic platform. This book is targeted at programmers who know basic Java on at least an intermediate level and would like to learn WebLogic Server. We assume that readers know about standard Java programming concepts such as exceptions and threads. However, we do not assume that readers know much about J2EE or application servers. Brief Overview of the Book J2EE Applications and BEA WebLogic Server contains both a descriptive narrative and examples for each major J2EE API, and a sample application that concludes the book. Using a step-by-step approach, the book introduces each major J2EE API and uses it to build a component of the WebAuction application, which supports an online auction site. Building the WebAuction application gives users the opportunity to explore significant areas of building a distributed Enterprise Java application, including: Overview of J2EE technologies (Chapter 2) Building presentation logic with servlets or Java Server Pages (JSPs); (Chapters 3 and 4) Establishing database connectivity and using transactions (Chapter 5) Using Remote Method Invocation, and the Naming and Directory Interface (Chapter 6) Using a message-oriented middleware layer to coordinate all the components and operations in your multi-tier, distributed WebAuction application (Chapter 7) Creating Enterprise Java Beans (Chapters 8-10) Integrating Internet mail (Chapter 11) Adding security (Chapter 12) Designing a distributed deployment (Chapter 13) Building and deploying the completed application (Chapter 14) Performing a capacity-planning exercise to assess the performance of the deployed application (Chapter 15) Chapter 1 presents a detailed overview of the book, with a roadmap and chapter summaries. Chapter 1 also lists system requirements and conventions. Chapter 2 surveys the J2EE technologies that are described in depth, with examples, in Chapters 3-12. About the Authors Michael Girdley is the Senior Product Manager for the BEA WebLogic Server, a role in which he acts as marketing liaison to over 200 engineers. An experienced application developer in Java, HTML, C, and C++, Michael is a co-author of Web Programming with Java (Sams-net Publishing, 1996) and Java Unleashed, Second Edition (Sams-net Publishing, 1997). Michael holds a bachelor's degree in computer science with honors from Lafayette College. Rob Woollen is a Senior Software Engineer at BEA Systems. He is currently the lead developer for the WebLogic Server EJB Container. Before joining BEA, Rob worked on UNIX kernel networking for Hewlett-Packard. Rob holds a bachelor's degree in computer science from Princeton University. Sandra L. Emerson is a technical writer and consultant with 20 years' experience in the software industry. She is a co-author of four computer trade books: The Business Guide to the UNIX System (Addison-Wesley); Database for the IBM PC (Addison-Wesley); Troff Typesetting for UNIX Systems (Prentice Hall PTR); and The Practical SQL Handbook (Addison-Wesley, Fourth Edition, 2001).}
}

@inproceedings{10.1109/ICSIP.2014.36,
author = {Rawoof, Abdul and Kulesh and Ray, Kailash Chandra},
title = {ARM Based Implementation of Text-to-Speech (TTS) for Real Time Embedded System},
year = {2014},
isbn = {9780769551005},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSIP.2014.36},
doi = {10.1109/ICSIP.2014.36},
abstract = {Since decades, real time hardware implementation of Text-To-Speech system has been drawing attention of the research community due to its various real time applications. These include reading aids for the blind, talking aid for the vocally handicapped and training aids and other commercial applications. All these applications demand the real time embedded platform to meet the real time specifications such as speed, power, space requirements etc. In this context the embedded processor ARM (Advanced RISC Machine), has been chosen as hardware platform to implement Text-To-Speech conversion. This conversion needs algorithms to perform various operations like parts of speech tagging, phrase marking, word to phoneme conversion and clustergen synthesis. These algorithms are coded and developed in C using eclipse IDE and finally implemented on commercially available ARM9 microcontroller (AT91SAM9263EJ-S). Experiments have been performed on ARM microcontroller using test cases. It has been observed that the performance of the ARM based implementation is very close to x86 implementation.},
booktitle = {Proceedings of the 2014 Fifth International Conference on Signal and Image Processing},
pages = {192–196},
numpages = {5},
keywords = {Text-to-Speech, Real Time Embedded System, ARM processor},
series = {ICSIP '14}
}

@inproceedings{10.1145/3180155.3182537,
author = {Bao, Lingfeng and Xing, Zhenchang and Xia, Xin and Lo, David and Hassan, Ahmed E.},
title = {Inference of development activities from interaction with uninstrumented applications},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3182537},
doi = {10.1145/3180155.3182537},
abstract = {Studying developers' behavior is crucial for designing effective techniques and tools to support developers' daily work. However, there are two challenges in collecting and analyzing developers' behavior data. First, instrumenting many software tools commonly used in real work settings (e.g., IDEs, web browsers) is difficult and requires significant resources. Second, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis.In this paper [1], to address these two challenges, we first use our ActivitySpace framework to improve the generalizability of the data collection. Then, we propose a Condition Random Field (CRF) based approach to segment and label the developers' low-level actions into a set of basic, yet meaningful development activities. To evaluate our proposed approach, we deploy the ActivitySpace framework in an industry partner's company and collect the real working data from ten professional developers' one-week work. We conduct an experiment with the collected data and a small number of initial human-labeled training data using the CRF model and the other three baselines (i.e., a heuristic-rules based method, a SVM classifier, and a random weighted classifier). The proposed CRF model achieves better performance (i.e., 0.728 accuracy and 0.672 macro-averaged F1-score) than the other three baselines.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {897},
numpages = {1},
keywords = {conditional random field, developers' interaction data, software development},
location = {<conf-loc>, <city>Gothenburg</city>, <country>Sweden</country>, </conf-loc>},
series = {ICSE '18}
}

@inproceedings{10.1109/ICSE43902.2021.00145,
author = {He, Xincheng and Xu, Lei and Zhang, Xiangyu and Hao, Rui and Feng, Yang and Xu, Baowen},
title = {PyART: Python API Recommendation in Real-Time},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00145},
doi = {10.1109/ICSE43902.2021.00145},
abstract = {API recommendation in real-time is challenging for dynamic languages like Python. Many existing API recommendation techniques are highly effective, but they mainly support static languages. A few Python IDEs provide API recommendation functionalities based on type inference and training on a large corpus of Python libraries and third-party libraries. As such, they may fail to recommend or make poor recommendations when type information is missing or target APIs are project-specific. In this paper, we propose a novel approach, PyART, to recommending APIs for Python programs in real-time. It features a light-weight analysis to derive so-called optimistic data-flow, which is neither sound nor complete, but simulates the local data-flow information humans can derive. It extracts three kinds of features: data-flow, token similarity, and token co-occurrence, in the context of the program point where a recommendation is solicited. A predictive model is trained on these features using the Random Forest algorithm. Evaluation on 8 popular Python projects demonstrates that PyART can provide effective API recommendations. When historic commits can be leveraged, which is the target scenario of a state-of-the-art tool ARIREC, our average top-1 accuracy is over 50% and average top-10 accuracy over 70%, outperforming APIREC and Intellicode (i.e., the recommendation component in Visual Studio) by 28.48%-39.05% for top-1 accuracy and 24.41%-30.49% for top-10 accuracy. In other applications such as when historic comments are not available and cross-project recommendation, PyART also shows better overall performance. The time to make a recommendation is less than a second on average, satisfying the real-time requirement.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1634–1645},
numpages = {12},
keywords = {real-time recommendation, data flow analysis, context analysis, Python, API recommendation},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s42979-022-01033-z,
author = {Kouhi, Mona and Rahmani, Mohsen},
title = {Design and Development of a Mobile Application for Teaching Triple Multiplication to Preschool Children},
year = {2022},
issue_date = {Mar 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {3},
number = {2},
url = {https://doi.org/10.1007/s42979-022-01033-z},
doi = {10.1007/s42979-022-01033-z},
abstract = {Due to the difficult educational condition caused by the COVID-19 pandemic, the design and development of mobile phone apps and technologies for teaching and learning are being more considered. Moreover, technology-based infrastructure for the use of distance education is developing rapidly. In this paper, an Android application called Triangular App (TriApp) was designed for teaching multiplication to preschool children. This application uses the Triple Multiplication method in which three numbers (including two numbers, and the product of multiplication of these two numbers) are considered as a group. By repeating and memorizing this group, if any of the numbers are not displayed, the child can guess the missed number. With TriApp, in addition to multiplication, children can also learn division. There is a test and competition section in which the child receives points by giving the correct answers, which increases the children's desire to learn. To evaluate the effectiveness of the TriApp, two preschool children groups were examined in 10 days. The former learned multiplication using TriApp and the later learned it using the multiplication table (repeated additions). The results showed that during test, 100% of the children that used the TriApp could answer asked multiplications correctly and only 40–70% of the children who were trained through multiplication table could answer asked multiplications correctly. Also, the children were satisfied 94% with the application, whereas, they were satisfied only 60% with the multiplication table. This mobile application was developed using Java programming language, Eclipse IDE, and SQLite database.},
journal = {SN Comput. Sci.},
month = {feb},
numpages = {13},
keywords = {Multiplication, Distance education and online learning, Preschool education, Mobile learning, Mobile application}
}

@book{10.5555/515925,
author = {Reed, Paul R.},
title = {Developing Applications with Java and Uml},
year = {2001},
isbn = {0201702525},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: PREFACE: This book focuses on the most powerful approach available today to model and build industrial-strength Java applications: the Unified Modeling Language (UML) adopted in 1997 by the Object Management Group (OMG). A project lifecycle and software process model are demonstrated (Rational's Unified Process) using a sample application from requirements gathering, using Use Cases, through implementation via the creation Java code from Class and Sequence diagrams. This sample application uses the latest Java technology frameworks such as Java Server Pages (JSP), Servlets, and most importantly, the Enterprise Java Bean 2.0 (EJB) server-side enabling technology for the implementation of the business rules. Products to implement these server-side solutions range from the Apache Tomcat server to commercial applications servers such as BEA's Weblogic server. Reason for the Book It took me many years to understand that writing a program was nothing more than a learned tactical skill. To program in a language like Java is to be a journeyman. But to somehow capture someone's requirements in an intelligent fashion and organize the necessary resources and resulting software into a cohesive deliverable, is the sign of a strategic craftsman. To me, the majority of Java books never consider Java in "the large." They focus on the small view, covering single Java enabled extensions such as JavaBeans, Servlets and Java Server Pages. Although these views, too, are necessary, unfortunately no one seems to touch on project planning, software process, and the methodology for building enterprise-status Java applications. This is a difficult topic to explore andpresent as the whole subject of process stirs many heartfelt debates and opinions. At the urging of many of my colleagues and supportive readers of my first book, Developing Applications with Visual Basic and UML, I have undertaken a similar project for Java. Who Should Read This Book This book is for anyone who wants to successfully build Java applications that can stand up over time. It provides an accurate road map for anyone to achieve the following goals. Review two processes, one commercially available through Rational Software called the Unified Process (UP) and one from the author's experiences called Synergy. The greatest emphasis will be placed on the Unified Process. Establish a sound project plan (presented in-depth in Appendix E). Estimate projects with confidence, rather than a rule-of-thumb approach. Understand and describe the requirements of the application using UML Use Cases. Create a sound design based UML Class and Sequence diagrams. Use a visual modeling tool such as, Rose, by Rational Software not only to create and track UML artifacts but also to generate skeletons for the component code. Although this author firmly believes that an automated code-generation process is a big factor contributing to successful projects, it by far is not mandatory. Use Java to build server-side Java functionality employing frameworks such as Java Server Pages (JSP), Servlets, and Enterprise JavaBeans 2.0 (EJB). Produce the code for the project using an evolutionary approach showing various technology options: 1). Servlets, JSP, and JavaBeans 2). Servlets, JSP, and Bean-Managed Persistence (BMP) 3). Servlets, JSP, and Container-Managed Persistence (CMP). Investigate the benefit of deploying Java applications on both open-source products like the Apache Tomcat server as well as using commercial Application Server products such as BEA's Weblogic application server. Anyone building Java applications today needs this book. What You Need to Know to Use This Book Maybe it's best to start out with what you don't need to know to benefit from this book. First, you don't need to know anything about the UML. I present the essential aspects of the UML and, more important, how they relate to Java deliverables. Although the UML is expressed with nine separate diagrams, you will benefit the most from a core set. Second, you don't need a formal background in object-oriented concepts (but it certainly wouldn't hurt). I discuss standard object constructs in the text in Chapter 2. Third, you should have some conversational understanding of what Enterprise JavaBeans are. For a really thorough treatment of Enterprise JavaBeans (EJB), you should focus on one of the many texts that cover them in more detail. A favorite book of mine is by Richard Monson-Haefel entitled Enterprise JavaBeans (O'Reilly). The reader would also benefit by having some exposure to Java Server Pages (JSP). A favorite book of mine is by Hans Bergsten entitled Java Server Pages (O'Reilly). This book does assume that you have a working knowledge of Java. Both the new Java programmer and the experienced Java programmer will benefit. However, I don't cover the basics of simple Java constructs, assuming that you already know these. I do briefly review the tenets of Java's support for object-oriented principals in Chapter 2, but only as a baseline for other topics related to the UML. If you have had no exposure to Java, buy this book anyway and open it after you have had some initial training in that programming language. This book places an emphasis on the most mainstream Java techniques and products that are used to build production applications. When I began this book I planned to cover all kinds of java technologies (i.e., applets, java applications talking to Servlets or JSPs). However, it quickly became apparent to me that the majority of my clients and my associate's clients were all pretty much cut from the same mold when you looked at their architecture. They consist of a light client browser on the front-end (with minimal JavaScript for syntax editing) a web server intercepting those browser requests with either Servlets and/or Java Server Pages acting as a broker within some container product which houses the business rules. These business rules are either implemented as JavaBeans or Enterprise JavaBeans. The container products range from open-source solutions like Apache Tomcat to commercial products. The two biggest of the commercial application server players I run across are BEA with their Weblogic product and IBM with their Websphere product. This doesn't mean there aren't more good commercial container products but these two vendors have the lion's share of the market. This book will utilize a light client-side technology (no applets or java applications), web server running Servlets and Java Server Pages who in turn message to either JavaBeans (Tomcat) or Enterprise JavaBeans (session and entity beans) residing in a commercial application server. In the case of the later, I have chosen to use BEA's Weblogic as my application server. Don't get discouraged if you are using another vendor's application server product because this book's coverage of EJB is based on the 2.0 specification. This release of EJB resolved many of the ambiguities that disallowed beans to be truly transportable across vendor implementations. So, regardless of your EJB vendor, you will be able to use the code built in this book. It would be unfair to say you will know, for instance, everything about EJBs after reading this book. If you already know about EJBs then this book will better help you put them into a sound design architecture. The emphasis is placed on the notation, UML, and the process, Unified Process and Synergy, in beginning, developing, and implementing, a software project using the Java language. The benefit of seeing an application from requirements gathering to implementation is the key to goal of this book. This is where I shall place my emphasis. Structure of the Book Following is a summary of the book's chapters and contents. Chapter 1: The Project DilemmaThis chapter reviews the current state of software development and my reasoning regarding why it's in the shape that it is today. It also reviews the concept of iterative and incremental software development and provides an overview of both the Unified Process from Rational Software and my Synergy methodology. It also touches on the primary components of the UML that will be covered in more depth later in the book. Chapter 2: Java, Object-Oriented, and the UMLThis chapter covers some of the benefits that result from the adoption of Java as a development environment. It presents these in the context of Java's implementation of encapsulation, inheritance, and polymorphism. It then maps the UML to various Java deliverables. Highlights include mapping the UML class to Java classes and Java interfaces; mapping use case pathways to Java entity, interface, and controller types of classes; and mapping component diagrams to Java classes and Java packages. Chapter 3: Getting the Project StartedThis chapter explores the case study used in the book, Remulak Productions. This fictional company sells musical equipment and needs a new order entry system. It introduces a project charter, along with a tool, called the event table, to help quickly solidify the application's features. Further, the chapter maps events to the first UML model, the use case. Chapter 4: Use CasesThis chapter reviews the use case, one of the central UML diagrams. Included is a template to document the use case. Actors and their roles in the use cases are defined. The chapter reviews the concept of use case pathways, as well as the project's preliminary implementation architecture. Also reviewed is an approach to estimating projects that are built by using the use case approach. Chapter 5: ClassesThis chapter explores the class diagram, the king of UML diagrams. It offers tips on identifying good class selections and defines the various types of associations. It also covers business rule categorization and how these rules can be translated into both operations and attributes of the class. Finally, it discusses the utilization of a visual modeling tool as a means to better manage all UML artifacts. Chapter 6: Building a User Interface PrototypeThis chapter reviews unique user interface requirements of each use case. It develops an early U/I prototype flow and an eventual graphical prototype. Finally, it maps what was learned during the prototype to the UML artifacts. Chapter 7: The Dynamic Elements of the ApplicationThis chapter discusses the dynamic models supported by the UML, exploring in depth the two key diagrams, often referred to as the interaction diagrams, sequence and collaboration. These are then directly tied back to the pathways found in the use cases. Other dynamic diagrams discussed include the state and activity diagrams. Chapter 8: The Technology LandscapeThis chapter covers the importance of separating logical services that are compliant with a model that separates services. It explores technology solutions specific to the Remulak Productions case study, including distributed solutions and the Internet using HTML forms, JSP and Servlets. Both JavaBeans and Enterprise JavaBeans as a solution for housing the business rules are also be explored. Chapter 9: Data Persistence: Storing the ObjectsThis chapter explores the steps necessary to translate the class diagram into a relational design to be supported by both Microsoft SQL Server and Oracle databases. It offers rules-of-thumb regarding how to handle class inheritance and the resulting possible design alternatives when translating to an RDMBS. This book will deliver solutions that range from roll-your-own persistence using JavaBeans and JDBC, all the way to Container Managed Persistence (CMP) features of the EJB 2.0 specification. The later removes all the requirements of the application to write SQL or control transactions. This chapter introduces the concept of Value objects to reduce network traffic as well as Data Access Objects that encapsulate SQL calls. Chapter 10: Applying the InfrastructureThis chapter finalizes the design necessary to implement the various layers of the application. It also presents the communication mechanism utilized between the layers and possible alternatives. Each class is delegated to one of three types: entity, boundary, or control. These are used as the basis for the design implementation and as the solution to providing alternative deployment strategies. Chapter 11: Constructing a Solution: Servlets, JSP, and JavaBeansThis chapter builds the first Architectural Prototype for Remulak and does not rely on Enterprise JavaBeans. Using the Maintain Relationships use case as the base, the various components are constructed. The primary goal of the Architectural prototype is to reduce risk early by eliminating any unknowns with the architecture. This chapter uses the Apache Tomcat server and introduces the concepts of user interface and use case controller classes. Chapter 12: Constructing a Solution: Servlets, JSP, and Enterprise JavaBeansThis chapter initially uses Rational Rose to generate EJB components. A primer on EJB is offered along with a thorough discussion of the transaction management options in the EJB environment. Session beans are utilized as the use case controller. Both a Container Managed Persistence (CMP) and Bean Managed Persistence (BMP) are presented. Leveraging the Data Access Objects created in the previous chapter is paramount to the success of a BMP implementation. Updates and Information I have the good fortune to work with top companies and organizations not only in the United States, but also in Europe, Asia, and South America. In my many travels, I am always coming across inventive ideas regarding how to use and apply the UML to build more-resilient applications that use not only Java but also C, C# and Visual Basic. Please visit my Web site at www.jacksonreed.com, where you can get the latest on the training and consulting services that I offer, as well as all of the source code presented in this book. I welcome your input and encourage you to contact me at prreed@jacksonreed.com.}
}

@book{10.5555/553611,
author = {Klander, Lars and Mercer, Dave},
title = {Access 2000 Developer's Black Book with Cdrom},
year = {1999},
isbn = {1576103498},
publisher = {Coriolis Group Books},
address = {USA},
abstract = {From the Book: Introduction Introduction This book is about power, the growing power of databases, computers, and networks to slash costs and dramatically increase effectiveness of communications and management. Databases touch everyone's lives in some way or another, and a clear understanding of what works and what doesn't puts that power within reach. This book is aimed at everyone who must participate in a database project to ensure success: database designers, end users, database administrators, senior managers, front-line managers, as well as those who must wear all these hats at once. • For experienced database designers and administrators, this book contains complete coverage of Microsoft Access 2000 in easy-to-understand (and use) examples, with plenty of reusable code and screen shots. • For managers and end users, this book contains plain-English explanations of how databases are constructed, what the limitations are, and a broad, exciting view of the potential. • For those who must act as designer, manager, and user, this book takes you from the most basic fundamentals to the most advanced programming steps, without requiring a degree in computer science. In every sense of the word, this book is a practical, day-to-day guide for people involved in building database solutions. Not only does it guide you through the phases of successful database projects (large and small) and the pitfalls that have ruined some, it teaches you the language and terminology used on all sides as you go: project management, process reengineering, relational models, programming fundamentals, and so on. The emphasis throughout the book is on enhancing communications, because clear and timely communication is the primary attribute of a successful database solution. Communication takes work. Everyone must be working from the same playbook for a database application to be effective and achieve widespread use. Traditionally, databases have been designed by computer scientists far removed from the day-to-day activities of work. A team of systems analysts would show up one day, gather what information they could about a process, then spend a year or two in the ivory tower building the application. The application would be instituted, workers trained to fill out the forms, and the reports would print, all according to the now-dated but assuredly very accurate specifications. For some applications, this system worked quite well, but for others it failed miserably. Where failure occurred, the primary cause was rapid change: changes in processes, requirements, business conditions, even changes in computer literacy. Today, because change is constant and the pace of change continues to accelerate, only excellent communication among everyone involved can overcome the swirling confusion born of change. This book gives everyone the playbook that they need to achieve these implementation goals. It takes the best of all traditional methodologies for rebuilding an organization's processes and for developing and constructing database solutions, explains them concisely, and blends them together into a powerful toolkit for building effective applications in a rapidly changing environment. The power of Microsoft Access 2000 combined with the proven methods outlined in this book increase the probability that your database application, no matter the size, will "work" from all perspectives. For the managing members of the team, the book helps you create a plan for effective and consistent implementation of your applications, whether destined for internal use throughout the enterprise or for public consumption. For those responsible for the creation of the implementation-the developers, power users, and users who will interact with the application on a regular basis-this book teaches you everything you need to know about making the application not only perform its tasks, but perform them well. No matter what environment you are developing for-from standalone databases at workstations to databases that will serve intranet and Internet users-this book teaches you how to address development issues in that environment and make sure your product not only works, but shines. Real-world examples, step-by-step explanations, and thousands of lines of program code all work together to ensure that you have all the tools you need to be successful. Contents Of This Book This book is divided into 8 parts, intended to guide you through the steps of database development with Access from beginning to end. Part I, "Fundamentals Of Information," contains three chapters that consider the nature of information and how information relates to the design of databases. The three chapters in the section, Chapter 1, "Foundations For Database Construction," Chapter 2, "The Nature Of Information," and Chapter 3, "Data Organization," guide you through the principles of information theory and the ways in which data is organized. Each chapter provides you with important information that you must understand to master effective techniques of database design. Part II, "Database Fundamentals," takes the information theory that you learned about in the first three chapters and brings it to the level of database design theory and principles. Chapter 4, "Relational Databases," introduces you to the principles of database design when working with relational databases like Access 2000. Chapter 5, "Database Structures," looks at the overall theory of database design and reviews the principles of relational database design that you learned in Chapter 4. Chapter 6, "Advanced Database Systems," considers the nature of advanced database architectures and the networks required to support them. By the time you finish Part II, you will have a solid knowledge base for database design-not only with Access, but with any relational database product. Part III, "Modern Database Implementation," moves on to some of the specific types of database uses in business today. Chapter 7, "Data Warehousing," covers the construction of data warehouses in depth. Chapter 8, "Applications and Operating Systems," covers practical application and operating systems (OS) issues, namely, how to decide whether to buy or make apps and OSs, and how to find and use them if you do decide to buy. Chapter 9, "Marketing," discusses the important considerations for you to keep in mind when preparing to distribute your Access products. From identifying your target market to measuring and adjusting your market strategy, effective marketing techniques can make a product successful or, if implemented poorly, can ensure it never sells a copy. Part IV, "Microsoft Access 2000 Overview," contains four chapters that address the specific improvements and changes to Access 2000, and the specific purposes for which Microsoft designed the Access 2000 product. Chapter 10, "Access 2000 Technologies," gives you a broad overview of some of the many component technologies that Access uses to simplify user access. Chapter 11, "New Features And Trends In Access 2000," considers some of the directions in which Microsoft has moved the Access product, including a discussion of the new Jet 4 engine and new integration with Microsoft's BackOffice products, specifically SQL Server. Chapter 12, "Access Purchasing And Installation," discusses such important implementation issues as who needs Access installations and what level they need, what the different types of Microsoft Office suites are, and specific installation concerns to keep in mind when purchasing the new Access 2000 product. Chapter 13, "Access 2000 Distribution And Training," addresses specific issues related to the deployment of the Access program in your enterprise. It also discusses built-in training support in the Access product and issues to consider when determining how and who to train. Part V, "Microsoft Access 2000 Usage," contains three chapters, each of which considers a general category of the target market for the Access product and how to design databases for that market. Chapter 14, "Access For Personal And Small Office/Home Office Use," addresses common uses of Access at home and in the small office setting. It discusses both common situations in which you might use Access databases and ways in which to create those databases. Chapter 15, "Using Access In A Corporate Environment," addresses common techniques for Access deployment within companies. It also contains your first introduction to the new Access Data Projects (ADPs) and their use as a SQL Server database front-end. Chapter 16, "Using Access For Scientific And Medical Purposes," considers common methods and implementations for Access in the scientific and medical communities. It also provides an introduction to the use of Access's graphing capabilities and presents useful information in both types of deployment environments. Part VI, "Database Application Design Reference," moves on to the creation of databases in Access 2000. The five chapters in this section provide you with a method that you can use to define and create databases to meet any need. Chapter 17, "Problem Definition And Design Planning," discusses the specific steps you should take in planning the design of a database to solve a particular problem and walks you through an extended example of these crucial steps in the design process. Chapter 18, "Planning And Design," moves on to the specific discussion of designing a database in accordance with the design planning that you performed in Chapter 17. Chapter 19, "Database Construction," shows you how to take an actual design diagram and convert it into table and database definitions in Access. Chapter 20, "Implementation-Beta Testing And Bug Checking," moves on to the testing and implementation phases of application design, including discussions of the testing process you should use and more. Chapter 21, "Completing The Implementation," discusses post-release improvements you can make to the application, including optimization, compacting, and repair of the database, as well as using the Access-provided tools to analyze and improve performance of your application. Part VII, "Microsoft Access 2000 GUI And VBA Programming Reference," covers the low-level, "nuts and bolts" of Access 2000 programming. The chapters take you from the initial creation of a database and its component tables through advanced programming with ActiveX Data Objects (ADO), Data Access Objects (DAO), and database management and security. Chapter 22, "Installation, Setup, And Configuration," discusses the installation specifics of the Access product, including the options you have during setup. It also introduces you to some of the specifics of Access database design-both for the standalone and the client-server environment. Chapter 23, "Developing Tables And Relationships," introduces you to the specifics of table creation and relationship definition, the core of database design. Chapter 24, "Creating Queries," takes you into the heart of relational database work, by teaching you how to create the different types of queries that lie at the heart of SQL's power. Chapter 25, "Creating Forms And Reports," teaches you the knowledge you need to create user interfaces and design effective reports that output data in the most usable form. Chapter 26, "Creating Macros And Modules," explains Access's macro language and introduces you to modules, which will contain Visual Basic for Applications code-code which will, in turn, unlock significant additional power for your database applications. Chapter 27, "Using Modules And Visual Basic For Applications," builds on the knowledge you gained in Chapter 26 to teach you what you need to know about writing VBA programs to "power-up" your Access applications. Chapter 28, "Working With DAO And ADO," introduces you to the database objects that VBA lets you use to manipulate Access, SQL Server, Oracle, and other ODBC- and OLE DB-compliant databases. Chapter 29, "Using Class Modules With Access," describes some basics of object-oriented programming and how you can use VBA class modules to implement custom objects within your Access 2000 applications. Chapter 30, "Advanced Database Design Techniques," shows you how to take advantage of VBA and Access's built-in features to make your applications more professional. It also focuses in-depth on the administration of security within your Access database. Part VIII, "Microsoft Access 2000 And Client-Server Development," contains five chapters that teach you about client-server programming with Access 2000 and different server-based database products, as well as how to create World Wide Web front-ends for Access or server databases. Chapter 31, "Client-Server Programming With Access 2000," introduces you to SQL Server and working with back-end products from Access. It also covers, in detail, important conceptual information about client-server design that is applicable to any back-end. Chapter 31 also introduces you to the Microsoft SQL Developer Engine (MSDE), a local implementation of SQL Server that you can use to design SQL Server databases on your development machine. Chapter 32, "Using Oracle And Access For Client-Server," teaches you the fundamental concepts of Oracle database design and the differences in development between Access front-ends for SQL Server and Oracle. Chapter 33, "Advanced Client-Server Techniques," takes the knowledge from Chapters 31 and 32 and sends it to the next level with important information about topics such as triggers and stored procedures, transaction processing, Access Data Projects (ADPs), and more. Chapter 34, "Web Front-End Development," moves to the largest client-server environment in the world-the Internet. It covers historically proven and commonly used techniques for exposing databases through HTML pages. Chapter 35, "Using Data Access Pages For Web Front Ends," moves on to Microsoft's proprietary Access front-end technology, Data Access Pages (DAPs), which let you develop highly customized, highly responsive front ends for your Access databases, all from within the Access Interactive Development Environment (IDE). In addition to a complete index, the book also contains an appendix of additional resources. From the first 9 chapters that step you through the fundamentals of database design and relate them to good programming practice and business process reengineering, to the next 26 chapters that cover every detail about how Access 2000 works (including how it interacts with the Web and mainframe databases), the purpose of this book is to build a common ground on which all people, from the novice user to the most sophisticated IT developer, can work together. Remember, people in organizations today recognize how important it is to make the powerful software tools on their desktops useful, and they need a tool like this book to make it happen.}
}

@book{10.5555/517956,
author = {Flannery, Ron M.},
title = {The  Informix Handbook with Cdrom},
year = {2000},
isbn = {0130122475},
publisher = {Pearson Higher Education},
edition = {1st},
abstract = {From the Book: PREFACE: Introduction: How to Use This Book What Makes This Book Unique This book is intended to be a comprehensive reference to Informix products. It provides a substantial amount of detail as well as reference information. You can find much of this information in the many Informix manuals, but this book organizes it into one place and helps you find additional information. In addition, the book works in conjunction with its CD and Web site (www.informixhandbook.com) to provide a complete and long-term, ongoing reference. We want this to be one reference that can be on the shelf of any Informix developer or administrator. The information is organized into various functional groups, simplifying the process of finding what you need. My vision in designing this book is this: Create one reference that will help every level of Informix database administrator, application developer, system administrator, and end user. Organize the information in an easy-to-find fashion for all major Informix product lines. In conjunction with the book, its CD, and Web site, provide appropriate examples and detail, directing the readers to additional places to look for information. Supply a reasonable amount of "behind-the-scenes" information but focus on functionality. Use authors that specialize in particular Informix products and are well known in the Informix world. Make this the book to choose if you would like one reference on Informix. That's a pretty tall order, isn't it This book does not claim to be "everything to everybody" but it can help you on many different levels. If it can't provide a specific answer, it will tell youwhereto find the answer. One of the major values with this book is that its Web site (www.informixhandbook.com) provides updated reference material for each chapter as well as numerous links that help you find more information. Many comprehensive Informix references in the world just don't exist--at least not one-volume references--and I'm one Informix devotee who wants to fill that gap. The book also spans different versions of Informix. We provide plenty of information to get you up and running with older versions of the product, including INFORMIX-SE and OnLine Server (version 5.x). We offer more of a focus on the family of Informix Dynamic Server (IDS) products, including IDS versions 7.x, IDS.2000, Informix Internet Foundation.2000, and the data warehouse products. You may already know the names of many of the authors of this book. They were chosen from the local and international user groups as well as Internet newsgroups, ensuring that you will learn from some of the most knowledgeable Informix experts in the world. When choosing authors, I considered critical the fact that they had a special focus in the areas about which they wrote. In addition, all authors worked within the design and vision of this book, helping provide a consistent look and feel. To ensure overall consistency, I read and edited all of the chapters after they were submitted. We make this book more enjoyable by sprinkling it with amusing stories from Informix employees and others in the industry. These stories help add spice and make your learning experience much more enjoyable. All the way from this introduction to the CD-ROM to the Web site, we are crafting this book to meet the needs of Informix users worldwide. Please learn how to use it and have fun with it! The Web Site: An Ongoing Reference This book was designed by technologists who believe in using the best tools for the job (I guess that's why we choose Informix products!). One of the major values that I think you can find with this book is that it has a very useful Web site. The site is designed to enhance and supplement the book on an ongoing basis. Using it in conjunction with this book, you will always be "current" on what is going on with Informix and the particular products. The site is found at www.informixhandbook.com and includes the following: Many informational Web site links about Informix and its products Enhanced and current information for each chapterfor example, if a product has changed or enhanced, we discuss it Further Web site links for each chapter Errata (error corrections, if any!) for each chapter Ability to use our monthly e-newsletter, which provides news and information about Informix and the book The latest Informix announcements and product information When the next edition of the book is released, most of the updates for each chapter will be included in the book. I truly intend the site to make this book a long-term, complete reference for your Informix needs. Please be sure to view the site and learn how to use it. If you want monthly news and updates about Informix, be sure to subscribe to our e-newsletter. As you read through each chapter, be sure to check the Web site for any new or updated information. I think that the site will prove to be a great value you are receiving by purchasing this book. Our Target Readers This book is designed to be an excellent tool for all levels of Informix users, including: Application developers Database administrators Informix server administrators End users The information is designed and organized in such a way that it should be easily accessible by all of these groups. As you'll soon see, the book is divided into six major functional sections, placing related information together. In addition, each chapter has numerous cross-references that help you know where to find more information. The book can help all types of users because it provides many levels of instructional information, detailed reference and review materials, and links for where to get more information. The remainder of this introduction explains how to use the book to its fullest capacity. What You Can Gain from This Book This book helps you understand current and future products and technologies related to Informix. We describe "how Informix works" and how to use Informix servers and their application development tools. We help you with database administration, server setup, and operating system interaction. We also discuss new technologies like Web/database applications and Informix Internet Foundation.2000 and how they fit into the future of Informix. As previously described, one of the main values of this book is its Web site. The site turns the book into an ongoing reference--it provides updated information on each chapter, errata (correction of errors in the book), and numerous Web site links that are grouped by functional category. The book provides references and working examples of the main functional areas of Informix. Our intent is to let you use the book in many different ways, each helping you find information for the Informix product lines. The extensive cross-referencing helps train you where to look for more complete answers. By consulting with many others in the Informix world, we believe that the organization of the book provides a long-desired "one-stop shopping" reference for Informix. This is accomplished by: Organizing the sections into major functional areas Creating a Web site that provides ongoing information about Informix and its products Selecting writers that are well known in the Informix world Providing examples that apply to major areas of Informix usage, spanning different products and servers Supplying excellent cross-references to finding more detailed information by using other reading materials and the World Wide Web Using the CD-ROM to efficiently allow searching and referencing information The design of this book follows these principles and should allow you to know exactly how to find the information you need, be it from this book or another reference. How to Use This Book, the CD, and the Web Site This book provides a unique way to get the information you need about Informix. From its beginnings, the book itself was structured to help you understand the core information about various aspects of Informix. But as the book progressed, so did the Web and other technologies. At that point, I decided to create an even better way to get the information to you. Here's how it works: This book usually serves as your starting point. You can try to find the exact information that you need in the appropriate chapter. If you can't, each chapter has references that direct you to more information elsewhere in the book, Informix manuals, the Web, or other books. If convenient, you can check the other books or manuals, or continue with the rest of this list. Check the book's Web site for the specific chapter. You might find updated information about the material or Web site links to direct you to more information. In addition, you can use the site's search feature to try to find an answer. After that, you can use the site's "Information about Informix" links. Finally, the site offers an e-newsletter that will keep you updated on news about Informix and the book. Check the book's CD-ROM for additional answers. You can use the well-organized information in the CD to find documentation or tools that you'll need. Also, be sure to watch the Web site for CD-ROM upgrades and search capabilities for the CD. All in all, the book, its CD, and Web site should help you find the answers you need. The following sections provide detail about each of these items. How This Book Is Organized Let's take a look at the organization of this book. The book is broken into six major functional sections. You will be able to easily determine where to look for answers. Within each of the six sections, chapters provide more detail. Here is a summary of the sections: I. Core Concepts of Informix This section describes the Informix Corporation, its products, and the future plans of Informix. We begin by giving a history of the Informix Corporation and how it evolved into what it is today. We describe the different types of Informix databases and tools and how they fit into your needs. Chapter 3 helps you get started; it teaches you how to use Informix's sample stores database and its utilities. Chapter 4 provides an extremely detailed view of what's behind the architecture of Informix, now and into the future. Chapter 5 describes the Informix version of SQL and how to use it. Finally, the last two chapters in this section show you how to access data in Informix databases and describe Informix's data warehousing direction. II. Informix SQL In this section, we provide detail about how to use Informix's version of SQL. We describe and show you how to create databases, tables, and indexes. We give detailed examples and tips about how to use the Informix SELECT, UPDATE, INSERT, and DELETE statements. Stored procedures and triggers are discussed, as well as other miscellaneous Informix commands. Finally, Chapter 16 explains some of the additions to Informix in version 7.30. All in all, this section gets you "online" with Informix and allows you to access data. III. Server Administration This section explains how to administer Informix database servers. The section is split into two sub-sections: Setting Up Informix and Ongoing Administration. We describe everything from simple INFORMIX-SE installations to detailed Informix Dynamic Server administration. We provide detail about how to set up and debug various servers on UNIX, NT, and Windows 2000, and describe how to work with the operating system. We help you set up your configuration parameters (the onconfig file, for example) and troubleshoot problems. We provide a comprehensive functional reference of the command-line utilities, such as onstat (complete syntax for this command is given in the appendix). Finally, we help you learn how to keep your servers up and running and how to back them up. As always, we tell you where to find more detailed information. IV. Performance Tuning The Performance Tuning section describes performance tuning both for application developers and Informix administrators. We explain how to use the tools that Informix provides, how to interpret the output, and how to make changes. For application developers, we offer a summary of fundamental design and tuning issues, and information about how to work with Informix tuning methodologies such as SET EXPLAIN and UPDATE STATISTICS. We give a detailed description of how to use the newer tuning tools, including SMI and the sysmaster database. For administrators, detailed sections explain how to work with a number of tools and methodologies and how to continue to monitor performance. V. Application Development This section explores the application development tools and methodologies available for Informix products. We explain how to make your databases work on a network by using Informix connectivity tools. We describe overall strategies for application development and how to plan for the future. Detailed sections are available about specific Informix tools like 4GL, Dynamic 4GL, Ace Reports, Perform Screens, and reporting tools. VI. Web Applications and Object Relational Databases This section focuses mostly on Web applications and how object relational database management systems (ORDBMSs) might fit into your future. Simply put, no one can deny that objects and the Web will enable database systems well into the 21st century. This section goes into detail about what these methodologies mean, how they are used, and how they might be used with Informix products. The first two chapters provide an overview of current Web technologies and how to use them in a database-enabled Web application. After that, we describe how to use Informix Internet Foundation.2000, its DataBlades, and other tools. Again, we help point you to the right resources so that you can remain on top of object technologies and how they fit into your future. VII. Appendices The appendices provides copies of many useful reference materials, Web site addresses, tools, and other information. The appendices alone will serve as an excellent desktop reference for just about any Informix user! We include Informix utilities (which are also on the CD-ROM) and describe how they work. We show you how to maximize the Informix resources like Informix Developer's Network, user groups, newsgroups, and the Web. The appendices also provides a comprehensive glossary of Informix-related terms and a more complete reference on INFORMIX-SQL and command-line utilities. Don't miss Appendix F, "Quick Up-and-Running Guide," which provides a quick reference of how to install and configure Informix products from the box to being online. After that, a list of sample queries and command line utilities help you see how to implement various Informix commands. Finally, the extremely detailed Glossary explains a large number of Informix-related terms and concepts. As you can see, we have very carefully split the sections of this book so that information is easy to find. We fully intend to provide an excellent reference for your Informix needs and make it very easy to use, now and long into the future. Now, let's find out about Informix and why we're doing this in the first place. Structure of Each Chapter The chapters are formatted to make finding information easy. Here is a summary of the items in each chapter: Summary of the information in the chapter. At the beginning of each chapter is a brief overview followed by a list of the major topics that are included in the chapter. List of topics, split functionally into subcategories. Within each chapter are several small sections, each explaining a certain category of information. Within each of these sections are sub-sections, providing details. By looking at the table of contents, you should be able to quickly find what you need. Notes, tips, warnings, Web links, and other chapters, code listings, and diagrams. The chapters are rich with specific items of interest (notes, tips, warnings) and references to other chapters, books, and Web sites. In addition, many graphical figures and code samples are provided to help enhance your understanding. References to other chapters. At the end of each chapter is a section called "For More Information." This points you to other chapters in the book that relate to the current chapter. Informix and Other References. Following the "For More Information" section, this tells you which Informix manuals and other books can further help you. I'm confident that you'll find the structure of these chapters very easy to use. You may want to take a quick look through some of the chapters or the table of contents to better understand this structure. Contents of the Web Site The Web site is one of the core parts of this book's solution. Because it is so important, I described it in the previous section, "The Web Site: An Ongoing Reference." To summarize, the site will be constantly updated, providing additional information about each chapter, as well as current and useful Web site links. The idea is to use the book in conjunction with the site to provide a complete solution. The site can be found at www.informixhandbook.com. Contents of the CD The CD includes a Web browser-driven interface that allows you to easily locate information by using your Web browser. Simply insert the CD and open the index.html file in the root directory. The CD includes a plethora of utilities and SQL files, allowing you to copy commands and utilities that you can use. In addition, the CD includes the following: Informix Guide to SQL Syntax The Administrator's Guide for Informix Dynamic Server (official Informix version for 7.3) A copy of Informix Dynamic Server for Linux and NT All of the book's SQL statements, scripts, and programs (i.e., all that were included as Listings) A computer-based training (SmartForce) course entitled "Managing the Instance" Several whitepapers Easy-to-use links to the book's Web siteif you are online when using the CD, you can use various links that send you directly to the proper portion of the Web site Hundreds of utilities for administering or programming for Informix products. Products are written in shell scripts, SQL, 4GL, and C. A trial copy of Server Studio from AGS (www.agsltd.com), a comprehensive GUI-based tool for Informix databases In addition, on the Web site we will offer enhanced search functionality and a possible upgrade for the CD. As you can see, the CD, Web site, and the book work together to give you the information you need about Informix. Considering the Future One of the goals of this book is to help you consider the future of technologyand Informixin your design and development of applications. We, the authors of this book, made a special effort to consider how database technology and application development will be evolving. And you should, too. What works now might not integrate or make use of upcoming technologies. This book and its Web site provide you with details about how technology is changing and how to take keep up with its evolution. Again, we provide many cross-references so that you're sure that you have the latest information. We have dedicated an entire section to Web technologies and object-relational databases. Section VI, "Web Applications and Object Relational Databases," describes object technology and how to use Web applications and Informix Internet Foundation.2000 as well as DataBlades. The Web is going to be a big part of the future of database and application development. Chapter 41, "Web Application Overview," explains the concepts behind Web computing, and Chapter 42, "Building Web/Database Applications," explains how to use Java and other tools to create an Informix-enabled Web application. Again, using the book along with its Web site should help you keep current about the best ways to create Informix applications. For More Information... This introduction provided an explanation of the goals and layout of this book. Please skim through the book so that you can really understand how and why we organized it the way we did. We're hoping that you find this book extremely easy to use, providing detail, reference, and even some amusement. Don't forget to use the Web site or the CD-ROM and its HTML-driven menu. Here are some other good places to start: To find many different tools and references, see this book's CD. For complete and up-to-date information on the material in the chapters and current Informix information, see the book's Web site at www.informixhandbook.com. For more history of Informix, see Chapter 1, "Now and the Future," and Chapter 2, "History of Informix: Live from Silicon Valley." For an explanation of Informix database servers and products, see Chapter 1, "Now and the Future." To find out how to start using Informix now, see Chapter 3, "Creating and Using the stores Database." For an explanation of Informix architecture, see Chapter 4, "Understanding Informix Architecture." For a complete reference and examples of Informix SQL, see Section II, "Using Informix SQL." For complete details about administration, see Section III, "Server Administration." To find out how to tune your Informix servers and programs, see Section IV, "Performance Tuning." To obtain a better understanding of how to develop applications in Informix, see Section V, "Application Development." For many references about the Web and object relational technology, see Section VI, "Web Applications and Object Relational Databases." For large amounts of reference material, see the book's appendices. Don't miss Appendix F, "Quick Up-and-Running Guide," or the intensive Glossary.}
}

@article{10.1007/s10664-017-9547-8,
author = {Bao, Lingfeng and Xing, Zhenchang and Xia, Xin and Lo, David and Hassan, Ahmed E.},
title = {Inference of development activities from interaction with uninstrumented applications},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9547-8},
doi = {10.1007/s10664-017-9547-8},
abstract = {Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.},
journal = {Empirical Softw. Engg.},
month = {jun},
pages = {1313–1351},
numpages = {39},
keywords = {Condition Random Field, Developers’ interaction data, Software development}
}

@article{10.1145/77556.77557,
author = {Frenkel, Karen A.},
title = {The European community and information technology},
year = {1990},
issue_date = {April 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/77556.77557},
doi = {10.1145/77556.77557},
abstract = {The world has watched Eastern Europe erupt into such political turmoil that historians are expected to call this period the Revolutions of 1989. Economic evolution was also underway as the Continent progressed toward a single European market. The goal—a market without national borders or barriers to the movement of goods, services, capital and people—was first outlined over 30 years ago by the 12 countries which became members of the Common Market. In the mid 1980s, the effort was renewed when these same countries approved an ambitious plan outlining hundreds of legislative directives and policies that would harmonize and re-regulate those of the member states. The measures are drafted by the European Commission, voted on   by the Council of Ministers, amended if necessary, and then assigned budgets by the Parliament. They include competition law, labor law, product regulation and standardization, taxation and subsidies, and quota and tariff guidelines. In 1987, the Single European Act created a timetable for the passage of legislation with a formal deadline for the removal of barriers by December 31, 1992, hence the term Europe '92 (EC '92). But many have described EC '92 as a process that will continue throughout the 1990s. The ouster of communist leaderships throughout Eastern Europe, however, has raised unexpected questions about the participation of the Eastern countries, and this could alter or delay the process.Nevertheless, the changes have begun and are taking place during the Information Revolution. It is therefore natural to ask what impact EC '92 will have on the computer industry. Inevitably, several of the directives and policies relate primarily, and many secondarily, to information technology. Table 2 lists the policies in effect and those being proposed. In the following pages, Communications presents several points of view regarding the impact of EC '92 on the information technology market in Europe.As of July 1988, the European information systems market was estimated at $90 billion by Datamation magazine and is expected by many to be the fastest growing market this decade. But during the last ten years, European-based computer companies have had difficulty keeping pace with American and Japanese firms. In 1988, European companies managed only a 20 percent market share on their own turf, according to market researcher International Data Corporation. Not much had changed since 1982 when their market share was 21 percent. As reported in the Wall Street Journal last January, European computer companies have been hindered by lack of economies of scale, narrow focus on national markets, and difficulty in keeping pace with Japanese and IJ.S. product innovations. But the occasion for the Journal article was the news that Germany's Siemens AG was merging with the ailing Nixdorf Computer AG. The result would possibly be the largest computer company based in Europe, and the sixth or seventh largest in the world. And in October of 1989, France's Groupe Bull announced the purchase of Zenith Electronics Corporation's personal computer unit. Bull claimed that it would become the sixth largest information service company in the world. Such restructurings have been predicted with the approach of EC '92, as corporate strategies would begin to take into account directives and trade rules regarding the computer and telecommunications industries. Smaller European and American computer companies are anticipating battle with giants like IBM and DEC, which have long-established European divisions or subsidiaries. IBM has been the leader in mainframes, minicomputers, and personal computers, but it is expected that all computer companies, European-based or not, will face greater competition in Europe.The Netherlands' NV Philips, the largest European semiconductor and consumer electronics company, says it has been preparing for EC '92 since the 1970s. And North American Philips Chairman Gerrit Jeelof has claimed company credit for initiating the 1987 European Act. In a speech delivered at a Business Week and Foreign Policy Association Seminar last May, Jeelof said that while American companies had forsaken consumer electronics, Philips and France's Thompson have held their own against the Japanese. But he indicated that American dominance of the European semiconductor market was a major impetus for EC '92. Jeelof said:
. . . because of the lack of European strength in the field of computers, the integrated circuits business in Europe is dominated by Americans. Europe consumes about 34 percent of all ICs in the world and only 18 percent are made in Europe by European companies. The rest are made by American companies or are imported. It is not a surprise then that in 1984 we at Philips took the initiative to stimulate a more unified European market. At the time, we called it Europe 1990. Brussels thought that 1990 was a bit too early and made it 1992. But it has been the electronics industry in Europe together with other major companies, that have been pushing for Europe 1992. Why did we want it? We wanted a more homogeneous total market in Europe and, based on that, we wanted to become more competitive. The process is on its way and obviously we see some reactions. If you take action, you get reaction.One reaction has been concern on the part of non-European companies and their governments that the EC is creating a protectionist environment, a “Fortress Europe.” As walls between nations are coming down, some fear that other more impenetrable ones are going up on the Continent's edges. Jeelof argues against this perception in another speech, “Europe 1992—Fraternity or Fortress,” reprinted in this issue in its entirety.Communications also presents an analysis of several trade rules relating to semi-conductors in “The Semiconductor Market in the European Community: Implications of Recent Rules and Regulations,” by Roger Chiarodo and Judee Mussehl, both analysts in the Department of Commerce Office of Microelectronics and Instruments. The authors outline the consequences of Europe's Rules of Origin, anti-dumping measures that are supposed to prevent companies from using assembly operations in an importing country to circumvent duty on imported products. In the United States, if the difference between the value of parts or components from the dumping country and the value of the final product is small, then duty will be placed on those parts or components used in U.S. assembly operations. By contrast, the EC rule says that if the value of parts or components exceeds 60 percent of the value of all parts and materials, then duty will be placed on those parts and materials upon assembly in Europe. Since 1968, origin was also determined according to “the last substantial process or operation” resulting in the manufacture of a new product. In the case of printed circuit boards, some countries interpreted this as assembly and testing, while others thought it meant diffusion. In 1982, the EC began harmonizing these interpretations, and as of 1989, the last substantial operation was considered diffusion: the selective introduction of chemical dopants on a semiconductor substrate. As a result, American and Japanese semi-conductor manufacturers have spent millions building foundries on European soil. To reveal the Japanese interpretation of such changes, Japanese Commerce Minister Eiichi Ono, with the Japanese Embassy in Washington, DC, expresses his country's impressions of EC '92 in this issue. In his speech, “Japan's View of EC '92,” delivered at an Armed Forces Communications and Electronics Association (AFCEA) conference on Europe '92, Ono states that while the EC's intentions might not be protectionist, they could 
become so upon implementation. His discussion focuses on semi-conductors and technology transfer issues.Although not a formal directive, in July 1988, the European Council decided to promote an internal information services market (the last “L” document in Table 2). To present the reasoning and objectives behind this initiative, we reprint the Communication from the Commission to the Council of Ministers, “The Establishment at Community Level of a Policy and a Plan of Priority Actions for the Development of an Information Services Market,” and the resulting July 1988 “Council Decision” itself. Funds allocated for 1989 and 1990 are approximately $36 million, $23 million of which was slated for a pilot/demonstration program called IMPACT, for Information Market Policy Actions. This may seem a pittance in comparison to the programs of other governments, but this Decision and other EC legislation are the first steps toward an EC industrial policy. Recognizing that Europe's non-profit organizations and the public sector play a very important role in providing database services, in contrast to the U.S. where the private sector is now seeding the production of such database services, IMPACT has prepared guidelines to help the public sector cooperate with the private sector in marketing information. These guidelines would also allow private information providers to use public data and add value to it to create commercial products. IMPACT is providing incentives to accelerate innovative services for users by paying 25 percent of a project's cost. After the first call for proposals, 16 of 167 projects proposed by teams composed of 300 organizations were funded. American-based companies can apply for funds if they are registered in Europe. Unlike the U.S., the EC allows registration regardless of who owns a company's capital. Projects funded are to develop databases that would be accessible to all members of the Community either on CD-ROM or eventually on a digital network, an ISDN for all Europe, as planned by the fifth recommendation listed in Table 2. One project in the works is a library of pharmaceutical patents on
 CD-ROM that will enable users to locate digitized documents. Users will also have direct access to on-line hosts for all kinds of patents. A tourism information database and a multi-media image bank of atlases are other pilot projects chosen, and another project will provide information on standards. Eventually, audiotext might be used to retrieve data by telephone instead of a computer terminal. When the initial projects have been completed, the Commission will inform the market place about the results of the implementation. Plans for a five-year follow-up program, IMPACT-2 are also under discussion.These projects depend to some extent on the implementation and passage of directives or the success of larger and better funded projects. On-line access to databases depends on the recommendation for an ISDN as well as on the standardization directive for information technology and telecommunications. The certification, quality assurance, and conformity assessment issues involved in that directive are too numerous and important to just touch on here and will be covered in a later issue of Communications. To make these databases accessible not only technically, but also linguistically, the EC has funded two automatic language translation projects called Systran and Eurotra. Systran is also the name of the American company in La Jolla, CA, known for its pioneering work in translation. In conjunction with the EC, Systran Translation Systems, Inc., has completed a translation system for 24 language pairs (English—French, French—English, for example, are two language pairs) for the translation of IMPACT- funded databases. The system resides on an EC mainframe; there will be on-line access by subscription; and it will also be available on IBM PS/2s modified to run VMS DOS. It is already on France's widespread Minitel videotext network.As this practical, market-oriented approach to technology implementation is beginning, Europe's cooperative research effort, ESPRIT, is also starting to transfer its results. Last year, the second phase, ESPRIT II, set up a special office for technology transfer. Its mission is to ensure the exploitation, for the benefit of European industry, of the fruits of the $1.5 billion ESPRIT I program that began in 1984, as well as the current $3.2 billion program (funding through 1992). The EC contributes half of the total cost, which is matched by consortia comprised of university and industry researchers from more than one country. About 40 percent of ESPRIT II's funds will be devoted to computer related-technologies.Every November, ESPRIT holds a week-long conference. Last year for the first time it devoted a day to technology transfer. Several successful technology transfers have occurred either from one member of the program to another or out of the program to a member of industry that had not participated in the research. An electronic scanner that detects and eradicates faults on chips, for example, was developed by a consortium and the patents licensed by a small company. This automatic design validation scanner was co-developed by CSELT, Italy, British Telecom, CNET, another telecom company in France, IMAG, France, and Trinity College, Dublin. The company that will bring it to market is ICT, Gmbh, a relatively small German company. It seems that in Europe, as in the United States, small companies and spin-offs like those found in the Silicon Valley here, are better at running quickly with innovative ideas, says an EC administrator.Another technology transfer success is the Supernode computer. This hardware and software parallel processing project resulted in an unexpected product from transputer research. The Royal Signal Radar Establishment, Inmos, Telmat, and Thorn EMI, all of the UK, APTOR of France, and South Hampton University and the University of Grenoble, all participated in the research and now Inmos has put the product on the market.Three companies and two universities participated in developing the Dragon Project (for Distribution and Reusability of ADA Real-time Applications through Graceful On-line Operations). This was an effort to provide effective support for software reuse in real-time for distributed and dynamically reconfigurable systems. The researchers say they have resolved the problems of distribution in real-time performance and are developing a library and classification scheme now. One of the companies, TXT, in Milan, will bring it to market.Several other software projects are also ready for market. One is Meteor, which is aimed at integrating a formal approach to industrial software development, particularly in telecommunications. The participants have defined several languages, called ASF, COLD, ERAE, PLUSS, and PSF for requirements engineering and algebraic methods. Another project is QUICK, the design and experimentation of a knowledge-based system development tool kit for real-time process control applications. The tool kit consists of a general system architecture, a set of building modules, support tools for construction, and knowledge-based system analysis of design methodology. The tool kit will also contain a rule-based component based on fuzzy logic. During the next two years, more attention and funds will be indirectly devoted to technology transfer, and the intention to transfer is also likely to be one of the guides in evaluating project proposals.Some industry experts maintain that high technology and the flow of information made the upheaval in Eastern Europe inevitable. Leonard R. Sussman, author of Power, the Press, and the Technology of Freedom: The Coming Age of ISDN (Freedom House, 1990), predicted that technology and globally linked networks would result in the breakdown of censorious and suppressive political systems. He says the massive underground information flow due to books, copiers, software, hardware, and fax machines, in Poland for example, indicates that technology can mobilize society. Knowing that computers are essential to an industrial society, he says, Gorbachev faced a dilemma as decentralized computers loosened the government's control over the people running them. Glasnost evolved out of that dilemma, says Sussman.Last fall, a general draft trade and economic cooperation accord was signed by the European Commission and the Soviet Union. And both American and Western European business interests are calling for the Coordinating Committee on Multilateral Export Controls (COCOM) to relax high technology export rules to the Eastern Bloc and the Soviet Union. The passage of that proposal could allow huge computer and telecommunications markets to open up. And perhaps the Revolutions of 1989 will reveal themselves to have been revolutions in communication and the flow of information due in part to high technology and the hunger for it.},
journal = {Commun. ACM},
month = {apr},
pages = {404–410},
numpages = {7},
keywords = {transborder data flow, statistics, standards, regulation}
}

@book{10.5555/579266,
author = {Unhelkar, Bhuvan},
title = {Process Quality Assurance for Uml-Based Projects},
year = {2002},
isbn = {0201758210},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: Purpose of this Book The convenor of the OOSIG (object-oriented Special Interest Group) of the Australian Computer Society is occasionally referred to as Chairperson. For past two years, this honorary and honourable title has been conferred upon me and, as the title suggests, it provides me with — amongst other things — the unenviable job of moving and organising chairs before the monthly meeting starts and ensuring they are stacked back against the wall after the meeting in the societys office is over. Getting the flipcharts and whiteboard ready, booking the room, sending the invitations, organising coffee and keeping the data projector light bulb from blowing up are some things keep the adrenaline level of the chairperson always on high. However, I had no such challenges to face when Canada-based Bran Selic, kindly addressed my SIG. Many members turned up to listen to one of the original contributors to the Unified Modelling Languages meta-model, particularly to the behind-the-scene stories. One of the interesting features of Brans talk was the candid highlighting of the strengths and weaknesses of the UML. Couple of reasons for UMLs popularity, as emerged during the talk were: UML is a standard and therefore accepted within the larger IT community, and UML came on the IT scene at the right time. The UML fills the void that existed in software development — a modelling mechanism that enables capture and expression of requirements, documentation of design, facilitates architectural discussion and supports software implementation. The modelling capabilities of the UML, supported by CASE tools, are widely usedin numerous practical software projects. Further, professional training courses on business analysis, architecture, design and testing are routinely based on the UML standard. The UML is also popular in the academic world as many university courses use the standard notations and diagrams of the UML to teach the students principles of software engineering. Finally, through relatively less-technical and more business-focused works, object technology and the UML have shown to be capable of being used in non-software related work, such as modelling business processes (BPR), business workflows, and even software workflows. Despite its popularity, however, the UML literature still needs discussion on and application of quality with UML. While we have some excellent literature on the processes of software development it seems to fall short of separate and detailed discussions on quality. On the other hand, works like Binders focus on the technical aspects of testing, using the UML notations, do not provide the process-aspect of improving the quality of software development. Indeed, none of this literature deserves any criticism for the lack of quality discussion — because these literary works do not purport to be discussing quality. The focus of these respectable and popular works is either development or testing. This book is written with an aim of directly addressing the paucity of literature in the area of process quality assurance for UML-based projects. Good Quality is all about satisfying the needs of the user. However, good is a highly subjective term. The reference-point against which quality is judged depends on time, place, and situation — all of which can change! Hence, the essential ingredients in producing good quality are: A product that satisfies the changing needs of the user A process that enables creation, verification and validation of such a product A common mechanism to establish communication Continuous improvement of the process of producing product When applied to software development, these quality requirements translate into producing a software product that would evolve, scale and change, according to the needs of its users — primarily the business. Not only do we need a process for developing such a software product, we also need significant checking and crosschecking of the models and processes that have built the software product. There is a need to ensure the syntactical correctness, semantic consistency and aesthetic symmetry in the models that will be used to produce good quality software. There is also a need to create, follow and check all necessary process steps in order to achieve maturity of processes that will result in good quality software products. Furthermore, these process steps must be executed iteratively, incrementally and sufficiently. Process steps should also be malleable enough to suit various development environments, and various types and sizes of projects. The specific and significant areas of quality related work required in a process incorporating the UML are addressed in this book. The quality techniques discussed in this book include how to organize the overall quality function, the process steps to be followed in creation of UML diagrams, the steps in verification and validation of these diagrams, when to conduct such verificat how the interpret the results of quality activities, who should create and validate the UML diagrams, and how to create a quality control (testing) strategy. Because of the process focus in this book, the techniques of creation of UML diagrams is assumed to be known to the readers. Summary of the Book This book is divided into 6 chapters as summarized below. Chapter 1: The Quality Game In this background chapter on quality assurance we discuss the elusive nature of quality in the context of software. Modelling, particularly with the UML, is shown as means to improve communication and quality and is conducted in the three distinct yet related modelling spaces of Problem, Solution and Background. Process is discussed in the context of its three dimensions of technology (what), methodology (how) and sociology (who). This is followed by discussion on the various checks (syntax, semantics and aesthetics) needed to validate and verify UML-based models and the checks of necessity, sufficiency and malleability needed for a good quality process. Organization of the quality function, and its application to various types of projects (development, integration, package implementation, outsourcing, data warehousing, and educational) as well as various sizes (small, medium, large) of projects are also discussed here. Chapter 2: Quality Environment: Managing the Quality function Process aspect of quality encompasses the management functions of creating and managing a quality environment. This is because software quality is not just verifying and validating what has been produced but also a sustained effort at following the discipline of producing models and software. This discipline encompasses the process or the steps involved in producing good models and good software. This part of this book comprehensively considers organization and execution of the quality function with detailed emphasis on the process of developing UML based software. In other words we discuss how the quality function is organized and carried out in UML-based projects. The people issues (who) is also given due relevance in this part of the book. Chapter 3: Quality Process Architecture This chapter discusses what constitutes such a process, and how it will be helpful in enhancing quality in a UML-based project. This chapter does not propose a new process, but discusses a most generic Process including the Technological, Methodological and Sociological dimensions — what constitutes a process, and what are its major dimensions of a process is described here. The technological dimension of a process deal with the what, the methodological dimension with the how and the sociological dimension with the who, of an overall process. These dimensions are described with common workday examples. Furthermore, the generic process also describes the most commonly used activities and tasks that should be there in any process. These activities and tasks, and the related roles and deliverables, are described with the aim of improving the discipline in a process, resulting in enhanced quality of UML-based deliverables and eventually the software product. Chapter 4: Enacting the Quality Software Process In this chapter we discuss the enactment of an example process including practical issues of configuring an iterative, incremental and parallel project plan, based on the process-components discussed in the previous chapter, are discussed here. We also discuss practical issues of tracking the progress of a project as well as modifying the project plan based on that tracking. An iterative and incremental project plan will facilitate better absorption of changes than a sequential project plan. Creation and management of such a changing plan, derived from the malleability aspect of the process, are also discussed here. This chapter discusses what happens when the rubber hits the road in terms of application of a process. Chapter 5: Estimates and Metrics for UML-based Projects This chapter discusses the important issues of measurements and estimates in UML-based software projects. Starting with an argument for the need to make good estimates, and how good metrics help in making good estimates, this chapter delves into the importance of these measures and estimates in improving the quality of models and processes in the project. Technical measures related to sizes and complexities of the UML artefacts and diagrams is also discussed. Estimates for the example implementation project using the UML are shown with a view to demonstrate the application and significance of metrics in a practical project. Chapter 6: Testing the product This chapter will discuss in detail the quality control and testing aspect of a quality lifecycle. While we discussed process quality in previous chapters, quality control, or testing, is a major process-component dedicated to verifying and validating the results of our efforts thus far in creating models and following a process. Good quality control is inherently negative as it is aimed at breaking everything in a system — its logic, its execution, its performance. Thus, although Quality control is an integral part of quality assurance, but is not synonymous with it. This separation is given its due importance in this separate part of this book. CD &amp; Potential Web Support The CD contains details of the chapters, diagrams, and a set of templates that can be customised for use in projects. Suggested metrics for improving quality (e.g. size of use cases, effort in creating classes) are also incorporated in the CD. Evaluation copies of relevant process tools that deal with quality process have also been provided, with permissions. Literary Audience There are a large number of books written on UML and similarly on processes. Their scope encompasses both academic research and practical applications. This book attempts to synergies the application of quality processes in UML-based projects. With the process focus, the reader is expected to be familiar with UML and its modelling techniques as the book does not purport to discuss the modelling techniques of the UML. However, a person responsible for quality assurance will find this work self-sufficient and may even be encouraged after reading this material to extend their understanding further in to UML. Semantics This author firmly believes in gender-neuter language. Person is therefore used wherever possible. However, in order to maintain simplicity of reading he has been used as freely, and has been balanced by equal, if not more, use of she. Terms like programmer and quality manager, unless otherwise mentioned, represent roles performed by actors. These terms dont tie down real people like you and me who, in a short span of time, can jump from the role of a programmer to a quality manager to a director and back. It is also recognised that people may be playing more than one role at a time. For example, a business analyst may also be a part-time academic or a researcher. We throughout the text primarily refers to the reader and the author — you and me. Occasionally, we refers to the general IT community of which the author is a member. We also refers to the teams in which the author has worked. Therefore, although this is a single author book, you may encounter we as a reference by the author to himself, as well as to the IT community. Real conversations, as you and I are having through this work, cannot be statically typed. Mapping to a Workshop The practical aspects of UML and Quality, displayed in this book, have been popular in seminars and conferences. Amongst many presentation, particular noteworthy are its acceptance as a tutorial at the UML2001 conference in Toronto, Canada and the two-day seminar series in Mumbai, Bangalore and Delhi, in India. Here is a generic outline of the two-day workshop based on this book. For the education and academic community, each chapter in this book can correspond to a 3-hour lecture topic, with earlier part of the semester used in simply creating the UML-based models based on the case study. Acknowledgements Encouragement and support can take various forms —a word of encouragement here, hint of a smile there! And then there are those detailed discussions and arguments with honest reviewers of the manuscript on what should be included and how it should be presented. This is interspersed with the arduous task of typing sections of the manuscript, drawing the figures and the rather trying job of proofreading someone elses writing. All this has come to me through many wonderful people whom I acknowledge here gratefully: Anurag Agarwal Rajeev Arora Craig Bates Paul Becker Christopher Biltoft Bhargav Bhatt Graham Churchley Kamlesh Chaudhary Sandra Cormack Joanne Curry Sanjeev Dandekar Edward DSouza Con DiMeglio Julian Edwards Nandu Gangal Athula Ginige David Glanville Mark Glikson Nitin Gupta Brian Henderson-Sellers Murray Heke Ivar Jacobson Sudhir Joshi Ashish Kumar Vijay Khandelwal Akshay Kriplani Yi-chen Lan Chinar &amp; Girish Mamdapur Javed Matin Sid Mishra Rahul Mohod Navyug Mohnot Narayana Munagala Karin Ovari Les Parcell Chris Payne Andrew Powell Abhay Pradhan Amit Pradhan Anand Pradhan Prabhat Pradhan Rajesh Pradhan Tim Redrup Tracey Reeve Prashant Risbud James Ross Magdy Serour Bran Selic Ashish Shah Paresh Shah Prince &amp; Nithya Soundararajan Pinku Talati Amit Tiwary Murat Tanik Asha Unhelkar Sunil Vadnerkar Suresh Velgapudi John Warner Houman Younessi Paul Becker, my editor at Addison-Wesley, has provided invaluable support in this work and deserves special recognition. Bearing with my delayed submissions, passing encouraging comments when the progress was slow and accommodating my changes up to the last minute are some of the traits of this considerate editor that are gratefully acknowledged. Finally, my family makes all this possible by just being around me even, and especially, when I am mentally lost. I am grateful to my wife Asha, my daughter Sonki Priyadarshini whose view on quality took a jagged turn as she stepped into her teens, my son Keshav Raja who can appreciate quality in cars, bikes and planes — which is the ability of these tools of the kindergarten trade to withstand punishment meted out by rather tough 6 year olds. Finally, this work acknowledges all trademarks of respective organisations, whose names andor tools have been used in this book. Specifically, I acknowledge the trademarks of Rational (for ROSETM), TogetherSoft (for TogetherControlCenterTM), Object-oriented Pty Ltd (for ProcessMentorTM) and eTrackTM. Critiques It reflects a healthy state of affairs within the IT world, and especially the UML and process community, if work of this nature receives its due share of criticism. All criticisms have an underlying rationale and that they should all be accepted in a positive and constructive vein. All comments on this work, both positive and negative will be accepted positively. Thus, to all my prospective critics, whose criticisms will not only enrich my own knowledge and understanding of the quality topics discussed in this book, but which will also add to the general wealth of knowledge available to the IT community, I wish to say a big thank you in advance. Bhuvan UnhelkarSydney, July 2001}
}

@inproceedings{10.1145/1362702.1362708,
author = {Kidd, Eric},
title = {Terrorism response training in scheme},
year = {2007},
isbn = {9781450378444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1362702.1362708},
doi = {10.1145/1362702.1362708},
abstract = {The Interactive Media Lab (IML) builds shrink-wrapped educational software for medical professionals and first responders. We have teams focusing on media production, script-level authoring, and low-level engine development. Our most recent project is Virtual Terrorism Response Academy. VTRA uses 3D simulations to teach students about radiological, chemical and biological weapons. Our software is now undergoing trials at government training centers and metropolitan police departments. VTRA consists of approximately 60,000 lines of Scheme, and a similar amount of C++. All of our product-specific code is in Scheme, and we make extensive use of macros and domain-specific languages.From 1987 to 2002, we used a C++ multimedia engine scripted in 5L, the "Lisp-Like Learning Lab Language". This was Lisp-like in name only; it used a prefix syntax, but didn't even support looping, recursion, or data structures. We needed something better for our next project! We ultimately chose to use Scheme, because (1) it was a well-known, general-purpose programming language, and (2) we could customize it extensively using macros. Migrating to Scheme proved tricky, because we needed to keep releasing products while we were building the new Scheme environment. We began by carefully refactoring our legacy codebase, allowing us to maintain our old and new interpreters in parallel. We then rewrote the front-end in a single, eight-day hacking session. But even once the Scheme environment was ready, few of our employees wanted to use it. In an effort to make Scheme programming more accessible, we invested significant effort in building an IDE. Today, our environment is much more popular---a third of our employees use it on a regular basis, including several professional artists.After migrating to Scheme, we added support for 3D simulations. And Scheme proved its worth almost immediately: we faced several hard technical problems, which we solved by building domain-specific languages using Scheme macros. First, we needed to simulate radiation meters. For this, we used a reactive programming language to implement a Model-View-Controller system. Second, we needed to guide students through the simulation and make teaching points. For this, we relied on a "goal system", which tracks what students need to accomplish and provides hints along the way. In both these cases, Scheme proved to be a significant competitive advantage. Not all problems have clean imperative solutions. A language which supports functional programming, macros, and combinator libraries allows us to do things our competitors can't.This summer, we'll be releasing our engine as open source, and starting work on a GUI editor. We welcome users and developers!},
booktitle = {Proceedings of the 4th ACM SIGPLAN Workshop on Commercial Users of Functional Programming},
articleno = {6},
numpages = {3},
location = {Freiburg, Germany},
series = {CUFP '07}
}

@book{10.5555/522237,
author = {Fieldhouse, Richard},
title = {Personal Oracle8 Explorer with Cdrom},
year = {1998},
isbn = {1576102505},
publisher = {Coriolis Group Books},
address = {USA},
edition = {10th},
abstract = {From the Book: As a programmer or budding developer of Oracle database applications, you're starting at the right place and with the right product by reading this book. I'll show you how you can become proficient in the use, design, and tuning of any Oracle database. This book is written to take anybody who has a basic grounding in the use of a PC and associated software through the maze of Oracle8. I say Oracle8, not Personal Oracle8, because while this book is based on Personal Oracle8, all of the features are exactly the same in every Oracle8 database, whether it's running on Unix, VMS, Windows NT, or Windows 95. The main difference is that on Windows 95 and Windows NT, the product is more graphically oriented-for example, it uses wizards to create tables. For this reason, when any procedures in this book are shown using graphical tools, I'll also describe how to do the procedure from the command line (which is sometimes the only way of doing things using such operating systems as Unix and VMS). By reading and understanding this book, you should accomplish four goals: • Design an efficient database from the ground up, using Oracle's object technology • Write simple and efficient SQL and PL/SQL code to manipulate the data within a database • Gain a working knowledge of the Oracle8 database and all its object-oriented features • Receive a large salary increase Over the past eight years as a database administrator (DBA), I have looked for books associated with Oracle and have noticed that good books aimed at the intermediate level-those you can pick up and quicklybecome productive with-are few and far between. For this reason, I decided to write my own book, based on my own experience. Starting my programming experience with version 5 of Oracle, I came to realize that you have to learn by trying first, then reading later. Because most projects in the workplace are on fairly tight schedules, you need to be able to pick up a piece of software and become as productive as possible in as short a time as possible. This book will be the guide to Oracle8 that I wish had been available when I was learning. At the time of this writing, no other book is available commercially, outside of the Oracle manuals, that deals with Personal Oracle8 and associated tools from an intermediate perspective. The purpose of this book is to cover the development of applications using Personal Oracle8 in conjunction with the documentation on the Personal Oracle8 CD-ROM. What is Oracle Oracle is an extremely complex and high-powered database; in fact, it is the most widely used database in the world. For these and many other reasons, it is generally recognized by computer programmers worldwide as the best. The Oracle database is one of the most technically advanced pieces of software available. One of Oracle's main selling points is that it and its applications are portable across any operating system. This means that an application developed using Personal Oracle8 could quite easily be scaled up and installed on a high-end Unix server with thousands of users accessing the data. One important point to make here is that the Oracle database will run on nearly any commercial operating system, using the same human interface, known as SQL*Plus or PL/SQL. First, let me take you through the progression of Oracle releases so you can visualize what the Oracle8 database is. Versions 6 and 7 of the Oracle database are relational, meaning that the data stored is referenced by unique identifiers and grouped by relationships. In technical speak, it uses primary and foreign keys (constraints) to enforce referential integrity. Version 8 is an object-relational database. This means that it still uses the relational theory, but has added extensions of object-oriented theory. In this book, these subjects will be dealt with in more detail, and you will use some of the object theory (even though you might not realize it). The extensions of object-oriented theory add further capability to the Oracle database, in some cases allowing the programmer to apply it more easily to real-life business applications. Intended Audience This book is aimed at the programmer or the home-computer hobbyist who wishes to find out more about Oracle. I have designed the book to be used in two different ways: • You can skim through chapters and just try the demos that appear throughout the book. This will give you a good understanding of how to use the product. You can later return to the related chapters to discover the theory behind the practice. • You can follow the chapters in order and gradually build up your expertise and overall knowledge, until you have a full working knowledge of how to build applications on Personal Oracle8. In writing this book, I have assumed that the reader has some knowledge of Windows 95 and has used a database (e.g., MS Access, FoxPro, or dBase). Other people who would benefit from reading this book are: • Oracle developers who wish to know more about developing/prototyping applications on Personal Oracle • College students who wish to get a good in-depth knowledge of relational databases • Other information technology professionals who wish to keep their skills up to date by using Personal Oracle8 Personal Oracle8 Personal Oracle8, a scaled-down version of Oracle's popular database, is aimed primarily at the desktop market (rather than Oracle's primary market, the high-end Unix or Windows NT servers). The Personal Oracle versions came about because Oracle wanted to give its customers a trial version of the software, which they could easily install and use. The primary difference between Oracle8 and Personal Oracle8 is that on the Personal version, only single-user access is available as a server-hence, the name Personal. Therefore, the software is ideally suited for use as a single-user prototyping tool. Again, because of Oracle's portability, a Personal Oracle database can be easily exported and imported into a large multiuser Oracle platform. This gives users an ideal development platform, because all features available on a high-end server are available to the user on the desktop, including the ability to enable a database for the Web. Personal Oracle8 is designed to be used in conjunction with Oracle's view of the future in information technology-the network computer (NC). Oracle8 (which according to its subtitle is "The Database for Network Computing") has been specifically designed with the NC architecture in mind. Some of the database's key features are: • The ability to support any number of users (in the server versions) • The ability to support any amount of data • Faster application development • Increase in cost effectiveness In reality, what do you get on the Personal Oracle8 installation CD-ROM You get one of the most technically advanced pieces of software available today, as well as a ready-made development environment for any database application-all in a low-priced, cost-effective database package for Windows 95. The Personal Oracle8 database is way ahead of its competitors in the desktop market. This book will show you how easy it is to use and implement this software, and, more importantly, it will help you understand what is going on in the database and why. Oracle's product suite Oracle Corporation produces a lot of software aimed at nearly every type of machine and operating system. A piece of software, which usually has a generic name such as Oracle Server, is available for virtually any type of operating system. The aim of this section is to give you an idea of where the Personal Oracle8 software sits in relation to other Oracle products. A list of Oracle's main product areas follows. Oracle8 Universal Server The Oracle8 Universal Server is, as the name implies, a server that will encompass all. The server is the core database, and a number of add-ons provide even more functionality. Some of these add-ons are listed and described here: • WebServer Option-Allows access to your Oracle database from the Web. This will give anyone with a Web browser the ability to access your Web pages. Okay, you say, what's so new about that This option allows you to tailor your Web pages to react differently to each user accessing them. For example, imagine having a Web page that realizes who you are and, accordingly, lets you access your information from the Internet. This is done by storing your information within the database; then, when you access the Web page, the database knows who you are and can show you relevant information. • Spatial Data Option-Widely used in very large databases, such as data warehouses. This option changes the way the data is stored, allowing for quicker access to data by using a different indexing strategy. • ConText-Basically, a text search engine embedded within the database. This allows you to search unstructured text within the database, because most information is of unstructured text format (e.g., newspaper articles). • OLAP-Stands for online analytical processing of data. This option allows you to store the data within the database in different dimensions. A dimension is the way the data is categorized. For a car dealership, for example, a dimension may be the customer name, the type of car purchased, or the amount of money spent. The OLAP option allows you to store your data in either a multidimensional or relational way. • Parallel Server-Allows you to have a single database that is accessed by multiple nodes. The database is stored on a shared disk array available to all nodes. This gives you more power, because a multiple-node database is more fault tolerant. If you have one machine and it fails, then you do not have access to the database. With Parallel Server, you have an instance of the database on each node, so if one goes down the other nodes will carry on. This is extremely useful if the database is mission critical. • Video Option-Allows you to store video and sound within the database. This video and sound can then be played back, in realtime, to anybody on the network. This is the technology behind "video on demand." This will allow multiple users to view the same piece of video whenever they want. Personal Oracle8 And Personal Oracle Lite Personal Oracle and Oracle Lite are scaled-down versions of their big brother, Oracle8 Universal Server. They provide access to the Oracle database on the desktop. Oracle Lite is a "lightweight relational database that runs in less than 1MB of RAM," according to Oracle. It is designed for use as a mobile application-i.e., to run on portable computers. Oracle Lite supports bidirectional replication with Oracle Universal Server. SQL*Plus All databases accept commands from the user in a common language: Structured Query Language (SQL). Oracle provides a tool, SQL*Plus, which is a standard SQL interpreter; the "Plus" is Oracle's added functionality. SQL*Plus is Oracle's command-line interface to the database. By using SQL*Plus, you can interrogate the database and format the output. SQL*Plus by itself is useful, but when you add the procedural capabilities of PL/SQL, you have a tool that can easily give you access to all of the features of Oracle8. SQL*Plus and PL/SQL are used in the creation of Oracle databases. The Personal Oracle8 database uses wizards to make tasks easier for you by converting your inputs into SQL commands, then executing them against the database. The procedural option allows you to use SQL*Plus type structures within procedures, giving you the flexibility needed to write simple or complex functions or packages. In this book, you'll see exactly how SQL*Plus and PL/SQL are used to create such procedures and functions. Oracle8 Enterprise Manager The Oracle8 Enterprise Manager (OEM) is the tool for managing the whole Oracle environment. The Enterprise Manager includes tools to monitor and interact with the database, job scheduling, and automated backup management facilities. This tool is primarily for systems administration use. Oracle8 Enterprise Manager supports new features of Oracle8-e.g., object partitioning, server backups, and security management. OEM also handles the recovery of a corrupt database by using Recovery Manager. This speeds up the recovery of databases by the database administrator. Database administration tools are available through the Enterprise Manager. Designer/2000 Designer/2000 is a business-process modeling tool. It gives the user the ability to design complex business objects and rules in an easy-to-understand way. The rules can then be implemented on the server automatically. This data modeling tool allows the designer to model an application independently of the implementation, thus giving the power to implement on multiple platforms from a single model. Developer/2000 The Developer/2000 package has three main components: Reports, Forms, and Graphics. These components combine to create an easy-to-use application development package. The easy-to-use Reports tool enables the user to create detailed and complex reports from any Oracle database. You can use Oracle Reports against a Personal Oracle8 database to produce business standard reports quickly and easily. The reports can include embedded graphics and can easily be Web enabled. Forms is an extremely powerful tool for creating front-end data-input screens. The Forms tool allows the user to create applications very quickly. When this incorporates reports, you can quite easily create and produce a professional application in a relatively short time. The Graphics tool gives picture representations of data within a database. For example, creating a pie chart from data retrieved from the database is quite easy using Oracle Graphics. If you incorporate this with either Oracle Reports or Oracle Forms, you can build up graphical reports and forms from the database with very little effort. Object Database Designer Object Database Designer is aimed at anybody who designs and builds Oracle databases and is specifically helpful when creating object-relational databases, because it includes the ability to use user-defined types within the database model. Once the database model is designed, it can then be automatically implemented, because the Object Database Designer will create the required SQL statements and execute them for you. The advantage of this is that the design is viewed graphically and a change of design can quickly be implemented on the database. The designer is tightly integrated with C++, the most common object-oriented programming language. The next step This Introduction gives you an idea of Oracle's history and product line. Now that you know where Personal Oracle is placed in the hierarchy of Oracle software, why not install it Chapter 1 covers the quick installation of Personal Oracle8.}
}

@inproceedings{10.1145/76619.76644,
author = {Mayer, P. J.},
title = {Entering the Ada systems design and coding market},
year = {1989},
isbn = {0897912853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/76619.76644},
doi = {10.1145/76619.76644},
abstract = {Advice is cheap, and we all know that you get what you pay for. There are many books, courses, and seminars on how to start a business. They have been written or are presented by professionals usually with far greater experience than I. While my general management background has been invaluable, the thing that best qualifies me to address this subject is the fact that we at Strictly Business Computer Systems have recently established an Ada programming shop.I'll share with you our experiences, from inception to the present. I must preface my remarks with the comment that they represent only our single effort in this area. I was fortunate in that my primary associates had successfully established and were operating a profitable business in the computer field, and it was their proven philosophy of adding value that became the keynote of our Ada effort.Additionally, we had the good fortune to make some valuable acquaintances early on in the process — relationships which enabled us to avoid some potentially costly pitfalls. Perhaps we can do the same for some of you.Now, to the subject at hand.What would seem to be the obvious first step in establishing any business is worth stating and that is the conscious act of making a commitment to the project. In our experience, the commitment was initially made about three years ago — two full years before the project was actually initiated. The delay occurred because the computer system integration business in which Strictly Business was totally immersed was growing at a pace that precluded devoting the time required to explore the Ada market.Then, a file less than a year ago, I joined Strictly Business with the sole responsibility of researching the Ada shop possibilities, and then managing the shop if the research was positive — which it obviously was. The fact that Strictly Business was willing to add me to the staff, as pure overhead from the business standpoint, clearly demonstrates that a true commitment existed. That commitment is really three-fold because undertaking such a project requires a dedication of, and money. Beyond that, you must assume the posture that characterizes the entrepreneur, and that is a total immersion in the business. You must identify with it and make it the focus of all that you do.If you and your organization are unwilling to pledge a full-fledged effort, your chances of success substantially diminish.Secondly, since the first phase of this project should be a marketing study, you must select an underlying theme that will provide a framework and give specific direction to your research. From the outset, we were convinced that within the Ada market a definite need existed for additional systems design and coding capacity. The corollary is that this appeared to provide a significant business opportunity. Our research was active — not passive or neutral. We saw an opportunity, and our purpose was to objectively and concretely confirm our perceptions.At each step in the process we were looking at what value was being added by the person, business or agency that we were exploring. Strictly Business was founded and has flourished on two basic concepts — namely, adding value through our involvement in each transaction and providing quality products and service to our clients. We scrupulously avoid being hardware and software “brokers” collecting fees for merely placing products with customers. We consider ourselves as partners with our clients and work to enhance their businesses with the products and services we provide.Having made the commitment and articulated your role and objectives, you must now begin the real work. This part of my message may be preaching to the choir. The fact that you are involved in the Ada community indicates that you have or are beginning to acquire a knowledge of the Ada marketplace. That's essential.Gather as much information as possible about every aspect of Ada. If you know the language, great. If not, that should not deter you from learning as much as you can exclusive of Ada per se. No one in our organization knew Ada before we began hiring our staff, yet several of us became knowledgeable and conversant enough to find our way around Ada circles — and in the Ada community, that's essential.Regardless of how much you learn in your explorations, the input of people active in Ada is indispensable. One of the most gratifying things our research revealed was the generosity and willingness of Ada experts to share their knowledge. We knocked on a lot of doors and did not find one that was not opened wide for us.Let me share with you some of the avenues we explored in trying to determine whet her or not a real Ada opportunity existed. We first had the advantage of coming from West Virginia whose senior U.S. Senator is Robert C. Byrd who has seen the potential of Ada and has for some years been one of its strongest advocates. With the assistance of two of his staff members, we were directed to the Software Valley Corporation which has been very much involved in bringing the advantages of Ada and Ada-related ventures to our Mountain State.Bob Verhotz, the Executive Director of Software Valley Corporation, in addition to other helpful suggestions, recommended that we contact Mr. Ralph Crafts. Bob had worked with Ralph on a number of occasions and spoke highly of his credentials and performance. We have not been disappointed.Ralph knows his way around the Ada community as well as anyone, and better than most. Almost a year ago, we employed Ralph as our consultant to define the state of the Ada market and give initial direction to our study. During intensive meetings with him, we received a great deal of background information and recommendations of additional areas into which we should extend our Ada network.These three initial contacts — Ralph, Software Valley, and Senator Byrd — confirmed that quality-conscious and professional systems developers could definitely find a place in the Ada market.At this point I think you can begin to see two things. The more obvious is the snowball effect of Ada contacts. Your first contact leads to two others which each lead to two or three more, and so on. The second thing is that we were strongly encouraged by each of these contacts, and our perceptions that excellent opportunities existed in Ada were reinforced. If anything, the potential began to look even greater than we had at first anticipated.Our tentacles, at that point, began to extend into additional areas of the Ada community. We have come to share Ralph's belief that the more people you know in this still relatively small group, the better off you are.We traveled to Washington to visit again with Senator Byrd's office. While there, with an introduction from the senator's staff, we also met with a number of people at the Ada Joint Programming Office, including the then-Air Force Deputy Director Major Al Kopp. More support and encouragement. On the same trip we cultivated an acquaintance at the Ada Information Clearinghouse. More support, encouragement, and a wealth of published information. We also briefly visited the STARS office and met with someone who was encouraging and informative about that extensive Ada project. Each of these organizations and individuals had a specific mission designed to enhance and increase the value of the Ada contribution.At that point we had begun to look at equipment and it was here that we found one of our more valuable allies and associates. From our initial contact with the personnel at RATIONAL we found them to be most helpful and open. Our sales representative made it possible for us to meet with two large firms handling major project work in Ada for the Defense Department.I don't need to tell you how valuable it can be to speak with someone who is engaged in the type of work you are contemplating and who has no ax to grind or hidden agendas as far as discussing things with you. Other vendors may have been equally helpful, but I doubt that any could have been more so. We met people doing actual project work in Ada for the government, extending our network and also making some contacts we would later pursue as we sought to put together our Ada staff.In March of this year, we attended the SlGAda conference in Phoenix where we researched a number of vendors, but more importantly, met others in the Ada community — on the commercial as well as the governmental side. We, admittedly, understood very little of the technical content of the meeting, but our purpose in attending was not technical in nature. We were networking, and our network was rapidly expanding.This might be a good point at which to remind you of the three-fold commitment required in this undertaking — time, energy, and money. By March our exploratory had gotten into its fifth month and had occupied practically all of my time and a substantial portion of the time of two of my colleagues at Strictly Business. Our travels had included a couple of trips to Washington and the trip to Phoenix as well as visits to Morgantown, WV (where the Software Valley Corporation is located) and Pittsburgh where we met with an active Ada development firm and some folks at the Software Engineering Institute of Carnegie-Mellon University. For a small firm such as ours, the budget for this venture was becoming substantial, but we were making valuable progress toward our objective.Speaking of budgets, probably the largest single start-up expenditure will be the development system you select. Spend sufficient time in making this decision. In equipment, you have a myriad of choices. With the recent validation of a large number of compilers, Ada development can be done, in one form or another, on anything from PC's to the much more sophisticated full-blown systems requiring major financial expenditures — and cost, at least in our case, was a significant consideration. But cost was only one factor.We also were concerned with other areas. Our initial plans called for a system to support ten (10) developers designing systems and/or writing code. Most hardware suppliers could accommodate that in one way or another. With our lack of experience in Ada, we were also looking for ease of familiarization and operation. And we were very much concerned with the level of support a supplier could provide. Who seemed most qualified and willing to “hold our hand,” as it were, until we gained some experience?The last major consideration was credibility. We knew that as a start-up operation gaining entree and establishing our credentials with potential contractors was critical. Our development system could say a lot about our commitment and dedication. Technical capabilities being a given, we were willing to pay some premium to project the most professional image. Bottom Line: find the system that will best enable us to efficiently and effectively develop software — to give our future clients value for their programming expenditures.We investigated three major suppliers — DEC and DG, both of whom seemed quite capable; and RATIONAL, whose development environment was written in and expressly for Ada.Weighing all the factors — system capabilities, support, ease of integration and use, reputation, cost, efficiency, etc. — we came down on the side of RATIONAL, and later decided that we would supplement them with SUN Microsystems work stations.We're happy with our decision and believe, as I said at the outset about the cost of advice, that you get what you pay for. At this time, we are confident that our system configuration will satisfy our objectives and meet our expectations. Something similar mayor may not be right for you. Your situation and needs, not our experience, should dictate your direction on equipment selection. We can only recommend that you thoroughly explore the alternatives.So where were we? We had done a lot of reading and travelling; met a lot of people with whom we'd like to be professionally associated: gotten a tremendous amount of encouragement that had been tempered with some pragmatic cautions; and made some preliminary system selections. Now we were getting down to the nitty-gritty — putting our plans and a proposal down on paper so that we could launch a sales effort to put together the financing needed to make it go.In formalizing your proposal or business plan, be prepared to spend a lot of hours at a desk with all of your background notes, a dictionary, a thesaurus, calculator, plenty of paper and pencils with generous erasers. With access to a word processor and a good spreadsheet program, you are facing a formidable task; without these two tools, it will seem, and may actually be, virtually “undoable.”Your proposal or business plan can take any of several forms, and no one is necessarily more or less appropriate or effective than any other. The plan should reflect your corporate style and philosophy. But regardless of the form, there are some elements which are indispensable.Your presentation must inspire confidence in a potential investor, assuming that you, like we, have to seek outside capital to launch your effort. The plan must clearly demonstrate that you have done your homework and thoroughly researched the subject and the market. It should deal with the principal players in your scenario, their credentials, and what they can contribute to the success of the venture — what value can each add? If yours is to be an extension of an existing business, the proposal must provide business and financial history in a realistic light, yet do so as favorably as possible. Finally, the plan must provide business forecasts in the form of projected financial statements and balance sheets. Have your accountant or someone with a strong financial background assist with the financials if that is not an area in which you have experience and confidence.Ultimately, the plan must convince its readers that you have (a) identified a need in the market and (b) that you are prepared and positioned to meet it. Experienced business pros will be reviewing the plan, so make the effort, and do it right. In preparing all of this information, keep in mind that an investor who decides to participate based on the plan will view it as your commitment. He very likely will measure your success, or lack of it, by using the plan as his yardstick. So, be conservative or at least realistic. Don't put anything into your plan that you might regret. if it were referenced some time later.One of our new acquaintances offered to review our proposal. He was doing Ada work so he could evaluate the presentation from that perspective. He was also very much involved with a managing board composed of experienced venture capitalists, so he could also take a look from that viewpoint. He gave us sound advice.My point is that you should have some disinterested parties whose opinions you value and respect, and who can freely and dispassionately critique your work, review it before you run with it. And, believe me, unless you are superhuman, you will go through several drafts and revisions before you submit the plan for outsider review. Our final plan was the sixth major revision, excluding the many internal changes and edits. Preparing an acceptable and effective plan is a humbling experience that will teach you the value of patience.One final note regarding your proposal — don't overlook its appearance. A copy of the plan and an introductory letter may be your only exposure as you try to get personal appointments to market your idea. Prepare them with care and attention to detail. Ensure that they reflect the high degree of professionalism that went into their re-search and preparation and which will characterize your business efforts. The content of the plan may not even be considered if the plan itself is not attractively presented.Now that you have what you believe is a good marketing piece, where do you go with it?Our objective was to secure local financing (within our community or at least within the state of West Virginia). We drew on personal contacts, a list of local venture capitalists that we obtained from the chamber of commerce, and suggestions offered by the CEO of one of the banks with whom we had an on-going personal and business relationship.We thoroughly explored various loan, grant, and incentive programs offered by municipal, county and state governments to attract business. If you have a university near you, they may have an office that assists with business start-ups. They may be very helpful if you choose to apply for loans or grants since this is an art form in itself. Don't overlook these potentially attractive sources of advice or capital; they could make the difference.Be prepared to make phone calls, personal visits and send written correspondence in cultivating potential investors. And be sure to have your ducks in line because most of these people did not accumulate their wealth or acquire their positions because they are fiscally naive or stupid. They are, by and large, very good business people who ask direct and probing questions and expect direct, succinct, supportable answers — and a wrong answer can quickly kill an opportunity.If local capital is not available, you will have to look farther afield. That's an area in which we can't offer much advice as we did not have to pursue it. We anticipated that if we had had to look elsewhere we would have to be even more on our toes, since we would give up the advantage of common ground. We would be negotiating on their turf rather than being from the same community as the people we were soliciting.One of the biggest difficulties we encountered was in selling something intangible. As sophisticated as many lenders and investors are, some are still uncomfortable with the computer field, and especially software, as an area of opportunity.Unless high tech businesses are already an established and accepted investment arena in your area, lenders may have difficulty grasping the concept of investing in intellectual property. Loans or investments for plant and equipment are a piece of cake — you can survey, touch, walk around or kick the tires of the collateral. In dealing with software, you lose that advantage, and many people are still wary of getting financially involved with something they can't see, touch, taste, or smell.Anticipate some initial skepticism and prepare to overcome it. BEGIN NOW. This is one area where you can't start sowing seeds and nurturing them too soon. Look for or create occasions to discuss with the financial powers in your community the role and advantages and success stories and opportunities in software development. When you come across a good article — one that's not too technical — that supports your point, send copies to appropriate people. Most will be read, and you'll be strengthening your case and laying a foundation you can build on later.Aside from the “intangibility factor,” we found that the key concern of potential investors is the make-up of your staff. If you have on board people with strong credentials and proven track records in Ada, your job will be much easier. We didn't. In fact, we had the chicken-and-egg situation of having financiers citing staff as a prerequisite on the one hand; and our inability to recruit and hire a staff until we had secured financing on the other. It was one of the most frustrating aspects of the whole process.We leaned heavily on the proven track records of those of us who were organizing the venture, even though they included no Ada experience. Special expertise has to be addressed, but good basic management skills and experience are highly regarded, well-respected, and carry a lot of weight. We also capitalized on the credentials of our consultant with whom we had reached an agreement for his continuing services after our start-up, making him a legitimate member of our team. It was true that we had considerable background in computer sales, and had on our staff experienced programmers doing custom work for clients, though not in Ada. Many people perceive experience in one area of the computer field as qualification to perform in what we knew to be largely unrelated areas. Since it worked to our advantage, we did not discourage that perception.While we were putting together our financing, we did some preliminary recruiting. We secured resumes and expressions of interest from programmers by contacting the colleges and universities that were graduating students from computer science programs in our area. From the outset, our objective had been to get our programmers locally, if possible. We believe that local residents, particularly in an area like ours, are more easily attracted to job opportunities near home and are more likely to remain with us because of their ties to the area.We recognized, however, that it was critical for us to attract at least one highly experienced Ada professional to direct the programming effort. We drew on the contacts we had made and also secured the services of two firms specializing in Ada placements. Use every tool you can muster, because this is a difficult area with the explosive growth that Ada is enjoying. Experienced people are hard to find, and you must be prepared for a difficult search and the possibility that you may not have adequately budgeted for this position. This person is key, however, and if you find the right one: the time, effort, and money expended in the search will have been well worth it.Look into training, particularly if you do as we did and recruit most of your staff with little or no practical Ada experience. Budget the time and money to allow for proper training of your people and recognize that they will be unproductive for some period of time after they come on board. We completed the hiring of our staff in early Fall. Theirs is a ten-week-long training program. We anticipate beginning work on our first contract no earlier than the first of the year. Our staff will have been on the payroll for more than three months before they take their first steps toward providing a return on the investment in them.So things have finally come together. With financing secured, you have ordered and scheduled installation of your system; hired and are training your staff; and are ready to undertake some work!Getting that first contract may be a challenge. Use every means at your disposal. If you can hire a professional who can bring contracts with him, so to speak, great! If the contacts you have made in your investigations can't help open doors for you, then you haven't been contacting the right people. If you have a Senator Byrd to lend support, bravo! Get all the help you can. Don't be bashful — most people are more than willing to lend as much help as they're able. Don't leave any stone unturned. And don't wait too late to begin looking.If, somehow, you can get a contract before you configure your shop it will certainly make it easier to attract financing. We were unable to do that. Few people will let a contract to a non-existent shop. We began to actively seek a contract as soon as we had our financing in place and our hiring underway.Use every advantage to secure that first contract, but recognize that future work will be contingent on your performance and the reputation for quality that you establish. Personal relationships will become much less a factor. Don't bite off more than you can chew on that first contract. Find something manageable that will give you some experience, allow you to establish some credibility, and is small enough to be completed in a reasonable length of time. And go all out to deliver the best product possible on or ahead of schedule. Then you're on our own, and relying on your performance record — and that's as it should be. The ball will be in your court and how you handle it will determine the flow of the game for the future.I've covered a lot of ground, and again I emphasize that this has been a review of our experience - a case study in which the last chapters are just now being written - and not a “how to” course, per se. In retrospect, I don't believe that there is much that we would do differently if we were to do it again. We approached the project as a marketing problem and treated it accordingly, drawing on the expertise of others in technical and financial areas. Some of the things we learned would enable us to compress the timeframe to establish a new venture if we were to do it again, but we are relatively well satisfied with how things went.Let me close by just saying that you can become discouraged if you allow it to happen. If you are like we were, the potential of the opportunity is so enormous and so obvious that you won't be able to easily accept the reluctance and skepticism of others. Why can't they see what's as plain as day to us? Why are things taking so long? Be patient and persist. If you're committed, do your homework, lay the groundwork, and do a good selling job, things will ultimately work out. Don't lose your sense of urgency; don't allow your interest to flag; and be patient…be patient…be patient.If we have been able to give you any ideas, then we've accomplished our objective. We wish you well. Thank you.},
booktitle = {Proceedings of the Conference on TRI-Ada '88},
pages = {567–580},
numpages = {14},
location = {Charleston, West Virginia, USA},
series = {TRI-Ada '88}
}

@book{10.5555/579355,
author = {Watkins, Damien and Hammond, Mark and Abrams, Brad},
title = {Programming in the .Net Environment},
year = {2002},
isbn = {0201770180},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: Introduction Developing large distributed software systems is a complex and interesting challenge. A number of architectures have been developed to simplify this task and distract developers from the many interoperability issues associated with developing such systems. This book is predominantly about one of those architectures, Microsoft's .NET Framework. An often-asked question is "so what is new in the .NET Framework " On one level the answer is "not much." To put this answer into context, the same may be said of most recent software advancements. As an example, C++ was a significant step forward but it was the amalgamation of the object oriented concepts of Simula 67 and the efficiency of C. Java also contained very little new science, with the concepts of virtual machines and class libraries having been commonplace for many years. So how then do these advancements contribute Often they contribute via synergy, the combination of known technologies in a new and different manner that allows developers to combine two powerful concepts in a single architecture. So it is with the .NET Framework. Although there are significant benefits to be gained by using the Framework, many readers will be relieved to see that many familiar concepts exist in the environment, although their implementation may have changed. For example, a major concept pervading the .NET Framework is object orientation. Recently this paradigm has seen enormous acceptance in many areas from Graphic User Interface (GUI) development through to network programming. The .NET Framework provides support for all of the object oriented concepts such as classification, information hiding, inheritance andpolymorphism. What is new in the .NET Framework is that language boundaries have been removed. The .NET Framework also extends these concepts in concert with other concepts. For example inheritance can be subject to security constraints; just because you can use a type it may not follow that you can subtype from that type. Audience It is important to understand the target audience that this book was written for in order to know if this book is for you. This book is targeted at software developers who wish to: Understand the philosophy and architecture of the .NET Framework, Produce generic frameworks, libraries, classes and tools to be used in the .NET Framework, and Use multiple languages to develop in the .NET Framework. As this book is targeted at software developers, it not only describes the goals and architecture of the .NET Framework but also demonstrates how the technology implements facilities and services to meet these goals. Understanding the philosophy and architecture of .NET is important for any distributed system developer even if they do not use the .NET Framework. Why The .NET Framework represents Microsoft's vision of distributed system development for the Internet. By understanding the architecture of the .NET Framework developers gain an insight into the issues associated with distributed systems development and Microsoft's solution to these issues. Once developers have an understanding of the .NET Framework's architecture, the next step is to develop software that utilizes the framework. The .NET Framework is not an abstract programming model, it is a full-featured system that allows developers to implement their sol and then make them available to other developers in a robust and secure environment. As the .NET Framework is language agnostic, developers can use the right language to develop parts of a system and then incorporate these parts together at runtime regardless of language differences. So who is this book not for This book is not an introduction to programming; readers should have experience developing software before reading this book. This book is also not the definitive guide to all aspects of the .NET Framework nor any single aspect of the .NET Framework. A single book that covered in detail all aspects of the .NET Framework would be almost indigestible. There will be books devoted to a single part of the .NET Framework, such as ASP.NET. Hopefully this book is a solid overview of fundamental aspects of the .NET architecture but for individual aspects, such as the security system, readers will have to refer to other texts or specific documentation for a complete treatment of a specific topic. Book Structure The structure of the book is fairly straightforward: The Introduction Section of the book describes the basic concepts and gives background information on the issues involved in distributed system development. The Runtime Section describes the issues that can be thought of as "Programming in the small." This section deals with issues such as defining types, storing metadata and executing programs. The Building and Deployment Section deals with the hard issues in distributed systems development. Issues such as assembling and developing software from components and deployment issues are covered here. This section also covers the B the libraries used to build applications. Finally the appendixes contain important peripheral information that does not fit into the first three sections of the book. This includes experience reports from people who have developed compliers for the .NET Framework. Our motivation for writing the book "So why are we writing the book " We have asked this question many times over. In late 1998 Monash University was asked if it would like to be involved with Microsoft in the development of the "next generation of COM", which was then known as the COM Object Runtime (COR). The invitation to join Project 7, the name for this multinational joint collaboration between Microsoft and a number of universities, came from James Plamondon at Microsoft Research. Why was Monash University chosen The major reason was because of our association with Bertrand Meyer and his object oriented programming language Eiffel. Even at this early stage Microsoft was firmly focused on having as many languages as possible supported by the runtime. Monash University accepted the invitation and Damien attend an overview in Atlanta during early 1999. The idea of writing this book was first discussed at that meeting. Having just attended the preview of COR, which was now known as Lightning at this time, Damien asked James Plamondon if he knew of anyone that was writing a book on Lightning. Even at this early stage it was clear that a number of books would be required to cover all of the aspects of Lightning but Damien also wanted to see that the small, but hopefully beneficial, involvement of Project 7 members was recorded. James encouraged Damien to consider writing such a book and, after a few years numerous changes, this book is the outcome of that conversation. The appendixes in this book are the acknowledgement of the work done by many people outside of Microsoft on Project 7. Mark Hammond has been involved in Python development since 1991, developing and maintaining the Win32 extensions for Python, which includes the PythonWin IDE and the support libraries for COM. Mark had been involved with Microsoft since the mid 1990s in Python related projects, most notably the ActiveScripting and ActiveDebugging extensions for the Python language. In 2000 Mark released his first book, Python Programming on Win32, co-authored with Andy Robinson. In 1998, due to the increasing popularity of the Python programming language, the Project 7 team decided that Python should be one of the initial languages ported to the platform. Mark's history with Microsoft meant that he was the obvious choice and being located in Melbourne Australia along with two other initial participants, Melbourne and Monash Universities, meant that a core group of Project 7 participants formed almost exactly on the other side of the world from Seattle. This is how Damien and Mark met. Brad was fortunate to be around for the birth of the Common Language Runtime, cutting his teeth in API design and working on fundamental types such as System.Object and System.String. He participated in the earliest design decisions that would later reflect across the breadth of the .NET Framework and in fact all .NET code. Brad was very enthusiastic about the cross language support being built into the runtime while leading the team that developed the Common Language Specification. Early in 1998 Adam Smith, a developer from another team, asked if he exposed properties from his library could VB (and other languages) consume his API. Brad did what any respectable Program Manager at Microsoft would do and called a one hour meeting to decide what features of the CLR would be available in all languages. That meeting didn't resolve the issue, in fact it took well over three years and thousands of hours from key architects inside of Microsoft such as Anders Hejlsberg, Peter Kukol, Paul Vick, Alan Carter, Scott Wiltamuth, George Bosworth, Lauren Feaux, Ian Ellison-Taylor, Herman Venter, Jonathan Caves, Francis Hogle, Mark Hall, Daryl Olander, Craig Symonds and Brian Harry to answer this question. Later we reviewed and honed the CLS with a group of key language innovators outside of Microsoft, the Project 7 members. It is through this effort that Brad met Damien and Mark. As a byproduct of working out what was to be included in the CLS many "Best Practices" were developed. Brad started writing these best practices down in what would later become the .NET Framework Design Guidelines document. This document lead the way in driving for consistency and usability across the APIs exposed in the .NET Framework. The work on the CLS and the Design Guidelines document lead Brad into a unifying role as we took disparate groups across Microsoft and formed the .NET Framework team. Through this effort Brad gained an appreciation for the value of the different parts of the .NET Framework as well as the need for consistent usage of concepts across them. In addition to his day job, Brad joined a very small team tasked with creating the CLI and C# Language standards first through ECMA and then through ISO. Again the CLS and Design Guidelines got a careful review and honing from this group and with great help from Jim Miller they were published as part of the International standards for the CLI standard. Brad loves to talk about the .NET Framework and how it simplifies the lives of developers so agreeing to do this book was a no-brainer! To partially complete the naming history of the .NET Framework it was known as Project 42 before COR and was subsequently called Lightning, COM+2.0 and NGWS (Next Generation Web Services) before finally being renamed to the .NET Framework only weeks before its launch at the PDC in Orlando in July 2000. From our personal viewpoint, the major satisfaction gained from working on Project 7, as with all experiences in life, has come not from developing a technology but from working with such a large, diverse and talented group of developers from all over the world. An interesting aside about the history of the .NET Framework is to look into each and every .NET Framework executable for the string "BSJB". This magic number refers to some of the original developers of the .NET Framework, Brain Harry, Susan Radke-Sproull, Jason Zander and Bill Evans. ECMA Standardization Core elements of the .NET Framework have been standardized by the European Computer Manufactures Association (ECMA.) A major reason for standardizing the .NET Framework is so that other implementations of the .NET Framework can be built. Apart from the commercial Windows based implementation, Microsoft has built a shared source implementations for Windows and BSD UNIX, hopefully other implementations from different groups will follow. For information on the standardization effort, interested readers should visit: http://www.ecma.ch and in particular the .NET Framework standardization effort at: http://www.ecma.ch/ecma1/STAND/ecma-335.htm The C# language standard is at: http://www.ecma.ch/ecma1/STAND/ECMA-334.htm You can find out more about the Shard Source Implementations at: http://msdn.microsoft.com/net/sscli}
}

@book{10.5555/560389,
author = {Brookshier, Daniel and Govoni, Darren and Krishnan, Navaneeth and Soto, Juan Carlos},
title = {JXTA: Java P2P Programming},
year = {2002},
isbn = {0672323664},
publisher = {Sams},
address = {USA},
abstract = {From the Book: Introduction By Daniel Brookshier When thinking about how to introduce this book, I thought I might start by welcoming you to a new concept in software. I have worked with many types of software, and I have programmed exclusively in Java since it was introduced in 1995. I've seen my share of new concepts and ideas that would change the world. Java has had the biggest impact in my life, and I believe the evidence shows that it has changed the computer world. What about JXTA Why should you or I use a technology that is so new and a departure from Web Services and client server technology When I started looking around, I found that JXTA is not so much a new concept as it is a revolution. Not a revolution in the sense of new or groundbreakinga revolution like the French Revolution. As with most situations where things go wrong, you blame those in power. The French had some rather large grievances with their government. Under Louis XIV and Louis XV, there was extravagant spending, unpopular wars on foreign soil, state bankruptcy, and high taxes imposed mostly on the common man. The French revolutionaries decided that the monarchy and the elite class all had to go. And, as they say, heads would roll. Peer-to-peer is a response to a sort of server-based tyranny. Client-server, multi-tier, and Web server technologies are like kings. Servers concentrate power and resources, limit access, and restrict an individual's ability to access and control his or her own data. This is not exactly an affront to our civil rights, but it can mean that a corporation has my data on their servers. There is also a barrier to the entry. The rich and noble born and elite of France controlled resources, and only large organizations have the resources to buy and maintain large servers. With the rise of Linux, you can create a shoestring operation, but you still need to pay for bandwidth and other resources. Servers hold the applications and data we use, but we have no stake or control in them. As individuals and even large groups, we cannot muster the resources to create our own servers unless we were born rich like a noble or have the resources like a corporation. Oust the king and suddenly you are looking for someone else to govern. The French, while architecting their revolution, had some of the same thoughts as today's JXTA developers. On August 26, 1789, the National Assembly of France approved a document, entitled Declaration of the Rights of Man and of the Citizen. They based it somewhat on the declaration of independence written in America. The French document seems to be more about individuals operating in a society and, thus, more like a peer-to-peer system. Let's look at a few of the articles of the declaration to see where the French revolutionaries and JXTA agree. Men are born and remain free and equal in rights. Social distinctions may be founded only upon the general good (Article 1). Peers also achieve social status via the information or unique processing they contribute. In a server world, the server has almost all resources, while clients have little or none. The principle of all sovereignty resides essentially in the nation. No body nor individual may exercise any authority which does not proceed directly from the nation (Article 3). JXTA creates a community where no individual computer has the ability to affect the entire network unless other member peers allow it. In a sense, this is like a democracy but on a more personal level, because you vote by participation in a group or application. Rights of individual computers are also granted by the protocols that every computer on the peer-to-peer network must follow. In a server environment, the client must follow the rules of the server software and the owners of the server. When there are many servers, there are also many masters, causing the clients to follow too many different and conflicting rules. Liberty consists in the freedom to do everything which injures no one else; hence the exercise of the natural rights of each man has no limits except those which assure to the other members of the society the enjoyment of the same rights. These limits can only be determined by law (Article 4). Liberty in JXTA, like real liberty, is difficult to define. But the key difference from client/server technology is the ability to be an acting part of the application. The benefits are a bit ethereal, but imagine the ability to truly control your data. You can also process the data at any time. Is this freedom Hard to say, but it is a start. JXTA promotes freedom as well as the right to punish those that abuse it. Even a free society has laws. For a network to succeed, there needs to be some way to know when others are harmed and provide a consequence to those responsible. In a P2P network, the ability of one member to do harm is limited. The redundancy of the network reduces the impact on the society of peers but, like any society, there are criminals (or at least perceived to be). JXTA has the notion of a credential. If a peer fails to be a good citizen, its rights may be forfeited and the credential invalidated. Server environments are a bit different. Beyond denial of service attacks, being a good or a bad client is a gray area, mainly because the applications are very constrained for normal users. The server is often cast as the villain, as a hoarder of data and even breaking the trust of clients by allowing the sale of a client's data. All the citizens have a right to decide, either personally or by their representatives, as to the necessity of the public contribution; to grant this freely; to know to what uses it is put; and to fix the proportion, the mode of assessment and of collection and the duration of the taxes (Article 14). Taxation should be compared to a service fee or cost to create a service. A JXTA peer determines the level of participation in the network and, thus, the cost of its hardware and other resources. Like a consumption tax, there is a tendency to pay more, the more you use the network. Due to redundancy and shared processing, all users benefit, rather than suffering because of poor hardware. Users make their own decisions on how they configure and use their P2P software. Inappropriate and draconian controls instituted by a server's owners or chosen software are eliminated. In another way, article 14 also shows the difference between server and P2P technology. With servers, an infrastructure must be maintained. Server software, because of its costs, looks like a government that requires a tax to operate that is usually flat rather than based on participation. With a peer network, peers share resources and each peer pays its share by its existence and level of participation. A society in which the observance of the law is not assured, nor the separation of powers defined, has no constitution at all (Article 16). This is sort of an obvious statement for JXTA. If you don't use JXTA protocols (our Constitution and basic laws), you cannot be a member of the community. If you are using JXTA and do abuse its community, you are usually just hurting yourself. Since property is an inviolable and sacred right, no one shall be deprived thereof except where public necessity, legally determined, shall clearly demand it, and then only on condition that the owner shall have been previously and equitably indemnified (Article 17). P2P started to become popular with the introduction of Napster. Sadly, the implication was that P2P was associated with piracy. Although Napster was originally formed with the idea that only valid owners of music would access digital versions, there was probably more piracy than legitimate use. Consequently, Napster has suffered in court with a severe reduction in the number of users. P2P networks, such as Gnutella, are also devoid of rights management. These systems cannot be taken to court as Napster was because they are truly distributed. However, because of their uncontrolled nature, corporations and ISPs are restricting their traffic, and individual users are being charged with crimes or losing rights to services. It is highly probable that these systems will be disabled or at least inconvenienced. The ultimate goal for JXTA is to be a good citizen and respect copyright and property laws. The reason is simple, without respectability, JXTA is seen as another Napster or Gnutella and will be filtered by ISPs and corporations. Respect others' rights to their property and you will be treated as a fellow citizen and allowed to use the Internet and corporate infrastructures. Most of us live in a commercial society, and we deal with commercial entities. Where there is unfair trade or criminal activity, the system of government or those affected will tend to remove those who abuse the system. Although you may argue that entities like record companies are not acting fairly, the fact is that the laws are currently written to protect themnot those who dislike the law and protest it by circumvention. Napster and the newer incarnations have not changed any laws through their public protests and active breaking of laws. We still need to follow the rule of law to succeed. JXTA Scale Another revolutionary idea of JXTA is what it empowers you to build. Without a central server, with its costs and limits, much more is possible. This does not necessarily mean new types of applications, just a greater scale than was possible in a server environment. A good example of the scalability of JXTA applications is simple catalog for e-commerce. Normally, you would need a large number of clustered servers to handle a large number of transactions. With JXTA, the catalog and its software are distributed automatically among peer computers. Instead of a server that must show the same catalog to millions of users, you just need one PC to distribute the first copy and any updates. All that needs to be centralized is the final order acceptance and credit card transaction, and even that is distributable to some extent. There are many benefits of a P2P catalog from cost savings to the ability of a user to access the catalog offline. The application also runs faster because the user is not as limited to his or her connection speed or waiting in a queue of other users. Add to this 100 percent availability to most users, and you ensure that the verities of the Internet or of a server farm are no longer a part of the risk equation. Another scale feature is raw computing power. In a server environment, each client has access only to limited resources that must be shared by all users. With JXTA, each peer has all the power of the machine it is running on, plus the shared power of all the other peers with which it is collaborating. Is JXTA a New Concept Just by reading this far, you may have seen very familiar concepts. In the prior examples on scale, it is very easy to associate the same goals with distributed computing. The examples of P2P throughout the book are all possible using other methods. However, the point of JXTA is not necessarily to replace these methods. JXTA is a platform with specific protocols to talk to other JXTA platforms in a peer-to-peer network. It is not an application or a library created to build specific applications. The reason JXTA exists is to enable the refactoring of many different applications in a P2P environment. Like the catalog example, the idea is to move away from centralized infrastructures to gain the benefits of a distributed system. RMI, CORBA, and Web Services are distant cousins of JXTA. They are either oriented toward a client/server or limited point-to-point communications. JXTA may seem to provide similar services, but the framework beneath is very different. For example, you can implement remote method invocation. The key difference between JXTA and others is that the delivery of the command to execute can span barriers like firewalls. The remote command can be sent to groups of computers or just a single computer, depending on the type of task. JXTA Risks I think we can safely agree that JXTA is not like anything else. Is JXTA something to bet your time as well as your fortune on There are risks. Some are new and others are well known. Some are being fixed as you read this book, and others simply need to be implemented on the current JXTA platform. The largest risk now is that JXTA will be in flux over the next couple of years. The good news is that the community of developers will try to keep the network stable for the purpose of keeping their products working. When you reach a certain point, developers learn to hate change, even when the project is open source. It is not all a bed of roses in other areas. There are aspects to a P2P system that can be problematic. In our catalog example, it does take time to propagate the catalog to all users. The same time delay is true of updates and transactions. We are at the start of the JXTA revolution. It is time to think revolutionarily thoughts. The reign of client server is about to fall. Read on and join the revolution. Viva la revolution! Daniel Brookshier JXTA Community Member, Java Consultant January 2002, Dallas, Texas What This Book Covers This book will only cover the Java J2SE reference platform implementation of JXTA. We will not cover the C++, J2ME, Pearl, or other languages that are being used to create JXTA platforms. The J2SE version is the reference platform and best for experimentation or explanation of JXTA protocols. Java is also the most popular language for JXTA development at this time. This book is intended to introduce new developers to the JXTA API and selected applications and services. Our goal is for the reader to understand P2P concepts and be able to build useful applications using JXTA. We do not cover detailed aspects of how the JXTA platform is implemented unless it adds value to the explanation on how to use it. Who Should Use This Book This book is written for readers who need an introduction to P2P and for those who want to learn JXTA. You should already be comfortable with Java. You do not need to know anything about JXTA or peer-to-peer programming. By the end of the book, you should be able to create simple P2P applications using JXTA and the J2SE JXTA reference platform. How This Book is Organized This book is organized with two goals. The first goal is to explain P2P and JXTA in general terms. The second goal is to create applications that use JXTA. Finally, we cover specific applications with the aim of furthering an understanding of JXTA while showing how more complete applications are written. This arrangement was chosen so that the reader can get an overview of JXTA and then build an understanding of how to use its various parts. Web Resources and Example Code You can download the source code for examples presented in this book from http://www.samspublishing.com. When you reach that page, enter this book's ISBN number (0672323664) in the search box to access information about the book and a Source Code link. The NetBeans IDE was used for much of the code that is found in the book. NetBeans is available at http://www.netbeans.org. Because Forte from Sun Microsystems is derived from NetBeans, it should work as well. You can also use your favorite editor or IDE, but the ANT scripts were created within NetBeans and Forte. Also on the site are files that can be used with MagicDraw from NoMagic at http://www.MagicDraw.com. The tool is written in Java and runs on most Java-compatible platforms. The demo version will allow you to browse and print the JXTA diagrams used in the book, but it will not allow you to save changes. The MagicDraw files follow the XMI standard for UML representation in XML, so other UML tools that support the standard should work (drawings may look different). © Copyright Pearson Education. All rights reserved.}
}

@book{10.5555/580423,
author = {Karlins, David},
title = {FrontPage 2002 Virtual Classroom},
year = {2001},
isbn = {0072191724},
publisher = {McGraw-Hill Professional},
abstract = {From the Book: Introduction Who Will Enjoy This You will, if you want to create attractive, feature-packed, and easy-to-use Web sites with FrontPage. Because of its Microsoft Office-like interface, FrontPage is a very accessible Web design tool. But beneath the surface, youll find powerful features that allow you to edit pictures, generate JavaScripts, and collect input data-features not available in any other Web design package. My goal with this book is to make those features accessible to both brand new Web designers, as well as veteran FrontPage designers who would like to add advanced features to their sites. Ive been teaching folks like yourself to use FrontPage for five years now. Ive written Microsoft authorized books on how to pass the Microsoft FrontPage MCSD exam (and Ive passed the Microsoft FrontPage Certified Professional exam myself). But Ive also taught people FrontPage who have never created a Web site before. Perhaps most importantly, I use FrontPage almost every day to create Web sites. Ive learned through trial and error the best ways to create Web sites with FrontPage, and also the best ways to learn FrontPage. Beginning level, intermediate, and many advanced FrontPage designers will all find important resources in this book. Many chapters approach concepts like tables, frames, and input forms on different levels. At first, new FrontPage users might want to try the more basic step-by-step sections early in the chapter, while more advanced developers will push the envelope using all the features covered in a chapter. And, this book is much more than a book. The accompanying CD-ROM, which is discussed in detail in the remainder of this introduction, has more than an hour of videos with demonstrations, tips, and candid advice. FrontPage 2002 Virtual Classroom CD This CD contains an exciting new kind of video-based instruction to help you learn FrontPage faster. We believe this learning tool is a unique development in the area of computer-based training. The author actually talks to you, right from your computer screen, demonstrating topics he wrote about in the book. Moving screencams and slides accompany the presentation, reinforcing what youre learning. The technology and design of the presentation were developed by Brainsville.com. The content on the CD-ROM was developed by OsborneMcGraw-Hill, David Karlins, and Brainsville.com. Patents (pending), copyright, and trademark protections apply to this technology and the name Brainsville.com. To ensure that the lessons play as smoothly as possible, please read the following directions for usage of the CD-ROM. Getting Started The CD-ROM is optimized to run under Windows 9598MENT2000 using the QuickTime player version 5 (or greater), from Apple. This CD-ROM is not designed to run on a Mac. If you dont have the QuickTime 5 player installed, you must install it either by downloading it from the Internet at http:www. quicktime.com, or running the Setup program from the CD-ROM. If you install from the Web, its fine to use the free version of the QuickTime player. You dont need to purchase the full version. To install the QuickTime player from the CD-ROM on a Windows PC: 1. Insert the CD-ROM in the drive. 2. Use Explorer or My Computer to browse to the CD-ROM. 3. Open the QuickTime folder. 4. Double-click the setup program there. 5. Follow the setup instructions on screen. Running the CD in Windows 9598MENT2000 Minimum Requirements: QuickTime 5 player Pentium II P300 (or equivalent) 64MB of RAM 8X CD-ROM Windows 95, Windows 98, Windows 2000, Windows ME, or Windows NT 4.0 with at least Service Pack 4 16-bit sound card and speakers FrontPage 2002 Virtual Classroom CD-ROM can run directly from the CD (see the following for running it from the hard drive for better performance if necessary) and should start automatically when you insert the CD in the drive. If the program does not start automatically, your system might not be set up to automatically detect CDs. To change this, you can do the following: 1. Choose Settings I Control Panel, and click the System icon. 2. Click the Device Manager tab in the System Properties dialog box. 3. Double-click the Disk drives icon and locate your CD-ROM drive. 4. Double-click the CD-ROM drive icon, and then click the Settings tab in the CD-ROM Properties dialog box. Make sure the Auto Insert Notification box is checked. This specifies that Windows will be notified when you insert a compact disc into the drive. If you dont care about the auto-start setting for your CD-ROM, and dont mind the manual approach, you can start the lessons manually. Heres how: 1. Insert the CD-ROM. 2. Double-click the My Computer icon on your Windows desktop. 3. Open the CD-ROM folder. 4. Double-click the startnow.exe icon in the folder. 5. Follow the instructions on screen to start. When the program autostarts, youll see a small window in the middle of your screen with an image of the book; click that image to launch the QuickTime player and start the lessons. The QuickTime player window should open and the Virtual Classroom introduction should begin running. On some computers, after the lesson loads you must click the Play button to begin. The Play button is the big round button with an arrow on it at the bottom center of the QuickTime player window. It looks like the play button on a VCR. You can click the links in the lower-left region of the QuickTime window to jump to a given lesson. The author will explain how to use the interface. The QuickTime player will completely fill a screen that is running at 800 x 600 resolution. (This is the minimum resolution required to play the lessons.) For screens with higher resolution, you can adjust the position of the player on screen, as you like. If you are online, you can click the Brainsville.com logo under the index marks to jump directly to the Brainsville.com Web site for information about additional video lessons from Brainsville.com. (See the description in the back of this book about the Web Design CD Extra for more details.) Improving PlayBack Your Virtual Classroom CD-ROM employs some cutting-edge technologies, requiring that your computer be pretty fast to run the lessons smoothly. Many variables determine a computers video performance, so we cant give you specific requirements for running the lessons. CPU speed, internal bus speed, amount of RAM, CD-ROM drive transfer rate, video display performance, CD-ROM cache settings and other variables will determine how well the lessons play. Our advice is to simply try the CD. The disk has been tested on laptops and desktops of various speeds, and in general, youll need at least a Pentium II-class computer running in excess of 300Mhz for decent performance. (If youre doing serious Web-design work, its likely your machine is at least this fast.) Close Other Programs For best performance, make sure you are not running other programs in the background while viewing the CD-based lessons. Rendering the video onscreen takes a lot of computing power, and background programs such as automatic e-mail checking, Web-site updating, or Active Desktop applets (such as scrolling stock tickers) can tax the CPU to the point of slowing the videos. Adjust the Screen Color Depth to Speed Up Performance Its possible that the authors lips will be out of synch with his voice, just like Web-based videos often look. There are a couple solutions: Lowering the color depth to 16-bit color makes a world of difference with many computers, laptops included. Rarely do people need 24-bit or 32-bit color for their work anyway, and it makes scrolling your screen (in any program) that much slower when running in those higher color depths. Try this: 1. Right-click the desktop and choose Properties. 2. Click the Settings tab. 3. In the Colors section, open the drop-down list box and choose a lower setting. If you are currently running at 24-bit (True Color) color, for example, try 16-bit (High Color). Dont use 256 colors, because video will appear very funky if you do. 4. OK the box. With most computers these days, you dont have to restart the computer after making this change. The video should run more smoothly now, because your computers CPU doesnt have to work as hard to paint the video pictures on your screen. If adjusting the color depth didnt help the synch problem, see the following section about copying the CDs files to your hard disk. When lessons are playing youre likely to not interact with the keyboard or mouse. Because of this, your computer screen might blank, and in some cases (such as with laptops) the computer might even go into a standby mode. Youll want to prevent these annoyances by turning off your screen saver and checking the power options settings to ensure they dont kick in while youre viewing the lessons. You make settings for both of these parameters from the Control Panel. 1. Open Control Panel, choose Display, and click the Screen Saver tab. Choose None for the screen saver. 2. Open Control Panel, choose Power Management, and set System Standby, Turn off Monitor, and Turn off Hard Disks to Never. Then click Save As and save this power setting as Brainsville Courses. You can return your power settings to their previous state, if you like, after you are finished viewing the lessons. just use the Power Schemes drop-down list and choose one of the factorysupplied settings, such as HomeOffice Desk. Copy the CD Files to the Hard Disk to Speed Up Performance The CD-ROM drive will whir quite a bit when running the lessons from the CD. If your computer or CD-ROM drive is a bit slow, its possible the authors lips will be out of synch with his voice, just like Web-based videos often look. The video might freeze or slow down occasionally, though the audio will typically keep going along just fine. If you dont like the CD constantly whirring, or you are annoyed by outof-synch video, you might be able to solve either or both problems by copying the CD-ROMs contents to your hard disk and running the lessons from there. To move CD content to your hard disk: 1. Using My Computer or Explorer, check to see that you have at least 650M free space on your hard disk. 2. Create a new folder on your hard disk (the name doesnt matter) and copy all the contents of the CD-ROM to the new folder. (You must preserve the subfolder names and folder organization as it is on the CD-ROM). 3. Start the program by opening the new folder and double-clicking the file startnow.exe. This will automatically start the lessons and run them from the hard disk. 4. (Optional) For convenience, you can create a shortcut to the startnow.exe file and place it on your desktop. You will then be able to start the program by clicking the shortcut. Update Your QuickTime Player The QuickTime software is updated frequently and posted on the Apple QuickTime Web site ( ). You can update your software by clicking Update Existing Software, from the Help menu in the QuickTime player. We strongly suggest you do this from time to time. Make Sure Your CD-Rom Drive is Set for Optimum Performance CD-ROM drives on IBM PCs can be set to transfer data using the DMA (Direct Memory Access) mode, assuming the drive supports this faster mode. If you are experiencing slow performance and out-of-synch problems, check this setting. These steps are for Windows 98 and Windows ME: 1. Choose Control Panel I System. 2. Click the Device Manager tab. 3. Click the plus (+) sign to the left of the CD-ROM drive. 4. Right-click the CD-ROM drive. 5. Choose Properties. 6. Click the Settings tab. 7. Look to see if the DMA check box is turned on (has a check mark in it). If selected, this increases the CD-ROM drive access speed. Some drives do not support this option. If the DMA check box remains selected after you restart Windows, this option is supported by the device. In Windows 2000, the approach is a little different. You access the drives settings via Device Manager as above, but click IDEATAPI Controllers. Right-click the IDE channel that your CD-ROM drive is on, choose Properties, and make the settings as appropriate. (Choose the device number, 0 or 1, and check the settings.) Typically its set to DMA If Available, which is fine. Its not recommended that you change these settings unless you know what you are doing! TroubleShooting This section offers solutions to common problems. Check for much more information about the QuickTime player, which is the software the Virtual Classroom CD uses to play. The CD Will Not Run If you have followed the instructions above and the program will not work, you might have a defective drive or CD. Be sure the CD is inserted properly in the drive. Test the drive with other CDs to see if they run. The ScreenCam Movie In! A Lesson Tangs If the author continues to talk, but the accompanying screencam seems to be stuck, just click the lesson index in the lower-left region of the QuickTime window to begin your specific lesson again. If this doesnt help, close the QuickTime window; then start the Virtual Classroom CD again. Volume Is Too Low or Totally Silent 1. Check your system volume first. Click the speaker icon next to the clock, in the lower-right corner of the screen. A little slider pops up. Adjust the slider, and make sure the Mute check box is not checked. 2. Next, if you have external speakers on your computer, make sure your speakers are turned on, plugged in, wired up properly, and the volume control on the speakers themselves is turned up. 3. Note that the QuickTime player also has a volume control setting. The setting is a slider control in the lower-left of the QuickTime player window. 4. The next place to look if youre still having trouble is in the Windows volume controls. Double-click the speaker next to the clock and it will bring up the Windows Volume Control sliders. Make sure the slider for Wave is not muted, and make sure its positioned near the top. For Technical Support Phone Hudson Software at (800) 217-0059 Visit Visit}
}

