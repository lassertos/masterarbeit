@article{10.1002/cae.21757,
author = {Gen\c{c}, Hasan Hakan and Aydin, Serkan},
title = {Education on visualization of some complex physics problems in programing environment},
year = {2016},
issue_date = {November 2016},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {24},
number = {6},
issn = {1061-3773},
url = {https://doi.org/10.1002/cae.21757},
doi = {10.1002/cae.21757},
journal = {Comput. Appl. Eng. Educ.},
month = {nov},
pages = {876–886},
numpages = {11},
keywords = {STEM, complex problems, computer aided, programming, visualization}
}

@inproceedings{10.1007/978-3-030-63212-0_8,
author = {\v{S}iaulys, Tomas},
title = {Engagement Taxonomy for Introductory Programming Tools: Failing to Tackle the Problems of Comprehension},
year = {2020},
isbn = {978-3-030-63211-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63212-0_8},
doi = {10.1007/978-3-030-63212-0_8},
abstract = {A large number of introductory programming environments for K-12 education have become widely used across the world. One of the main ideas behind these environments is introducing basic programming concepts more effectively by incorporating different visualization strategies. There have been attempts to classify introductory programming tools, however, certain critical aspects have not yet been discussed within the existing classifications, especially those related to user engagement in the programming environment. In this paper we introduce an engagement taxonomy for introductory programming tools (ETIP) built on a concept of engagement taxonomy for software visualization and previous classifications of programming learning tools. The new taxonomy is then used to inclusively review introductory programming environments for secondary education used today with a focus on user engagement in a learning environment. Our review illustrates how majority of introductory programming tools do not fully explore the ways visualizations could help with tackling the problems of beginner programming comprehension. There is still a lack of knowledge about the importance of the level of engagement in visual introductory programming tools and the suggested taxonomy could be used for future research of computer science education.},
booktitle = {Informatics in Schools. Engaging Learners in Computational Thinking: 13th International Conference, ISSEP 2020, Tallinn, Estonia, November 16–18, 2020, Proceedings},
pages = {94–106},
numpages = {13},
keywords = {Introductory programming, Software visualization, Engagement taxonomy},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/3392063.3394437,
author = {Pires, Ana Cristina and Rocha, Filipa and de Barros Neto, Antonio Jos\'{e} and Sim\~{a}o, Hugo and Nicolau, Hugo and Guerreiro, Tiago},
title = {Exploring accessible programming with educators and visually impaired children},
year = {2020},
isbn = {9781450379816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3392063.3394437},
doi = {10.1145/3392063.3394437},
abstract = {Previous attempts to make block-based programming accessible to visually impaired children have mostly focused on audio-based challenges, leaving aside spatial constructs, commonly used in learning settings. We sought to understand the qualities and flaws of current programming environments in terms of accessibility in educational settings. We report on a focus group with IT and special needs educators, where they discussed a variety of programming environments for children, identifying their merits, barriers and opportunities. We then conducted a workshop with 7 visually impaired children where they experimented with a bespoke tangible robot-programming environment. Video recordings of such activity were analyzed with educators to discuss children's experiences and emergent behaviours. We contribute with a set of qualities that programming environments should have to be inclusive to children with different visual abilities, insights for the design of situated classroom activities, and evidence that inclusive tangible robot-based programming is worth pursuing.},
booktitle = {Proceedings of the Interaction Design and Children Conference},
pages = {148–160},
numpages = {13},
keywords = {accessible, children, educators, programming, robots, tangible, visual impairments},
location = {London, United Kingdom},
series = {IDC '20}
}

@inproceedings{10.23919/FRUCT.2017.8071326,
author = {Mordvinov, Dmitry and Litvinov, Yurii and Bryksin, Timofey},
title = {TRIK studio: Technical introduction},
year = {2017},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
url = {https://doi.org/10.23919/FRUCT.2017.8071326},
doi = {10.23919/FRUCT.2017.8071326},
abstract = {This paper presents TRIK Studio - an environment for visual (and textual) programming of robotic kits, which is used in educational organizations across Russia and Europe. First part of the article provides overview of the system - its purpose, features, differences from similar programming environments, general difficulties of robot programming and solutions proposed by TRIK Studio. Second part presents implementation details of TRIK Studio and its most interesting components. This article combines five fields of study: robotics, domain-specific visual modeling, education, formal methods and methods of program analysis. Main contribution of this article is detailed technical description of TRIK Studio as complex and successful open-source cross-platform robot programming environment written in C++/Qt, and first part of the article can also be interesting for teachers as it provides an overview of existing robot programming tools and related problems.},
booktitle = {Proceedings of the 20th Conference of Open Innovations Association FRUCT},
pages = {296–308},
numpages = {13},
keywords = {Code generation, Data-flow languages, Educational robotics, Robotics, Visual programming},
location = {Saint-Petersburg, Russia},
series = {FRUCT '17}
}

@phdthesis{10.5555/163789,
author = {Chou, Wen-Chung},
title = {The effects of programming instruction in procedural programming and logic programming environments on problem-solving ability},
year = {1993},
publisher = {University of Illinois at Urbana-Champaign},
address = {USA},
abstract = {The purposes of this study were to evaluate the effectiveness of programming instruction on students' problem-solving abilities, and to explore whether a logic programming or a procedural programming environment results in different degrees of cognitive transfer in terms of procedural, declarative, and conditional thinking skills. In an attempt to identify factors which influence programming learning, this study investigated interaction effects between types of programming instruction and students' aptitude variables, gender and computer anxiety. Three intact classes of 35 students each from the Department of Mathematics and Science Education at Pingtung Teacher's College in Taiwan were randomly assigned to a procedural programming group (QuickBASIC), a logic programming group (Turbo Prolog), and a control group. The study followed a nine-week schedule, the first and the last weeks designated for administration of pre-and post-tests, respectively. An ANCOVA analysis and a multiple regression analysis were used to test statistical hypotheses about programming effects and interactions.Findings of this study indicate that procedural programming is effective in developing procedural thinking skills, while logic programming is effective in developing declarative thinking skills. Neither programming environment had a significant effect on the development of conditional thinking skills. In terms of cognitive development, female students performed better than male students in a logic programming environment, while the reverse was true in a procedural programming environment. No significant interaction was found between computer anxiety and types of programming instruction.Implications of this study for teaching practice are (a) serious consideration of including both procedural programming and logic programming environments in the school curriculum, (b) developing optimum computing environments that foster all procedural thinking, declarative thinking, and conditional thinking skills, (c) cultivating students' abilities to solve daily-life oriented problems with logic programming environments, and (d) adapting programming environments to individual differences. In addition, future studies should concentrate more on identifying near transfer rather than far transfer effects. Recommendations for future research are provided in this dissertation.},
note = {UMI Order No. GAX93-28999}
}

@inproceedings{10.1145/3563657.3596042,
author = {Goswami, Lahari and Zeinoddin, Pegah Sadat and Estier, Thibault and Cherubini, Mauro},
title = {Supporting Collaboration in Introductory Programming Classes Taught in Hybrid Mode: A Participatory Design Study},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596042},
doi = {10.1145/3563657.3596042},
abstract = {Hybrid learning modalities, where learners can attend a course in-person or remotely, have gained particular significance in post-pandemic educational settings. In introductory programming courses, novices’ learning behaviour in the collaborative context of classrooms differs in hybrid mode from that of a traditional setting. Reflections from conducting an introductory programming course in hybrid mode led us to recognise the need for re-designing programming tools to support students’ collaborative learning practices. We conducted a participatory design study with nine students, directly engaging them in design to understand their interaction needs in hybrid pedagogical setups to enable effective collaboration during learning. Our findings first highlighted the difficulties that learners face in hybrid modes. The results then revealed learners’ preferences for design functionalities to enable collective notions, communication, autonomy, and regulation. Based on our findings, we discuss design principles and implications to inform the future design of collaborative programming environments for hybrid modes.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {1248–1262},
numpages = {15},
keywords = {collaboration, hybrid classroom, participatory design, programming environment},
location = {<conf-loc>, <city>Pittsburgh</city>, <state>PA</state>, <country>USA</country>, </conf-loc>},
series = {DIS '23}
}

@inproceedings{10.1145/2676723.2693622,
author = {Weintrop, David},
title = {Minding the Gap Between Blocks-Based and Text-Based Programming (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2693622},
doi = {10.1145/2676723.2693622},
abstract = {Graphical blocks-based programming environments, such as Scratch and Snap!, are becoming increasingly popular tools for introducing learners to programming in formal educational settings. However, a growing body of research is finding that students struggle when transitioning from these tools to more conventional, text-based programming languages. To better understand students' difficulties and begin to explore potential solutions to facilitate this transition, a 10-week, quasi-experimental study was conducted with 80 students across three high-school introductory programming classes. Each class spent five weeks working with different version of a blocks-based programming tool, each of which integrated text-based programming in a different way. After working in the introductory environments, students transitioned to Java for the remainder of the study. The goal of this project is to understand the affects of blocks-based programming on students' emerging understandings, document challenges students face in transitioning from blocks-based to text-based programming, and investigate potential ways to bridge these two modalities. To answer these questions, a mixed-method approach was taken that included cognitive interviews with learners, automated collection of student authored programs, and pre/mid/post attitudinal and content assessments.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {720},
numpages = {1},
keywords = {blocks-based programming, high school computer science, introductory programming environments},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1145/949344.949394,
author = {Gray, Kathryn E. and Flatt, Matthew},
title = {ProfessorJ: a gradual introduction to Java through language levels},
year = {2003},
isbn = {1581137516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/949344.949394},
doi = {10.1145/949344.949394},
abstract = {In the second-semester programming course at the University of Utah, we have observed that our students suffer unnecessarily from a mismatch between the course content and the programming environment. The course is typical, in that it exposes students to Java a little at a time. The programming environments are also typical, in that they report compilation and run-time errors in the jargon of professional programmers who use the full Java language. As a result, students rely heavily on teaching assistants to interpret error messages, and valuable classroom time is wasted on syntactic diversions.ProfessorJ is our new programming environment that remedies this problem. Like other pedagogical environments, such as BlueJ and DrJava, ProfessorJ presents the student with a simplified interface to the Java compiler and virtual machine. Unlike existing environments, ProfessorJ tailors the Java language and error messages to the students' needs. Since their needs evolve through the course, ProfessorJ offers several language levels, from Beginner Java to Full Java.},
booktitle = {Companion of the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {170–177},
numpages = {8},
location = {Anaheim, CA, USA},
series = {OOPSLA '03}
}

@inproceedings{10.1109/FIE44824.2020.9273982,
author = {Carlos Begosso, Luiz and Ricardo Begosso, Luiz and Aragao Christ, Natalia},
title = {An analysis of block-based programming environments for CS1},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE44824.2020.9273982},
doi = {10.1109/FIE44824.2020.9273982},
abstract = {This Research Full Paper presents our experience in analyzing and selecting block-based programming environments to support the teaching of algorithms for the students starting the introductory courses of a Computer Science major. The teaching of algorithms and programming concepts to students of the first years of Computer Science and Engineering courses has been a major challenge because students often have difficulty understanding the logic and abstraction, leading to a high dropout rate. Some strategies have been conducted to further the mission of helping students understand better those basic concepts, but this topic still remains a major problem for students in the initial grades of those courses. In previous projects developed at our university, we have already proposed the use of learning objects and gamification, with very positive results. One of the questions that arise when we adopt new teaching approaches is to know how this new path will contribute to the student’s learning. In this project, we conducted a study on eight block-based programming environments and sought to identify which aspects of those environments comply with the Computer Science reference curriculum. Our work was based on the joint task force on Computing Curricula conducted by the ACM and IEEE Computer Society CS2013 curriculum guidelines for undergraduate programs in Computer Science. We studied the virtual programming environments Alice, MIT App Inventor, Blockly Games, Code.org, Gameblox, Pencil Code, Microsoft MakeCode and Scratch. Then, we crossed the characteristics of each, identified the positive and negative points of each teaching environment in relation to the topics established by the guidelines. We have classified the main characteristics of those programming environments, establishing criteria such as: prior programming knowledge requirements; ease of interaction with users; programming language code; availability of documentation for learning; programming practices addressed by the environment; and ease of learning programming. We believe that this work can contribute to the selection process of a suitable programming environment to be adopted in an introductory course of computer programming.},
booktitle = {2020 IEEE Frontiers in Education Conference (FIE)},
pages = {1–5},
numpages = {5},
location = {Uppsala}
}

@inproceedings{10.1145/299649.299718,
author = {Ginat, David and Shifroni, Eyal},
title = {Teaching recursion in a procedural environment—how much should we emphasize the computing model?},
year = {1999},
isbn = {1581130856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/299649.299718},
doi = {10.1145/299649.299718},
abstract = {Recursion is a powerful and essential computational problem solving tool, but the concept of recursion is difficult to comprehend. Students that master the conventional programming construct of iteration in procedural programming environments, find it hard to utilize recursion.This study started as a test of CS College students' utilization of recursion. It was conducted after they have completed CS1, where they studied recursion with the C programming language. The test revealed that students adhere to the iterative pattern of "forward accumulation", due to their confidence with the iteration construct, but lack of trust of the recursion mechanism. These results motivated us to get more insight into the nature of recursion difficulties and ways to overcome them.In this paper we describe the difficulties we observed, and present a declarative, abstract, approach that contributed to overcome them. We question the emphasis that should be put on the basic computing model when presenting recursion, and argue for emphasis on the declarative approach for teaching recursion formulation in a procedural programming environment.},
booktitle = {The Proceedings of the Thirtieth SIGCSE Technical Symposium on Computer Science Education},
pages = {127–131},
numpages = {5},
keywords = {problem decomposition, recursive formulation},
location = {New Orleans, Louisiana, USA},
series = {SIGCSE '99}
}

@inproceedings{10.1145/3287324.3293798,
author = {Akram, Bita and Min, Wookhee and Wiebe, Eric and Mott, Bradford and Boyer, Kristy Elizabeth and Lester, James},
title = {Assessing Middle School Students' Computational Thinking Through Programming Trajectory Analysis},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293798},
doi = {10.1145/3287324.3293798},
abstract = {With national K-12 education initiatives such as "CSForAll," block-based programming environments have emerged as widely used tools for teaching novice programming. A key challenge presented by block-based programming environments is assessing students' computational thinking (CT) and programming competencies. Developing assessment methods that can evaluate students' use of CT practices such as testing and refining, and developing and using appropriate algorithms, can help teachers evaluate students learning and provide appropriate scaffolding. In this work, we utilize an evidence-centered assessment design approach to devise a three-dimensional assessment to evaluate students' CT competencies based on evidence extracted from their programming trajectories in a block-based programming environment. In this assessment, the first dimension assesses students' knowledge of essential CT concepts, the second dimension assesses students' dynamic testing and refining strategies, and the third dimension assesses their overall problem-solving efficiency. We apply the assessment framework to data collected from students' interactions with a game-based learning environment designed to develop middle-grade students' CT competencies and programming skills. The results demonstrate that students' knowledge of basic CT constructs, such as appropriate use and combination of control structures, serves as the foundation for designing and implementing effective algorithms. Further, we assessed students testing and refining strategies over the three dimensions of novelty, positivity, and scale. The results demonstrate that students with higher algorithmic capabilities tend to make more novel, positive, and small-scale changes. The results reveal distinctive patterns in students' approaches to computational thinking problem solving and make a step toward identifying and assessing productive computational thinking practices.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1269},
numpages = {1},
keywords = {block-based programming, computational thinking assessment, evidence-centered design, game-based learning, programming trajectories},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/62115.62131,
author = {LeBlanc, Thomas J. and Scott, Michael L. and Brown, Christopher M.},
title = {Large-scale parallel programming: experience with BBN butterfly parallel processor},
year = {1988},
isbn = {0897912764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/62115.62131},
doi = {10.1145/62115.62131},
abstract = {For three years, members of the Computer Science Department at the University of Rochester have used a collection of BBN Butterfly™ Parallel Processors to conduct research in parallel systems and applications. For most of that time, Rochester's 128-node machine has had the distinction of being the largest shared-memory multiprocessor in the world. In the course of our work with the Butterfly we have ported three compilers, developed five major and several minor library packages, built two different operating systems, and implemented dozens of applications. Our experience clearly demonstrates the practicality of large-scale shared-memory multiprocessors, with non-uniform memory access times. It also demonstrates that the problems inherent in programming such machines are far from adequately solved. Both locality and Amdahl's law become increasingly important with a very large number of nodes. The availability of multiple programming models is also a concern; truly general-purpose parallel computing will require the development of environments that allow programs written under different models to coexist and interact. Most important, there is a continuing need for high-quality programming tools; widespread acceptance of parallel machines will require the development of programming environments comparable to those available on sequential computers.},
booktitle = {Proceedings of the ACM/SIGPLAN Conference on Parallel Programming: Experience with Applications, Languages and Systems},
pages = {161–172},
numpages = {12},
location = {New Haven, Connecticut, USA},
series = {PPEALS '88}
}

@inproceedings{10.5555/1973598.1973643,
author = {Yang, Hung-Jen and Yang, Hsieh-Hua and Wang, Cheng Chung and Huang, Kuo-Yan},
title = {Social interactions of the computerized collaborative problem solving on micro chip},
year = {2006},
isbn = {9608457432},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {We have investigated collaborative problem solving in a teaching experiment, which was organized for 32 senior university students in the computerized collaborative problem solving learning environment. The participating teacher was trained by us and students had available kits, interfaces and computers equipped with 8051 micro chip programming tools. Student activities were video recorded and the analysis proceeded through writing video protocols, edited into episodes and then classified into categories. Categories were mainly derived empirically. In the analysis, we used concepts such as collaboration and problem solving, in accordance with social constructivism. The data showed that typical learning processes were collaborative (51% of all episodes) as well as dynamic problem-solving processes, inseveral stages. Students worked quite independently of the teacher, as they learned to use the programming tool autonomously in their technology projects. It appears, however, that more teacher support, such as introducing handbooks, planning tools and advanced programming skills, would have been an advantage. And it also appears the non-group interations would provide help in the problem solving process. Some ideas about further development of study processes in modern learning environments are discussed.},
booktitle = {Proceedings of the 5th WSEAS International Conference on Applied Computer Science},
pages = {227–233},
numpages = {7},
keywords = {collaborative learning, computerized, problem solving, social interactions},
location = {Hangzhou, China},
series = {ACOS'06}
}

@article{10.1016/j.compedu.2016.01.010,
author = {Chao, Po-Yao},
title = {Exploring students' computational practice, design and performance of problem-solving through a visual programming environment},
year = {2016},
issue_date = {April 2016},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {95},
number = {C},
issn = {0360-1315},
url = {https://doi.org/10.1016/j.compedu.2016.01.010},
doi = {10.1016/j.compedu.2016.01.010},
abstract = {This study aims to advocate that a visual programming environment offering graphical items and states of a computational problem could be helpful in supporting programming learning with computational problem-solving. A visual problem-solving environment for programming learning was developed, and 158 college students were conducted in a computational problem-solving activity. The students' activities of designing, composing, and testing solutions were recorded by log data for later analysis. To initially unveil the students' practice and strategies exhibited in the visual problem-solving environment, this study proposed several indicators to quantitatively represent students' computational practice (Sequence, Selection, Simple iteration, Nested iteration, and Testing), computational design (Problem decomposition, Abutment composition, and Nesting composition), and computational performance (Goal attainment and Program size). By the method of cluster analysis, some empirical patterns regarding the students' programming learning with computational problem-solving were identified. Furthermore, comparisons of computational design and computational performance among the different patterns of computational practice were conducted. Considering the relations of students' computational practice to computational design and performance, evidence-based suggestions on the design of supportive programming environments for novice programmers are discussed. A visual problem-solving environment was proposed to support programming learning.Students exhibited different patterns of computational practice in the environment.Patterns of computational practice were correlated with computational design and performance.},
journal = {Comput. Educ.},
month = {apr},
pages = {202–215},
numpages = {14},
keywords = {Computer programming, Students programming patterns, Visual problem solving}
}

@article{10.4018/IJWLTT.2017100106,
author = {Papadakis, Stamatios and Kalogiannakis, Michail and Orfanakis, Vasileios and Zaranis, Nicholas},
title = {The Appropriateness of Scratch and App Inventor as Educational Environments for Teaching Introductory Programming in Primary and Secondary Education},
year = {2017},
issue_date = {October 2017},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {4},
issn = {1548-1093},
url = {https://doi.org/10.4018/IJWLTT.2017100106},
doi = {10.4018/IJWLTT.2017100106},
abstract = {Teaching programming is a complex task. The task is even more challenging for introductory modules. There is an ongoing debate in the teaching community over the best approach to teaching introductory programming. Visual block-based programming environments allow school students to create their own programs in ways that are more accessible than in textual programming environments. These environments designed for education allow students to program without the obstacle of syntax errors errors in typing commands found in traditional text-based languages. In this paper, the authors focus on the use of App Inventor and Scratch as blocks-based programming environments designed explicitly with novices in mind. In the authors' analysis, both Novice Programming Environments NPEs seemed to be attractive platforms for introducing fundamental concepts in computer programming and both look appealing for both majors and non-majors.},
journal = {Int. J. Web-Based Learn. Teach. Technol.},
month = {oct},
pages = {58–77},
numpages = {20},
keywords = {App Inventor for Android AIA, Novice Programmers, Novice Programming Environments NPEs, Primary Education, Scratch, Secondary Education}
}

@inproceedings{10.1109/ICALT.2005.310,
author = {Evangelidis, Georgios},
title = {WIPE " Pilot Testing and Comparative Evaluation},
year = {2005},
isbn = {0769523382},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICALT.2005.310},
doi = {10.1109/ICALT.2005.310},
abstract = {This paper discusses the pilot testing and evaluation of a database-driven web-based programming environment called WIPE (Web Integrated Programming Environment). WIPE is a teaching tool for secondary education students that are introduced to the principles of programming. The programming environment was used in secondary schools in Greece and the results of its evaluation demonstrate that it successfully deals with the difficulties novices meet.},
booktitle = {Proceedings of the Fifth IEEE International Conference on Advanced Learning Technologies},
pages = {928–932},
numpages = {5},
series = {ICALT '05}
}

@phdthesis{10.5555/AAI28087160,
author = {Blanchard, Jeremiah J. and Boyer, Kristy and Huggins-Manley, Anne and Weintrop, David and Wilson, Joseph},
advisor = {P., Anthony, Lisa and M, Gardner-McCune, Christina},
title = {Building Bridges: Dual-Modality Instruction and Introductory Programming Coursework},
year = {2020},
isbn = {9798728297147},
publisher = {University of Florida},
address = {USA},
abstract = {Blocks-based programming environments have become commonplace in introductory computing courses in K-12 schools and some college level courses. In comparison, most college-level introductory computer science courses teach students text-based languages which are more commonly used in industry and research. However, the literature provides evidence that students may face difficulty moving to text-based programming environments even when moving from blocks-based environments, and some perceive blocks-based environments as inauthentic. Bi-directional dual-modality programming environments, which provide multiple representations of programming language constructs (such as blocks and text) and allow students to transition between them freely, offer a potential solution to issues of authenticity and syntax challenges for novices and those with prior experience in blocks by making clear the connection between blocks and text representations of programs. While previous research has investigated transition from blocks-based to textual environments, there is limited research on dual-modality programming environments.The goal of my dissertation work is to identify how use of bi-directional dual-modality programming environments connects with learning in introductory programming instruction at the college level. I have developed a bi-directional dual-modality Java language plugin and evaluated the use of said tool within an introductory computer science (CS1) course. In my work I analyzed understanding and retention of specific computing / programming concepts, how any connections vary according to prior programming experience, and in what ways dual-modality programming environments affect the classroom learning experience.},
note = {AAI28087160}
}

@book{10.5555/2636689,
author = {Venit, Stewart and Drake, Elizabeth},
title = {Prelude to Programming},
year = {2014},
isbn = {013374163X},
publisher = {Addison-Wesley Professional},
edition = {6th},
abstract = {Prelude to Programming is appropriate for Pre-Programming and Introductory Programming courses in community colleges, 4-year colleges, and universities. No prior computer or programming experience is necessary although readers are expected to be familiar with college entry-level mathematics. Prelude to Programming provides beginning students with a language-independent framework for learning core programming concepts and effective design techniques. This approach gives students the foundation they need to understand the logic behind program design and to establish effective programming skills. The Sixth Edition offers students a lively and accessible presentation as they learn core programming concepts including data types, control structures, data files and arrays, and program design techniques such as top-down modular design and proper program documentation and style. Problem-solving skills are developed when students learn how to use basic programming tools and algorithms, which include data validation, defensive programming, calculating sums and averages, and searching and sorting lists. Teaching and Learning Experience This program presents a better teaching and learning experiencefor you and your students. It provides: A Language-Independent, Flexible Presentation: The text has been designed so that instructors can use it for students at various levels. Features that Help Solidify Concepts: Examples, exercises, and programming challenges help students understand how concepts in the text apply to real-life programs. Real Programming Experience with RAPTOR: Students gain first-hand programming experience through the optional use of RAPTOR, a free flowchart-based programming environment. Support Learning: Resources are available to expand on the topics presented in the text.}
}

@inproceedings{10.5555/648138.746794,
author = {Laforenza, Domenico},
title = {Programming High Performance Applications in Grid Environments},
year = {2001},
isbn = {3540426094},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The need for realistic simulations of complex systems relevant to the modeling of several modern technologies and environmental phenomena increasingly stimulates the development of advanced computing approaches. Nowadays it is possible to cluster or couple a wide variety of resources including supercomputers, storage systems, data sources, and special classes of devices distributed geographically and use them as a single unified resource, thus forming what is popularly known as a "computational grid" [1,2].Grid Computing enables the development of large scientific applications on an unprecedented scale. Grid-aware applications (meta-applications, multidisciplinary applications) make use of coupled computational resources that cannot be replicated at a single site. In this light, grids let scientists solve larger or new problems by pooling together resources that could not be coupled easily before. Designing and implementing grid-aware applications often require interdisciplinary collaborations involving aspects of scientific computing, visualization, and data management [3]. Multi-disciplinary applications are typically composed of large and complex components, and some of them are characterized by huge high performance requirements [4,5,6,7]. In order to get better performance, the challenge is to map each component onto the best candidate computational resource having a high degree of affinity with the software component. This kind of mapping is a non-trivial task. Moreover, it is well known that, in general, the programmer's productivity in designing and implementing efficient parallel applications on high performance computers remains a very time-consuming task. Grid computing makes the situation worse as heterogeneous computing environments are combined so that the programmer must manage an enormous amount of details. Consequently, the development of grid programming environments that would enable programmers to efficiently exploit this technology is an important and hot research issue. A grid programming environment should include interfaces, APIs, utilities and tools so as to provide a rich development environment. Common scientific languages such as C, C++, Java and Fortran should be available, as should application-level interfaces like MPI and PVM. A range of programming paradigms should be supported, such as message passing and distributed shared memory. In addition, a suite of numerical and other commonly used libraries should be available.Today, an interesting discussion is opened about the need to thinkat new abstract programming models and develop novel programming techniques addressing specifically the grid, which would deal with the heterogeneity and distributed computing aspects of grid programming [8].In this talk, after an introduction on the main grid programming issues, an overview of the most important approaches/projects conducted in this field worldwide will be presented. In particular, the speaker's contribution in designing some grid extension for a new programming environment will be shown. This workconstitutes a joint effort conducted by some academic and industrial Italian partners, in particular the Department of Computer Science of the Pisa University and CNUCE-CNR, in the frameworkof the ASI-PQE2000 National Project aimed at building ASSIST (A Software development System based on Integrated Skeleton Technology) [9,10,11]. The main target for the ASSIST Team is to build of a new programming environment for the development of high performance applications, based on the integration of the structured parallel programming model and the objects (components) model. In this way, ASSIST should be available for a wide range of hardware platforms from the homogeneous parallel computers (MPP, SMP, CoWs) to the heterogeneous ones (Grids).},
booktitle = {Proceedings of the 8th European PVM/MPI Users' Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface},
pages = {8–9},
numpages = {2}
}

@inproceedings{10.1145/3328778.3372699,
author = {Kong, Minji},
title = {Increasing Understanding of Students' Programming Process through Scratch Programming Event Data Analysis},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3372699},
doi = {10.1145/3328778.3372699},
abstract = {Computational thinking (CT) is a problem solving approach that is becoming prominent in educational settings. To help ease students into the world of programming, visual programming tools such as Scratch are used. Researchers are also developing various tools in an attempt to assess students' proficiency in thought processes that make up CT. However, current existing tools mainly utilize students' final code products when evaluating their demonstration of CT, losing their opportunity to understand students' actual learning and programming procedures. This work presents a logging methodology that records programming actions of students in the Scratch block coding environment to enable an in-depth analysis of students' programming process. With this logging methodology, we conducted a case study with introductory programmers to study how the logs of students' programming processes can provide insight into how they practice CT during programming.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1422},
numpages = {1},
keywords = {block-based-programming, computational-thinking, logging, novice-programmer-learning},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/3328778.3366865,
author = {Blanchard, Jeremiah and Gardner-McCune, Christina and Anthony, Lisa},
title = {Dual-Modality Instruction and Learning: A Case Study in CS1},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366865},
doi = {10.1145/3328778.3366865},
abstract = {In college-level introductory computer science courses, students traditionally learn to program using text-based languages which are common in industry and research. This approach means that learners must concurrently master both syntax and semantics. Blocks-based programming environments have become commonplace in introductory computing courses in K-12 schools and some colleges in part to simplify syntax challenges. However, there is evidence that students may face difficulty moving to text-based programming environments when starting with blocks-based environments. Bi-directional dual-modality programming environments provide multiple representations of programming language constructs (in both blocks and text) and allow students to transition between them freely. Prior work has shown that some students who use dual-modality environments to transition from blocks to text have more positive views of text programming compared to students who move directly from blocks to text languages, but it is not yet known if there is any impact on learning. To investigate the impact on learning, we conducted a study at a large public university across two semesters in a CS1 course (N=673). We found that students performed better on typical course exams when they were taught using dual-modality representations in lecture and were provided dual-modality tools. The results of our work support the conclusion that dual-modality instruction can help students learn computational concepts in early college computer science coursework.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {818–824},
numpages = {7},
keywords = {computer science education, cs1, blocks-based programming environments, dual-modality programming environments, novice programmers, programming languages},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/3290511.3290525,
author = {Nagashima, Kazuhei and Cho, Shinya and Horikoshi, Masayuki and Manabe, Hiroki and Kanemune, Susumu and Namiki, Mitaro},
title = {Design and development of bit arrow: a web-based programming learning environment},
year = {2018},
isbn = {9781450365178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290511.3290525},
doi = {10.1145/3290511.3290525},
abstract = {In Japan, all high school students will study programming in next teaching guidelines. It is important to use suitable environment or tool for programming education. Some current textbooks show programs written in JavaScript and instruct students to use text editor and browser as development environment. But such environment has some problems; it requires students to manage multiple applications, to find errors with little information and to type long statements. We developed "Bit Arrow", an online programming environment. The environment helps students to find errors. Also the environment provides API to write statements shortly. In this report, we describe design and evaluation of Bit Arrow from students' log data.},
booktitle = {Proceedings of the 10th International Conference on Education Technology and Computers},
pages = {85–91},
numpages = {7},
keywords = {Javascript, education support, programming education, web-based environment},
location = {Tokyo, Japan},
series = {ICETC '18}
}

@article{10.1002/cae.6180020105,
author = {Cheng, Harry H.},
title = {Pedagogically effective programming environment for teaching mechanism design},
year = {1994},
issue_date = {January 1994},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2},
number = {1},
issn = {1061-3773},
url = {https://doi.org/10.1002/cae.6180020105},
doi = {10.1002/cae.6180020105},
abstract = {The CH programming environment is an open system. Users can enhance the system through its various user interfaces. CH is specially designed for applications in mechanical systems engineering, although it is applicable to many other disciplines as well. CH has been successfully used as a teaching and learning tool for an undergraduate course, Computer-Aided Mechanism Design, at the University of California, Davis in Fall 1993. In this article we will present the CH programming environment and programming features developed for teaching and student learning. We will describe how a teaching toolbox is developed and used for teaching mechanism design. Source codes in the teaching toolbox are available to students so that they can study the software implementation of algorithms and modify the codes to solve similar problems. Although the developed teaching toolbox is specific for instruction on mechanism design, the CH programming environment and ideas presented in this article are general, and they are applicable to instructional improvement for a wide range of subjects in engineering.},
journal = {Comput. Appl. Eng. Educ.},
month = {jan},
pages = {23–39},
numpages = {17}
}

@inproceedings{10.1145/3568812.3603492,
author = {Tran, Caryn and O'Rourke, Eleanor},
title = {UUnderstanding Novices’ Perceptions of “Authentic” Programming},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603492},
doi = {10.1145/3568812.3603492},
abstract = {Authentic learning, characterized by engagement with real-world problems and tools, has long been of interest in education due to its impact on student motivation and learning outcomes [2, 7]. In computer science (CS) education, however, students and teachers face the challenge of balancing the desire to teach and learn "real" programming with the need for a gentle and scaffolded introduction to this highly abstract and cognitively demanding discipline [4]. As a tool-dependent discipline, the tension between authentic and scaffolded is particularly evident in the perceived in-authenticity of educational programming tools. While scaffolded blocks-based programming tools are approachable [14] and beneficial for learning [3, 10], they are often perceived as less authentic by high school students [4, 14], which can be demotivating. Conversely, "real" text-based programming, while authentic, can be difficult and intimidating, creating a barrier to learning and engagement [10, 14]. This dichotomy exemplifies a challenge in CS education: how can we provide an authentic learning experience through tools that are both approachable and representative of authentic programming practice? Addressing this challenge necessitates understanding what "authenticity" means in the context of CS education. Authenticity, a multi-dimensional and complex concept, encompasses dimensions of real-world relevance, disciplinary relevance, and personal relevance, each of which can be further decomposed [8, 9, 11, 12, 13]. Crucially, it is each individual student’s perception of authenticity, rather than an objective measure, that impacts their learning [2, 5]. While efforts have been made to create more authentic educational programming tools and curricula [1, 4, 6], these efforts adopt a top-down approach, with limited understanding of students’ rich, multi-faceted perceptions of authentic programming. Our study takes a bottom-up approach. We aim to first understand high school students’ perceptions of authentic programming. Our research questions for this study are: (1) What do students mean by “real programming”? (2) Do theories of authenticity accurately model students’ perception of authentic educational programming tools? (3) How do students assess existing educational programming tools’ authenticity and what affects that assessment? (4) How does identity, background, and experience affect perceptions of authenticity? We employ a mixed-methods approach, combining quantitative surveys with qualitative interviews. Informed by frameworks [8] and models [11] of authentic learning, we have designed a survey instrument to explore our research questions. Subsequent interviews will identify the characteristics of a given tool that lead to student perceptions of its authenticity, and probe how these perceptions affect student motivation to learn using said tool. This first stage of our research will enhance our understanding of the qualities of programming tools that affect students’ perception of authenticity. This understanding could lead to insights about the design of authentic learning tools and how to match students with educational programming tools that they find to be authentic. A later stage will leverage those insights to discover design techniques to create a sense of authenticity without sacrificing scaffolded learning. Overall, our research aims to contribute to an understanding of authentic learning in CS education and to develop theories and methods to design for perceived authenticity.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {37–38},
numpages = {2},
keywords = {authentic learning, high school, programming, student perception},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3328778.3366943,
author = {Lytle, Nicholas and Milliken, Alexandra and Catet\'{e}, Veronica and Barnes, Tiffany},
title = {Investigating Different Assignment Designs to Promote Collaboration in Block-Based Environments},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366943},
doi = {10.1145/3328778.3366943},
abstract = {Pair Programming is often employed in educational settings as a means of promoting collaboration and scaffolding the assignment difficulty for teams. While much research supports its inclusion as a pedagogical practice at the university level, some research has demonstrated in K-12 contexts, it can potentially lead to inequitable learning enviroments and create dynamics between partners that might negatively effect novice learners. New block-based programming environments like Netsblox have attempted to address this by creating ways for both partners to program simultaneously, but this feature has yet to be examined in detail. In this paper, we introduce several modes of Collaboration afforded by Netsblox. This includes Pair-Separate, Pair-Together, and Partner Puzzles - a mode that Splits the necessary blocks to build the assignment between team members. From an initial pilot study involving 25 pairs of middle and high school students, we find that most pairs preferred working on assignments in the Partner Puzzle mode as it presented a fun challenge to teams. We end on recommendations for building assignments using this methodology and future research directions investigating the role of collaboration in programming},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {832–838},
numpages = {7},
keywords = {block-based environments, collaboration, pair-programming},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/1595356.1595361,
author = {Kiesm\"{u}ller, Ulrich},
title = {Diagnosing learners' problem solving strategies using learning environments with algorithmic problems in secondary education},
year = {2008},
isbn = {9781605583853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595356.1595361},
doi = {10.1145/1595356.1595361},
abstract = {At schools special learning and programming environments are often used in the field of algorithm. Particularly with regard to informatics lessons in secondary education they should help novices to learn the basics of programming. In several parts of Germany (e. g. Bavaria) these fundamentals are even taught in the 7th grade, when pupils are 12 to 13 years old. Age-based designed learning and programming environments such as Karel, the robot and Kara, the programmable ladybug, are employed there, however learners still underachieve. One possible approach to improve both teaching and learning process is specifying the knowledge concerning the learners' individual problem solving strategies, when they create their solutions in consideration of the solution attempt's quality.A goal of the research project described here is being able to identify and categorise several problem solving strategies automatically. Due to this knowledge learning and programming environments can be improved which will optimise the informatics lessons, in which they are applied. Therefore the environments must be enhanced with special analytic and diagnostic modules, whose results can be given to the learner in the form of individualized system feedback messages in the future.In this text preliminary considerations are demonstrated. The research methodology as well as the design and the implementation of the research instruments are explained. We describe first studies, whose results are presented and discussed.},
booktitle = {Proceedings of the 8th International Conference on Computing Education Research},
pages = {16–24},
numpages = {9},
keywords = {Kara, algorithms, didactics of informatics, problem solving process, secondary computer science education, tool-based analysis},
location = {Koli, Finland},
series = {Koli '08}
}

@phdthesis{10.5555/125370,
author = {Vogeli, Christian Bruce},
title = {The applications of Hypercard in an educational environment},
year = {1991},
publisher = {Columbia University},
address = {USA},
abstract = {This study investigates the HyperCard programming environment from the perspectives of the educator, student and software developer. It is designed to provide a full-spectrum evaluation of HyperCard and its utility in education. First, the study attempts to determine if HyperCard is appropriate as an instructional language. Second, the study examines HyperCard's potential as a programming environment for teachers and students. Finally, this report provides educators with extensive HyperCard programming examples and lesson plans. Two courses in HyperCard programming were designed and taught by the investigator. One course was presented to in-service teachers, the other to high school students. Student's work was collected and evaluated for inclusion in the report. Three individuals from each course were selected as clinical study subjects. In-depth case studies chronicling each subject's experiences and development are included in the report. The investigator also undertook two educationally relevant programming projects with HyperCard. One project involved the development of an administrative scheduling package for a high school. The second project used HyperCard to develop a library control and query package for a medium-sized library. These projects were intentionally selected to challenge HyperCard's capabilities.Based upon the evaluations conducted, it appears that HyperCard can be mastered by teachers and students with little or no programming experience. Some of the in-service teachers with prior programming experience appeared to have difficulty adapting to certain aspects of the HyperCard environment. These individuals seemed reluctant to discard certain outdated programming habits and techniques. Students in the high school class achieved greater overall success in HyperCard programming. Information gleaned from the investigator's experiences developing the software projects indicates that HyperCard is well-suited for the development of complex programs for use in educational settings. HyperCard facilitated the inclusion of graphics, sounds and other sophisticated features in the investigator's programs.HyperCard seems to possess excellent potential for enriching education. HyperCard appears to be an effective medium for presenting introductory and advanced computer programming topics. HyperCard may also be used to create educational software which appropriately incorporates the capabilities of modern computing hardware. Further research is needed to evaluate HyperCard's multimedia-oriented capabilities and their applications in education.},
note = {UMI Order No. GAX91-21218}
}

@inproceedings{10.1145/2839509.2850528,
author = {Wong, Gary K.W. and Zhu, Kening and Ma, Xiaojuan and Huen, John},
title = {The Development of Internationalized Computational Thinking Curriculum in Hong Kong Primary Education (Abstract Only)},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2850528},
doi = {10.1145/2839509.2850528},
abstract = {Computational Thinking (CT) has been widely introduced and investigated in recent years, particularly in the U.S. since the born of visual, block-based, drag-drop programming environments such as Kodu, Scratch, Minecraft and App Inventor. Although the user interface is mainly in English, the characteristics of these easy-to-use, game-based, and interactive tools attract many teachers and researchers in the world to pay much attention to the possibilities and opportunities of introducing these tools to students. Recently, some primary school teachers in Hong Kong begin to independently introduce some of these programming tools to students at age 7 - 11 as a part of learning activities in their computer lessons. Their motives are similar but not the same, such as making a fun learning and teaching experience, motivating students for active and collaborative participation, and introducing CT concepts to develop generic skills (e.g. problem solving skills, creativity, and critical thinking). However, there is an absence of well-developed and planned curriculum for "coding education" to introduce computational thinking systematically to students in the local context with expected learning outcomes. Due to the uniqueness of K-12 curriculum in Hong Kong, the existing curriculum model in the U.S. may need to be customized and redesigned to become suitable for integrating into the curriculum in Hong Kong. In this poster, it describes the first proposed coding education curriculum in Hong Kong primary education (Primary 4 to Primary 6) with relevant objectives, structures, contents, and learning outcomes. A new pedagogical design framework for CT is introduced in this poster, which could be generalizable and yet to be evaluated. This new curriculum will serve as the curriculum guide to local teachers, and is the first research initiative of a three-year longitudinal study investigating the impact of CT activities to students particularly in Hong Kong. The experience of this curriculum development for CT concepts in K-12 education can inspire teachers and researchers in other parts of the world when adopting and internationalizing CT activities based on the curriculum model developed under the U.S. education.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {685},
numpages = {1},
keywords = {coding education, computational thinking, international outreach for cs education, k-12 curriculum development},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@proceedings{10.1145/1352678,
title = {ICDL '07: Proceedings of the 2007 international conference on Dynamic languages: in conjunction with the 15th International Smalltalk Joint Conference 2007},
year = {2007},
isbn = {9781605580845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Although Smalltalk is one of the oldest object-oriented programming languages, its conception and programming environment can still be considered as a design pearl and as a beacon in the realm of programming languages and programming environments. Other dynamic languages (such as Lisp, Scheme, Self,...) were similarly influential in expanding what software engineers can express with their programs.With the rising popularity of languages like Ruby, Python, Javascript, PHP, and with the growing challenges of aspect-orientation, pervasive computing, mobile code, and context-aware computing, dynamic languages are a worthy topic for further research. Therefore, ESUG decided to broaden the scope of the formerly "Smalltalk only" research track of its yearly meeting in order to enable cross-fertilization with research conducted using other dynamic languages. This way we hope to obtain more significant scientific results on various aspects of dynamicity in programming languages.This volume holds the papers that were presented during the 2007 edition of the conference which was held in Lugano, Switzerland. After careful reviewing by at least three reviewers we selected 11 papers out of 16 for inclusion in the conference. We took great care to avoid conflicts of interest by ensuring that reviewers did not have any formal connections to one of the authors of the papers they reviewed, namely (a) working in the same institution, university, or research group, (b) having written joint papers, (c) supervised earlier work, (d) had family ties, (e) or otherwise felt uncomfortable reviewing the work. The papers were reviewed using the typical academic standards: (a) present sound scientific work (a relevant problem, a convincing solution described in sufficient detail to allow replication, a sound validation, cite related work), (b) help the community (have something interesting to say to researchers working on dynamic programming languages in general and Smalltalk in particular), (c) reports something worthwhile for further reference (other researchers will cite this work in the future).},
location = {Lugano, Switzerland}
}

@inproceedings{10.1109/ICSE.2017.59,
author = {Barik, Titus and Smith, Justin and Lubick, Kevin and Holmes, Elisabeth and Feng, Jing and Murphy-Hill, Emerson and Parnin, Chris},
title = {Do developers read compiler error messages?},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.59},
doi = {10.1109/ICSE.2017.59},
abstract = {In integrated development environments, developers receive compiler error messages through a variety of textual and visual mechanisms, such as popups and wavy red underlines. Although error messages are the primary means of communicating defects to developers, researchers have a limited understanding on how developers actually use these messages to resolve defects. To understand how developers use error messages, we conducted an eye tracking study with 56 participants from undergraduate and graduate software engineering courses at our university. The participants attempted to resolve common, yet problematic defects in a Java code base within the Eclipse development environment. We found that: 1) participants read error messages and the difficulty of reading these messages is comparable to the difficulty of reading source code, 2) difficulty reading error messages significantly predicts participants' task performance, and 3) participants allocate a substantial portion of their total task to reading error messages (13%--25%). The results of our study offer empirical justification for the need to improve compiler error messages for developers.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {575–585},
numpages = {11},
keywords = {compiler errors, eye tracking, integrated development environments, programmer comprehension, reading, visual attention},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1145/1594399.1594402,
author = {Kiesm\"{u}ller, Ulrich},
title = {Diagnosing Learners’ Problem-Solving Strategies Using Learning Environments with Algorithmic Problems in Secondary Education},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/1594399.1594402},
doi = {10.1145/1594399.1594402},
abstract = {At schools special learning and programming environments are often used in the field of algorithms. Particularly with regard to computer science lessons in secondary education, they are supposed to help novices to learn the basics of programming. In several parts of Germany (e.g., Bavaria) these fundamentals are taught as early as in the seventh grade, when pupils are 12 to 13 years old. Designed age-based learning and programming environments such as Karel the robot and Kara, the programmable ladybug, are used, but learners still underachieve. One possible approach to improving both the teaching and the learning process is to specify the knowledge concerning the learners’ individual problem solving strategies, their solutions, and their respective quality.A goal of the research project described here is to design the learning environment so that it can identify and categorize several problem-solving strategies automatically. Based on this knowledge, learning and programming environments can be improved, which will optimize the computer science lessons in which they are applied. Therefore, the environments must be enhanced with special analytic and diagnostic modules, the results of which can be given to the learner in the form of individualized system feedback messages in the future.In this text preliminary considerations are demonstrated. The research methodology as well as the design and the implementation of the research instruments are explained. We describe first studies, whose results are presented and discussed.},
journal = {ACM Trans. Comput. Educ.},
month = {sep},
articleno = {17},
numpages = {26},
keywords = {Kara, Secondary computer science education, algorithms, didactics of informatics, problem solving process, tool-based analysis}
}

@inproceedings{10.1145/2897586.2897613,
author = {Ghorashi, Soroush and Jensen, Carlos},
title = {Jimbo: a collaborative IDE with live preview},
year = {2016},
isbn = {9781450341554},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897586.2897613},
doi = {10.1145/2897586.2897613},
abstract = {Team collaboration plays a key role in the success of any multi-user activity. Software engineering is a highly collaborative activity, where multiple developers and designers work together to solve a common problem. Meaningful and effective designer-developer collaboration improves the user experience, which can improve the chances of success for the project. Learning to program is another activity that can be implemented in a more collaborative way, students can learn in an active style by working with others. The growth of online classes, from small structured seminars to massive open online courses (MOOCs), and the isolation and impoverished learning experience some students report in these, points to an urgent need for tools that support remote pair programming in a distributed educational setting.In this paper, we describe Jimbo, a collaborative integrated development environment (IDE) that we believe is beneficial and effective in both aforementioned activities. Jimbo integrates many features that support better collaboration and communication between designers and developers, to bridge communication gaps and develop mutual understanding. These novel features can improve today's CS education by bringing students closer to each other and their instructors as well as training them to collaborate which is consistent with current practices in software engineering.},
booktitle = {Proceedings of the 9th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {104–107},
numpages = {4},
keywords = {IDE, Jimbo, collaboration, collaborative learning, communication, designer-developer collaboration, distance learning, live preview, pair programming, user awareness, web development},
location = {<conf-loc>, <city>Austin</city>, <state>Texas</state>, </conf-loc>},
series = {CHASE '16}
}

@inproceedings{10.1145/3478432.3499122,
author = {Yagubyan, Abel and Garcia, Dan},
title = {Seamless Embedding of Programming IDEs into Computer-Based Testing Software},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478432.3499122},
doi = {10.1145/3478432.3499122},
abstract = {Interest in computer-based assessment has increased in recent years, most certainly due to a shift to online learning due to the COVID pandemic. Instructors are creating questiongenerators for Computer Science classes on PrairieLearn (PL), an open-source platform developed at the University of Illinois at Urbana-Champaign PrairieLearn. The software generates differentvariants of each question to students through randomization. The challenge up to now has been that automatically graded coding problems in RISC-V or Snap!, some of the significant languages used in undergraduate Computer Science courses at our university, weren't possible to do within the software. Thequestion could be displayed, but then the student would have to load their favorite integrated development environment (IDE), code it up, and thenreturn to PL to upload their solution. This poster discusses our approach to embedding interactive development environments for Venus (RISC-V) and Snap! directly into PrairieLearn, so students never have to leave the browser tab!},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1168},
numpages = {1},
keywords = {computer-based testing, mastery learning},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@book{10.5555/3066196,
author = {Ibrahim, Dogan},
title = {BBC Micro: Bit 27 Projects for Students Level 1 - Simple Projects},
year = {2016},
isbn = {1526205092},
publisher = {Dogan Ibrahim},
edition = {1st},
abstract = {The BBC micro:bit is a credit sized computer based on a highly popular and high performance ARM processor. The device is designed by a group of 29 partners for use in computer education in the UK and will be given free of charge to every secondary school student in the UK. This book is about the use of the BBC micro:bit computer in practical projects. The BBC micro:bit computer can be programmed using several different programming languages, such as Microsoft Block Editor, Microsoft Touch Develop, mikroPython, and JavaScript. The easy and popular Block Editor programming tool is used in this book. This is a graphical programming tool which is ideal for the beginners such as the year 7 students in UK. The book covers more than 27 tested and working projects using the Block Editor programming. The following are given for each project: Title of the project Description of the project Aim of the project Difficulty level Program listing Results Try for yourself}
}

@article{10.1007/s10639-022-11445-2,
author = {Xu, Enwei and Wang, Wei and Wang, Qingxia},
title = {A meta-analysis of the effectiveness of programming teaching in promoting K-12 students’ computational thinking},
year = {2022},
issue_date = {Jun 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-022-11445-2},
doi = {10.1007/s10639-022-11445-2},
abstract = {Computational thinking is considered to be an important competence in the intelligent era, and the incorporation of computational thinking as an integral part of school education beginning in childhood has been proposed. However, the ways in which computational thinking can be taught more effectively the context of in K-12 programming teaching remain unclear. This paper reports the results of a meta-analysis of 28 empirical studies on K-12 programming teaching that were published in international education journals in the 21st century to determine which teaching methods and programming tools are most effective in promoting the computational thinking of K-12 students. The results show that (1) programming teaching can promote the improvement of K-12 students’ computational thinking (ES = 0.72, z = 9.9, P &lt; 0.01), with an overall effect at the upper-middle level (95% CI[0.60,0.83]); (2) scaffolding programming (ES = 1.84, z = 11.9, P &lt; 0.01) and problem-based programming (ES = 1.14, z = 5.57, P &lt; 0.01) are the most effective teaching methods and can significantly promote the development of K-12 students’ computational thinking (chi2 = 40.58, P &lt; 0.01); (3) since differences in the effect of programming tools between groups are not significant (Chi2 = 6.47, P = 0.09), it is impossible to determine which programming tools are most effective; and (4) intervention duration (ES = 0.72, z = 11.9, P &lt; 0.05, 95% CI[0.60, 0.83]) and learning scaffold (ES = 0.83, z = 6.27, P &lt; 0.05, 95% CI[0.57, 1.09]) are both key moderating variables that affect the improvement of computational thinking. Based on these results, suggestions are provided for future research and practice.},
journal = {Education and Information Technologies},
month = {nov},
pages = {6619–6644},
numpages = {26},
keywords = {Computational thinking, Programming tool, Teaching method, Effectiveness, Meta-analysis}
}

@article{10.5555/1089053.1089068,
author = {Chen, Zhixiong and Marx, Delia},
title = {Experiences with Eclipse IDE in programming courses},
year = {2005},
issue_date = {December 2005},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {21},
number = {2},
issn = {1937-4771},
abstract = {This paper reports our experiences using Eclipse for Java in our two-semester Java programming course sequence. Eclipse for Java is a fully featured professional Integrated Development Environment (IDE) with many advanced features. These features may overwhelm some of our students, especially those who have no programming background. In order to balance the use of an IDE that connects potential programmers with a professional community and our students' learning needs, we have conducted various experiments for the last two years. We found that for teaching-oriented colleges like ours, one effective way to reach that balance is to introduce Java concepts and syntax using the JDK directly for several weeks at first and then gradually present IDE features using Eclipse for Java for the rest of the course sequence. Students who have gone through this process were able to learn language syntax and problem solving skills in addition to understanding the value of an IDE, giving them a level of confidence and excitement for using a professional development tool.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {104–112},
numpages = {9}
}

@inproceedings{10.1145/3450613.3456833,
author = {Morshed Fahid, Fahmid and Tian, Xiaoyi and Emerson, Andrew and B. Wiggins, Joseph and Bounajim, Dolly and Smith, Andy and Wiebe, Eric and Mott, Bradford and Elizabeth Boyer, Kristy and Lester, James},
title = {Progression Trajectory-Based Student Modeling for Novice Block-Based Programming},
year = {2021},
isbn = {9781450383660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450613.3456833},
doi = {10.1145/3450613.3456833},
abstract = {Block-based programming environments are widely used in computer science education. However, these environments pose significant challenges for student modeling. Given a series of problem-solving actions taken by students in block-based programming environments, student models need to accurately infer problem-solving students’ programming abilities in real time to enable adaptive feedback and hints that are tailored to students’ abilities. While student models for block-based programming offer the potential to support student-adaptivity, creating student models for these environments is challenging because students can develop a broad range of solutions to a given programming activity. To address these challenges, we introduce a progression trajectory-based student modeling framework for modeling novice student block-based programming across multiple learning activities. Student trajectories utilize a time series representation that employs code analysis to incrementally compare student programs to expert solutions as students undertake block-based programming activities. This paper reports on a study in which progression trajectories were collected from more than 100 undergraduate students engaging in a series of block-based programming activities in an introductory computer science course. Using progression trajectory-based student modeling, we identified three distinct trajectory classes: Early Quitting, High Persistence, and Efficient Completion. Analysis of these trajectories revealed that they exhibit significantly different characteristics with respect to students’ actions and can be used to accurately predict students’ programming behaviors on future programming activities compared to competing baseline models. The findings suggest that progression trajectory-based student models can accurately model students’ block-based programming problem solving and hold potential for informing adaptive support in block-based programming environments.},
booktitle = {Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {189–200},
numpages = {12},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}

@inproceedings{10.5555/582910.786750,
author = {Wyeth, Peta and Purchase, Helen C.},
title = {Programming without a Computer: A New Interface for Children under Eight},
year = {2000},
isbn = {0769505155},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Electronic Blocks are a new programming interface, designed for children aged between three and eight years. The Electronic Blocks programming environment includes sensor blocks, action blocks and logic blocks. By connecting these blocks children can program structures that interact with the environment. The Electronic Block programming interface design is based on principles of developmentally appropriate practices in early childhood education. As a result the blocks provide young children with a programming environment that allows them to explore quite complex programming principles. The simple syntax of the blocks provides opportunities for young children unavailable through the use of traditional programming languages. The blocks allow children to create and use simple code structures. The Electronic Block environment provides a developmentally appropriate environment for planning overall strategies for solving a problem, breaking a strategy down into manageable units, and systematically determining the weakness of the solution. Electronic Blocks are the physical embodiment of computer programming. They have the unique dynamic and programmable properties of a computer minus its complexity.},
booktitle = {Proceedings of the First Australasian User Interface Conference},
pages = {141},
keywords = {Early Childhood Education, Electronic Building Blocks, Physical Programming},
series = {AUIC '00}
}

@inproceedings{10.1145/2676723.2691916,
author = {Trower, Jake and Gray, Jeff},
title = {Creating New Languages in Blockly: Two Case Studies in Media Computation and Robotics (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2691916},
doi = {10.1145/2676723.2691916},
abstract = {Introducing programming concepts to children early in their education can be beneficial because the type of problem solving that encompasses computational thinking is becoming increasingly relevant in our daily lives. A relatively new breed of programming environments has emerged to address this need. Visual programming languages (VPLs) allow programming logic to be represented with diagrams that illustrate its execution flow. Popular VPLs (e.g., Scratch, Snap, Alice, App Inventor) exist as full-featured, stand-alone programming environments with diagrammatic representations of the program instructions. This representation removes the syntactical barrier to entry that may exist with textual languages.Blockly, developed by Google, is a type of block language development kit that allows the rapid construction of new block-based visual programming languages to address a specific pedagogical or content focus. This poster provides a brief tutorial on the steps used to create a new Blockly environment, along with two case studies demonstrating the power of Blockly. The two environments introduced are focused on the manipulation of images via operations on pixels (Pixly), and programmatic control of a Sphero robot (Spherly). The construction and specific details of these two environments are described and illustrated with html, xml, and Javascript code examples and some of their potential applications. More information about Pixly and Spherly can be found at: http://outreach.cs.ua.edu/pixly and http://outreach.cs.ua.edu/spherly},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {677},
numpages = {1},
keywords = {blockly, media computation, robotics programming},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1007/978-3-319-43518-3_7,
author = {Tetsumura, Naoki and Oshima, Toru and Koyanagi, Ken'Ichi and Masuta, Hiroyuki and Motoyoshi, Tatsuo and Kawakami, Hiroshi},
title = {Interface Design Proposal of Card-Type Programming Tool},
year = {2016},
isbn = {9783319435176},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-43518-3_7},
doi = {10.1007/978-3-319-43518-3_7},
abstract = {We developed a card-type programming tool "Pro-Tan" as an education tool for beginners in programming. This tool was designed to be usable without any instruction on its operations. In the experiment, we did not tell the users how to use any of the tools. The subjects were instructed to create a program by themselves. We found problems of Pro-Tan's design in the experiment. In this paper, we report the system configuration of Pro-Tan and problem of Pro-Tan's design.},
booktitle = {Proceedings, Part II, of the 9th International Conference on Intelligent Robotics and Applications - Volume 9835},
pages = {71–77},
numpages = {7},
keywords = {Active learning, Beginners, Programming education tool, RFID system, Tangible interface},
location = {Tokyo, Japan},
series = {ICIRA 2016}
}

@inproceedings{10.1109/C5.2006.43,
author = {Fujioka, Takeshi and Takada, Hideyuki and Kita, Hajime},
title = {What Does Squeak Provide Students with?--- A Comparative Study of Squeak eToy and Excel VBA as Tools for Problem-Solving Learning in High School---},
year = {2006},
isbn = {0769525636},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/C5.2006.43},
doi = {10.1109/C5.2006.43},
abstract = {We have designed a Problem-based Learning (PBL) curriculum called ISEC-SeT, which is designed for information science education in high schools using computer programming as a tool for problem solving. We adopted Squeak eToy and Excel VBA as programming environments for the PBL, and have practiced ISEC-SeT at Horikawa High School in Kyoto, Japan, from October 2004 to September 2005. Evaluation by teachers and students focused on the students' presentations and essays on the projects shows that the students achieved problem solving abilities through the curriculum and that Squeak eToy provides them with a better environment for PBL than Excel VBA.},
booktitle = {Proceedings of the Fourth International Conference on Creating, Connecting and Collaborating through Computing},
pages = {42–49},
numpages = {8},
series = {C5 '06}
}

@inproceedings{10.1145/800037.800962,
author = {Meinke, John G. and Beidler, John A.},
title = {Alternatives to the traditional first course in computing},
year = {1981},
isbn = {0897910362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800037.800962},
doi = {10.1145/800037.800962},
abstract = {The first course in Computer Science at the University of Scranton has evolved over a number of years as a course in problem solving utilizing the computer. Bearing in mind that such a course should provide relatively standard programming tools, the course uses a structured derivative of FORTRAN promoting top-down stepwise refinement in programming methodology as well as encourages the utilization of “packaged programs”. We now have a course that provides a solid foundation for computer science majors as well as offers an excellent introduction to computing to those students for whom the course has a service purpose. This has been accomplished with a minimal faculty at a small university.},
booktitle = {Proceedings of the Twelfth SIGCSE Technical Symposium on Computer Science Education},
pages = {57–60},
numpages = {4},
keywords = {Algorithm development, Stepwise refinement, Structured programming, Top-down design},
location = {St. Louis, Missouri, USA},
series = {SIGCSE '81}
}

@inproceedings{10.1145/971300.971403,
author = {K\"{o}lling, Michael and Barnes, David J.},
title = {Enhancing apprentice-based learning of Java},
year = {2004},
isbn = {1581137982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/971300.971403},
doi = {10.1145/971300.971403},
abstract = {Various methods have been proposed in the past to improve student learning by introducing new styles of working with assignments. These include problem-based learning, use of case studies and apprenticeship. In most courses, however, these proposals have not resulted in a widespread significant change of teaching methods. Most institutions still use a traditional lecture/lab class approach with a strong separation of tasks between them. In part, this lack of change is a consequence of the lack of easily available and appropriate tools to support the introduction of new approaches into mainstream courses.In this paper, we consider and extend these ideas and propose an approach to teaching introductory programming in Java that integrates assignments and lectures, using elements of all three approaches mentioned above. In addition, we show how the BlueJ interactive programming environment [7] (a Java development environment aimed at education) can be used to provide the type of support that has hitherto hindered the widespread take-up of these approaches. We arrive at a teaching method that is motivating, effective and relatively easy to put into practice. Our discussion includes a concrete example of such an assignment, followed by a description of guidelines for the design of this style of teaching unit.},
booktitle = {Proceedings of the 35th SIGCSE Technical Symposium on Computer Science Education},
pages = {286–290},
numpages = {5},
keywords = {Java, objects-first, pedagogy},
location = {Norfolk, Virginia, USA},
series = {SIGCSE '04}
}

@inproceedings{10.1145/800104.803356,
author = {Tam, W. C. and Busenberg, S. N.},
title = {Practical experience in top-down structured software production in an academic setting},
year = {1977},
isbn = {9781450374071},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800104.803356},
doi = {10.1145/800104.803356},
abstract = {Much has been said about the importance of teaching top-down program design and structured programming in computer programming courses. However, instruction in these concepts has usually been limited to short homework assignments and at most to term projects. This type of experience is very different from the production programming environment encountered in industry, where the problems tackled are generally more complex and on a larger scale. Also, in many cases industrial programs are produced by a programming team under constraints in both time and resources. For students who aspire to a career in the software area, experience in a realistic production programming environment is desirable. Such experience is not provided in the traditional courses and novel ways have to be devised in order to bring it on campus.At Harvey Mudd College, an academic program, called the Mathematics Clinic, has been institured with the aim of providing this type of realistic experience to the student. The Mathematics Clinic brings problems from industry to be studied and solved by small teams of students under faculty supervision. The problems are selected for their educational value, but attention is paid to the feasibility of producing results that are of value to the sponsoring industrial concern.The general organization of the Clinic program has been described by Spanier (1). In the present paper, a production programming project undertaken by the Mathematics Clinic is described with emphasis placed on the mode of instruction and the experience gained by the students.},
booktitle = {Proceedings of the Seventh SIGCSE Technical Symposium on Computer Science Education},
pages = {31–36},
numpages = {6},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '77}
}

@inproceedings{10.1145/3572549.3572610,
author = {Liu, Hong and Hossain, Md Nour and Alnusair, Awny},
title = {Fostering Active Learning in Introductory Programming Courses by Utilizing Multiple Programming Tools and Enrichment Activities},
year = {2023},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572549.3572610},
doi = {10.1145/3572549.3572610},
abstract = {Teaching introductory computer programming for first-year college students has been challenging. The challenges stem from the fact that teachers often struggle to find the best pedagogical strategies that address the cognitive needs and learning styles of a diverse group of learners. This paper discusses the results of a systematic study we have conducted to assess the effectiveness of improving students’ learning through active engagement, participation, and commitment. In particular, the study combines the utilization of multiple programming tools and learning aids along with carefully designed collaborative problem-solving activities and lab-based projects. The results of the study show evidence that such a combination promotes active learning and sharpens students’ problem-solving skills.},
booktitle = {Proceedings of the 14th International Conference on Education Technology and Computers},
pages = {379–385},
numpages = {7},
keywords = {Active learning, Problem-based learning, Programing Tools},
location = {<conf-loc>, <city>Barcelona</city>, <country>Spain</country>, </conf-loc>},
series = {ICETC '22}
}

@inproceedings{10.1145/1822090.1822134,
author = {Friese, Stefan},
title = {Measuring of and reacting to learners' progress in logic programming courses},
year = {2010},
isbn = {9781605588209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1822090.1822134},
doi = {10.1145/1822090.1822134},
abstract = {To be able to support learners adequately in logic programming courses, it is crucial to know about their current level of competency during the learning process. Based on this, the course can be adapted and individual support can be given. In a traditional lecture the learning process is often a black box for the teacher, so there is no possibility to incorporate it. This paper describes how an architecture consisting of an online programming environment, private blogs and a reporting application can be used to solve the problem and shows how it was applied in the context of university lectures.},
booktitle = {Proceedings of the Fifteenth Annual Conference on Innovation and Technology in Computer Science Education},
pages = {152–154},
numpages = {3},
keywords = {learning process evaluation, programming language education, reflection},
location = {Bilkent, Ankara, Turkey},
series = {ITiCSE '10}
}

@article{10.1145/2677089,
author = {Reardon, Susan and Tangney, Brendan},
title = {Smartphones, Studio-Based Learning, and Scaffolding: Helping Novices Learn to Program},
year = {2014},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
url = {https://doi.org/10.1145/2677089},
doi = {10.1145/2677089},
abstract = {This article describes how smartphones, studio-based learning, and extensive scaffolding were used in combination in the teaching of a freshman Introduction to Programming course. To reduce cognitive overload, a phased approach was followed in introducing programming concepts and development environments, beginning with the visual programming environment Scratch and culminating with Java development for Android smartphones. Studio-based learning, a pedagogical approach long established in the fields of architecture and design education, was used as the basis for a collaborative social constructivist—and constructionist—approach to learning. Smartphones offered students the potential to develop applications for a context that is both immediate and clearly relevant to the ways in which they utilize and interact with technology.The research was carried out over three full academic years and included 53 student participants. An exploratory case study methodology was used to investigate the efficacy of the approach in helping to overcome the barriers faced by novice programmers. The findings indicate that the approach has merit. The students were motivated and engaged by the learning experience and were able to develop sophisticated applications that incorporated images, sound, arrays, and event handling. There is evidence that aspects of the studio-based learning approach, such as the scope that it gave students to innovate and the open feedback during student presentations, provided a learning environment that was motivating. Overall, the combination of smartphones, studio-based learning, and appropriate scaffolding offers an effective way to teach introductory programming courses.},
journal = {ACM Trans. Comput. Educ.},
month = {dec},
articleno = {23},
numpages = {15},
keywords = {Smartphones, contextualized learning}
}

@inproceedings{10.1109/ICASSP.2018.8461781,
author = {Jald\'{e}n, Joakim and Moreno, Xavier Casas and Skog, Isaac},
title = {Using the Arduino Due for Teaching Digital Signal Processing},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICASSP.2018.8461781},
doi = {10.1109/ICASSP.2018.8461781},
abstract = {This paper describes an Arduino Due based platform for digital signal processing (DSP) education. The platform consists of an in-house developed shield for robust interfacing with analog audio signals and user inputs, and an off-the-shelf Arduino Due that executes the students' DSP code. This combination enables direct use of the Arduino integrated development environment (IDE), with its low barrier to entry for students, its low maintenance need and cross platform interoperability, and its large user base. Relevant hardware and software features of the platform are discussed throughout, as are design choices made in relation to learning objectives, and the planned use of the platform in our own DSP course.},
booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {6468–6472},
numpages = {5},
location = {Calgary, AB, Canada}
}

@inproceedings{10.1145/3412382.3458780,
author = {Stein, Gordon and Jean, Devin and L\'{e}deczi, \'{A}kos},
title = {Distributed Virtual CPS Environment for K12: Demo Abstract},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3458780},
doi = {10.1145/3412382.3458780},
abstract = {While educational robotics and makerspaces are useful to modern STEM education, they introduce both physical and economic barriers to entry. By creating a simulated, networked environment, we can facilitate instruction on cyber-physical systems and related topics while reducing cost and complexity. The virtual environment created is connected to a block-based programming language, NetsBlox, to allow students to engage with the curriculum regardless of programming experience. The networked simulation and collaborative programming environment combine to become especially effective for distance learning. This demonstration showcases example scenarios providing students with a simple interface to interact with a simplified sensor network in a small area of a smart city and solve robot challenges.},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {394–395},
numpages = {2},
keywords = {computer science education, distance learning, educational robotics},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@inproceedings{10.5555/645549.659020,
author = {Draper, Bruce A. and Bins, Jos\'{e} and Baek, Kyungim},
title = {ADORE: Adaptive Object Recognition},
year = {1999},
isbn = {3540654593},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many modern computer vision systems are built by chaining together standard vision procedures, often in graphical programming environments such as Khoros, CVIPtools or IUE. Typically, these procedures are selected and sequenced by an ad-hoc combination of programmer's intuition and trial-and-error. This paper presents a theoretically sound method for constructing object recognition strategies by casting object recognition as a Markov Decision Problem (MDP). The result is a system called ADORE (Adaptive Object Recognition) that automatically learns object recognition control policies from training data. Experimental results are presented in which ADORE is trained to recognize five types of houses in aerial images, and where its performance can be (and is) compared to optimal.},
booktitle = {Proceedings of the First International Conference on Computer Vision Systems},
pages = {522–537},
numpages = {16},
series = {ICVS '99}
}

@book{10.5555/1796091,
author = {Bischof, C. and Bischof, C. and Bucker, M. and Gibbon, P. and Joubert, G. and Lippert, T.},
title = {Parallel Computing: Architectures, Algorithms and Applications - Volume 15 Advances in Parallel Computing},
year = {2008},
isbn = {158603796X},
publisher = {IOS Press},
address = {NLD},
abstract = {ParCo2007 marks a quarter of a century of the international conferences on parallel computing that started in Berlin in 1983. The aim of the conference is to give an overview of the state-of-the-art of the developments, applications and future trends in high performance computing for all platforms. The conference addresses all aspects of parallel computing, including applications, hardware and software technologies as well as languages and development environments. Special emphasis was placed on the role of high performance processing to solve real-life problems in all areas, including scientific, engineering and multidisciplinary applications and strategies, experiences and conclusions made with respect to parallel computing. The book contains papers covering: 1) Applications; The application of parallel computers to solve computationally challenging problems in the physical and life sciences, engineering, industry and commerce. The treatment of complex multidisciplinary problems occurring in all application areas was discussed. 2) Algorithms; Design, analysis and implementation of generic parallel algorithms, including their scalability, in particular to a large number of processors (MPP), portability and adaptability and 3) Software and Architectures; Software engineering for developing and maintaining parallel software, including parallel programming models and paradigms, development environments, compile-time and run-time tools. A number of symposia on specialized topics formed part of the scientific program. The following topics were covered: Parallel Computing with FPGAs, The Future of OpenMP in the Multi-Core Era, Scalability and Usability of HPC Programming Tools, DEISA: Extreme Computing in an Advanced Supercomputing Environment and Scaling Science Applications on Blue Gene. The conference was organized by the renowned research and teaching institutions Forschungszentrum Julich and the RWTH Aachen University in Germany.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences}
}

@book{10.5555/2857306,
author = {Gander, Walter},
title = {Learning MATLAB: A Problem Solving Approach},
year = {2015},
isbn = {3319253263},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This comprehensive and stimulating introduction to Matlab, a computer language now widely used for technical computing, is based on an introductory course held at Qian Weichang College, Shanghai University, in the fall of 2014. Teaching and learning a substantial programming language arent always straightforward tasks. Accordingly, this textbook is not meant to cover the whole range of this high-performance technical programming environment, but to motivate first- and second-year undergraduate students in mathematics and computer science to learn Matlab by studying representative problems, developing algorithms and programming them in Matlab. While several topics are taken from the field of scientific computing, the main emphasis is on programming. A wealth of examples are completely discussed and solved, allowing students to learn Matlab by doing: by solving problems, comparing approaches and assessing the proposed solutions.}
}

@article{10.5555/827120.827126,
author = {Eronen, Pasi J. and Sutinen, Erkki and Vesisenaho, Mikko and Virnes, Marjo},
title = {Kids' club as an ICT-based learning laboratory},
year = {2002},
issue_date = {2002},
publisher = {Institute of Mathematics and Informatics},
address = {LTU},
volume = {1},
number = {1},
issn = {1648-5831},
abstract = {Kids' Club is a research laboratory where school children of age 10 to 14, in collaboration with university students and researchers, apply and create novel information and communication technologies (ICT) for learning. The technical environment includes visualization and concretization tools, such as a visual programming environment, control technologies, and programmable bricks. As of pedagogical models, the laboratory makes use of problem-based learning (PBL), creative problem solving, and group processes Preliminary results show that the environment provides a promising platform for developing educational technologies by getting immediate and constructive feedback from potential users. In addition, visual and particularly concretizing tools offer an attractive learning environment for learning abstract skills, like programming.},
journal = {Informatics in Education},
month = {sep},
pages = {61–72},
numpages = {12},
keywords = {ICT in education, concretizing tools, constructionism, kids' club, problem-based learning, technology education}
}

@inproceedings{10.1145/236452.236537,
author = {K\"{o}lling, Michael and Rosenberg, John},
title = {Blue—a language for teaching object-oriented programming},
year = {1996},
isbn = {089791757X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/236452.236537},
doi = {10.1145/236452.236537},
abstract = {Teaching object-oriented programming has clearly become an important part of computer science education. We agree with many others that the best place to teach it is in the CS1 introductory course. Many problems with this have been reported in the literature. These mainly result from inadequate languages and environments. Blue is a new language and integrated programming environment, currently under development explicitly for object-oriented teaching. We expect clear advantages from the use of Blue for first year teaching compared to using other available languages. This paper describes the design principles on which the language was based and the most important aspects of the language itself.},
booktitle = {Proceedings of the Twenty-Seventh SIGCSE Technical Symposium on Computer Science Education},
pages = {190–194},
numpages = {5},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGCSE '96}
}

@inproceedings{10.5555/1620113.1620136,
author = {Yakir, Ari and Kaminka, Gal},
title = {An integrated development environment and architecture for soar-based agents},
year = {2007},
isbn = {9781577353232},
publisher = {AAAI Press},
abstract = {It is well known how challenging is the task of coding complex agents for virtual environments. This difficulty in developing and maintaining complex agents has been plaguing commercial applications of advanced agent technology in virtual environments. In this paper, we discuss development of a commercial-grade integrated development environment (IDE) and agent architecture for simulation and training in a high-fidelity virtual environment. Specifically, we focus on two key areas of contribution. First, we discuss the addition of an explicit recipe mechanism to Soar, allowing reflection. Second, we discuss the development and usage of an IDE for building agents using our architecture; the approach we take is to tightly-couple the IDE to the architecture. The result is a complete development and deployment environment for agents situated in a complex dynamic virtual world.},
booktitle = {Proceedings of the 19th National Conference on Innovative Applications of Artificial Intelligence - Volume 2},
pages = {1826–1832},
numpages = {7},
location = {Vancouver, British Columbia, Canada},
series = {IAAI'07}
}

@article{10.1007/s10639-021-10811-w,
author = {Humble, Niklas},
title = {Teacher observations of programming affordances for K-12 mathematics and technology},
year = {2022},
issue_date = {May 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {4},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-021-10811-w},
doi = {10.1007/s10639-021-10811-w},
abstract = {With future shortage of professionals with programming and computing skills, many countries have made programming part of kindergarten – grade 12 curriculum (K-12). A possible approach is to make programming part of an already existing subject. Sweden has chosen this approach and in 2017 programming was integrated in the subject content of K-12 mathematics and technology. Integrating programming is at the expense of extra workload on teachers. Teachers affected by these changes will face new challenges in their teaching and learning activities. The aim of the study is to examine K-12 teachers’ use and perceived affordances of programming as a tool for teaching and learning activities in mathematics and technology. Data were collected through focus group discussions with three teacher teams in mathematics and technology from three K-12 schools in the mid Sweden region. 21 teachers participated in the study. Thematic analysis with a mixture of deductive and inductive coding were used to analyse the data. Theory of affordances was used to structure findings in themes of interests and answer the study’s aim and research questions. Results show that the teachers use a variety of programming tools in their teaching and learning activities. The use of programming in mathematics and technology can be understood in five main perceived affordances: 1) Play, 2) Discovery, 3) Adaptation, 4) Control, and 5) Freedom; which relate to both student motivation and subject content. Teachers also perceive obstacles and opportunities in using programming, that relates to different programming tools’ ability to support teaching and learning activities. The findings of this study can be drawn upon by teachers and other stakeholders in the integration of programming in K-12 education, and in the design of teaching and learning activities with programming.},
journal = {Education and Information Technologies},
month = {may},
pages = {4887–4904},
numpages = {18},
keywords = {Programming, Teaching and learning, K-12 education, Affordances, Teacher perspective}
}

@inproceedings{10.1145/3162087.3162092,
author = {G\"{u}lbahar, Yasemin and Kalelio\u{g}lu, Filiz},
title = {Competencies of High School Teachers and Training Needs for Computer Science Education},
year = {2017},
isbn = {9781450363389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3162087.3162092},
doi = {10.1145/3162087.3162092},
abstract = {The computer science discipline is evolving with problems in both technological and pedagogical aspects almost worldwide. With the advent of new technologies and approaches for teaching programming at all ages, many countries including Turkey have revised their computer science curriculum. These revisions have resulted in serious training needs being highlighted for teachers with inadequate competencies to meet the expected learning outcomes. Hence, the purpose of this study was to explore; (a) the self-perceived competencies of teachers about the topics in the curriculum, (b) perceptions about programming, programming tools and approaches, and (c) contribution of university education to their teaching profession. The findings revealed that most teachers believe they are not sufficiently competent to be an effective computer science teacher. Related to this finding, most of them especially mentioned their training needs for programming, emerging tools and technologies. Plus more than half of the participants think that the higher education curriculum is inadequate to meet teacher expectations and to create competent teachers.},
booktitle = {Proceedings of the 6th Computer Science Education Research Conference},
pages = {26–31},
numpages = {6},
keywords = {Computer science teacher competencies, computer science teacher training},
location = {Helsinki, Finland},
series = {CSERC '17}
}

@inproceedings{10.2312/eged.20171024,
author = {Toisoul, Antoine and Rueckert, Daniel and Kainz, Bernhard},
title = {Accessible GLSL shader programming},
year = {2017},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.2312/eged.20171024},
doi = {10.2312/eged.20171024},
abstract = {Teaching fundamental principles of Computer Graphics requires a thoroughly prepared lecture alongside practical training. Modern graphics programming rarely provides a straightforward application programming interface (API) and the available APIs pose high entry barriers to students. Shader-based programming of standard graphics pipelines is often inaccessible through complex setup procedures and convoluted programming environments. In this paper we discuss an undergraduate entry level lecture with its according lab exercises. We present a programming framework that makes interactive graphics programming accessible while allowing to design individual tasks as instructive exercises to solidify the content of individual lecture units. The discussed teaching framework provides a well defined programmable graphics pipeline with geometry shading stages and image-based post processing functionality based on framebuffer objects. It is open-source and available online.},
booktitle = {Proceedings of the European Association for Computer Graphics: Education Papers},
pages = {35–42},
numpages = {8},
location = {Lyon, France},
series = {EG '17}
}

@article{10.5555/2835377.2835394,
author = {Eckroth, Joshua},
title = {Foundations of a cross-disciplinary pedagogy for big data},
year = {2016},
issue_date = {January 2016},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {31},
number = {3},
issn = {1937-4771},
abstract = {The increasing awareness of "big data" is transforming the academic and business landscape across many disciplines. Yet, big data programming environments are still too complex for non-programmers to utilize. To our knowledge, only computer scientists are ever exposed to big data processing concepts and tools in undergraduate education. Furthermore, non-computer scientists may lack sufficient common ground with computer scientists to explain their specialized big data processing needs. In order to bridge this gap and enhance collaboration among persons with big data processing needs and persons who are trained in programming and system building, we propose the foundations of a cross-disciplinary pedagogy that exposes big data processing paradigms and design decisions at an abstract level. With these tools, students and experts from different disciplines can more effectively collaborate on solving big data problems.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {110–118},
numpages = {9}
}

@inproceedings{10.1145/2063348.2063374,
author = {Prabhu, Prakash and Jablin, Thomas B. and Raman, Arun and Zhang, Yun and Huang, Jialu and Kim, Hanjun and Johnson, Nick P. and Liu, Feng and Ghosh, Soumyadeep and Beard, Stephen and Oh, Taewook and Zoufaly, Matthew and Walker, David and August, David I.},
title = {A survey of the practice of computational science},
year = {2011},
isbn = {9781450311397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063348.2063374},
doi = {10.1145/2063348.2063374},
abstract = {Computing plays an indispensable role in scientific research. Presently, researchers in science have different problems, needs, and beliefs about computation than professional programmers. In order to accelerate the progress of science, computer scientists must understand these problems, needs, and beliefs. To this end, this paper presents a survey of scientists from diverse disciplines, practicing computational science at a doctoral-granting university with very high research activity. The survey covers many things, among them, prevalent programming practices within this scientific community, the importance of computational power in different fields, use of tools to enhance performance and software productivity, computational resources leveraged, and prevalence of parallel computation. The results reveal several patterns that suggest interesting avenues to bridge the gap between scientific researchers and programming tools developers.},
booktitle = {State of the Practice Reports},
articleno = {19},
numpages = {12},
location = {Seattle, Washington},
series = {SC '11}
}

@inproceedings{10.1145/2616498.2616568,
author = {Feldhausen, Russell and Bell, Scott and Andresen, Daniel},
title = {Minimum Time, Maximum Effect: Introducing Parallel Computing in CS0 and STEM Outreach Activities Using Scratch},
year = {2014},
isbn = {9781450328937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2616498.2616568},
doi = {10.1145/2616498.2616568},
abstract = {This paper presents our experiences and outcomes using Scratch to teach parallel computing concepts to students just learning about computer science. We presented versions of this material to middle school and high school girls during a STEM workshop and then to undergraduate university students enrolled in an introductory computer science course. Using the Scratch development environment, students are able to build, modify and observe the changes in the performance of applications which utilize multi-threaded, concurrent, operations. This includes scenarios which involve more advanced topics such as race conditions and mutex locks.Developing these materials has allowed us to introduce these concepts in a programming environment much earlier than we have previously, giving instructors in down-stream courses the ability to build upon this early exposure. Survey results show that this approach resulted in a significant increase in both of these areas. For example, the number of students in our CS0 course who felt they could apply parallel programming to other problems using Scratch more than doubled, rising from 25 to 55 (out of 61 students that responded to both surveys). Likewise, the number of students who felt they understood what parallel programming means rose from 27 to 56. These results were achieved after just one class period. Similarly, 27 of the 37 girls responding to the workshop survey felt that they were capable of learning to write computer programs and 22 of 41 indicated they had an interest in a job using HPC to solve problems.},
booktitle = {Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment},
articleno = {75},
numpages = {7},
location = {Atlanta, GA, USA},
series = {XSEDE '14}
}

@article{10.1287/isre.8.1.25,
author = {Kim, Jinwoo and Lerch, F. Javier},
title = {Why Is Programming Sometimes So Difficult? Programming as Scientific Discovery in Multiple Problem Spaces},
year = {1997},
issue_date = {March 1997},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {8},
number = {1},
issn = {1526-5536},
url = {https://doi.org/10.1287/isre.8.1.25},
doi = {10.1287/isre.8.1.25},
abstract = {Our theoretical framework views programming as search in three problem spaces: rule, instance, and representation. The main objectives of this study are to find out how programmers change representation while working in multiple problem spaces, and how representation change increases the difficulty of programming tasks. Our theory of programming indicates that programming is similar to the way scientists discover and test theories. That is, programmers generate hypotheses in the rule space and test these hypotheses in the instance space. Moreover, programmers change their representations in the representation space when rule development becomes too difficult or alternative representations are available. We conducted three empirical studies with different programming tasks: writing a new program, understanding an existing program, and reusing an old program. Our results indicate that considerable cognitive difficulties stem from the need to change representations in these tasks. We conclude by discussing the implications of viewing programming as a scientific discovery for the design of programming environments and training methods.},
journal = {Info. Sys. Research},
month = {mar},
pages = {25–50},
numpages = {26},
keywords = {empirical studies of programmers, multiple problem spaces, object-oriented programming, scientific discovery}
}

@article{10.2478/v10065-009-0017-9,
author = {Kluszczy\'{n}Ski, Rafa\l{} and Mikulski, \L{}Ukasz and Nowicki, Marek and Ba\l{}A, Piotr},
title = {Contests Hosting Service as a tool to teach programming},
year = {2009},
issue_date = {Number 1 / 2009},
publisher = {Versita},
address = {Warsaw, POL},
volume = {9},
number = {1},
issn = {1732-1360},
url = {https://doi.org/10.2478/v10065-009-0017-9},
doi = {10.2478/v10065-009-0017-9},
abstract = {Computer science would not exist without the concept of algorithm. Therefore design of algorithms plays an important role in education while implementation is usually considered to be straightforward. Increasing variety of programming languages, wealth of possible constructions, programming environments and tools makes programming difficult for the beginners.Apart from the idea of problem solution, it is important to teach programming skills. Size of classes of 10-20 pupils and a limited number of lessons and their short time are the major problem. The teacher has to check solution of every pupil, compile it and run tests. This is definitely a time-consuming process which makes teaching difficult. In this paper the authors present the use of problem solutions validation systems during classes. With the help of such a system called ZawodyWEB, the authors teach algorithms and programming for the secondary school students.},
journal = {Ann. UMCS, Inf.},
month = {jan},
pages = {213–224},
numpages = {12}
}

@inproceedings{10.1145/29650.29662,
author = {Chase, B. B. and Hood, R. T.},
title = {Selective interpretation as a technique for debugging computationally intensive programs},
year = {1987},
isbn = {0897912357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/29650.29662},
doi = {10.1145/29650.29662},
abstract = {As part of Rice University's project to build a programming environment for scientific software, we have built a facility for program execution that solves some of the problems inherent in debugging large, computationally intensive programs. By their very nature such programs do not lend themselves to full-scale interpretation. In moderation however, interpretation can be extremely useful during the debugging process. In addition to discussing the particular benefits that we expect from interpretation, this paper addresses how interpretive techniques can be effectively used in conjunction with the execution of compiled code. The same implementation technique that permits interpretation to be incorporated as part of execution will also permit the execution facility to be used for debugging parallel programs running on a remote machine.},
booktitle = {Papers of the Symposium on Interpreters and Interpretive Techniques},
pages = {113–124},
numpages = {12},
location = {St. Paul, Minnesota, USA},
series = {SIGPLAN '87}
}

@article{10.1016/j.compedu.2006.04.005,
author = {Vel\'{a}zquez-Iturbide, J. \'{A}ngel and Pareja-Flores, Crist\'{o}bal and Urquiza-Fuentes, Jaime},
title = {An approach to effortless construction of program animations},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {50},
number = {1},
issn = {0360-1315},
url = {https://doi.org/10.1016/j.compedu.2006.04.005},
doi = {10.1016/j.compedu.2006.04.005},
abstract = {Program animation systems have not been as widely adopted by computer science educators as we might expect from the firm belief that they can help in enhancing computer science education. One of the most notable obstacles to their adoption is the considerable effort that the production of program animations represents for the instructor. We present here an approach to reduce such a workload based on the automatic generation of visualizations and animations. The user may customize them in a user-friendly way to construct more expressive program animations. These operations are carried out by means of a user-friendly manipulation based on the metaphor of office documents. We have applied this approach to the functional paradigm by extending the WinHIPE programming environment. Finally, we report on the successful results of an evaluation performed to measure its ease of use.},
journal = {Comput. Educ.},
month = {jan},
pages = {179–192},
numpages = {14},
keywords = {Authoring tools and methods, Evaluation of CAL systems, Improving classroom teaching, Interactive learning environments, Programming and programming languages}
}

@inbook{10.5555/949921.949938,
author = {Pareja-Flores, C. and Vel\'{a}zquez-Iturbide, J. \'{A}.},
title = {Program execution and visualization on the web},
year = {2003},
isbn = {159140102X},
publisher = {IGI Global},
address = {USA},
abstract = {Programming is a demanding task with an education program that requires the assistance of complex tools such as programming environments, algorithm animators, problem graders, etc. In this chapter, we give a comprehensive presentation of tools for program execution and visualization on the Web. We summarize the technical evolution of these tools, describe educational uses, report lessons learned, and look at formal evaluations of their educational effectiveness. We also deal with a closely related matter, namely, collections of Web documents containing programming exercises. Finally, we outline our view of future trends in the use of the Web for programming education, and we give our personal conclusions. This chapter is of interest to educators and researchers, because it gives a comprehensive presentation of the main issues and results of a field where most of the contributions are sparse in the literature.},
booktitle = {Web-Based Education: Learning from Experience},
pages = {236–259},
numpages = {24}
}

@inproceedings{10.1145/1822090.1822167,
author = {Kiesmueller, Ulrich and Sossalla, Sebastian and Brinda, Torsten and Riedhammer, Korbinian},
title = {Online identification of learner problem solving strategies using pattern recognition methods},
year = {2010},
isbn = {9781605588209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1822090.1822167},
doi = {10.1145/1822090.1822167},
abstract = {Learning and programming environments used in computer science education give feedback to the users by system messages. These are triggered by programming errors and give only "technical" hints without regard to the learners' problem solving process. To adapt the messages not only to the factual but also to the procedural knowledge of the learners, their problem solving strategies have to be identified automatically and in process. This article describes a way to achieve this with the help of pattern recognition methods. Using data from a study with 65 learners aged 12 to 13 using a learning environment for programming, a classification system based on hidden Markov models is trained and integrated in the very same environment. We discuss findings in that data and the performance of the automatic online identification, and present first results using the developed software in class.},
booktitle = {Proceedings of the Fifteenth Annual Conference on Innovation and Technology in Computer Science Education},
pages = {274–278},
numpages = {5},
keywords = {algorithms, computer science education, pattern recognition, problem solving strategies, secondary education, tool-based analysis},
location = {Bilkent, Ankara, Turkey},
series = {ITiCSE '10}
}

@inproceedings{10.1109/ACIT-CSI.2015.73,
author = {Kato, Yoshitaka and Ozaki, Masaya and Kani, Jun'ya and Ito, Nobuhiro and Kawabe, Yoshinobu},
title = {Developing Compiler for Nihongo Programming Language PEN},
year = {2015},
isbn = {9781467396424},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACIT-CSI.2015.73},
doi = {10.1109/ACIT-CSI.2015.73},
abstract = {When a novice programmer starts learning computer programming, C or Java are often employed. They are general purpose and useful languages, but for Japanese novice programmers these programming languages are difficult. One reason for this difficulty is English keywords employed in these programming languages, that is, these words are not in Japanese learner's native language. Recently, for Japanese novice programmers, "Nihongo-based" programming languages have been developed in Japan. PEN is a Nihongo-based programming language for education. A programming environment has been developed for PEN, and the environment has an interpreter. However, there has not been developed a compiler for PEN. In this paper, we aim to develop a compiler for PEN language, actually, we develop a translator from PEN into C, and we finally obtain a binary code with a C compiler such as gcc.},
booktitle = {Proceedings of the 2015 3rd International Conference on Applied Computing and Information Technology/2nd International Conference on Computational Science and Intelligence},
pages = {387–392},
numpages = {6},
series = {ACIT-CSI '15}
}

@inproceedings{10.1145/3397537.3397561,
author = {Santos, Andr\'{e} L.},
title = {Javardise: a structured code editor for programming pedagogy in Java},
year = {2020},
isbn = {9781450375078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397537.3397561},
doi = {10.1145/3397537.3397561},
abstract = {The syntax of a programming language is the textual form - that conforms to a grammar - to express instructions of a programming model. The key idea of structured code editors is to constrain editing to syntactically valid program code, that is, the modifications ensure that the source code always conforms to the grammar. Syntax is considered an entry barrier when learning how to program. In this work we rehash the concept of structured code editors targeting programming education. We present Javardise, a structured editor for a subset of the Java language, and discuss its features in the light of programming pedagogy.},
booktitle = {Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming},
pages = {120–125},
numpages = {6},
keywords = {Java, Structured editors, programming pedagogy},
location = {Porto, Portugal},
series = {Programming '20}
}

@article{10.1016/j.neucom.2022.11.056,
author = {Komorniczak, Joanna and Ksieniewicz, Pawe\l{}},
title = {         problexity—An open-source Python library for supervised learning problem complexity assessment},
year = {2023},
issue_date = {Feb 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {521},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2022.11.056},
doi = {10.1016/j.neucom.2022.11.056},
journal = {Neurocomput.},
month = {feb},
pages = {126–136},
numpages = {11},
keywords = {Problem complexity, Classification, Regression, Python}
}

@inproceedings{10.1145/3027063.3027129,
author = {Melcer, Edward},
title = {Moving to Learn: Exploring the Impact of Physical Embodiment in Educational Programming Games},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3027129},
doi = {10.1145/3027063.3027129},
abstract = {There has been increasing attention paid to the necessity of Computational Thinking (CT) and CS education in recent years. To address this need, a broad spectrum of animation programming environments and games have been created to engage learners. However, most of these tools are designed for the touchpad/mouse and keyboard, and few have been evaluated to assess their efficacy in developing CT/programming skills. This is problematic when trying to understand the validity of such designs for CS education, and whether there are alternative approaches that may prove more effective. My dissertation work helps address this problem. After creating a framework based on a meta-review that carefully dissects embodiment strategies in learning games, I am building and evaluating tangible and augmented reality versions of a CT game. I plan to examine how these different forms of physical interaction help to facilitate and enhance meaning-making during the learning process, and whether/how they improve related learning factors such as self-belief and enjoyment.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {301–306},
numpages = {6},
keywords = {computational thinking, educational games, embodied cognition, embodied interaction, physical embodiment, programming},
location = {<conf-loc>, <city>Denver</city>, <state>Colorado</state>, <country>USA</country>, </conf-loc>},
series = {CHI EA '17}
}

@inproceedings{10.1145/2593801.2593809,
author = {Landh\"{a}u\ss{}er, Mathias and Hey, Tobias and Tichy, Walter F.},
title = {Deriving time lines from texts},
year = {2014},
isbn = {9781450328463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593801.2593809},
doi = {10.1145/2593801.2593809},
abstract = {We investigate natural language as an alternative to programming languages. Natural language would empower anyone to program with minimal training. In this paper, we solve an ordering problem that arises in natural-language programming. An emprical study showed that users do not always provide the strict sequential order of steps needed for execution on a computer. Instead, temporal expressions involving "before", "after", "while", "at the end", and others are used to indicate an order other than the textual one. We present an analysis that extracts the intended time line by exploiting temporal clues. The technique is analyzed in the context of Alice, a 3D programming environment, and AliceNLP, a system for programming Alice in ordinary English. Extracting temporal order could also be useful for analyzing reports, question answering, help desk requests, and big data applications.},
booktitle = {Proceedings of the 3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {45–51},
numpages = {7},
keywords = {Alice, Natural language processing, end-user programming, programming with natural language, temporal expressions, temporal reasoning, time lines},
location = {Hyderabad, India},
series = {RAISE 2014}
}

@inproceedings{10.1145/3159450.3162177,
author = {Shaffer, Clifford A. and Brusilovsky, Peter and Koedinger, Kenneth R. and Edwards, Stephen H.},
title = {CS Education Infrastructure for All: Interoperability for Tools and Data Analytics (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162177},
doi = {10.1145/3159450.3162177},
abstract = {CS Education makes heavy use of online educational tools like IDEs, Learning Management Systems, eTextbooks, interactive programming environments, and other smart content. Instructors and students would benefit from greater interoperability between tools. CS Ed researchers increasingly make use of the large collections of data generated by click streams coming from them. However, we all face barriers that slow progress: (1) Educational tools do not integrate well. (2) Information about CS learning process and outcome data generated by one system is not compatible with that from other systems. (3) CS problem solving and learning (e.g., coding solutions) is different from the type of data (discrete answers to questions or verbal responses) that current educational data mining focuses on. This BOF will discuss ways that we might support and better coordinate efforts to build community and capacity among CS Ed researchers, data scientists, and learning scientists toward reducing these barriers. CS Ed infrastructure should support broader re-use of innovative learning content that is instrumented for rich data collection, formats and tools for analysis of learner data, and best practices to make large collections of learner data available to researchers. Achieving these goals requires engaging a large community of researchers to define, develop, and use critical elements of this infrastructure to address specific data-intensive research questions.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1063},
numpages = {1},
keywords = {LTI, computer science education research, infrastructure, interoperability, smart content, student analytics},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1109/MSR.2017.10,
author = {Chatterjee, Preetha and Gause, Benjamin and Hedinger, Hunter and Pollock, Lori},
title = {Extracting code segments and their descriptions from research articles},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.10},
doi = {10.1109/MSR.2017.10},
abstract = {The availability of large corpora of online software-related documents today presents an opportunity to use machine learning to improve integrated development environments by first automatically collecting code examples along with associated descriptions. Digital libraries of computer science research and education conference and journal articles can be a rich source for code examples that are used to motivate or explain particular concepts or issues. Because they are used as examples in an article, these code examples are accompanied by descriptions of their functionality, properties, or other associated information expressed in natural language text. Identifying code segments in these documents is relatively straightforward, thus this paper tackles the problem of extracting the natural language text that is associated with each code segment in an article. We present and evaluate a set of heuristics that address the challenges of the text often not being colocated with the code segment as in developer communications such as online forums.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {91–101},
numpages = {11},
keywords = {code snippet description, information extraction, mining software repositories, text analysis},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3408877.3432534,
author = {Lyulina, Elena and Birillo, Anastasiia and Kovalenko, Vladimir and Bryksin, Timofey},
title = {TaskTracker-tool: A Toolkit for Tracking of Code Snapshots and Activity Data During Solution of Programming Tasks},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432534},
doi = {10.1145/3408877.3432534},
abstract = {The process of writing code and use of features in an integrated development environment (IDE) is a fruitful source of data in computing education research. Existing studies use records of students' actions in the IDE, consecutive code snapshots, compilation events, and others, to gain deep insight into the process of student programming.In this paper, we present a set of tools for collecting and processing data of student activity during problem-solving. The first tool is a plugin for IntelliJ-based IDEs (PyCharm, IntelliJ IDEA, CLion). By capturing snapshots of code and IDE interaction data, it allows to analyze the process of writing code in different languages --- Python, Java, Kotlin, and C++. The second tool is designed for the post-processing of data collected by the plugin and is capable of basic analysis and visualization. To validate and showcase the toolkit, we present a dataset collected by our tools. It consists of records of activity and IDE interaction events during solution of programming tasks by 148 participants of different ages and levels of programming experience. We propose several directions for further exploration of the dataset.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {495–501},
numpages = {7},
keywords = {activity tracking, code tracking, ide instrumentation, programming education},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@article{10.1007/s10846-015-0202-6,
author = {\'{A}lvarez, Ainhoa and Larra\~{n}aga, Mikel},
title = {Experiences Incorporating Lego Mindstorms Robots in the Basic Programming Syllabus: Lessons Learned},
year = {2016},
issue_date = {January   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {81},
number = {1},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-015-0202-6},
doi = {10.1007/s10846-015-0202-6},
abstract = {Basic Programming is a first year mandatory course of the Computer Engineering degree. Both students and teachers face difficulties in this course, which has high failure and drop-out rates. Several authors have proposed the use of visual programming environments and robots to overcome the difficulties of this course, some of which have been successful. This paper presents the two-year experiment using Lego Robots carried out at the University of the Basque Country (UPV/EHU) with around 100 students, along with the results. Satisfactory results have been obtained regarding both motivation and the perception of the students of their learning process; moreover the drop-out rate decreased even though no statistical significance was obtained regarding the final marks of the course. From those results and the analysis of the data it was derived that robot sessions should be more integrated in the curriculum, giving them greater relevance in the final marks. In addition, it is indispensable to classify course students and adapt learning sessions to each student type due to the high student heterogeneity.},
journal = {J. Intell. Robotics Syst.},
month = {jan},
pages = {117–129},
numpages = {13},
keywords = {Basic Programming, Lego Mindstorms, Robots in Computer Engineering Education}
}

@article{10.5555/3202163.3202166,
title = {Comparing novice programing environments for use in secondary education: App Inventor for Android vs. Alice},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1–2},
issn = {1753-5255},
abstract = {Coding is part of logical thinking and is one of the basic skills which are known as '21st-century skills'. Coding acquisition is necessary as it is used in a wide range of occupations. However, computer programing is difficult to learn and programing courses often have high drop-out rates. Novice programmers suffer from a wide range of difficulties and deficits. Research in teaching and learning programing across different countries and educational contexts reveal that novice programmers face the same challenges in their efficiency of writing, debugging and running programs. These difficulties have led those involved in the teaching of programing to further consider the most effective ways that can facilitate novice programmers in learning the basic programing concepts. Visual programing environments which support the construction of programs through a drag-and-drop interface are among the most popular coding tools for teaching novice programmers. In this paper, we investigate the use of Alice and App Inventor for Android, with regard to their effectiveness for teaching and learning programing in secondary education students.},
journal = {Int. J. Technol. Enhanc. Learn.},
month = {jan},
pages = {44–72},
numpages = {29}
}

@phdthesis{10.5555/1571208,
author = {Westcott, Sandra},
advisor = {Bergin, Joseph},
title = {Effectiveness of using digital game playing in a first-level programming course},
year = {2008},
isbn = {9780549933465},
publisher = {Pace University},
address = {USA},
abstract = {In this high tech world of globalization it is paramount that students know how to think critically, and know how to identify, analyze, and solve problems quickly and effectively. Computer programming courses can provide students with a good foundation in these basic skill sets however, in many U.S. colleges and universities student enrollment in computer related majors is declining. In the fall of 2006 only 1.1 percent of incoming freshmen expressed any interest in computer science as a major. Enrollment in Computer Science needs to increase if we are to remain competitive. Often students enrolled in introductory computer programming courses find the subject difficult. Several studies have concluded that even after students successfully complete an introductory programming course, they still find it difficult to design and code programming solutions. This research investigates the effectiveness of using digital game  playing  to bring computer programming education into the world of experience of novice programming students enrolled in a college-level introductory computer programming course. The research determines whether or not digital game  playing  improves the effective transfer of the students’ problem solving, critical thinking, logical, and programming knowledge from game playing to a formal programming environment. This research also explores whether this method is more effective for certain majors.},
note = {AAI3338734}
}

@inproceedings{10.1145/2591635.2591641,
author = {Feautrier, Paul},
title = {Author retrospective for array expansion, array shrinking, or there and back again},
year = {2014},
isbn = {9781450328401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591635.2591641},
doi = {10.1145/2591635.2591641},
abstract = {In the late 1980's, I was recovering from a long stint as the manager of Paris University computing facility (yes, there were still computing facilities, and PCs were just coming of age) and I was teaching old fashioned automatic parallelization to postgraduate students. I had heard of scalar expansion and privatization, and it was a natural question whether it could be extended to arrays. I first concocted partial solutions - for instance, valid only for the innermost loop - and then decided to try for the general case: converting array accesses to single assignment form. I soon realized that this implied finding the source, or last write before a given read, and that the solution must be a function of the position of the read in the temporal execution of the program. It was obvious that this could not be done for arbitrary complex programs, hence I specified a set of restriction: the polyhedral model. I also introduced the execution order, now known as the 'happens-before' relation. Finding the last write then became an integer programming problem with some unfamiliar features: lexicographic order took the place of the economic function, the problem had to be solved exactly, and the coordinates of the read operation were acting as parameters. Hence, I had first to build PIP (a parametric integer programming tool [2]) before solving my problem. PIP was developed on a 80286 PC, using Borland TurboC and LeLisp.It then took me about two years to have an improved form of the ICS paper published by a journal [3]. Here, the emphasis was more on single assignment conversion and its use for program comprehension. I also formalized a comparison algorithm, which is needed when there are several potential sources. But it was not until [4] that I managed to prove its termination.Meanwhile, the ICS paper had attracted attention from the other side of the Atlantic. Most important was Bill Pugh's work [6], in which the problem was reformulated in term of affine relations, and solved by Bill's own linear programming tool, Omega. I remember that we exchanged our benchmarks, and found that our results were equivalent. An early example of reproducible research!!},
booktitle = {ACM International Conference on Supercomputing 25th Anniversary Volume},
pages = {6},
numpages = {1},
location = {Munich, Germany}
}

@inproceedings{10.5220/0005922204730482,
author = {Oliveira, Francisco C. de M. B. and Freitas, Adriano T. de and Araujo, Thiago A. C. de and Silva, Lidiane C. and Queiroz, Bruno da S. and Soares, \'{E}der F.},
title = {IT Education Strategies for the Deaf},
year = {2016},
isbn = {9789897581878},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005922204730482},
doi = {10.5220/0005922204730482},
abstract = {IT related jobs present a good opportunity for better paying positions for people with disabilities (PWD). Online training seems to be an interesting approach, due to its reach. Building and delivering online content for this population is challenging, especially for those who are deaf or hearing impaired (DHI). We face many problems in the process of teaching the DHI student. Initially we have language related issues, the vocabulary of the Brazilian Sign Language (Libras) is poor when it comes to specific content such as IT. Then, we confront the problem of having very few tutors versed in Libras. Content format, how can we present the information to the DHI considering visual aspects__ __ Accessible learning objects, accessible programming environment, online collaboration between DHI and Non-DHI. Real word task analysis are all discussed in the current text. We present a series of studies our lab conducted and is conducting as we create and deliver IT online content for the PWD.},
booktitle = {Proceedings of the 18th International Conference on Enterprise Information Systems},
pages = {473–482},
numpages = {10},
keywords = {Accessible Content, Employability, Online Collaboration, Sign Language.},
location = {Rome, Italy},
series = {ICEIS 2016}
}

@article{10.1007/s10639-022-11019-2,
author = {\c{C}akiro\u{g}lu, \"{U}nal and \c{C}evik, undefinedsak},
title = {A framework for measuring abstraction as a sub-skill of computational thinking in block-based programming environments},
year = {2022},
issue_date = {Aug 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {7},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-022-11019-2},
doi = {10.1007/s10639-022-11019-2},
abstract = {In order to teach Computational Thinking (CT) skills to young students, Block-Based Programming Environments (BBPEs) are integrated into secondary school computer science (CS) education curricula. As a CT skill, abstraction is one of the prominent skills, which is difficult to enhance and measure. Researchers developed some scales for measuring abstraction in BBPEs; however, it is still quite difficult to measure abstraction and understand students’ abstraction behaviors. The aim of this study is to suggest tasks that could help enhance students’ abstraction skills while teaching CT via block-based programming. In addition, a rubric to score the students’ abstraction behaviors in the problem solving process was created and validated. A framework with regard to the definitions of abstraction skill was adopted and the way to isolate it from other CT-skills was proposed. As a result, pattern recognition, generalizing, decomposition, focusing and eliminating were defined as indicators of abstraction in the problem solving process via BBPEs. The study also informed computer science educators about the relations between teaching CT via BBPEs, affordances of BBPEs and nature of abstraction.},
journal = {Education and Information Technologies},
month = {aug},
pages = {9455–9484},
numpages = {30},
keywords = {Block based environments, Abstraction, Computational thinking, Problem solving}
}

@inproceedings{10.1109/FIE.2018.8659327,
author = {Granada, Rafael Pinto and Barwaldt, Regina and Esp\'{\i}ndola, Danbia Bueno},
title = {Glossary of computational terms as a stimulus to programming logic: a case study with deaf students},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE.2018.8659327},
doi = {10.1109/FIE.2018.8659327},
abstract = {This research full paper presents that Brazilian deaf community has conquered rights in the area of education, and in recent years, with the increase of the enrollment of deaf people in educational establishments, there is a need to elaborate new specific signs of LIBRAS for technical terms of specific courses in different areas. The objective of this work is develop a computational glossary of signals in LIBRAS with the reserved words of the SuperLOGO programming environments and the NXT robotics software. In addition, programming logic and geometry learning in deaf students will be stimulated. The work will be carried out in a bilingual high school, in the mathematics discipline. To validate the glossary and the development process of the students’ knowledge will be used categories of pedagogical evaluation and content analysis. The use of these programming softwares combined with the glossary demonstrated the ability to build knowledge in programming logic and geometry that would be hampered by the language barrier. We conclude that the signs developed for the glossary enable the deaf to access complex terms of computing, providing the same possibilities of educational development as their fellow listeners.},
booktitle = {2018 IEEE Frontiers in Education Conference (FIE)},
pages = {1–7},
numpages = {7},
location = {San Jose, CA, USA}
}

@inproceedings{10.1145/3230977.3231013,
author = {Fagerlund, Janne},
title = {A Study on the Assessment of Introductory Computational Thinking via Scratch Programming in Primary Schools},
year = {2018},
isbn = {9781450356282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230977.3231013},
doi = {10.1145/3230977.3231013},
abstract = {Computational thinking (CT), a transversal intellectual foundation integral to computer science, is making its way into compulsory comprehensive education worldwide. Students are expected to attain skills and knowledge in such interdisciplinary CT principles as Algorithmic thinking, Data representation, and Debugging. Problem-solving by designing and manipulating interactive media with Scratch, a graphical programming tool, is popular especially at the primary school level. However, there has been confusion regarding how introductory CT can be operationalized for educational practice. Teachers and students need research-based knowledge for setting appropriate learning goals in addition to instruments for formative assessment that potentially improve the quality of learning. This study contributes to these issues by developing the assessment for learning of CT via Scratch in primary school settings. A review on prior studies involving the assessment of CT-related computational ideas in Scratch has led to the conceptualization of a revised assessment framework. Next steps in the study are analyzing fourth grade students' (N=58) Scratch projects and exploring complementary methods for analyzing CT in video recordings of the students' programming processes.},
booktitle = {Proceedings of the 2018 ACM Conference on International Computing Education Research},
pages = {264–265},
numpages = {2},
keywords = {assessment, computational thinking, education, graphical programming, primary school, scratch},
location = {Espoo, Finland},
series = {ICER '18}
}

@inproceedings{10.1109/IPDPSW.2015.51,
author = {Finlayson, Ian and Mueller, Jerome and Rajapakse, Shehan and Easterling, Daniel},
title = {Introducing Tetra: An Educational Parallel Programming System},
year = {2015},
isbn = {9781467376846},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPSW.2015.51},
doi = {10.1109/IPDPSW.2015.51},
abstract = {Despite the fact that we are firmly in the multicore era, the use of parallel programming is not as widespread as it could be - in the software industry or in education. There have been many calls to incorporate more parallel programming content into undergraduate computer science education. One obstacle to doing this is that the programming languages most commonly used for parallel programming are detailed, low-level languages such as C, C++, Fortran (with OpenMP or MPI), OpenCL and CUDA. These languages allow programmers to write very efficient code, but that is not so important for those whose goal is to learn the concepts of parallel computing. This paper introduces a parallel programming language called Tetra which provides parallel programming features as first class language features, and also provides garbage collection and is designed to be as simple as possible. Tetra also includes an integrated development environment which is specifically geared for debugging parallel programs and visualizing program execution across multiple threads.},
booktitle = {Proceedings of the 2015 IEEE International Parallel and Distributed Processing Symposium Workshop},
pages = {746–751},
numpages = {6},
keywords = {Debugging, Education, Parallel Programming},
series = {IPDPSW '15}
}

@inproceedings{10.1145/3306214.3338570,
author = {Hattori, Keisuke and Hirai, Tatsunori},
title = {An intuitive and educational programming tool with tangible blocks and AR},
year = {2019},
isbn = {9781450363143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306214.3338570},
doi = {10.1145/3306214.3338570},
abstract = {In recent years, there has been a trend in teaching programming in elementary schools around the world. When teaching programming to lower grader students, robots and puzzles are often used to learn programming for easy understanding. However, those tools have limitations in execution results. Higher graders often use visual programming as a learning material. However, there is a problem that visual programming requires one computer for each individual student, and many students have to learn how to use a computer first. Therefore we propose a programming tool using tangible blocks and AR. This makes it possible to learn programming intuitively with fewer restrictions. Our tool is operated using a smartphone and tangible blocks without using a computer. By using AR, it is possible to create an intuitive programming that can interact with reality. We asked teachers who have experience teaching programming to children to assess the usefulness of our tool within programming education in school. As a result, there was an opinion that it might be suitable for multi-person programming.},
booktitle = {ACM SIGGRAPH 2019 Posters},
articleno = {24},
numpages = {2},
keywords = {AR, creativity, education, physically coding, programming},
location = {Los Angeles, California},
series = {SIGGRAPH '19}
}

@book{10.5555/3122732,
author = {Milutinovi, Veljko and Salom, Jakob and Trifunovic, Nemanja and Giorgi, Roberto},
title = {Guide to DataFlow Supercomputing: Basic Concepts, Case Studies, and a Detailed Example},
year = {2016},
isbn = {3319367587},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This unique text/reference describes an exciting and novel approach to supercomputing in the DataFlow paradigm. The major advantages and applications of this approach are clearly described, and a detailed explanation of the programming model is provided using simple yet effective examples. The work is developed from a series of lecture courses taught by the authors in more than 40 universities across more than 20 countries, and from research carried out by Maxeler Technologies, Inc. Topics and features: presents a thorough introduction to DataFlow supercomputing for big data problems; reviews the latest research on the DataFlow architecture and its applications; introduces a new method for the rapid handling of real-world challenges involving large datasets; provides a case study on the use of the new approach to accelerate the Cooley-Tukey algorithm on a DataFlow machine; includes a step-by-step guide to the web-based integrated development environment WebIDE.}
}

@book{10.5555/2785658,
author = {Milutinovic, Veljko and Salom, Jakob and Trifunovic, Nemanja and Giorgi, Roberto},
title = {Guide to DataFlow Supercomputing: Basic Concepts, Case Studies, and a Detailed Example},
year = {2015},
isbn = {3319162284},
publisher = {Springer Publishing Company, Incorporated},
abstract = {This unique text/reference describes an exciting and novel approach to supercomputing in the DataFlow paradigm. The major advantages and applications of this approach are clearly described, and a detailed explanation of the programming model is provided using simple yet effective examples. The work is developed from a series of lecture courses taught by the authors in more than 40 universities across more than 20 countries, and from research carried out by Maxeler Technologies, Inc. Topics and features: presents a thorough introduction to DataFlow supercomputing for big data problems; reviews the latest research on the DataFlow architecture and its applications; introduces a new method for the rapid handling of real-world challenges involving large datasets; provides a case study on the use of the new approach to accelerate the Cooley-Tukey algorithm on a DataFlow machine; includes a step-by-step guide to the web-based integrated development environment WebIDE.}
}

@article{10.1109/2.58220,
author = {Computer Staff},
title = {Gigabit Network Testbeds},
year = {1990},
issue_date = {September 1990},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {23},
number = {9},
issn = {0018-9162},
url = {https://doi.org/10.1109/2.58220},
doi = {10.1109/2.58220},
abstract = {A description is given of five testbeds developed to examine gigabit applications and network technologies: Aurora, Blanca, Casa, Nectar, and Vistanet. Aurora is an experimental wide-area network testbed whose main objective is to explore and evaluate technologies for phase three of the proposed gigabit National Research and Education Network. The goals of the Blanca network research program are to develop technologies supporting gigabit/second networks, to develop programming tools supporting advanced network-based applications, and to explore the relationships between network technology paradigms and application requirements. The intent of the Casa testbed is to demonstrate that distributed supercomputing using wide-area high-speed networks can provide new levels of computational resources for leading-edge scientific problems. For the Nectar testbed project, a gigabit/second or higher experimental network will be developed to connect a variety of high-performance hosts. The Vistanet research project is intended to help determine whether networks based on emerging public-network standards will satisfy the goals of the National Research and Education Network and to provide information on specifications for those standards.},
journal = {Computer},
month = {sep},
pages = {77–80},
numpages = {4}
}

@inproceedings{10.5555/3618408.3619075,
author = {Khalili, Mohammad Mahdi and Zhang, Xueru and Abroshan, Mahed},
title = {Loss balancing for fair supervised learning},
year = {2023},
publisher = {JMLR.org},
abstract = {Supervised learning models have been used in various domains such as lending, college admission, face recognition, natural language processing, etc. However, they may inherit pre-existing biases from training data and exhibit discrimination against protected social groups. Various fairness notions have been proposed to address unfairness issues. In this work, we focus on Equalized Loss (EL), a fairness notion that requires the expected loss to be (approximately) equalized across different groups. Imposing EL on the learning process leads to a non-convex optimization problem even if the loss function is convex, and the existing fair learning algorithms cannot properly be adopted to find the fair predictor under the EL constraint. This paper introduces an algorithm that can leverage off-the-shelf convex programming tools (e.g., CVXPY (Diamond and Boyd, 2016; Agrawal et al., 2018)) to efficiently find the global optimum of this non-convex optimization. In particular, we propose the ELminimizer algorithm, which finds the optimal fair predictor under EL by reducing the non-convex optimization to a sequence of convex optimization problems. We theoretically prove that our algorithm finds the global optimal solution under certain conditions. Then, we support our theoretical results through several empirical studies.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {667},
numpages = {20},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}

@article{10.5555/2037151.2037174,
author = {Tuttle, Sharon M.},
title = {Introducing programming in a functions-first manner, using the "Program by Design" approach},
year = {2011},
issue_date = {October 2011},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {27},
number = {1},
issn = {1937-4771},
abstract = {Matthias Felleisen was awarded the 2011 SIGCSE Award for Outstanding Contribution to Computer Science Education for his and his research team's work on the TeachScheme! project, which was begun in 1995, and is now called "Program by Design". This functions-first approach to teaching introductory programming and problem-solving features a robust software design methodology called the design recipe, and emphasizes good software engineering practices, such as early testing, from the beginning. These are combined with an integrated development environment (IDE), DrRacket, designed for beginners, featuring different language levels of the Scheme-like language, Racket, that combines simple syntax with customized error messages and support for graphics and animation accessible even for beginning programmers. The Program by Design project then includes support for transitioning to object-oriented programming in Java. However, many of this approach's concepts can be applied in different programming languages (such as Java, Python, or C++), or one can choose to start with this approach and then transition to different programming paradigms and/or languages at a pace appropriate to your situation.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {101},
numpages = {1}
}

@inproceedings{10.1145/3012430.3012495,
author = {Pinto-Llorente, Ana M and Mart\'{\i}n, Sonia Casillas and Gonz\'{a}lez, Marcos Cabezas and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e}},
title = {Developing computational thinking via the visual programming tool: lego education WeDo},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012495},
doi = {10.1145/3012430.3012495},
abstract = {This study seeks to extend the existing research on the use of visual programming tools to work and develop computational thinking. We show the primary education students' perceptions of the use of the software Lego Education WeDo in the subject of natural sciences to promote the computational thinking. We tried to test the following hypotheses: Students will learn to build and program 3D models with Lego Education WeDo (H1), students will think creatively to solve the problems (H2), Lego Education WeDo will help pupils to know the relationship between cause and effect (H3), and the tasks developed will allow pupils to reflect about the possibilities they have and to find the correct answer (H4). Based on the result analysis there were evidences of the effectiveness of the project to increase the participants' awareness of the computational thinking. The research also concluded that according to learners' perception, the way in which activities were designed provided them possibilities to learn to build models in 3D and program them. Moreover, the findings of the study also demonstrated that the success of the project also depended on the teacher's role as a guide in the teaching-learning process.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {45–50},
numpages = {6},
keywords = {collaborative learning, computational thinking, introductory programming, natural sciences, visual programming tool},
location = {<conf-loc>, <city>Salamanca</city>, <country>Spain</country>, </conf-loc>},
series = {TEEM '16}
}

@inproceedings{10.1109/LaTiCE.2013.44,
author = {Simon},
title = {Soloway's Rainfall Problem Has Become Harder},
year = {2013},
isbn = {9780769549606},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/LaTiCE.2013.44},
doi = {10.1109/LaTiCE.2013.44},
abstract = {Discussing the use of plans in programming and in programming education, Solo way describes a programming task that has come to be known as the rainfall problem. This problem was used in a number of empirical experiments in the 1980s and 1990s, and was generally recognised as being quite difficult for student programmers. This paper reports that when the rainfall problem was recently used as an examination question in an introductory programming course, the students performed extremely poorly on it. These students are presumably no better than the many students who have been set this problem in the past, but it also appears that the problem has become harder than it was 20 years ago. For example, the problem assumes that loop-controlled keyboard input is standard, whereas in many programming environments nowadays the standard has become event-driven GUI input. As a consequence of this change, students are less likely to be familiar with loop-controlled keyboard input and with the use of a sentinel to terminate input, another feature of the rainfall problem. While there is potential value in comparing the performance of today's students with that in the literature of past decades, it is important to consider changes in technology that might impose a different level of challenge on the same problem.},
booktitle = {Proceedings of the 2013 Learning and Teaching in Computing and Engineering},
pages = {130–135},
numpages = {6},
keywords = {Soloway, programming education, rainfall},
series = {LATICE '13}
}

@inproceedings{10.1145/3478431.3499393,
author = {Staub, Jacqueline and Chothia, Zaheer},
title = {Large-Scale Analysis of Error Frequencies in Logo Programming},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499393},
doi = {10.1145/3478431.3499393},
abstract = {With the widespread introduction of computer science as a new school subject, programming is experiencing a strong and promising upswing in primary school education. Programming concepts can be taught in a constructive manner, fostering students' precision and algorithmic problem-solving skills. Simultaneously, this form of learning is also inherently challenging and error-prone. During a year-long study, we collected a trace of all failing Logo programs committed in the xlogo programming environment. Analyzing this data set allows us to confirm that Logo errors follow the same patterns as many other programming languages. In particular, we shed a light on the following three areas of Logo programming: (i) we determine which programming errors occur and their respective frequency, (ii) we verify previous work confirming that a handful of error classes account for the vast majority of all errors committed, and (iii) we highlight that some errors are rooted in deep misunderstandings of programming concepts rather than mere syntactical flaws. A core tenet, we argue, is that whilst errors cannot be eliminated entirely they offer an opportunity for both researchers and pupils alike.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
pages = {571–577},
numpages = {7},
keywords = {frequency analysis, programming education, runtime error, syntax errors},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{10.5555/580131.857009,
author = {Vestdam, Thomas and N"rmark, Kurt},
title = {Aspects of Internal Program Documentation " An Elucidative Perspective},
year = {2002},
isbn = {0769514952},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {It is difficult and challenging to comprehend the internal aspects of a program. The internal aspects are seen as contrasts to end user aspects and interface aspects. Internal program documentation is relevant for almost any kind of software. The internal program documentation represents the original as well as the accumulated understanding of the program, which is very difficult to extract from the source program and its modifications over time. Elucidative Programming is a documentation technique that originally is inspired by Literate Programming. As an important difference between the two, Elucidative Programming does not call for any reorganization of the source programs, as required by Literate Programming tools. Elucidative Programming provides for mutual navigation in between program source files and sections of the documentation. The navigation takes place in an Internet browser applying a two-framed layout. In this paper we investigate the applicabilityof Elucidative Programming in a number of areas related to internal program documentation. It is concluded that Elucidative Programming can solve a number of concrete problems in the areas of program tutorials, frameworks, and program reviews. In addition we see positive impacts of Elucidative Programming in the area of programming education.},
booktitle = {Proceedings of the 10th International Workshop on Program Comprehension},
pages = {43},
series = {IWPC '02}
}

@inproceedings{10.1145/3638067.3638076,
author = {Zen, Eliana and Da Costa, Vinicius Kruger and Tavares, Tatiana Aires},
title = {Understanding the Accessibility Barriers Faced by Learners with Visual Impairments in Computer Programming},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638076},
doi = {10.1145/3638067.3638076},
abstract = {Assistive Technology (AT) has facilitated the integration and inclusion of people with disabilities in society. Regarding the visually impaired, the use of AT can be crucial in ensuring access to Information and Communication Technologies. However, in technical and higher-level courses related to computing, the use of AT may not be sufficient to ensure that these students comprehend the concepts addressed in the subjects and have access to all the resources provided by the tools necessary for their professional training. This is because screen readers, the main accessibility resource used by the visually impaired to interact with digital systems, generally perform a linear reading of the content available in the graphical interface, which can demand more time and effort from users. Additionally, this approach may limit access to information. This is particularly relevant in the case of Integrated Development Environment (IDEs), which have complex visual interfaces that may not be accessible to screen readers if accessibility requirements are not correctly implemented. Therefore, this study seeks to identify the barriers to interaction with IDEs faced by students in Computer Programming classes. The research involved surveying 12 professors of disciplines related to the area of Computer Programming who taught students with visual impairment. Additionally, interviews were conducted with 6 students and graduates with visual impairments from Technical and Higher Education courses in the area of Computing. The data were analyzed using Content Analysis. The results confirmed those already identified in the literature and also revealed new barriers.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {65},
numpages = {11},
keywords = {Accessibility, Assistive Technology, Computer Programming, Human-Computer Interaction, Visual Impairment},
location = {<conf-loc>, <city>Macei\'{o}</city>, <country>Brazil</country>, </conf-loc>},
series = {IHC '23}
}

@mastersthesis{10.5555/AAI28104555,
author = {Smith, R. P.},
title = {Software Portability},
year = {1986},
isbn = {9798662517028},
publisher = {Open University (United Kingdom)},
abstract = {This thesis represents the submission to the Open University for the degree of Master of Philosophy. The research topic lies within the discipline of computer science and in particular problems associated with the transportability of computer software are investigated. The study period was organised into 3 parts:• A literature search to look at what work had been done in the area.• The design of an intermediate language for a compiler.• The specification, design and implementation of a functional programming language in order to develop a portable programming environment. This thesis covers the work of the period Sep. 1978 to Jun. 1982, all of which was performed by part time study. The Thesis comprises 4 major sections and an appendix which shows program listings. The sections are as follows:• Section 1: The introduction• Section 2: The result of a literature search.• Section 3: A design for a compiler intermediate language, Ivor, covering the architecture of the underlying abstract machine and the structure of the intermediate language.• Section 4: The design and implementation of a function based programming language. Mule, including the structure of the language and a description of the implementation.},
note = {AAI28104555}
}

@article{10.1145/13677.22723,
author = {Joyce, Jeffrey and Lomow, Greg and Slind, Konrad and Unger, Brian},
title = {Monitoring distributed systems},
year = {1987},
issue_date = {May 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {0734-2071},
url = {https://doi.org/10.1145/13677.22723},
doi = {10.1145/13677.22723},
abstract = {The monitoring of distributed systems involves the collection, interpretation, and display of information concerning the interactions among concurrently executing processes. This information and its display can support the debugging, testing, performance evaluation, and dynamic documentation of distributed systems. General problems associated with monitoring are outlined in this paper, and the architecture of a general purpose, extensible, distributed monitoring system is presented. Three approaches to the display of process interactions are described: textual traces, animated graphical traces, and a combination of aspects of the textual and graphical approaches. The roles that each of these approaches fulfill in monitoring and debugging distributed systems are identified and compared. Monitoring tools for collecting communication statistics, detecting deadlock, controlling the non-deterministic execution of distributed systems, and for using protocol specifications in monitoring are also described.Our discussion is based on experience in the development and use of a monitoring system within a distributed programming environment called Jade. Jade was developed within the Computer Science Department of the University of Calgary and is now being used to support teaching and research at a number of university and research organizations.},
journal = {ACM Trans. Comput. Syst.},
month = {mar},
pages = {121–150},
numpages = {30}
}

@article{10.1145/2700519,
author = {Xinogalos, Stelios},
title = {Object-Oriented Design and Programming: An Investigation of Novices’ Conceptions on Objects and Classes},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
url = {https://doi.org/10.1145/2700519},
doi = {10.1145/2700519},
abstract = {The Object-Oriented Programming (OOP) technique is nowadays the most popular programming technique among tertiary education institutions. However, learning OOP is a cognitively demanding task for undergraduate students. Several difficulties and misconceptions have been recorded in the literature for both OOP concepts and languages, mainly Java. This article focuses on reviewing and advancing research on the most fundamental OOP concepts, namely, the concepts of “object” and “class” and their role during program execution. The results of a long-term investigation on the subject are presented, focusing on a study exploring undergraduate students’ conceptions on “objects” and “classes.” The study advances related research on categories of conceptions on “objects” and “classes” by providing quantitative results, in addition to qualitative results, regarding the frequency of the recorded conceptions. Nearly half the students seem to comprehend the modeling and static/dynamic aspects of the concepts “object” and “class.” Implications for achieving a deep conceptual understanding of text, action, and modeling aspects of these fundamental concepts are also discussed. Information regarding the programming environments utilized in the course and key features of the applied teaching approach are presented, in order to facilitate both a better understanding of the context and a better employment of the results of the presented study. Finally, proposals for enhancing the contribution of this and similar studies are made.},
journal = {ACM Trans. Comput. Educ.},
month = {jul},
articleno = {13},
numpages = {21},
keywords = {Object-oriented programming, class, conceptions, misconceptions, object, teaching/learning programming}
}

@inproceedings{10.1145/3328778.3366838,
author = {Piwek, Paul and Savage, Simon},
title = {Challenges with Learning to Program and Problem Solve: An Analysis of Student Online Discussions},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366838},
doi = {10.1145/3328778.3366838},
abstract = {Students who study problem solving and programming (in a language such as Python) at University level encounter a range of challenges, from low-level issues with code that won't compile to misconceptions about the threshold concepts and skills. The current study complements existing findings on errors, misconceptions, difficulties and challenges obtained from students after-the-fact through instruments such as questionnaires and interviews. In our study, we analysed the posts from students of a large cohort (textasciitilde1500) of first-year University distance learning students to an online 'Python help forum' - recording issues and discussions as the students encountered specific challenges. Posts were coded in terms of topics, and subsequently thematically grouped into Python-related, problem solving/generic programming related, and module specific. We discuss the set of topics and rank these in terms of the number of forum discussions in which they occur (as a proxy for their prevalence). The top challenges we identified concern student understanding and use of a mix of programming environments (in particular, Python IDLE for offline programming and CodeRunner for programming quizzes) and code fragment problems. Apart from these, Python-specific topics include, among others, collections, functions, error messages, iteration, outputting results, indentation, variables and imports. We believe that the results provide a good insight into the challenges that students encounter em as they learn to program. In future work we intend to study the discussions in further detail in terms of theories of conceptual change.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {494–499},
numpages = {6},
keywords = {challenges, misconceptions, online student discussions, problem solving, programming, python, threshold concepts and skills},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.5555/1169107.1169155,
author = {Naftel, Andrew and Throuvalas, Antonios and Evans, Gareth},
title = {Machine assessment of shape copying tests using Zernike moment descriptors},
year = {2006},
isbn = {0889865809},
publisher = {ACTA Press},
address = {USA},
abstract = {Visual Motor Integration tests, which involve a subject copying geometric shapes, are often used as one of a battery of tests to assess the needs of a child who may have a specific learning difficulty (SpLD). As part of the Dyslexia Early Screening Test (DEST), the resulting freehand sketches are assessed by an expert who analyses them visually and decides on the degree of similarity between the sketches and a shape copying scoring template. The assessment is time-consuming and rather subjective. In this paper, we investigate the use of Zernike moment descriptors as a feature extraction technique for training a k-nearest neighbour classifier to recognise and automatically assign scores to a set of hand-sketched shapes. A prototype shape copying assessment system DESCAR has been implemented using the Matlab programming environment. Scoring classification accuracy has been evaluated on a test corpus of 840 sketches comprising 120 different drawings of each of 7 different shapes used in the DEST study [16]. Experimental results show that machine score accuracy rates in the range 63.6-77.9% can be obtained in the correct assignment of scores when compared to a human expert assessor. Accuracy rates depend on geometric shape, order of Zernike moments and choice of classification test used.},
booktitle = {Proceedings of the 24th IASTED International Conference on Signal Processing, Pattern Recognition, and Applications},
pages = {262–267},
numpages = {6},
keywords = {classification, dyslexia testing, moment descriptors, sketch recognition},
location = {Innsbruck, Austria},
series = {SPPRA'06}
}

@inproceedings{10.1145/3241815.3242586,
author = {Rahman, Farzana},
title = {Leveraging Visual Programming Language and Collaborative Learning to Broaden Participation in Computer Science},
year = {2018},
isbn = {9781450359542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241815.3242586},
doi = {10.1145/3241815.3242586},
abstract = {Engaging underrepresented populations of women and minorities in Computer Science (CS) represents our greatest untapped resource for increasing the STEM workforce. In recent years, tremendous efforts have been geared towards developing learning materials to increase the interest of underrepresented students in CS. More recently, CS education researchers are beginning to recognize the need to apply the learning sciences to develop age- and grade-appropriate curricula and pedagogies for developing computing competencies among children. One effective approach to build learning competencies among young underrepresented students is through Collaborative Learning, which is an educational approach that involves groups of learners working together to solve a problem or create a product. Our goal, in this paper, is to report our experiences on designing and delivering a curriculum that teaches programming to middle school students using App Inventor through collaborative learning. Our curriculum is developed on the hypothesis that visual programming environment, in this case, App Inventor, present an alternative way of learning programming, which in the collaborative learning environment can enhance programming competencies and interests in underrepresented students. In this experience report, we will describe how we implemented this curriculum as a block course; present our lessons learned, and few findings from the evaluation.},
booktitle = {Proceedings of the 19th Annual SIG Conference on Information Technology Education},
pages = {172–177},
numpages = {6},
keywords = {appinventor, collaborative learning, programming fundamentals, visual programming language},
location = {Fort Lauderdale, Florida, USA},
series = {SIGITE '18}
}

@book{10.5555/3265452,
author = {Felleisen, Matthias and Findler, Robert Bruce and Flatt, Matthew and Krishnamurthi, Shriram},
title = {How to Design Programs: An Introduction to Programming and Computing},
year = {2018},
isbn = {0262534800},
publisher = {The MIT Press},
abstract = {A completely revised edition, offering new design recipes for interactive programs and support for images as plain values, testing, event-driven programming, and even distributed programming. This introduction to programming places computer science at the core of a liberal arts education. Unlike other introductory books, it focuses on the program design process, presenting program design guidelines that show the reader how to analyze a problem statement, how to formulate concise goals, how to make up examples, how to develop an outline of the solution, how to finish the program, and how to test it. Because learning to design programs is about the study of principles and the acquisition of transferable skills, the text does not use an off-the-shelf industrial language but presents a tailor-made teaching language. For the same reason, it offers DrRacket, a programming environment for novices that supports playful, feedback-oriented learning. The environment grows with readers as they master the material in the book until it supports a full-fledged language for the whole spectrum of programming tasks. This second edition has been completely revised. While the book continues to teach a systematic approach to program design, the second edition introduces different design recipes for interactive programs with graphical interfaces and batch programs. It also enriches its design recipes for functions with numerous new hints. Finally, the teaching languages and their IDE now come with support for images as plain values, testing, event-driven programming, and even distributed programming.}
}

@inproceedings{10.1145/2839509.2850525,
author = {Wagner, Amber and Gray, Jeff and Marghitu, Daniela and Stefik, Andreas},
title = {Raising the Awareness of Accessibility Needs in Block Languages (Abstract Only)},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2850525},
doi = {10.1145/2839509.2850525},
abstract = {Block languages (e.g., Scratch, Snap!, Alice, App Inventor, Blockly) offer a gentle introduction to programming and have been adopted widely in both K-12 and CS0 courses. However, block languages often are dependent on the mouse/keyboard for input and typically are visual in their output and representation. Because of these dependencies, students with a disability (e.g., mobility limitations or vision impairment) generally are unable to use block languages, thereby reducing the opportunities for broader participation in computational learning activities. Given the increasing need to broaden the participation of computing to those with diverse skills and backgrounds, it is important that the tools used to initiate the earliest entre into computing do not erect immediate roadblocks that impede initial interest and opportunity. There are many variations of user interfaces and assistive technologies that benefit those who may have difficulties utilizing traditional Graphical User Interfaces (GUIs), but these tools often cannot be used universally across block languages. As more block languages are being developed and integrated into K12 and University curriculum, it is imperative that accessible solutions are discussed and implemented. These discussions require participation from the block language developer community, accessible computing community, and those educators who encounter accessibility needs among the students in their classrooms. The goal of this lightning talk is to call attention to the need for more accessible block-based programming environments and to spark conversation surrounding possible standard accessibility APIs that could possibly be supported by block language environment tool developers.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {497},
numpages = {1},
keywords = {accessibility, block languages, broadening participation},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@book{10.5555/3055864,
author = {Valle, Andrea},
title = {Introduction to SuperCollider},
year = {2016},
isbn = {3832540172},
publisher = {Logos Verlag},
address = {DEU},
abstract = {Originally developed by James McCartney in 1996 and now an open source project, SuperCollider is a software package for the synthesis and control of audio in real time. Currently, it represents the state of the art in the field of audio programming: there is no other software available that is equally powerful, efficient or flexible. Yet, SuperCollider is often approached with suspicion or awe by novices, but why? One of the main reasons is the use of a textual user interface. Furthermore, like most software packages that deal with audio, SuperCollider prerequisites a series of skills, ranging from expertise in analog/digital signal processing, to musical composition, to computer science. However, as the beginner overcomes these initial obstacles and understands the powerful flexibility of SuperCollider, what once were seen as weaknesses become its strengths. SuperCollider's features also mean versatility in advanced software applications, generality in terms of computer modelling, and expressivity in terms of symbolic representations. This book aims at providing a brief overview of, and an introduction to, the SuperCollider programming environment. It also intends to informally present, by employing SuperCollider, a series of key notions relevant to what is broadly referred to as computer music. Andrea Valle is a researcher/aggregate professor in film, photography and television at the University of Turin-DAMS, and is active as a musician and composer. He has been a SuperCollider user since 2005.}
}

@inproceedings{10.1145/2576768.2598264,
author = {De Melo, Vin\'{\i}cius Veloso},
title = {Kaizen programming},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598264},
doi = {10.1145/2576768.2598264},
abstract = {This paper presents Kaizen Programming, an evolutionary tool based on the concepts of Continuous Improvement from Kaizen Japanese methodology. One may see Kaizen Programming as a new paradigm since, as opposed to classical evolutionary algorithms where individuals are complete solutions, in Kaizen Programming each expert proposes an idea to solve part of the problem, thus a solution is composed of all ideas together. Consequently, evolution becomes a collaborative approach instead of an egocentric one. An idea's quality (analog to an individual's fitness) is not how good it fits the data, but a measurement of its contribution to the solution, which improves the knowledge about the problem. Differently from evolutionary algorithms that simply perform trial-and-error search, one can determine, exactly, parts of the solution that should be removed or improved. That property results in the reduction in bloat, number of function evaluations, and computing time. Even more important, the Kaizen Programming tool, proposed to solve symbolic regression problems, builds the solutions as linear regression models - not linear in the variables, but linear in the parameters, thus all properties and characteristics of such statistical tool are valid. Experiments on benchmark functions proposed in the literature show that Kaizen Programming easily outperforms Genetic Programming and other methods, providing high quality solutions for both training and testing sets while requiring a small number of function evaluations.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {895–902},
numpages = {8},
keywords = {collaborative problem solving, curve-fitting, evolutionary algorithm, genetic programming, linear regression, symbolic regression},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@article{10.1145/2872521,
author = {K\"{o}lling, Michael and McKay, Fraser},
title = {Heuristic Evaluation for Novice Programming Systems},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
url = {https://doi.org/10.1145/2872521},
doi = {10.1145/2872521},
abstract = {The past few years has seen a proliferation of novice programming tools. The availability of a large number of systems has made it difficult for many users to choose among them. Even for education researchers, comparing the relative quality of these tools, or judging their respective suitability for a given context, is hard in many instances. For designers of such systems, assessing the respective quality of competing design decisions can be equally difficult.Heuristic evaluation provides a practical method of assessing the quality of alternatives in these situations and of identifying potential problems with existing systems for a given target group or context. Existing sets of heuristics, however, are not specific to the domain of novice programming and thus do not evaluate all aspects of interest to us in this specialised application domain.In this article, we propose a set of heuristics to be used in heuristic evaluations of novice programming systems. These heuristics have the potential to allow a useful assessment of the quality of a given system with lower cost than full formal user studies and greater precision than the use of existing sets of heuristics. The heuristics are described and discussed in detail. We present an evaluation of the effectiveness of the heuristics that suggests that the new set of heuristics provides additional useful information to designers not obtained with existing heuristics sets.},
journal = {ACM Trans. Comput. Educ.},
month = {jun},
articleno = {12},
numpages = {30},
keywords = {HCI, heuristic evaluation, introductory programming tools}
}

@inproceedings{10.1145/3408877.3432389,
author = {Lin, Xinyue and Connors, James and Lim, Chang and Hott, John R.},
title = {How Do Students Collaborate? Analyzing Group Choice in a Collaborative Learning Environment},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432389},
doi = {10.1145/3408877.3432389},
abstract = {Collaborative learning has been effective and widely adopted in Computer Science education. Existing studies have controlled for group sizes by assigning members to determine the optimal collaboration environment, with some focusing on a peer-programming environment and others observing a wider range of sizes and tasks.We analyzed collaboration trends through an observational study of 189 students in a large upper-level Computer Science algorithms course, which uses a less-constrained collaborative setting. In the course, the collaboration policy encourages students to choose their own groups for each assignment, up to four other students, offering insight into how groups evolve in size and membership when students are given the freedom to self-select. Since each student is required to submit their own individual work, we collected information about the grade and self-reported collaborators of each research participant for nine assignments, including written and coding homework.Our results show that any collaboration improved individual performance on average. For programming assignments, groups of size four were optimal. Across both written and programming assignments, larger groups performed better, including chains of collaboration greater than the course policy allowed. However, sizes 4-5 performed best within the bounds of the policy. We also demonstrate that factors impacting collaboration include homework difficulty, time of grade release, students' relative performance with respect to the class, as well as the homework type.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {212–218},
numpages = {7},
keywords = {collaboration groups, collaborative learning, computer science education, group formation},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/800174.809773,
author = {Saib, Sabina H.},
title = {Issues in Ada's future sponsored by ACM/adatc (Panel Discussion)},
year = {1982},
isbn = {0897910850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800174.809773},
doi = {10.1145/800174.809773},
abstract = {The Ada programming language has gone public. The Department of Defense originally sponsored the effort in order to develop a high level language which could be used in real time computer systems. Such systems are written in assembly language or in obscure languages such as CMS (Navy), JOVIAL (Air Force), and TACPOL (Army). Each of the high level languages in current use by the services has deficiencies which require the use of assembly language in almost all applications. In fact, most of the real time applications which are written supposedly in a high level language are written actually in assembly language because the compilers accept assembly language statements. This sad state of affairs has resulted because none of the current language can handle all of the requirements of real time applications. For example, there exist at least four dialects of JOVIAL (J2, J3, J3B, and J73) none of which have a basic input or output capability.Some of the problems which result from the use of assembly language and obscure languages are logistical rather than technical: a small number of people know these languages; there is a dearth of training material and courses; there is no portability of programs or people between systems which use these languages; the programming environments for each of the languages is poor; the languages are not available on many computer systems.},
booktitle = {Proceedings of the ACM '82 Conference},
pages = {118–120},
numpages = {3},
series = {ACM '82}
}

@inproceedings{10.1145/3408877.3432554,
author = {Branco, Andr\'{e} and Dutra, Claudia and Zumpichiatti, D\'{e}bora and Campos, Francisco Augusto and SantClair, Gabriel and Mello, Jhulian and Moreira, Jo\~{a}o Victor and Godinho, Julia and Marotti, Julia and Gomide, Janaina},
title = {Programming for Children and Teenagers in Brazil: A 5-year Experience of an Outreach Project},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432554},
doi = {10.1145/3408877.3432554},
abstract = {There has been a worldwide surge in programming education initiatives for children and teenagers. In Brazil, this trend faces some challenges, namely inadequate infrastructure of most schools, notably public ones, that lack access to computers and tablets, and basic education curricular requirements not contemplating computer science concepts. This article reports on the five-year experience of an outreach project from a public university in Brazil. The project aims to promote computer science education and to teach programming to children and teenagers. Undergraduate engineering students who participate in the project as members engage in activities such as planning the courses and their schedules, creating partnerships with local schools and other educational projects, giving lectures, producing scientific research and educational materials, as well as promoting the project on social media. The courses use free online programming tools, Python, MIT App Inventor, and Arduino to cover fundamental concepts of programming and computational thinking. They vary approaches and tools according to the age range and available technological resources of the target audience. The use of unplugged activities means to assist in learning and to circumvent computer access problems. Furthermore, they serve for introducing basic programming concepts in classes and motivating students with dynamic activities. Over its five-year existence, the project has achieved its purpose, by reaching a total of 2639 students through 45 workshops and 94 courses. It has provided courses in eleven public schools, created two booklets and one app as free educational material, along with presented papers and posters in scientific conferences.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {411–417},
numpages = {7},
keywords = {computational thinking, computer science education, didactic strategies, k-12, programming},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@techreport{10.5555/867115,
author = {Presberg, David and Jaeger, Christopher},
title = {Porting the Distributed Array Query and Visualization Tool for High Performance Fortran to the SP2},
year = {1996},
publisher = {Cornell University},
address = {USA},
abstract = {Distributed Array Query and Visualization (DAQV) is a Parallel Tools Consortium sponsored project to create a tool for visualizing distributed data in High Performance Fortran (HPF). The DAQV tool is currently maintained at the University of Oregon, and our goals here at the Cornell Theory Center (CTC) are to verify the work done by the people there, and port the DAQV tool and its associated visualization clients to our IBM SP2 and IBM''s HPF compiler, XL HPF. We describe in this paper the installation of the DAQV tool first on an SGI Onyx, using Portland Group Inc.''s HPF compiler, pghpf, which was already supported by DAQV. We make various modifications to the distribution to generalize the installation, and then port the DAQV tool to the IBM SP2, also using the pghpf compiler. Finally, we accomplish the port of DAQV to IBM''s XL HPF compiler. We describe our approaches to overcoming various obstacles encountered in the porting process. These include re-analyzing the DAQV design to accommodate distinctions between the pghpf and XL HPF compiler run time implementations. The end result is not a single portable reference implementation of the DAQV tool, as was originally planned, but rather two different portable implmentations that demonstrate different ways in which a vendor may choose to perform the gathering of distributed data from HPF programs in a DAQV-like tool. Keywords: multiprocessors, parallel programming tools, data access, DAQV, data distribution, data parallel, Fortran, HPF, SP2, SPMD}
}

@inproceedings{10.1145/2676723.2677337,
author = {Garcia, Daniel D. and Ding, Wei and Cohen, Joseph and Ericson, Barbara and Gray, Jeff and Reed, Dale},
title = {One-Day Activities for K-12 Face-to-Face Outreach},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2677337},
doi = {10.1145/2676723.2677337},
abstract = {The recent successes of Computer Science Education Week and code.org's Hour of Code have meant that more K-12 students than ever are being given an authentic, engaging and eye-opening exposure to the wonders of computer science. There are resources aplenty to help high school and college faculty with outreach. These range from easy-to-learn, open-ended programming environments (Scratch, Alice, Snap!), to online coding challenges (code.org, Lite-bot), to non-computer activities with live performances (CS Unplugged, cs4fn), to having the entire outreach experience delivered "in a box", thanks to NCWIT.We wanted to bring educators together to share experiences with what they've done specifically with a one-day event, given these vast resources. Now that there are so many online coding experiences, it is enough to shuttle young students into a computer room, point their browser at one of these experiences, and answer questions as they come up? Is it important to include hands-on and hands-off (e.g., nifty demos, inspiring talks) components, and if so, in what order? What do different demographics find the most engaging? Is there any chance that we can do "damage", since these highlight-reel experiences might over-simplify how hard some of the problems are, and that not every important result has a flashy payoff? Do some of the early experiences leave students with the impression that computer science is only (say) apps, interactive multimedia programs or solving mazes? Finally, when it's over, what follow-up is appropriate? Participants on the panel will share best practices, common pitfalls, and advice.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {520–521},
numpages = {2},
keywords = {computer science education, k-12, outreach},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@article{10.5555/1791129.1791185,
author = {Toth, David},
title = {Our experiences incorporating robotics into our service course: poster session},
year = {2010},
issue_date = {June 2010},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {25},
number = {6},
issn = {1937-4771},
abstract = {A number of colleges and universities are using robots as a focus in introductory computer science courses to try to stimulate students' interest in computer science. Several colleges have integrated robots into their CS1 courses or even built an entire course around robots [1, 2, 3, 4]. Other colleges have integrated robots into CS0 courses [5, 6]. In this poster, we present the experiences we had when incorporating robotics into our service course, which is designed to be neither a CS0 nor a CS1 course.Our course, Introduction to Information Technology, aims to introduce students to the concepts in information technology and their uses in the workplace. Because of breadth of this subject, we are only able to devote 1--1.5 weeks to any particular topic, including introducing students to computer programming. The course is almost entirely populated by students who will not major in computer science, although on rare occasions, a student taking this course will continue on to CS1 and the computer science major. Previously, we introduced programming with Scratch and also had students play a game called LightBot, which requires students to "program" a virtual robot to light up tiles [7, 8]. Many of the students had a lot of success with LightBot and enjoyed it. Conversely, a large portion of the students did not appear to be engaged with Scratch.This year, following the success of using LightBot, we considered alternate methods to introduce programming and chose to use robots in the course to introduce students to computer programming. We used the Scribbler robots from Parallax, Inc. in three of our four sections of Introduction to Information Technology this fall [9]. The Scribbler robot comes with a graphical programming environment that can be used by students instead of using the Basic Stamp editor that also comes with the robot. Students were taught how to use the graphical programming environment to make the Scribblers move, make decisions using if/else statements, repeat tasks with a while loop, and detect obstacles using the robot's sensors. The students were encouraged to play with the robots for the 50 minute class period to get comfortable with them. They were told that during the following class, which was 100 minutes, their assignment would be to program the robots to navigate a simple U-shaped maze. The students were told that their robot should be able to be placed at either entrance of a maze by the professor and be able to enter the maze and exit out the other side.We had mixed results and encountered several unexpected challenges when conducting the lab. The success rate of the students was low and few students were able to complete the exercise the way we had hoped they would. This may have been due to some of the many problems we encountered while conducting this lab. However, the reason for changing the vehicle for introducing programming was to try to engage more students and a greater proportion of the students appeared to be engaged in trying to make the robot traverse the maze than had been engaged by Scratch last year. We also noticed that several students who had not seemed to be particularly engaged during the course were working hard on the lab and appeared to be enjoying themselves. Given that we were primarily looking to engage the students more successfully than last year, the lab may have worked better than the success rate indicated.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {256–258},
numpages = {3}
}

@phdthesis{10.5555/932896,
author = {Kerkiz, Nabil Fouad and Bouldin, Dan},
title = {Development and experimental evaluation of partitioning algorithms for adaptive computing systems},
year = {2000},
isbn = {0493035788},
publisher = {The University of Tennessee},
abstract = {Multi-FPGA systems offer the potential to deliver higher performance solutions than traditional computers for some low-level computing tasks. This requires a flexible hardware substrate and an automated mapping system. CHAMPION is an automated mapping system for implementing image processing applications in multi-FPGA systems under development at the University of Tennessee. CHAMPION will map applications in the Khoros Cantata graphical programming environment to hardware. The work described in this dissertation involves the automation of the CHAMPION back-end design flow, which includes the partitioning problem, netlist to structural VHDL conversion, synthesis and placement and routing, and host code generation. The primary goal is to investigate the development and evaluation of three different k-way partitioning approaches. In the first and the second approaches, we discuss the development and implementation of two existing algorithms. The first approach is a hierarchical partitioning method based on topological ordering (HP). The second approach is a recursive algorithm based on the Fiduccia and Mattheyses bipartitioning heuristic (RP). We extend these algorithms to handle the multiple constraints imposed by adaptive computing systems. We also introduce a new recursive partitioning method based on topological ordering and levelization (RPL). In addition to handling the partitioning constraints, the new approach efficiently addresses the problem of minimizing the number of FPGAs used and the amount of computation, thereby overcoming some of the weaknesses of the HP and RP algorithms.},
note = {AAI9996361}
}

@article{10.1109/TE.2023.3281825,
author = {Seth, Arjit and Redonnet, Stephane and Liem, Rhea P.},
title = {MADE: A Multidisciplinary Computational Framework for Aerospace Engineering Education},
year = {2023},
issue_date = {Dec. 2023},
publisher = {IEEE Press},
volume = {66},
number = {6},
issn = {0018-9359},
url = {https://doi.org/10.1109/TE.2023.3281825},
doi = {10.1109/TE.2023.3281825},
abstract = {Contribution: A multidisciplinary computational framework to support undergraduate engineering education is introduced. It is posed as an open-source tool that students can use to understand and apply computational methods commonly required in engineering problems. Background: This framework development is motivated by past experiences of teaching courses as segregated disciplines within an aerospace engineering curriculum. Reviewing pedagogical approaches shows that current university practices do not adequately emphasize or demonstrate the importance of interdisciplinary relationships. This framework is also motivated by the apparent need for engineering graduates to study these relationships by learning computational engineering via programming, in which proficiency is becoming increasingly important for their future careers. Intended Outcomes: 1) To effectively teach multidisciplinary engineering concepts to students via the integration of cross-disciplinary content from various courses and 2) To improve computational and programming literacy among undergraduate students in engineering studies. Application Design: A multidisciplinary design optimization approach is adopted to develop a framework called Multidisciplinary Aircraft Design Education (MADE), which provides functionalities for studying engineering disciplines within a computational programming environment. MADE makes the related course contents more accessible and intuitive to students by using reactive computational notebooks in lectures, tutorials, and assignments. Findings: MADE is assessed based on the learning outcomes and feedback from undergraduate students of two courses within an aerospace engineering curriculum. These assessments indicate an improved understanding of multidisciplinary concepts and their applications via computational programming, as reflected in the positive responses from the students.},
journal = {IEEE Trans. on Educ.},
month = {jun},
pages = {622–631},
numpages = {10}
}

@book{10.5555/555119,
author = {Austell-Wolfson, Barry M. and Otieno, Derek},
title = {The  Complete Book of C Programming},
year = {1999},
isbn = {0130960934},
publisher = {Prentice Hall PTR},
address = {USA},
edition = {1st},
abstract = {From the Publisher:This comprehensive book teaches the reader how to design and write portable elementary, intermediate and advanced batch and interactive ANSI C programs in their entirety, that are easy to read, debug, modify and maintain. The book and accompanying programs comprise a total package designed to satisfy all ANSI C needs in any of the programming environments that embrace the C Standard. The authors incorporate a  build-as-you go method  beginning the text with a complete billing application programming problem and progressively solving the problem chapter by chapter as programming knowledge grows thereby training the reader to develop programs in a modular, top-down way, as well as maintain and modify code. It also provides useful techniques for maintaining and modifying older “legacy” programs and covers information processing and system concepts. Building Blocks, Using Variables, Introduction to I/O, Expressions, Operators, and Type Conversion, Loops and Conditional Statements, Arrays, Pointers and Strings, Functions, String-Handling &amp; Buffer Functions, Scope and Duration, The Preprocessor, Byte Structure and Bit Manipulation, Complex Data Types and Type Conversion, Files, Dynamic Data Structures and Memory Allocation, Working with the System, Projects &amp; Program Chaining, Controlling the PC Console - Escape Sequences, Memory and Interrupts on the PC, Video Services Interrupts on the PC, Direct Memory Access on the PC, Graphics Mode and the Mouse on the PC, ASCII/EBCDIC Characters, and Extended Keyboard Codes. For programmers, systems administrators, or anyone responsible for programming or maintaining programs and systems written in ANSI C.}
}

@phdthesis{10.5555/1559634,
author = {Patel, Imran S.},
advisor = {Gilbert, John R.},
title = {Empirical evaluation of software development productivity in high performance computing},
year = {2008},
isbn = {9780549699514},
publisher = {University of California at Santa Barbara},
address = {USA},
abstract = {Although advances in parallel hardware continue to deliver a measurable increase in execution performance, corresponding improvements in the programmability and thereby productivity of HPC systems remain hard to quantify. Several factors such as the parallel programming language, the hardware architecture, and programmer aptitude influence the achieved performance and the required effort. While there exists a rich body of work to characterize system and application performance, the effect of factors such as programming languages and programmer workflows on performance and productivity is poorly understood. Traditional benchmarks narrowly focus on raw performance and fail to evaluate these factors. This dissertation presents an empirical approach to study the effect of these factors, especially programming languages, on productivity using experimental studies with human subjects. We leverage the development experience of students in HPC classroom studies to collect empirical data for our experimental analysis. Collecting high-quality data across multiple classroom studies is a challenging problem. We present a data collection methodology and an operational system that captures an accurate and concise view of the development workflow. Our system uses instrumented tools along with a high-level development framework to capture development activities of subjects with minimal intrusiveness. We address practical issues such as reproducibility, noise, and privacy that arise during data analysis. Using our system, we have consistently collected several high-quality datasets during a series of replicated studies.The dissertation presents a statistical approach for using this data to formally evaluate several hypotheses about the relative productivity of two parallel language extensions, MPI and UPC, that represent two mainstream parallel programming models. We formulate hypothesis tests for comparing the performance and productivity of these two models and present several statistically significant results. While the statistical method analyzes the data related to the developed software, a workflow model explains the development process and effort of the programmer. We propose a Hidden Markov Model to representatively model the workflow followed by subjects when using a given programming language. We present results from training and building these models on our datasets. We also describe GRID*  p , a MATLAB-based programming environment that demonstrates the productivity and performance of high-level development tools for large-scale computations on the Grid. Results of developing two typical parallel applications show that GRID*  p  delivers promising performance for highly parallel applications with minimal development effort.},
note = {AAI3316493}
}

@article{10.1145/3077618,
author = {Qian, Yizhou and Lehman, James},
title = {Students’ Misconceptions and Other Difficulties in Introductory Programming: A Literature Review},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3077618},
doi = {10.1145/3077618},
abstract = {Efforts to improve computer science education are underway, and teachers of computer science are challenged in introductory programming courses to help learners develop their understanding of programming and computer science. Identifying and addressing students’ misconceptions is a key part of a computer science teacher's competence. However, relevant research on this topic is not as fully developed in the computer science education field as it is in mathematics and science education. In this article, we first review relevant literature on general definitions of misconceptions and studies about students’ misconceptions and other difficulties in introductory programming. Next, we investigate the factors that contribute to the difficulties. Finally, strategies and tools to address difficulties including misconceptions are discussed.Based on the review of literature, we found that students exhibit various misconceptions and other difficulties in syntactic knowledge, conceptual knowledge, and strategic knowledge. These difficulties experienced by students are related to many factors including unfamiliarity of syntax, natural language, math knowledge, inaccurate mental models, lack of strategies, programming environments, and teachers’ knowledge and instruction. However, many sources of students’ difficulties have connections with students’ prior knowledge. To better understand and address students’ misconceptions and other difficulties, various instructional approaches and tools have been developed. Nevertheless, the dissemination of these approaches and tools has been limited. Thus, first, we suggest enhancing the dissemination of existing tools and approaches and investigating their long-term effects. Second, we recommend that computing education research move beyond documenting misconceptions to address the development of students’ (mis)conceptions by integrating conceptual change theories. Third, we believe that developing and enhancing instructors’ pedagogical content knowledge (PCK), including their knowledge of students’ misconceptions and ability to apply effective instructional approaches and tools to address students’ difficulties, is vital to the success of teaching introductory programming.},
journal = {ACM Trans. Comput. Educ.},
month = {oct},
articleno = {1},
numpages = {24},
keywords = {Misconceptions, conceptual change, constructivism, difficulties, introductory programming}
}

@article{10.1007/s100090100054,
author = {Engblom, Jakob and Ermedahl, Andreas and Sj\"{o}din, Mikael and Gustafsson, Jan and Hansson, Hans},
title = {Worst-case execution-time analysis for embedded real-time systemsTHANKSREF="*"ID="*"This work was performed within the Advanced Software Technology (ASTEC, http: //www.astec.uu.se) competence center, supported by the Swedish National Board for Industrial and Technical Development (NUTEK, http://www.nutek.se).},
year = {2003},
issue_date = {August    2003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {4},
number = {4},
issn = {1433-2779},
url = {https://doi.org/10.1007/s100090100054},
doi = {10.1007/s100090100054},
abstract = {In this article we give an overview of the worst-case execution time (WCET) analysis research performed by the WCET group of the ASTEC Competence Centre at Uppsala University. Knowing the WCET of a program is necessary when designing and verifying real-time systems. The WCET depends both on the program flow, such as loop iterations and function calls, and on hardware factors, such as caches and pipelines. WCET estimates should be both safe (no underestimation allowed) and tight (as little overestimation as possible). We have defined a modular architecture for a WCET tool, used both to identify the components of the overall WCET analysis problem, and as a starting point for the development of a WCET tool prototype. Within this framework we have proposed solutions to several key problems in WCET analysis, including representation and analysis of the control flow of programs, modeling of the behavior and timing of pipelines and other low-level timing aspects, integration of control flow information and low-level timing to obtain a safe and tight WCET estimate, and validation of our tools and methods. We have focussed on the needs of embedded real-time systems in designing our tools and directing our research. Our long-term goal is to provide WCET analysis as a part of the standard tool chain for embedded development (together with compilers, debuggers, and simulators). This is facilitated by our cooperation with the embedded systems programming-tools vendor IAR Systems.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = {aug},
pages = {437–455},
numpages = {19},
keywords = {Embedded systems, Hard real-time, Programming tools, Software architecture, WCET analysis}
}

@inproceedings{10.1145/3361721.3361729,
author = {Tsarava, Katerina and Leifheit, Luzia and Ninaus, Manuel and Rom\'{a}n-Gonz\'{a}lez, Marcos and Butz, Martin V. and Golle, Jessika and Trautwein, Ulrich and Moeller, Korbinian},
title = {Cognitive Correlates of Computational Thinking: Evaluation of a Blended Unplugged/Plugged-In Course},
year = {2019},
isbn = {9781450377041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361721.3361729},
doi = {10.1145/3361721.3361729},
abstract = {Coding as a practical skill and computational thinking (CT) as a cognitive ability have become an important topic in education and research. It has been suggested that CT, as an early predictor of academic success, should be introduced and fostered early in education. However, there is no consensus on the underlying cognitive correlates of CT in young elementary school children. Therefore, the present work aimed at (i) assessing CT and investigating its associations to established cognitive abilities, and (ii) evaluating a newly developed CT course for elementary school children.As such, 31 7-10-year-old children took part in 10 lessons of a structured CT course. The course aimed at introducing and fostering CT concepts in both unplugged and plugged-in ways, incorporating life-size board games, Scratch, Scratch for Arduino, and Open Roberta programming environments. In a pre-/post-test design, we assessed several cognitive abilities using standardized tests on nonverbal-visuospatial and verbal reasoning abilities, numeracy, as well as short-term memory, and measured CT using an adapted version of the only existing validated test CTt, to accommodate it to the younger sample.We identified significant associations between CT and nonverbal-visuospatial reasoning, as well as different aspects of numeracy (e.g., fact retrieval and problem completion). In line with recent theoretical accounts and empirical investigations for other age groups, these findings specify the underlying cognitive mechanism of CT in elementary school. Moreover, our results indicated that students were able to specifically improve their CT abilities through the course, as assessed by the adapted version of the CTt.},
booktitle = {Proceedings of the 14th Workshop in Primary and Secondary Computing Education},
articleno = {24},
numpages = {9},
keywords = {cognitive skills, computational thinking, computational thinking assessment, computational thinking curriculum},
location = {Glasgow, Scotland, Uk},
series = {WiPSCE '19}
}

@inproceedings{10.1109/ICHIT.2008.243,
author = {Heo, Sun-Young and Kim, Eun-Gyung},
title = {A Study on the Improvement of Query Processing Performance of OWL Data Based on Jena2},
year = {2008},
isbn = {9780769533285},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICHIT.2008.243},
doi = {10.1109/ICHIT.2008.243},
abstract = {The present web has not provided the function of more than the information store in that the collected and retrieved information should be interpreted and be purified though the present Web has a merit that can search correlated information easily through simple connections between documents. So, to overcome the limitation of the existing web like this, researches for Semantic web ontology languages (RDF, RDFS, OWL and so on) prescribed as standard of W3C and for the related technologies have conducted actively. Also, recently the interest for Jena2, Java framework, including rule-based inference engine and the programming environment for the languages like RDF, FDFS, OWL, SPARQL and so on has been heightened. Jena2 has problems that the performance for simple selection operation and the performance for the query that join operations are required slow down, and the performance in processing OWL data slows down because storing document information in a single table. This paper designed and realized OWL Translator to move to the relational database designing in this paper the data being in Multiple Translator and Jena Relational Database for storing each data information in a separate table after dividing the meaning of OWL documents into Class, Property, Individual to solve this problem. Also, this paper compared the processing performances of the engine quality realizing by using Multiple Translator and OWL Translator and the engine realizing by applying as it is the functions providing in Jena2 by using University Ontology providing in SPARQL engine of Jena2.},
booktitle = {Proceedings of the 2008 International Conference on Convergence and Hybrid Information Technology},
pages = {678–681},
numpages = {4},
keywords = {ERD, Inference Engine, Jena, OWL, OWL-QL, Ontology, Semantic Web},
series = {ICHIT '08}
}

@inproceedings{10.1007/978-3-030-77961-0_60,
author = {Murphy, Erin and Rasin, Alexander and Furst, Jacob and Raicu, Daniela and Tchoua, Roselyne},
title = {Ensemble Labeling Towards Scientific Information Extraction (ELSIE)},
year = {2021},
isbn = {978-3-030-77960-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77961-0_60},
doi = {10.1007/978-3-030-77961-0_60},
abstract = {Extracting scientific facts from unstructured text is difficult due to challenges specific to the complexity of the scientific named entities and relations to be extracted. This problem is well illustrated through the extraction of polymer names and their properties. Even in the cases where the property is a temperature, identifying the polymer name associated with the temperature may require expertise due to the use of complicated naming conventions and by the fact that new polymer names are being “introduced” into the lexicon as polymer science advances. While domain-specific machine learning toolkits exist that address these challenges, perhaps the greatest challenge is the lack of—time-consuming, error-prone and costly—labeled data to train these machine learning models. This work repurposes Snorkel, a data programming tool, in a novel approach as a way to identify sentences that contain the relation of interest in order to generate training data, and as a first step towards extracting the entities themselves. By achieving 94% recall and an F1 score of 0.92, compared to human experts who achieve 77% recall and an F1 score of 0.87, we show that our system captures sentences missed by both a state-of-the-art domain-aware natural language processing toolkit and human expert labelers. We also demonstrate the importance of identifying the complex sentences prior to extraction by comparing our application to the natural language processing toolkit.},
booktitle = {Computational Science – ICCS 2021: 21st International Conference, Krakow, Poland, June 16–18, 2021, Proceedings, Part I},
pages = {750–764},
numpages = {15},
keywords = {Information extraction, Data labeling, Relations extraction, Snorkel, Data programming, Polymers},
location = {Krakow, Poland}
}

@inproceedings{10.1145/3328778.3372680,
author = {Williams, Renaldo and Garcia, Dan},
title = {CodeKey - An Online Code Editor to Study Code Patterns and Enhance Student Performance in CS Courses},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3372680},
doi = {10.1145/3328778.3372680},
abstract = {In the past several years, there has been an increase in web-based compilers that allow students to learn how to code using a browser. Many Universities use online code editors for their large Computer Science (CS) courses. For example, the CS200 course at UC Berkeley uses Jupyter Notebooks to teach Python for data science to 800+ students. All the students in the course must write and submit their code assignments in the web-browser. These online code editors for large CS courses presents several benefits. One benefit is that it becomes easier to monitor the steps that a student takes to solve a coding problem since keystrokes can be tracked using Javascript. Another benefit is that the code written by students can be stored in one central database, creating less barriers for code analysis. The CodeKey project aims to take advantage of analyzing code patterns of students in a CS course in order to find key insights. CodeKey aims to find these insights by monitoring the interactions (i.e. clicks and keystrokes) of students as each student attempts to solve a coding problem. The goal is to study the code patterns of students in a CS course in order to understand similarities and differences between students who perform well on a problem and students who do not. We also aim to study how revealing these coding patterns to a student can increase his understanding of how to solve a difficult coding problem by showing common mistakes, and by showing simple steps that lead to the correct solution.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1357},
numpages = {1},
keywords = {computer science education, intelligent tutor systems, web-compilers},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/3481312.3481318,
author = {Strong, Glenn and North, Ben},
title = {Pytch — an environment for bridging block and text programming styles (Work in progress)},
year = {2021},
isbn = {9781450385718},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3481312.3481318},
doi = {10.1145/3481312.3481318},
abstract = {Block-based programming languages, and Scratch in particular, are widely used to introduce young learners to programming. As these students progress through their education, they want or need to transition to using text-based systems and encounter a number of challenges as they do so. Issues with syntax, text editing, and memorisation are all significant, but the change of programming paradigm is also a challenge. This paper discusses the design and development of a system to help students make the transition to text-based programming environments more easily. Sprites, animations and sound form the basis of most Scratch programs and these engaging features become unavailable at the same time as students are facing transition difficulties related to text editing and program structure. From programs designed in an actor-based event-driven system with easy concurrency, students have to move to procedural or class-based programs where multimedia features are accessed quite differently and programs are designed around explicit event loops. In this paper we introduce a new programming system, Pytch, which embodies “Scratch-Oriented programming” in Python. Using a web-based environment that requires no local setup, students can build Python programs using the familiar sprites and concurrent event-driven model learned in Scratch. The system offers the programming model inspired by Scratch through a Python library and a runtime augmented with a form of managed concurrency. The motivation and related work are discussed, and the system is presented in its current form. The next stage will be to evaluate the effectiveness of the system with users.},
booktitle = {Proceedings of the 16th Workshop in Primary and Secondary Computing Education},
articleno = {22},
numpages = {4},
keywords = {Education, Games, Programming languages, Python, Scratch},
location = {Virtual Event, Germany},
series = {WiPSCE '21}
}

@inproceedings{10.1145/800179.810207,
author = {Miller, Mark L. and Goldstein, Ira P.},
title = {Problem solving grammars as formal tools for intelligent CAI},
year = {1977},
isbn = {9781450339216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800179.810207},
doi = {10.1145/800179.810207},
abstract = {AI-CAI, the application of artificial intelligence techniques to the design of personal learning environments, is an enterprise encompassing both theoretical and practical concerns. In the short term, the process of developing and testing intelligent tutoring programs serves as a new experimental vehicle for exploring alternative cognitive and pedagogical theories. In the long term, such programs will supplement the educational supervision and guidance provided by human teachers.The lesson of AI-CAI to date has been that the critical component for a successful system is a model of the expertise to be conveyed which is modular, comprehensible, and articulate. Hence, as a step toward an AI-CAI tutor for elementary graphics programming, a rule-based theory of the planning and debugging of programs is explored.The rules are formalized as a context free grammar. This grammar is used to reveal the constituent structure of problem solving episodes, by parsing protocols in which programs are written, tested and debugged. This is illustrated by the analysis of a session with a beginning student. The virtues of the approach for constructing models of individual students' skills are discussed; limitations and extensions of the approach are also considered.},
booktitle = {Proceedings of the 1977 Annual Conference},
pages = {220–226},
numpages = {7},
keywords = {Artificial intelligence, Computer aided instruction, Computer science education, Computer uses in education, Context free grammars, Information processing psychology, Logo, Personal learning environments, Planning and debugging, Problem solving, Program understanding, Programming environments, Protocol analysis, Structured programming},
location = {Seattle, Washington},
series = {ACM '77}
}

@article{10.1023/B:EMSE.0000027778.69251.1f,
author = {Vok\'{a}\v{c}, Marek and Tichy, Walter and Sj\o{}berg, Dag I. K. and Arisholm, Erik and Aldrin, Magne},
title = {A Controlled Experiment Comparing the Maintainability of Programs Designed with and without Design Patterns—A Replication in a Real Programming Environment},
year = {2004},
issue_date = {September 2004},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1023/B:EMSE.0000027778.69251.1f},
doi = {10.1023/B:EMSE.0000027778.69251.1f},
abstract = {Software “design patterns” seek to package proven solutions to design problems in a form that makes it possible to find, adapt and reuse them. To support the industrial use of design patterns, this research investigates when, and how, using patterns is beneficial, and whether some patterns are more difficult to use than others. This paper describes a replication of an earlier controlled experiment on design patterns in maintenance, with major extensions. Experimental realism was increased by using a real programming environment instead of pen and paper, and paid professionals from multiple major consultancy companies as subjects. Measurements of elapsed time and correctness were analyzed using regression models and an estimation method that took into account the correlations present in the raw data. Together with on-line logging of the subjects’ work, this made possible a better qualitative understanding of the results. The results indicate quite strongly that some patterns are much easier to understand and use than others. In particular, the Visitor pattern caused much confusion. Conversely, the patterns Observer and, to a certain extent, Decorator were grasped and used intuitively, even by subjects with little or no knowledge of patterns. The implication is that design patterns are not universally good or bad, but must be used in a way that matches the problem and the people. When approaching a program with documented design patterns, even basic training can improve both the speed and quality of maintenance activities.},
journal = {Empirical Softw. Engg.},
month = {sep},
pages = {149–195},
numpages = {47},
keywords = {Controlled experiment, design patterns, qualitative results, real programming environment}
}

@article{10.1007/s10639-023-11625-8,
author = {Dilmen, Kaan and Kert, Serhat Bahad\i{}r and U\u{g}ra\c{s}, Tuba},
title = {Children’s coding experiences in a block-based coding environment: a usability study on code.org},
year = {2023},
issue_date = {Sep 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {9},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-023-11625-8},
doi = {10.1007/s10639-023-11625-8},
abstract = {Programming education is an important educational process that enables the development of children's problem solving and algorithmic thinking skills. It is known that children frequently encounter syntax problems in coding activities. Many block-based programming software has been developed to eliminate this difficulty in the learning process. Block-based programming software is widely used all over the world because of its colorful features and providing a coding environment that children can learn easily. However, analyzes on the usefulness of such widely used block-based programming software cannot be found in the literature. In this study, the usability of code.org block-based coding environment was analyzed through the coding practices of children. The study group was consisted of 14 children aged between 9 and 13. Analyzes were made in terms of efficacy, efficiency, and satisfaction. For the efficacy analysis of the programming environment, it was observed that all the children completed the tasks assigned to them. In efficiency analysis; task times, task step counts, need for assistance in the process of using software, overall focus data, heat maps, eye scanning data and focus levels in the guided area of the participants were examined. In satisfaction analysis; satisfaction level of participants was examined. As a result of the research; usability data for Code.org environment has been tried to be presented in detail. In the efficacy dimension, while there were generally no problems regarding the task completion status of the participants; in efficiency dimension, suggestions were made regarding the placement of the blocks, block sizes and application methods. In satisfaction dimension, it was seen that children faced with problems during the block search process.},
journal = {Education and Information Technologies},
month = {feb},
pages = {10839–10864},
numpages = {26},
keywords = {Coding education, Programming, Usability, Block-based environment, Secondary education}
}

@article{10.1006/ijhc.1994.1032,
author = {Davies, Simon P.},
title = {Knowledge restructuring and the acquisition of programming expertise},
year = {1994},
issue_date = {April 1994},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {40},
number = {4},
issn = {1071-5819},
url = {https://doi.org/10.1006/ijhc.1994.1032},
doi = {10.1006/ijhc.1994.1032},
abstract = {This paper explores the relationship between knowledge structure and organization and the development of expertise in a complex problem-solving task. An empirical study of skill acquisition in computer programming is reported, providing support for a model of knowledge organization that stresses the importance of knowledge restructuring processes in the development of expertise. This is contrasted with existing models which have tended to place emphasis upon schemata acquisition and generalization as the fundamental modes of learning associated with skill development. The work reported in this paper suggests that a fine-grained restructuring of individual schemata takes place during the later stages of skill development. It is argued that those mechanisms currently thought to be associated with the development of expertise may not fully account for the strategic changes and the types of error typically found in the transition between intermediate and expert problem solvers. This work has a number of implications. Firstly, it suggests important limitations of existing theories of skill acquisition. This is particularly evident in terms of the ability of such theories to account for subtle changes in the various manifestations of skilled performance that are associated with increasing expertise. Secondly, the work reported in this paper attempts to show how specific forms of training can give rise to this knowledge restructuring process. It is argued that the effects of particular forms of training are of primary importance, but these effects are often given little attention in theoretical accounts of skill acquisition. Finally, the work presented here has practical relevance in a number of applied areas including the design of intelligent tutoring systems and programming environments.},
journal = {Int. J. Hum.-Comput. Stud.},
month = {apr},
pages = {703–726},
numpages = {24}
}

@inproceedings{10.1145/3545947.3569611,
author = {Malan, David J. and Carter, Jonathan and Liu, Rongxin and Zenke, Carter},
title = {Providing Students with Standardized, Cloud-Based Programming Environments at Term's Start (for Free)},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3569611},
doi = {10.1145/3545947.3569611},
abstract = {For CS50 at Harvard, we have long provided students with a standardized programming environment, to avoid start-of-term technical difficulties that might otherwise arise if students had to install and configure compilers, interpreters, and debuggers on their own Macs and PCs. (For many students, "hello, world" is challenge enough on day 0, without also encountering "command not found" at the same time!) We originally provided students with shell accounts on a university-managed cluster of systems. We then transitioned to a cloud-based equivalent so as to manage the systems ourselves, root access and all. We transitioned thereafter to client-side virtual machines, to scale to more students and enable GUI-based assignments. We have since transitioned to web-based environments, complete with code tabs, terminal windows, and file explorers, initially implemented atop AWS Cloud9 and now, most recently, GitHub Codespaces, an implementation of Visual Studio (VS) Code in the cloud, free for teachers and students alike. In this workshop, we'll discuss the pedagogical and technological advantages and disadvantages of every approach and focus most of our time, hands-on, on using and configuring GitHub Codespaces itself for teaching and learning. Along the way, attendees will learn how to create their own Docker images and "devcontainers" for their own classes and any languages they teach. Attendees will learn what is possible educationally by writing their own VS Code extensions as well. And how, at term's end, to "offboard" students to VS Code itself on their own Macs and PCs, so as to continue programming independent of Codespaces.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1183},
numpages = {1},
keywords = {cli, code, code editor, command-line interface, container, docker, editor, graphical user interface, gui, ide, integrated development environment, programming, terminal window, text editor, web app, web application},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@phdthesis{10.5555/909456,
author = {Van Houten, Karen Jane Hamilton},
title = {Kl, a well-structured beginning computer language},
year = {1980},
publisher = {University of Idaho},
address = {USA},
abstract = {One of the most exciting aspects of modern computer science is the transformation of programming from an art to a science. Program reliability, software management, and structured programming are all helping produce quality programs. An important contribution to the ever increasing excellence of software is the improvement of computer science education at the earliest levels. A new programming language, KL, has been designed and developed specifically for use in introductory computer science courses. The developmental goals of KL were: (1) inclusion of structured-programming tools, (2) simplicity for the user, (3) straightforward implementation, and (4) adaptability to small computers.These goals were accomplished in a number of ways. The structure of KL requires that each statement begin with a keyword and end with a semi-colon. This decreases the implementation effort and adds to the simplicity of the language. This form also eliminates the necessity for reserved words which often cause difficulty for the user. The keywords have been chosen to be meaningful and, when possible, short. Restricting the number of different statements available supports all four goals.The tools needed for abstraction are included in KL. User-defined data types are available in KL. Procedures and functions are also available for dividing a computer problem into subtasks.KL is a suitable language for teaching programming for several reasons. It requires structured programming techniques because no other constructs are available. This should start good programming habits. The concepts of structured programming can be learned using KL and later applied to other less structured languages. The concepts of structured programming are language independent and KL enables the learning of these concepts early in a computer scientist's career.},
note = {AAI8019797}
}

@inproceedings{10.1145/3626253.3633427,
author = {Malan, David J. and Liu, Rongxin and Zenke, Carter and Lloyd, Doug},
title = {Providing Students with Standardized, Cloud-Based Programming Environments at Term's Start (for Free)},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633427},
doi = {10.1145/3626253.3633427},
abstract = {For CS50 at Harvard, we have long provided students with a standardized programming environment, to avoid start-of-term technical difficulties that might otherwise arise if students had to install and configure compilers, interpreters, and debuggers on their own Macs and PCs. (For many students, "hello, world" is challenge enough on day 0, without also encountering "command not found" at the same time!) We originally provided students with shell accounts on a university-managed cluster of systems. We then transitioned to a cloud-based equivalent so as to manage the systems ourselves, root access and all. We transitioned thereafter to client-side virtual machines, to scale to more students and enable GUI-based assignments. We have since transitioned to web-based environments, complete with code tabs, terminal windows, and file explorers, initially implemented atop AWS Cloud9 and now, most recently, GitHub Codespaces, an implementation of Visual Studio (VS) Code in the cloud, free for teachers and students alike. In this workshop, we'll discuss the pedagogical and technological advantages and disadvantages of every approach and focus most of our time, hands-on, on using and configuring GitHub Codespaces itself for teaching and learning. Along the way, attendees will learn how to create their own Docker images and "devcontainers" for their own classes and any languages they teach. Attendees will learn what is possible educationally by writing their own VS Code extensions as well. And how, at term's end, to "offboard" students to VS Code itself on their own Macs and PCs, so as to continue programming independent of Codespaces.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1903},
numpages = {1},
keywords = {cli, code, code editor, command-line interface, container, docker, editor, graphical user interface, gui, ide, integrated development environment, programming, terminal window, text editor, web app, web application},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@article{10.1145/3563296,
author = {Ritschel, Nico and Fronchetti, Felipe and Holmes, Reid and Garcia, Ronald and Shepherd, David C.},
title = {Can guided decomposition help end-users write larger block-based programs? a mobile robot experiment},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563296},
doi = {10.1145/3563296},
abstract = {Block-based programming environments, already popular in computer science education, have been successfully used to make programming accessible to end-users in domains like robotics, mobile apps, and even DevOps. Most studies of these applications have examined small programs that fit within a single screen, yet real-world programs often grow large, and editing these large block-based programs quickly becomes unwieldy. Traditional programming language features, like functions, allow programmers to decompose their programs. Unfortunately, both previous work, and our own findings, suggest that end-users rarely use these features, resulting in large monolithic code blocks that are hard to understand. In this work, we introduce a block-based system that provides users with a hierarchical, domain-specific program structure and requires them to decompose their programs accordingly. Through a user study with 92 users, we compared this approach, which we call guided program decomposition, to a traditional system that supports functions, but does not require decomposition. We found that while almost all users could successfully complete smaller tasks, those who decomposed their programs were significantly more successful as the tasks grew larger. As expected, most users without guided decomposition did not decompose their programs, resulting in poor performance on larger problems. In comparison, users of guided decomposition performed significantly better on the same tasks. Though this study investigated only a limited selection of tasks in one specific domain, it suggests that guided decomposition can benefit end-user programmers. While no single decomposition strategy fits all domains, we believe that similar domain-specific sub-hierarchies could be found for other application areas, increasing the scale of code end-users can create and understand.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {133},
numpages = {26},
keywords = {block-based programming, mobile robots, program decomposition}
}

@inproceedings{10.1145/3551349.3556939,
author = {Feldmeier, Patric and Fraser, Gordon},
title = {Neuroevolution-Based Generation of Tests and Oracles for Games},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556939},
doi = {10.1145/3551349.3556939},
abstract = {Game-like programs have become increasingly popular in many software engineering domains such as mobile apps, web applications, or programming education. However, creating tests for programs that have the purpose of challenging human players is a daunting task for automatic test generators. Even if test generation succeeds in finding a relevant sequence of events to exercise a program, the randomized nature of games means that it may neither be possible to reproduce the exact program behavior underlying this sequence, nor to create test assertions checking if observed randomized game behavior is correct. To overcome these problems, we propose Neatest, a novel test generator based on the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. Neatest systematically explores a program’s statements, and creates neural networks that operate the program in order to reliably reach each statement—that is, Neatest learns to play the game in a way to reliably cover different parts of the code. As the networks learn the actual game behavior, they can also serve as test oracles by evaluating how surprising the observed behavior of a program under test is compared to a supposedly correct version of the program. We evaluate this approach in the context of Scratch, an educational programming environment. Our empirical study on 25 non-trivial Scratch games demonstrates that our approach can successfully train neural networks that are not only far more resilient to random influences than traditional test suites consisting of static input sequences, but are also highly effective with an average mutation score of more than 65%.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {72},
numpages = {13},
keywords = {Automated Testing, Game Testing, Neuroevolution, Scratch},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@article{10.5555/3205191.3205192,
author = {Manaris, Bill},
title = {Computing in the arts: the algorithm is the medium},
year = {2018},
issue_date = {June 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {33},
number = {6},
issn = {1937-4771},
abstract = {Algorithms have existed for at least 2,000 years (e.g., Euclid's algorithm). In music and art, algorithms appear as early as Guido d'Arezzo (ca. 1000 A.D.), and in compositions by Bach, Mozart, John Cage, Iannis Xenakis, among others. Modern examples include data sonification for scientific or aesthetic purposes, such as sonifying biosignals, images, orbits of planets, and human movement (e.g., dance), among others. This talk will focus on Computing in the Arts (CITA), an NSF-funded model curriculum, which combines creativity, problem solving, and computer programming to prepare students for graduate school and careers in technology and arts industries of the 21st century. CITA is part of the new movement to combine art and design with science, technology, engineering and math (STEM + Art = STEAM). Several examples will be presented, including:• SoundMorpheus (an innovative interface for positioning sounds via arm movements);• Diving into Infinity (a motion-based system which explores depictions of infinity in M.C. Escher's works); and• JythonMusic (a programming environment for developing interactive music experiences and systems).Bill Manaris is Professor of Computer Science, and Director of the Computing in the Arts program at the College of Charleston. His areas of expertise include computer music, human-computer interaction and artificial intelligence. He explores interaction design, modeling of aesthetics and creativity, sound spatialization, and telematics. As an undergraduate, he studied computer science and music at the University of New Orleans, and holds M.S. and Ph.D. degrees in Computer Science from the University of Louisiana. He also studied classical and jazz guitar. Recently, he published a textbook in Computer Music and Creative Programming. His research has been supported by the National Science Foundation, Google, IBM, the Louisiana Board of Regents, and the Stavros Niarchos Foundation.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {5–6},
numpages = {2}
}

@article{10.5555/1629116.1629136,
author = {K\"{o}lling, Michael},
title = {Greenfoot: introduction to Java with games and simulations},
year = {2010},
issue_date = {January 2010},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {25},
number = {3},
issn = {1937-4771},
abstract = {• This tutorial will demonstrate Greenfoot, a programming environment developed by the creators of BlueJ, that allows teaching of object-oriented programming concepts -- using Java -- in a highly engaging and motivating context.• One of the major problems in teaching computing today is the lack of interest in many young people. Computing suffers from a disastrous reputation (to a great extent wrongly). It is perceived as boring, geeky, unrewarding, and antisocial. Popular preconceptions of overweight male teenagers with thick glasses and skin problems sitting alone in windowless cellar rooms in front of a computer screen with pizza boxes strewn around them do little to attract a more diverse group to computer science.• Greenfoot is designed to enable teachers to bring fun and engagement back into computing, while teaching real programming concepts, and without trivializing the subject matter. Students quickly start to program graphical, interactive applications, such as games and simulations. A very diverse set of possible projects and sample programs serves to attract groups of students who would not normally take an interest in programming.• The Greenfoot environment provides tools for students to achieve real successes quickly, while providing tools for teachers to illustrate and discuss fundamental concepts of object-oriented programming. Once completed, student work can easily be published and shared on the Internet.• Greenfoot should be of interest to anyone teaching Java, especially in early programming courses, at schools and colleges.• Greenfoot is available from www.greenfoot.org. The tutorial is practically oriented and allows participants to use Greenfoot in their classroom immediately. Audience members with laptops will be encouraged to play along during the tutorial, giving a chance to get some first-impression, hands-on experience with the software during the session.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {117},
numpages = {1}
}

@inproceedings{10.5555/1369599.1369663,
author = {Zong, Nuannuan and Adjouadi, Malek and Ayala, Melvin},
title = {Artificial neural networks approaches for multidimensional classification of acute lymphoblastic leukemia gene expression samples},
year = {2005},
isbn = {9608457297},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Accurate classification of human blood cells plays a decisive role in the diagnosis and treatment of diseases. Artificial Neural Networks (ANNs) have been consistently used as a trusted classification tool for this type of analysis. In the present case study, two approaches are implemented on two different parametric data clusters in a multidimensional space using ANNs trained with cross-validation. Beckman-Coulter Corporation supplied flow cytometry data of numerous patients as training sets for the first approach to exploit the physiological characteristics of the different blood cells provided. The goal was to establish a programming tool for the identification of different white blood cell categories of a given blood sample and provide information to medical doctors in the form of diagnostic references for the specific disease state that is considered for this study, namely Acute Lymphoblastic Leukemia (ALL). Successful initial results of this first approach have been published. The second approach is focusing on the gene expression profiling of ALL to classify its six subtypes. Generated by the oligonucleotide microarrays, this data provides additional insights into the biology underlying the clinical differences between these leukemia subgroups. With the application of the hypothesis space, along with the learning bias, the system is also trained to assess the inherent problem of data overlap and be able to recognize abnormal blood cell patterns. An analysis of the systems regarding computational load and receiver-operating characteristic (ROC) was conducted. The algorithms as proposed provide solutions to data overlap from our initial results. And by applying ANNs, the classification accuracy of the first approach is remarkably improved up to 100% and 92% for the second approach.},
booktitle = {Proceedings of the 9th WSEAS International Conference on Computers},
articleno = {64},
numpages = {6},
keywords = {acute lymphoblastic leukemia (ALL), artificial neural networks (ANNs), cross-validation, gene expression profiling, microarrays, receiver operating characteristics (ROC) analysis, white blood cell},
location = {Athens, Greece},
series = {ICCOMP'05}
}

@article{10.1145/3427596,
author = {Kim, Han Sung and Kim, Soohwan and Na, Wooyoul and Lee, Woon Jee},
title = {Extending Computational Thinking into Information and Communication Technology Literacy Measurement: Gender and Grade Issues},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
url = {https://doi.org/10.1145/3427596},
doi = {10.1145/3427596},
abstract = {As Information and Communication Technology (ICT) literacy education has recently shifted to fostering computing thinking ability as well as ICT use, many countries are conducting research on national curriculum and evaluation. In this study, we measured Korean students’ ICT literacy levels by using the national measurement tool that assesses abilities of the IT (Information Technology) area and the CT (Computational Thinking) area. A research team revised an existing ICT literacy assessment tool for the IT test and developed a new CT test environment in which students could perform actual coding through a web-based programming tool such as Scratch. Additionally, after assessing ICT literacy levels, differences in ICT literacy levels by gender and grade were analyzed to provide evidence for national education policies. Approximately 23,000 elementary and middle school students participated in the 2018 national assessment of ICT literacy, accounting for 1% of the national population of students. The findings demonstrated that female students had higher literacy levels in most sub-factors of IT and CT areas. Additionally, in the areas of strengths and weaknesses, the ratio of below-basic achievement among male students was at least two times greater than that of female students. Nonetheless, male students scored higher on CT automation, a coding item that involved problem solving using Scratch. Looking at the difference according to grade level, the level improved as the school year increased in elementary school, but there was no difference in middle school. When analyzing the detailed elements of middle school students, the automation factor of seventh grade students was found to be higher than eighth and ninth grade students. Based on these results, this study discussed some implications for ICT and computing education in elementary and middle schools.},
journal = {ACM Trans. Comput. Educ.},
month = {jan},
articleno = {5},
numpages = {25},
keywords = {21st century abilities, ICT literacy, computational thinking, elementary education, secondary education}
}

@phdthesis{10.5555/1236791,
author = {Muezzinoglu, Mehmet K.},
advisor = {Zurada, Jacek M.},
title = {Approximate dynamic programming for anemia management},
year = {2006},
isbn = {9780542825583},
publisher = {University of Louisville},
address = {USA},
abstract = {The focus of this dissertation work is the formulation and improvement of anemia management process involving trial-and-error. A two-stage method is adopted toward this objective. Given a medical treatment process, a discrete Markov representation is first derived as a formal translation of the treatment process to a control problem under uncertainty. A simulative numerical solution of the control problem is then obtained on-the-fly in the form of a control law maximizing the long-term benefit at each decision stage. Approximate dynamic programming methods are employed in the proposed solution. The motivation underlying this choice is that, in reality, some patient characteristics, which are critical for the sake of treatment, cannot be determined through diagnosis and remain unknown until early stages of treatment, when the patient demonstrates them upon actions by the decision maker. A review of these simulative control tools, which are studied extensively in reinforcement learning theory, is presented. Two approximate dynamic programming tools, namely SARSA and  Q -learning, are introduced. Their performance in discovering the optimal individualized drug dosing policy is illustrated on hypothetical patients made up as fuzzy models for simulations. As an addition to these generic reinforcement learning methods, a state abstraction scheme for the considered application domain is also proposed. The control methods of this study, capturing the essentials of a drug delivery problem, constitutes a novel computational framework for model-free medical treatment. Experimental evaluation of the dosing strategies produced by the proposed methods against the standard policy, which is being followed actually by human experts in Kidney Diseases Program, University of Louisville, shows the advantages for use of reinforcement learning in the drug dosing problem in particular and in medical decision making in general.},
note = {AAI3228046}
}

@inproceedings{10.1145/268084.268090,
author = {Hendrix, T. Dean and Barowski, Larry A. and Cross, James H.},
title = {A visual development environment for multi-lingual curricula},
year = {1997},
isbn = {0897918894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/268084.268090},
doi = {10.1145/268084.268090},
abstract = {Although a computer science curriculum may use a single language as its "core" language, many curricula require students to learn and use multiple languages for course or practicum work. Students benefit from the exposure to other languages and other language models. However, a problem arising from the multi-lingual nature of a curriculum is the necessity to learn and use different development environments and language front-ends. GRASP (Graphical Representations of Algorithms, Structures, and Processes) is a software engineering tool currently being successfully utilized as a common development environment for the multi-lingual computer science curriculum at Auburn University. Besides providing a common front-end for different languages, GRASP also provides automated visualization of source code in the form of the control structure diagram and the complexity profile graph. This paper describes GRASP and its current use in the computer science curriculum. GRASP is freely available via the Internet at the following URL: http://www.eng.auburn.edu/grasp},
booktitle = {Proceedings of the Twenty-Eighth SIGCSE Technical Symposium on Computer Science Education},
pages = {20–24},
numpages = {5},
location = {San Jose, California, USA},
series = {SIGCSE '97}
}

@phdthesis{10.5555/193426,
author = {Repenning, Alexander},
title = {Agentsheets: a tool for building domain-oriented dynamic, visual environments},
year = {1993},
publisher = {University of Colorado at Boulder},
address = {USA},
abstract = {Cultures deal with their environments by adapting to them and simultaneously changing them. This is particularly true for technological cultures, such as the dynamic culture of computer users. To date, the ability to change computing environments in non-trivial ways has been dependent upon the skill of programming. Because this skill has been hard to acquire, most computer users must adapt to computing environments created by a small number of programmers. In response to the scarcity of programming ability, the computer science community has concentrated on producing general-purpose tools that cover wide spectrums of applications. As a result, contemporary programming languages largely ignore the intricacies arising from complex interactions between different people solving concrete problems in specific domains.This dissertation describes Agentsheets, a substrate for building domain-oriented, visual, dynamic programming environments that do not require traditional programming skills. It discusses how Agentsheets supports the relationship among people, tools, and problems in the context of four central themes: (1) Agentsheets features a versatile construction paradigm to build dynamic, visual environments for a wide range of problem domains such as art, artificial life, distributed artificial intelligence, education, environmental design, and computer science theory. The construction paradigm consists of a large number of autonomous, communicating agents organized in a grid, called the agentsheet. Agents utilize different communication modalities such as animation, sound, and speech. (2) The construction paradigm supports the perception of programming as problem solving by incorporating mechanisms to incrementally create and modify spatial and temporal representations. (3) To interact with a large number of autonomous entities Agentsheets postulates participatory theater, a human-computer interaction scheme combining the advantages of direct manipulation and delegation into a continuous spectrum of control and effort. (4) Metaphors serve as mediators between problem solving-oriented construction paradigms and domain-oriented applications. Metaphors are used to represent application semantics by helping people to conceptualize problems in terms of concrete notions. Furthermore, metaphors can simplify the implementation of applications. Application designers can explore and reuse existing applications that include similar metaphors.},
note = {UMI Order No. GAX94-23532}
}

@phdthesis{10.5555/913492,
author = {Mcginnis, Denise R.},
title = {The effects of structured design techniques on software development in a business programming course},
year = {1987},
publisher = {The University of Toledo},
abstract = {A major problem in the area of software development is how to produce a quality software product at the lowest possible cost. Software is fast becoming the largest cost component in a computer information system. Even after large sums of money have been spent on development of software, it may still be incorrect. This is the major reason that the highest percentage of a computing budget of many businesses is spent on maintenance of this software. The purpose of this study was to examine the effects of a design walkthrough upon software development in the college programming environment. Both third and fourth generation languages were used for this research. Students in the experimental group were required to participate in a design walkthrough. Time required to develop a program, compiles used in development, and correctness of the final solution were studied.The sample included 148 undergraduate students enrolled in the College of Business Administration at the University of Toledo. Eight sections of three different courses were randomly assigned to the control or experimental group. Students in the experimental group were asked to design a two-level control break program with one of several design techniques and then participate in a design walkthrough with the course instructor prior to coding the program.There was a significant difference in the correctness of the student's program solutions for both the third and fourth generation languages. Those students in the experimental group developed programs that were more correct than those who did not participate in the design walkthroughs. There was no significant difference in the mean time to develop the software, the mean compiles used in development, or mean time spent with the instructor for the experimental and control groups.The results support the hypothesis that walkthroughs help to reduce the number of errors in completed software. However, the results did not support the hypothesis that the use of structured design techniques reduces the number of resources needed for program development.},
note = {AAI8722200}
}

@book{10.5555/1717944,
author = {Henry, M. and Stevens, H.},
title = {A Primer of Ecology with R},
year = {2009},
isbn = {0387898816},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {Ecology is more quantitative and theory-driven than ever before, and A Primer of Ecology with R combines an introduction to the major theoretical concepts in general ecology with a cutting edge open source tool, the R programming language. Starting with geometric growth and proceeding through stability of multispecies interactions and species-abundance distributions, this book demystifies and explains fundamental ideas in population and community ecology. Graduate students in ecology, along with upper division undergraduates and faculty, will find this to be a useful overview of important topics. In addition to the most basic topics, this book includes construction and analysis of demographic matrix models, metapopulation and source-sink models, host-parasitoid and disease models, multiple basins of attraction, the storage effect, neutral theory, and diversity partitioning. Several sections include examples of confronting models with data. Chapter summaries and problem sets at the end of each chapter provide opportunities to evaluate and enrich one's understanding of the ecological ideas that each chapter introduces. R is rapidly becoming the lingua franca of quantitative sciences, and this text provides a tractable introduction to using the R programming environment in ecology. An appendix provides a general introduction, and examples of code throughout each chapter give readers the option to hone their growing R skills. "The distinctive strength of this book is that truths are mostly not revealed but discovered, in the way that R-savvy ecologistsempirical and theoreticalwork and think now. For readers still chained to spreadsheets, working through this book could be a revolution in their approach to doing science." (Stephen P. Ellner, Cornell University) "One of the greatest strengthsis the integration of ecological theory with examples ... pulled straight from the literature." (James R. Vonesh, Virginia Commonwealth University)}
}

@book{10.5555/1477711,
author = {Randolph, Nick and Gardner, David},
title = {Professional Visual Studio 2008},
year = {2008},
isbn = {0470229888},
publisher = {Wrox Press Ltd.},
address = {GBR},
abstract = {Professional Visual Studio 2008 Microsoft Visual Studio 2008 is the latest version in the ongoing evolution of the Integrated Development Environment (IDE), and this resource examines the diverse facets of the IDEfrom common tasks to intricate functions to the powerful tools that accompany the main code editing and design windows. Written by a unique author duo and offering an in-depth look at the powerful and fascinating features and techniques of the IDE, this book explores each aspect of the development life cycle from the perspective of how Visual Studio 2008 can make your life easier. Each chapter is packed with examples that illustrate uses for various tools, commands, and shortcuts of Visual Studio 2008. You will gradually learn to identify where a feature is used, conclude how you can use it to its fullest potential, and then seamlessly apply that feature to help solve real-world problems. What you will learn from this book How to create project templates and wizards Methods for using IntelliSense, code refactoring, class modeling, and unit testing Tips for using DataSets, LINQ, and Synchronization Services for working with data How to build web applications using ASP.NET AJAX, Silverlight, and ASP.NET MVC Ideas for building Office and Mobile applications, WPF, WCF, and WF projects Ways to effectively analyze and identify bugs using the advanced debugging features How to automate repetitive tasks using the Visual Studio 2008 add-ins and macros Suggestions for using Visual Studio Team System components coupled with Team Foundation Server Techniques for building more secure applications Who this book is for This book is for programmers who want to become proficient with the latest version of Visual Studio and are interested in the advanced capabilities of the IDE. Wrox Professional guides are planned and written by working programmers to meet the real-world needs of programmers, developers, and IT professionals. Focused and relevant, they address the issues technology professionals face every day. They provide examples, practical solutions, and expert education in new technologies, all designed to help programmers do a better job.}
}

@inproceedings{10.1145/3372923.3404798,
author = {Kitromili, Sofia and Jordan, James and Millard, David E.},
title = {What Authors Think about Hypertext Authoring},
year = {2020},
isbn = {9781450370981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372923.3404798},
doi = {10.1145/3372923.3404798},
abstract = {Despite significant research into authoring tools for interactive narratives and a number of established authoring platforms, there is still a lack of understanding around the authoring process itself, and the challenges that authors face when writing hypertext and other forms of interactive narratives. This has led to a monolithic view of authoring, which has hindered tool design, resulting in tools that can lack focus, or ignore important parts of the creative process. In order to understand how authors practise writing, we conducted semi-structured interviews with 20 interactive narrative authors. Using a qualitative analysis, we coded their comments to identify both processes and challenges, and then mapped these against each other in order to understand where issues occurred during the authoring process. In our previous work we were able to gather together a set of authoring steps that were relevant to interactive narratives through a review of the academic literature. Those steps were: Training/Support, Planning, Visualising/Structuring, Writing, Editing, and Compiling/Testing. In this work we discovered two additional authoring steps, Ideation and Publishing that had not been previously identified in our reviews of the academic literature - as these are practical concerns of authors that are invisible to researchers. For challenges we identified 18 codes under 5 themes, falling into 3 phases of development: Pre-production, where issues fall under User/Tool Misalignment and Documentation; Production, adding issues under Complexity and Programming Environment; and Post-production, replacing previous issues with longer term issues related to the narrative's Lifecycle. Our work shows that the authoring problem goes beyond the technical difficulties of using a system, rather it is rooted in the common misalignment between the authors' expectations and the tools capabilities, the fundamental tension between expressivity and complexity, and the invisibility of the edges of the process to researchers and tool builders. Our work suggests that a less monolithic view of authoring would allow designers to create more focused tools and address issues specifically at the places in which they occur.},
booktitle = {Proceedings of the 31st ACM Conference on Hypertext and Social Media},
pages = {9–16},
numpages = {8},
keywords = {authoring, authoring tools, authors, digital interactive narratives, digital interactive storytelling, hypertext fiction, interactive fiction},
location = {Virtual Event, USA},
series = {HT '20}
}

@article{10.1016/S1571-0661(05)80684-X,
author = {Andrew, Gordon and Carolyn, Pitts and Andrew, Talcott},
title = {Preface},
year = {1998},
issue_date = {May 1998},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {10},
number = {C},
issn = {1571-0661},
url = {https://doi.org/10.1016/S1571-0661(05)80684-X},
doi = {10.1016/S1571-0661(05)80684-X},
abstract = {This issue of ENTCS is an unrefereed conference record of talks presented at the Second Workshop on Higher Order Operational Techniques in Semantics (HOOTS II) held at Stanford University, December 8-11, 1997. The meeting was organised by A. Gordon, A. Pitts and C. Talcott with generous sponsorship from Harlequin Ltd, NSF and ONR. The first HOOTS workshop was held October 28-30, 1995 as part of the University of Cambridge Isaac Newton Institute research programme on Semantics of Computation (July-Dec 1995).The study of operational techniques for higher-order languages is now a thriving area, with much research activity going on world-wide. An important open problem is a theory of program equivalence for languages with higher-order features, including functions and objects. Techniques for defining and reasoning about equivalence and other properties of higher-order programs have emerged in distinct communities, including the concurrency, functional programming and type theory communities. The purpose of the HOOTS workshops was to bring researchers from these communities together to discuss current trends in the theory of operational semantics, its application to higher-order languages and its connection with more established semantic techniques.Papers presented at HOOTS II covered a broad range of topics: techniques such as bisimulation and logical relations for reasoning about contextual equivalence alternative program relations such as operational subsumption, and evaluation rules for program contexts operational models including adaptation of big-step evaluation semantics to provide capabilities of small-step and denotational semantics forms, flow graphs, and history dependent automata higher-order programming calculi including: imperative call-by-need lambda calculus, action calculi, process calculi for reasoning about mobility and security, interaction of actors and pi calculus agents approaches to program analysis and verification, including: logics for control flow analysis, monadic type systems, and diagramatic specification notation for actor systems; programming environment tools such a type systems for Java byte-code, and higher-order program units for modularity.Programs and participants lists for HOOTS I and II and other information about HOOTS, past and future can be found here},
journal = {Electron. Notes Theor. Comput. Sci.},
month = {may},
pages = {1},
numpages = {1}
}

@inproceedings{10.5555/792762.793279,
author = {Beynon, Michael D. and Andrade, Henrique and Saltz, Joel},
title = {Low-Cost Non-Intrusive Debugging Strategies for Distributed Parallel Programs},
year = {2002},
isbn = {0769517455},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Debugging is an important and challenging component of the software development cycle. The utilization of proper tools that help trace execution, inspect variable values, do postmortem analysis, dynamically attach to running processes, among other tasks can greatly increase programmer productivity by reducing the time to understand incorrect behavior. It is well known that many more hours are spent debugging software than compiling source code [11], therefore having the appropriate tools and making the best use of information provided by them is fundamental. When a programmer deals with parallel programs in shared or distributed memory settings, debugging becomes quite complicated given the various potential interactions that may occur between threads and processes. Data races, deadlocks, synchronization issues, and communication problems must be dealt with in addition to the traditional sequential (single process) problems such as memory leaks, memory overruns, etc. Since debugging is clearly important in both sequential and parallel/distributed environments, much effort has been focused on this task. It ranges from standardization initiatives [6] and list of requirements [3], to specific research prototypes [5, 14, 15], and commercial and open source products [4, 7, 9, 10, 13].GDB [12] is freely available and omnipresent in most research labs and universities, which contributes to its status as a debugging tool of choice for many programmers writing sequential code. On the other hand, it does not target the particular debugging issues presented by parallel/distributed applications as does, for example, TotalView [4]. Nevertheless, GDB's newer versions are well capable of dealing with multi-threaded programs. There are also many capable parallel/cluster programming environments that go beyond the ideas presented in this paper. We present this work as strategies that can be implemented without capital expenditure 1 somewhat easily into existing projects without adopting a programming environment.In this work, we will show how five low-cost and non-intrusive techniques that work using free commodity tools such as GDB can be used to improve the debugging process of multi-threaded and/or distributed parallel programs. These techniques have been used in the development of two major software middlewares DataCutter [2] and MQO [1] and have proven their value by lowering the time necessary to detect and correct bugs.},
booktitle = {Proceedings of the IEEE International Conference on Cluster Computing},
pages = {439},
series = {CLUSTER '02}
}

@phdthesis{10.5555/2521911,
author = {Deng, Qian},
advisor = {Chen, Youping},
title = {Coarse-graining atomistic dynamics of fracture by finite element method: formulation, parallelization and applications},
year = {2011},
isbn = {9781267377678},
publisher = {University of Florida},
address = {USA},
abstract = {The accurate prediction of material behaviors is one of the most important fields in both science and engineering communities. Understanding the mechanisms of material behaviors sometimes needs us to study the material over a wide span of length scales. In this work we present a new methodology which is able to coarse-grain the atomistic dynamics of fracture by finite element method. First, based on the Atomistic Field Theory (AFT) (Chen and Lee 2005, Chen et al. 2006, Chen 2009), a finite elements method with built in atomistic information is presented. Then a high efficiency parallel code for large scale computation is described. This code was written in FORTRAN language and uses the standard parallel programming environment message passing interface (MPI). The performance of the parallel code was tested on the supercomputer Trestle of SDSC (San Diego Supercomputer Center). Through the comparison of the coarse-grained (CG) simulation results with the molecular dynamics (MD) simulation results, it is found that the new CG method is able to predict the crack tip stress, dynamic crack propagation and even crack branching, with results similar to that of the atomic-level molecular dynamicssimulations. Finally, both 2D (2 dimensional) and 3D (3 dimensional) dynamic fracture problems were computed through the CG method. In 2D dynamic fracture simulations, the relationship between stress waves and crack propagations was studied. It is found that the stress waves reflected back from the boundary can trigger the dynamic crack branching. In 3D simulations, the dynamic fractures under different loading were simulated. The largest 3D model is composed of over 0.1 million elements which are equivalent to over 0.1 billion atoms. To show the performance of the parallel code in dealing with large number of processors, the crack surface evolution in this model was simulated using 512 processors. All of simulations conducted in this study show the robustness of the parallel code in dealing with dynamic fracture problems. (Full text of this dissertation may be available via the University of Florida Libraries web site. Please check http://www.uflib.ufl.edu/etd.html)},
note = {AAI3514944}
}

@article{10.1155/2022/7048698,
author = {Zhou, Jieqiong and Wei, Zhenhua and Shi, Jianwei and Khan, Mohammad Ayoub},
title = {Teaching Application and Evaluation of Ideological and Political Courses Based on Multisource Data Fusion},
year = {2022},
issue_date = {2022},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2022},
issn = {1939-0114},
url = {https://doi.org/10.1155/2022/7048698},
doi = {10.1155/2022/7048698},
abstract = {Domestic education and scientific nature of realization automatic evaluation have become issues of concern. The integration of information technology in the teaching field of ideological and political theory courses in colleges and universities (hereinafter referred to as “ideological and political courses”) inevitably has an important impact on the teaching of traditional ideological and political courses. ASP.NET is the main interface technology of .NET. .NET is an environment that can provide support for building development and execution in multiple languages, and realizes the functions of language development, code compilation, building configuration, program operation, and object interaction. The system development tool is ASP.NET (Visual Studio 2013), the background database development tool is Microsoft SQL Server 2008, and the system development environment is Windows 7 \texttimes{}64. The core of the system is multisource data fusion. Firstly, the data are preprocessed to extract useful information data to form data object fusion. Then, convert the code table into a transaction library. In view of the large amount of data to be mined and easily affect mining quality, the improved Apriori-P algorithm based on partitioning in association with rule technology is applied. This algorithm is used to generate frequent itemsets. According to the given minimum confidence, a data fusion rule is generated. Then, the fusion result is generated from the corresponding data report. Before data fusion, system administrators should select data sources from historical databases or existing evaluation databases. When querying rules, different users can log in to the system to query their own data content, students can query the teacher’s evaluation results, teachers can query their own evaluation results, and school administrators can view the data fusion results. After evaluating teachers through the campus network, all evaluation data can be integrated and analyzed, and the system automatically generates analysis results corresponding to users. The stability of the teaching evaluation system is better, and the system performance can reach 89% at 5,000 person-hours. The degree of integration between the form of informatization teaching and the teaching content of ideological and political courses, and the information literacy of teachers of ideological and political courses in colleges and universities need to be further improved. This research helps in scientific education. For the problems existing in the current research, starting from the three aspects of teaching concept, teaching mode, and teachers’ information literacy, it proposes corresponding effective ways to promote the deep integration of information technology and ideological and political teaching in colleges and universities.},
journal = {Sec. and Commun. Netw.},
month = {jan},
numpages = {9}
}

@phdthesis{10.5555/266873,
author = {Iyer, Lakshmi},
title = {Parallel optimal clustering},
year = {1997},
publisher = {University of Georgia},
address = {USA},
abstract = {Cluster analysis is a generic term coined for procedures that are used objectively to group entities based on their similarities and differences. The primary objective of these procedures is to group n items into up to K mutually exclusive clusters so that items within each cluster are relatively homogeneous in nature while the clusters themselves are distinct. Statistical methods attempt to lower the interaction between each observation and the group mean or median. In contrast, optimal clustering techniques not only finds the best possible solution, but also accounts for total group interaction. In this research, we develop, implement, and evaluate a parallel algorithm (PGROUPS) to solve clustering problems to optimality. PGROUPS was implemented using one to eight processors on the IBM PowerParallel System (SP-2) at University Computing &amp; Network Services at The University of Georgia using the xlf (f77) Fortran compiler under the AIX Version 3.2 (Unix) operating system. Our programming environment is the Parallel Virtual Machine (PVM) and we used the user mode (the communication channel switch) so that more than one process can be activated on a single processor. PGROUPS is based on the model and serial solution methodology (GROUPS) due to Aronson and Klein (1989) and Klein and Aronson (1991).Prior to developing PGROUPS, the serial solution methodology was enhanced by developing a new heuristic to obtain the initial upperbound and eliminating the evaluation of lower bounds for some nodes that are already bound fathomed. The two enhancements decreased the overall solution CPU time for GROUPS, for the test problems, by an average of 27.1%, with a maximum reduction of 53.2%.Test results of PGROUPS indicate that we obtain near linear to superlinear speedups (both absolute and relative) with overall average efficiency also close to one and greater than one. We also achieved even load balancing with each Slave Process performing an equal share of work.We expect that this parallel algorithm will have a significant bearing on our ability to solve relatively large-scale clustering problems to optimality. Thus, solutions to real-world, complex clustering problems, which could not be solved due to lack of efficient algorithms, can now be attempted.},
note = {UMI Order No. GAX97-35526}
}

@proceedings{10.1145/1066129,
title = {eclipse '04: Proceedings of the 2004 OOPSLA workshop on eclipse technology eXchange},
year = {2004},
isbn = {9781450377980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This foreword sketches the history of the Eclipse project. It
then presents the context of the present workshop, the eclipse
Technology Exchange (eTX), which was held on October 24, 2004 at
OOPSLA 2004, Vancouver, British Columbia, Canada.&lt;b&gt;The Eclipse Project&lt;/b&gt;The Eclipse Project began in April 1999 at IBM's OTI laboratory.
It was initially conceived as a successor product for the VisualAge
family of software development tools. VisualAge was a commercially
successful IDE, but it was also a closed environment built on
proprietary APIs. It did not integrate well with other vendors'
tools, and only the IBM/OTI team could enhance or extend the
product. Moreover, it was becoming apparent from customer
experience that more was required than a simple re-write of
VisualAge. In fact, there was growing demand for a tool integration
platform --- a programming environment that would provide kernel
IDE functionality, but also allow developers, third party vendors
and users to seamlessly add their own extensions, personalizations,
and enhancements.The Eclipse team set out to identify the essential kernel
concepts underlying the VisualAge product line (or any other IDE
for that matter). In effect, they wanted to strip out all of the
functionality within an IDE that was specific to a particular
programming language, development task, or programming model. The
hope was that there would be substantial residual function left
behind, that could then be restructured to form a content-neutral,
and programming language-neutral, foundation on which IDEs and
similar products could be built from components. It was a bold
venture, since there was no guarantee that anything practically
useful would result.What they discovered was Eclipse: a tool integration platform
together with a set of components (plugins, in the Eclipse
vernacular) that could be seamlessly assembled into a wide variety
of software development products. The Java Development Toolkit
(JDT) --- the Eclipse Java IDE --- became their proof-point. It was
built in parallel by a separate team, which operated independently
from the Eclipse Platform project. The JDT team had no special
privileges; they had to use the same APIs as any third party
product and were allowed no "back door" access to Platform kernel
functionality. The intent was that, despite these constraints, the
finished JDT should be indistinguishable from a purpose-built,
vertically integrated IDE product like VisualAge. This goal was
realized, the Eclipse Project was a success, and the Eclipse
Community was born.In the years since the Eclipse code base was released into open
source by IBM, its growth has been nothing short of spectacular.
Tens of thousands download the Eclipse SDK every week from over
fifty mirror sites around the globe. Thousands of Eclipse plugins
are now available from open source and commercial suppliers.
Software vendors are now shipping several hundred commercial
products based on Eclipse. Approximately 60 companies are members
of the Eclipse Foundation, which hosts Eclipse open source
development. The first Eclipse Developer Conference (EclipseCON
2004) was held in February 2004 in Anaheim, California. Over 220
companies and organizations from nearly 25 countries were
represented.&lt;b&gt;ECLIPSE AND COMPUTER SCIENCE RESEARCH&lt;/b&gt;It has been particularly interesting to see the uptake of
Eclipse within the research community. In retrospect one could
perhaps have anticipated this. Computing is, after all, an
empirical discipline --- ideas must be implemented to be validated.
For software researchers in particular, the computer becomes our
laboratory. We necessarily build on the work of those who have gone
before, and of course as time progresses our technology pyramids
keep getting higher. Complexity is our bane: the low-hanging fruit
were picked long ago, and most interesting problems are just not
simple. Consequently, experimentation usually requires complex
infrastructure, plumbing, as we often call it. Most researchers
spend far too much time building (and rebuilding, and fixing) this
plumbing, and far too little time actually developing new ideas.
Given the nature of research, there are seldom any applicable
standards for such infrastructure (these only come much later when
the research has matured into products). Consequently researchers
up to now have had to live and work in their own vertical towers,
sharing their ideas but only infrequently sharing code. The only
common programming platform among researchers was "emacs", and
while this continues to be very flexible, it lags far behind
industrial-scale IDEs in terms of functionality.But Eclipse changes this context. It provides a means to create
and share that necessary common infrastructure, particularly for
investigators in such areas as programming languages, tools, and
environments. Researchers can focus more of their time on their
real mission of innovation, and much less on the tedious plumbing
tasks. Moreover, Eclipse-based implementations are built from
commercial-quality components, resulting in robust demonstration
systems that make it much easier for researchers to publicize and
promote their work.What specifically does Eclipse offer researchers that makes it
so attractive? First, it is an extensible platform for integrating
components, which comes replete with a large number of commercial
quality components out of the box. It runs on nearly all operating
systems and GUI combinations, and is one of the few Java
implementations that actually realizes that language's "write once
run everywhere" potential (rather than the typical "write once test
everywhere" experience). Perhaps most importantly, it is available
in open source with a generous non-viral license. Finally, it has
tremendous visibility due to broad based industry support, which
includes the backing of such powerhouse firms as IBM, HP, SAP,
Intel, and many more.&lt;b&gt;ECLIPSE AND COMPUTER SCIENCE EDUCATION&lt;/b&gt;There are numerous challenges in education these days such as
distance, limited resources and the recognized need to make
learning a personalized and active experience. Many educators are
consequently looking at how technology can address these challenges
and enhance learning in the classroom and beyond. For computer
science education, Eclipse has already been widely adopted as an
IDE to support programming. The advantages for some are that it is
free, platform independent and industrially relevant. But beyond
these obvious advantages, other researchers have recognized that
Eclipse provides an excellent infrastructure for developing
learning tools. These tools can leverage the wealth of technology
already present in the Eclipse community, as well as benefit from
integration with other learning tools developed by other
researchers and educators. The result of these multiple efforts is
the emergence of Eclipse as an effective and powerful platform to
support research in educational technologies and an improved
learning experience in many settings.&lt;b&gt;THE ECLIPSE TECHNOLOGY EXCHANGE&lt;/b&gt;That idea that Eclipse would provide exactly the rich, open and
robust platform that IT researchers needed was not initially an
obvious one, and so it needed to be promoted within the academic
community. IBM and eclipse.org set out to popularize these ideas by
hosting a series of workshops and birds-of-a-feather events at
various software research conferences. This ad hoc program
gradually evolved into the eclipse Technology Exchange (eTX)
workshops, the most recent of which was held at OOPSLA 2004 in
Vancouver. These events provide a forum for researchers who are
using Eclipse to network and share their experiences and their
code.The foundation for a successful eTX is a set of high quality,
refereed presentations, which serve to illustrate the breadth and
vitality of the Eclipse research and teaching communities. The
papers in this volume are exemplary in this regard. Several
describe plugins that build on the Eclipse Platform to offer new
programming tools, such as for aspect browsing; debugging
distributed applications; profiling and monitoring program
behaviour; visualization of complex data; and feature modeling, a
technique used in product-line development to model similarities
and differences of products. About half of the papers describe the
use of Eclipse for teaching object-oriented programming and
software engineering, in both classroom and distance settings. One
such paper addresses distributed collaborative programming. Others
describe Eclipse courseware plugins for code-based tutorials;
visualization of computer organization concepts; and tracking
student programming projects.Each paper illustrates the advantages that Eclipse offers
researchers and teachers. They describe rich full-featured
implementations, which can be used and modified by other
researchers/teachers in their respective areas, at minimal cost in
terms of incremental programming effort. This works because they
all leverage the rich infrastructure and base of components
provided by Eclipse. And of course by developing their own new
components, each of these projects extends that base and enables
others to build on their work --- a virtuous cycle that is creating
an eco-system around Eclipse and enriching the entire software
research and teaching communities.},
location = {Vancouver, British Columbia, Canada}
}

@phdthesis{10.5555/915812,
author = {Lee, Taejae and Browne, James C. and Werth, John},
title = {Software reuse in parallel programming environments},
year = {1989},
publisher = {The University of Texas at Austin},
abstract = {To date, reuse of software has not had its anticipated effect on improvements in software productivity. This is because we do not fully understand the concepts behind reusability and because there has been relatively little experimentation with reusability systems. In this research we attack these problems in three ways: (1) An investigation of the conceptual foundations of reuse for a parallel programming environment based on the Unified Computation Graph Model designed by Dr. James C. Browne at the University of Texas, Austin. (2) A realization of these concepts in a software base management system, ROPE, to support reuse in such an environment. (3) An experimental evaluation of the effectiveness of ROPE. The research addresses each of the fundamental steps of finding, understanding, modifying, and composing reusable components: (1) The problem of finding components is addressed by a new classification method, called the structured relational classification method. This method appears to be an effective technique for combining the strengths of relational methods in the maintenance and query areas with the strengths of more traditional methods in the browsing area. (2) For understanding components, we have introduced design analysis methods which basically flow from the UCGM model itself. (3) Modifying components is addressed in several ways. First through a suitable definition of generic designs and secondly through techniques for composing and decomposing graphs. (4) Composition of components is discussed in detail and a framework is laid for a calculus of composition of components. This required a formalization of some new aspects of the UCGM model and definitions and theorems about the structure of UCGM.The reusability system ROPE was built, tested and used by a variety of people. Each of the concepts discussed above was realized to some degree in the final system though the theory outstripped the implementation in several areas. This was a very substantial programming project.A fairly extensive evaluation of ROPE was done. The initial set of experiments has clearly established the effectiveness of CODE and ROPE in promoting component reuse in programs of modest size and complexity and in delivery of nearly error free programs with relatively little effort.},
note = {AAI9005613}
}

@phdthesis{10.5555/929168,
author = {Lee, Wonjun},
advisor = {Srivastava, Jaideep},
title = {Quality-of-service provisioning for multimedia applications},
year = {1999},
isbn = {0599358882},
publisher = {University of Minnesota},
address = {USA},
abstract = {Over the past several years, systems are increasingly supporting a new class of multimedia applications that need to trade off Quality of Service with respect to resource constraints. In particular, soft-real time applications, such as  continuous media  (  CM \'{y}) systems, have an important property, viz, they allow for graceful adaptation of the application Quality-of-Service (QoS), and therefore are able to have acceptable performance with reduced resource utilization. The use of graceful adaptation of the application for admission control leads to an integrated admission control and service negotiation protocol. This dissertation specifically deals with the policies the system adopts, which are referred to as the application admission and negotiation process. For this, we use the constructs of  resource demand functions  and  benefit functions , which simplify the mechanisms for graceful adaptation of the applications. We adopt  welfare economic theories  into our system model and define a  price-based framework  for resource allocation and QoS support in distributed multimedia systems. By formulating a computational economy and finding its competitive equilibrium using market-based techniques that are widely used in economic theories, we can solve the distributed resource allocation problem. In the first part of the dissertation, we present a QoS-based resource management model for multimedia applications for both single resource allocation and multiple resource environments. The second part of the dissertation focuses on the problem of the  Soft-QoS framework , including QoS-based admission control, negotiations, and scheduling for continuous media applications. In this dissertation, we develop novel admission control and QoS negotiation algorithms using benefit and resource demand functions. In the next chapter, our effort focuses on disk bandwidth-based admission control and scheduling in video servers. The fourth part of the dissertation involves actual system implementation and performance analysis; that is, QoS-based evaluation of the file system and distributed system services for continuous media provisioning. We designed and developed a QoS-based continuous media server system which was incorporated into the  HDIMI  (  Heterogeneous Distributed Multimedia Information Management for the Infosphere ) and  Presto  projects, which were Air Force-sponsored at the University of Minnesota. Our CM servers were implemented both on Socket programming environments and on CORBA. The comparison of performance between such various network protocols on the Internet was also done. (Abstract shortened by UMI.)},
note = {AAI9934977}
}

@phdthesis{10.5555/AAI28991918,
author = {Formoso, Laura Dapena},
advisor = {Miguel, Leit\~{a}o, J.},
title = {Video-Monitorization System for a Realistic Driving Simulator},
year = {2014},
isbn = {9798209808770},
publisher = {Instituto Politecnico do Porto (Portugal)},
abstract = {In the last few years, ISEP in collaboration with FEUP and other universities, created a realistic driving simulator called DriS, which had the objective to help in researches of different areas, as civil engineer, computer graphics, psychology, education, etc.The result of this thesis pretends to help the professionals who analyse the data collected in each driving experience, in order to allow them the study of the driver's reactions at different obstacles during a ride, at the same time.DriS simulator consists in one white screen where the driving simulators environments are projected, in one real car to make the driving experience and four cameras placed in the car. Of these four cameras, three are inside the car and one of them outside the car. Each camera is focused in one specific and critical part of the driving: the road, the driver, the pedals and the controls (gearshift, steering wheel, wiper controls, etc.).Each one of the camera records a video that is save in one computer placed in the control room, inside the Laborat\'{o}rio de An\'{a}lise de Tr\'{a}fego in FEUP. Also, a text file is saved in this computer. This text file contains some information about the driver's experience, as it can be the car coordinates, the speed of the car, the time, etc.The work of this thesis arises in order to improve the way on how professionals analyse and perform data collected from a DriS driving experience. For that purpose, was created a video-­‐monitorization system, consists in a video application, that allows load and player four videos simultaneously as well as a text file which contains all the data collected from the experience. All of them will be time-coordinated and the user could move forward and backward through them using a slider. Also, as a basic video player, contains some buttons to control the status of the video (play, stop, pause) allowing the professionals analyse with detail the four videos and the data.Take advantage of the new progresses in software development, the application was made in C++ using the Qt library, and its integrated development environment the Qt Creator, which made easier the implementation.At the end of this report (Chapter 4) is attached a user manual in order to explain and help the professionals to use the application.},
note = {AAI28991918}
}

@inproceedings{10.1145/2462476.2462493,
author = {Matsuzawa, Yoshiaki and Okada, Ken and Sakai, Sanshiro},
title = {Programming process visualizer: a proposal of the tool for students to observe their programming process},
year = {2013},
isbn = {9781450320788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462476.2462493},
doi = {10.1145/2462476.2462493},
abstract = {We have developed a tool that enables learners to observe the process by which they program through visualization of data that are recorded in the source code editor. One purpose of the tool is to assist learners by using the Personal Software Process (PSP) to allow them to analyze the process by which they program by using the tool after completing a programming task. The proposed tool has functions for A) replaying a process using animation; B) automatic calculation of metrics; C) support for inputting subtasks; and D) process analysis report generation. An evaluation experiment was conducted with participants from the second-level introductory programming course at our university. The results were that 1) the accuracy of effort estimation dropped, although we clearly found that the reason for the drop was the difficulty of the second assignment; 2) according to a questionnaire, students reported both the effectiveness of the observation task and the effectiveness of the tool; and 3) there was large differences between students in terms of the description level of subtasks.},
booktitle = {Proceedings of the 18th ACM Conference on Innovation and Technology in Computer Science Education},
pages = {46–51},
numpages = {6},
keywords = {process, programing education, psp, visualize},
location = {Canterbury, England, UK},
series = {ITiCSE '13}
}

@inproceedings{10.1145/3482632.3483098,
author = {Gu, Ran},
title = {Analysis on the Construction of Diversified Innovation and Entrepreneurship Education Mode in Colleges under the Background of "Internet +"},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3483098},
doi = {10.1145/3482632.3483098},
abstract = {Entrepreneurship through the use of Internet platforms has now become a common situation in the social environment, and as college students themselves, they are also the most direct beneficiaries of this type of entrepreneurial platform. However, it should be noted that in today's rapid Internet development environment Next, as a college student, it is not only the improvement of opportunities, but also the existence of great challenges. By discussing the current problems in college students’ entrepreneurship and innovation education, the article points out that the government, colleges should establish a unified management entrepreneurship and innovation education system. Promote "Internet +" entrepreneurship and innovation skills training, enrich multi-student entrepreneurial practice activities, and more entrepreneurship projects are introduced in colleges, while enhancing the role of teachers as entrepreneurial mentors.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1126–1130},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/330908.331912,
author = {K\"{o}lling, Michael and Rosenberg, John},
title = {Objects first with Java and BlueJ (seminar session)},
year = {2000},
isbn = {1581132131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/330908.331912},
doi = {10.1145/330908.331912},
abstract = {Object-oriented languages have been taught for some time at universities. The most common approach has been to teach those constructs required for imperative programming first and to introduce the notion of classes and objects somewhat later in the course. More recently, many educators have been promoting the notion of teaching about classes and objects first. This helps students to adopt the object-oriented paradigm at an early stage and encourages them to focus on the application structure before beginning coding. Most new textbooks have followed such an approach.While this method has clear advantages, it is not easy to realise in practice. This is partly a result of the languages used for teaching. However, we would argue that the major difficulty comes from the lack  of program development environments and tools which themselves fully embrace the object-oriented paradigm.The use of Java as the language for teaching addresses some of the problems. Java with its clean support for the object-oriented paradigm is now widely regarded as a suitable choice for introductory teaching. The choice of environment, however, remains an issue.The view of the development environment as a major difficulty in Java courses is further supported by numerous reports of educators relating their experiences with teaching introductory Java courses. While Java was consistently described as an excellent language for teaching the object-oriented paradigm, the environments available are regularly identified as a significant source of problems. These may be  divided into two areas:
The environments are designed for professional programmers. They are too complex and have a steep learning curve. Thus valuable teaching time is spent teaching the students how to use the environment and this detracts from the principles of programming.Most of the existing environments fail to fully adopt the object-oriented paradigm. Users of the environment must deal with files, lines of code and directory hierarchies rather than classes, objects and relationships.In this seminar we will argue the case that the requirements for teaching the object-oriented paradigm and Java can only be satisfied by the provision of a program development environment specifically designed for teaching.We will introduce BlueJ, a relatively new development environment which addresses all of these issues. We will show how the unique features of this environment can be used to create an introductory Java course that fully embraces the “object first” approach and supports the presentation of a cleaner picture of the paradigm than previously possible.BlueJ is based heavily on earlier work by us on a language and environment called Blue. BlueJ is a complete Java development environment, written entirely in Java. It provides graphical support for object-oriented design, abstracts over files and the operating system and provides fully integrated support for a design, edit, compile and test cycle. In addition, BlueJ supports interactive creation of objects and interactive calling of methods of objects. This provides support for incremental development, one of the major advantages of object-orientation. It includes an easy-to-use debugger and support for applications and applets.One of the main differences between BlueJ and other environments is its distinct focus on a teaching context. It combines powerful tools with an easy-to-use interface, avoiding the complexity that creates so many problems when using existing environments in a classroom.BlueJ has been used very successfully for two semesters as Monash University.The presentation will provide the context in which the BlueJ project has been developed. We will discuss the design principles for BlueJ, the major aims of the project and our experiences with using it in class. A demonstration of the current version of BlueJ will be given. We will also demonstrate a set of examples and problems which can be used in a first Java course and show how the course structure can be improved and support teaching “objects first” with the availability of an environment that fully supports the paradigm.BlueJ is available free of charge and can be used by any interested institution. Details of how to obtain a copy of BlueJ will be provided at the seminar.},
booktitle = {Proceedings of the Thirty-First SIGCSE Technical Symposium on Computer Science Education},
pages = {429},
location = {Austin, Texas, USA},
series = {SIGCSE '00}
}

@book{10.5555/829549,
author = {Raymond, Eric S.},
title = {The Art of UNIX Programming},
year = {2003},
isbn = {0131429019},
publisher = {Pearson Education},
abstract = {"Reading this book has filled a gap in my education. I feel a sense of completion, understand that UNIX is really a style of community. Now I get it, at least I get it one level deeper than I ever did before. This book came at a perfect moment for me, a moment when I shifted from visualizing programs as things to programs as the shadows cast by communities. From this perspective, Eric makes UNIX make perfect sense." --Kent Beck, author of Extreme Programming Explained, Test Driven Development, and Contributing to Eclipse"A delightful, fascinating read, and the lessons in problem-solvng are essential to every programmer, on any OS." --Bruce Eckel, author of Thinking in Java and Thinking in C++Writing better software: 30 years of UNIX development wisdomIn this book, five years in the making, the author encapsulates three decades of unwritten, hard-won software engineering wisdom. Raymond brings together for the first time the philosophy, design patterns, tools, culture, and traditions that make UNIX home to the world's best and most innovative software, and shows how these are carried forward in Linux and today's open-source movement. Using examples from leading open-source projects, he shows UNIX and Linux programmers how to apply this wisdom in building software that's more elegant, more portable, more reusable, and longer-lived.Raymond incorporates commentary from thirteen UNIX pioneers: Ken Thompson, the inventor of UNIX. Ken Arnold, part of the group that created the 4BSD UNIX releases and co-author of The Java Programming Language. Steven M. Bellovin, co-creator of Usenet and co-author of Firewalls and Internet Security. Stuart Feldman, a member of the Bell Labs UNIX development group and the author of make and f77. Jim Gettys and Keith Packard, principal architects of the X windowing system. Steve Johnson, author of yacc and of the Portable C Compiler. Brian Kernighan, co-author of The C Programming Language, The UNIX Programming Environment, The Practice of Programming, and of the awk programming language. David Korn, creator of the korn shell and author of The New Korn Shell Command and Programming Language. Mike Lesk, a member of the Bell Labs development group and author of the ms macro package, the tbl and refer tools,lex and UUCP. Doug McIlroy, Director of the Bell Labs research group where UNIX was born and inventor of the UNIX pipe. Marshall Kirk McKusick, developer of the 4.2BSD fast filesystem and a leader of the 4.3BSD and 4.4BSD teams. Henry Spencer, a leader among early UNIX developers, who created getopt, the first open-source string library, and a regular-expression engine used in 4.4BSD.}
}

@article{10.1145/2700514,
author = {Park, Thomas H. and Dorn, Brian and Forte, Andrea},
title = {An Analysis of HTML and CSS Syntax Errors in a Web Development Course},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
url = {https://doi.org/10.1145/2700514},
doi = {10.1145/2700514},
abstract = {Many people are first exposed to code through web development, yet little is known about the barriers beginners face in these formative experiences. In this article, we describe a study of undergraduate students enrolled in an introductory web development course taken by both computing majors and general education students. Using data collected during the initial weeks of the course, we investigate the nature of the syntax errors they make when learning HTML and CSS, and how they resolve them. This is accomplished through the deployment of openHTML, a lightweight web-based code editor that logs user activity. Our analysis reveals that nearly all students made syntax errors that remained unresolved in their assessments, and that these errors continued weeks into the course. Approximately 20% of these errors related to the relatively complex system of rules that dictates when it is valid for HTML elements to be nested in one another. On the other hand, 35% of errors related to the relatively simple tag syntax determining how HTML elements are nested. We also find that validation played a key role in resolving errors: While the majority of unresolved errors were present in untested code, nearly all of the errors that were detected through validation were eventually corrected. We conclude with a discussion of our findings and their implications for computing education.},
journal = {ACM Trans. Comput. Educ.},
month = {mar},
articleno = {4},
numpages = {21},
keywords = {Web development, code editors, computational literacy}
}

@inproceedings{10.1145/1384271.1384395,
author = {Kiesmueller, Ulrich and Brinda, Torsten},
title = {How do 7th graders solve algorithmic problems? a tool-based analysis},
year = {2008},
isbn = {9781605580784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1384271.1384395},
doi = {10.1145/1384271.1384395},
abstract = {Informatics education, not only in higher but also in secondary education, is often assisted by special learning software to teach the fundamental ideas of algorithms [2]. In this context pupils also learn the basics of programming using didactically reduced, textbased or visual programming languages. Therefore in Germany, in some federal countries (for example Bavaria), where the basics of algorithms are already taught in the 7th grade (age 12 to 13 years), age-based learning and programming environments, such as Karel, the robot and Kara, the programmable ladybug [1], are used. Although the design of these environments is age-based, working with them to solve algorithmic problems often causes problems in the classroom. These tools give feedback to the learners based on the analysis of a current solution attempt without taking the previous problem solving process into account. The system messages are often rather technical and therefore hardly helpful especially for weaker learners to enable them to correct arisen problems by themselves. In order to give optimal support to pupils in these situations and therefore improve the learning processes, the learner-system interaction of the used educational software environments should be enhanced and better be adapted to the learners? individual problem solving strategies.The main objective of this research project is to find out, to what extent the automated diagnosis of a problem solving strategy of a learner is possible, and to what extent this knowledge can be used to enhance the learner-system interaction. Starting from the advantages and disadvantages of standardized process observation methods, two software-based research instruments for the system supported diagnosis of the individual proceedings, using the learning environment Kara, were designed and implemented. With the first component learner-system interactions are recorded, the second one provides functions to analyse the collected data. Using test-cases gives a first idea of the quality of the solution attempts.The requirements for the software components resulted from several test scenarios with a small number of participants with different qualification in computer science (from novices to graduating computer science students). During these tests each individual was observed by a researcher and additionally interviewed afterwards. A first version of the implemented instruments was tested in case studies with more than 100 participants (12 to 13 years old) from Bavarian grammar schools to evaluate the suitability for daily use. During the studies the learners were asked to solve three given tasks in a session of 45 minutes, provided by the Kara system, individually (one pupil per computer), but communication between the test persons was allowed. The tasks required knowledge of the control structures (sequence, selection, iteration).The results of these studies indicate that it is possible to identify and to evaluate different problem solving patterns with the help of the developed instruments. To identify different types of learners? strategies it is necessary to combine the various kinds of visualizations of the collected data. To support automatic categorization pattern-recognition methods will be used. The collected ordinal (test-case results) and nominal data can be used for analyses of the correlation between different factors (for example number of error messages or program executions compared with the assessment of the solution attempt) with methods of descriptive statistics.},
booktitle = {Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education},
pages = {353},
numpages = {1},
keywords = {algorithms, didactics of informatics, kara, problem solving process, secondary computer science education, tool-based analysis},
location = {Madrid, Spain},
series = {ITiCSE '08}
}

@article{10.1145/3632530,
author = {Mansoor, Niloofar and Peterson, Cole S. and Dodd, Michael D. and Sharif, Bonita},
title = {Assessing the Effect of Programming Language and Task Type on Eye Movements of Computer Science Students},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
url = {https://doi.org/10.1145/3632530},
doi = {10.1145/3632530},
abstract = {Background and Context: Understanding how a student programmer solves different task types in different programming languages is essential to understanding how we can further improve teaching tools to support students to be industry-ready when they graduate. It also provides insight into students’ thought processes in different task types and languages. Few (if any) studies investigate whether any differences exist between the reading and navigation behavior while completing different types of tasks in different programming languages.Objectives: We investigate whether the use of a certain programming language (C++ versus Python) and type of task (new feature versus bug fixing) has an impact on performance and eye movement behavior in students exposed to both languages and task types.Participants: Fourteen students were recruited from a Python course that taught Python as an introductory programming language.Study Method: An eye tracker was used to track how student programmers navigate and view source code in different programming languages for different types of tasks. The students worked in the Geany Integrated Development Environment (IDE, used also in their course) while eye-tracking data was collected behind the scenes making their working environment realistic compared to prior studies. Each task type had a Python and C++ version, albeit on different problems to avoid learning effects. Standard eye-tracking metrics of fixation count and fixation durations were calculated on various areas of the screen and on source code lines. Normalized versions of these metrics were used to compare across languages and tasks.Findings: We found that the participants had significantly longer average fixation duration and total fixation duration adjusted for source code length during bug fixing tasks than the feature addition tasks, indicating bug fixing is harder. Furthermore, participants looked at lines adjacent to the line containing the bug more often before looking at the buggy line itself. Participants who added a new feature correctly made their first edit earlier compared to those who failed to add the feature. Tasks in Python and C++ have similar overall fixation duration and counts when adjusted for character count. The participants spent more time fixating on the console output while doing Python tasks. Overall, task type has a bigger effect on the overall fixation duration and count compared to the programming language.Conclusions: CS educators can better support students in debugging their code if they know what they typically look at while bug fixing. For new feature tasks, training students not to fear edits to learn about the code could also be actively taught and encouraged in the classroom. CS education researchers can benefit by building better IDE plugins and tools based on eye movements that guide novices in recognizing bugs and aid in adding features. These results will lead to updating prior theories on mental models in program comprehension of how developers read and understand source code. They will eventually help in designing better programming languages and better methods of teaching programming based on evidence on how developers use them.},
journal = {ACM Trans. Comput. Educ.},
month = {jan},
articleno = {2},
numpages = {38},
keywords = {Program comprehension, source code, C++, Python, bug fixing, new feature tasks, programming education, learning behavior, eye-tracking study}
}

@book{10.5555/2566754,
author = {Choudhary, Hariom},
title = {C++ Programming- Final Golden Edition.: Beginners To Experts Approach Guide - With Easy Learning &amp; Problem Analysis to Program Design &amp; Development.},
year = {2013},
isbn = {1492719161},
publisher = {CreateSpace Independent Publishing Platform},
address = {North Charleston, SC, USA},
edition = {5th},
abstract = {C++11 has arrived: thoroughly master it, with the definitive new guide from C++ creator Bjarne Stroustrup, C++ Programming Language, Fourth Edition! The brand-new edition of the world's most trusted and widely read guide to C++, it has been comprehensively updated for the long-awaited C++11 standard. Extensively rewritten to present the C++11 language, standard library, and key design techniques as an integrated whole, Stroustrup thoroughly addresses changes that make C++11 feel like a whole new language, offering definitive guidance for leveraging its improvements in performance, reliability, and clarity. C++ programmers around the world recognize Bjarne Stoustrup as the go-to expert for the absolutely authoritative and exceptionally useful information they need to write outstanding C++ programs. Now, as C++11 compilers arrive and development organizations migrate to the new standard, they know exactly where to turn once more: Stoustrup's C++ Programming Language, Fourth Edition. C++ Programming in Easy Steps instructs you how to program in the powerful C++ language, giving complete examples that illustrate each aspect. C++ Programming in Easy Steps begins by explaining how to download and install a free C++ compiler so you can quickly begin to create your own executable programs by copying the book's examples. It demonstrates all the C++ language basics before moving on to provide examples of Object Oriented Programming. The book concludes by demonstrating how you can use your acquired knowledge to create programs graphically in the free Microsoft Visual C++ Express Integrated Development Environment (IDE). C++ Programming in Easy Steps makes no assumption you have previous knowledge of any programming language so it's ideal for the newcomer to computer programming. It has an easy-to-follow style that will appeal to programmers moving from another programming language, and to the student who is studying C++ programming at school or college, and to those seeking a career in computing who need a fundamental understanding of object oriented programming. Want to learn to code? Want to learn C++? Struggling to follow your lecturer or books and tutorials written for experts? You're not alone. As a professional C++ developer and former Harvard teaching fellow, I know what you need to know to be a great C++ programmer, and I know how to teach it, one step at a time. I know where people struggle, and why, and how to make it clear. I cover every step of the programming process, including: Getting the tools you need to program and how to use them Basic language feature like variables, loops and functions How to go from an idea to code A clear, understandable explanation of pointers Strings, file IO, arrays, references Classes and advanced class design C++-specific programming patterns Object oriented programming Data structures and the standard template library (STL) Key concepts are reinforced with quizzes and over 75 practice problems.}
}

@article{10.5555/1292428.1292462,
author = {Hunt, John M. and Matzko, Sarah},
title = {Retooling a curriculum},
year = {2007},
issue_date = {December 2007},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {23},
number = {2},
issn = {1937-4771},
abstract = {Computer Science has problems attracting and retaining students. One response has been the development of new curricula, which often originate at large research universities. This paper looks at the issue of adapting one such curriculum to a small liberal arts college. Our focus is on changing the supporting tool set - language, development environment, editors, etc. We discuss the criteria that lead to these choices, particularly the related to differences in students.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {195–201},
numpages = {7}
}

@inproceedings{10.1145/3183440.3195061,
author = {Santos, Alan and Sales, Afonso and Fernandes, Paulo and Kroll, Josiane},
title = {Challenge-based learning: a brazilian case study},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3195061},
doi = {10.1145/3183440.3195061},
abstract = {Mobile application development (MAD) has became, or is considering to be a part of the academic curricula in Computer Science courses. However, training students on mobile application development inherits the challenges of teaching software engineering where the target computer is a device that has a large number of features accessible by software. Furthermore, the most related experience in teaching students reveals difficulties in developing software engineering competencies. In this paper we present results from a case study conducted in four universities in Brazil. We have investigated the adoption of Challenge-Based Learning (CBL) framework and agile practices for training students in software engineering applied in mobile application development environments.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {155–156},
numpages = {2},
keywords = {agile practices, challenge-based learning, mobile, software development, software engineering education},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@phdthesis{10.5555/AAI28649526,
author = {Ross, Anne Spencer and Jennifer, Mankoff, and Meredith, Ringel Morris, and Mark, Harniss,},
advisor = {James, Fogarty, and O, Wobbrock, Jacob},
title = {A Large Scale, Multi Factor Approach to Understanding and Improving Mobile Application Accessibility},
year = {2021},
isbn = {9798480642926},
publisher = {University of Washington},
abstract = {Accessibility failures in mobile applications (apps) create barriers for disabled people and people who use assistive technologies. Given the growing role of apps in everyone's daily life, equitable access is imperative. Toward this goal, I created a conceptual framework for understanding and improving app accessibility at scale inspired by epidemiology. My epidemiology‑inspired framework poses app accessibility failures as "diseases" in a "population" of apps. This perspective forefronts a population-level perspective within an ecosystem of factors that impact app accessibility (e.g., developer tools, guidelines, company culture, and many more). In this dissertation, I demonstrate my thesis that applying my epidemiology‑inspired framework, which emphasizes large‑scale and multi‑factor approaches, (1) can reveal population‑level trends of accessibility failures, (2) can aid in identifying a range of factors that impact app accessibility, and (3) can inform the design of tools for identifying and repairing accessibility failures in apps.To enhance our understanding of the state of app inaccessibility, I performed the first large‑scale analyses of Android app accessibility. My results measured the prevalence of accessibility failures across apps and identified classes of elements that frequently had accessibility failures. Missing labels was one of the most prevalent failures; 23% of the 8,901 apps had more than 90% of their image-based elements missing labels. Reflecting a less frequent but severe accessibility failure, 8% of 9,999 tested apps were completely unusable with many assistive technologies, such as screen readers. Apps with such failures disproportionately came from the Education category.Using a multi‑factor assessment, partially guided by my large‑scale analyses, I identified accessibility shortcomings in environmental factors such as programming tools, developer guidelines, and inter-team dynamics that could contribute to app inaccessibility at scale. For example, I identified a set of game engines and cross-platform tools (e.g., Unity, Adobe Air) that frequently produced apps that were unusable with many assistive technologies. Another factor I assessed was Android's implementation documentation, finding many instances of missing labels in the example code snippets.Toward improving app accessibility, I explored techniques for enhancing developer tools, testing tools, and third‑party repairs. In developer tools, I present novel designs and techniques that aim to improve the efficiency, effectiveness, and education of developers by tightening the runtime-implementation feedback loop, leveraging screen context to provide more specific repairs, and using new visualization and interaction tool interface designs. Within testing tools, I prototyped an extension to Google's Accessibility Scanner to allow human annotation of automated results. Professional accessibility testers found the tool promising and discussed factors beyond tools, such as knowledge gaps and social dynamics with the developer teams, which affected their testing. Addressing repairs after an app is released, I present a proof-of-concept for third-party repair that people who use screen readers found useful while highlighting factors around trust, repair-production infrastructure, and co-creation that would impact real-word deployment.Throughout my dissertation, I leverage my epidemiology‑inspired concepts and language to highlight the value of my research and place it within the larger space of work on app accessibility. Together, this research aims to expand our understanding of app accessibility at scale and inform efforts to improve it.},
note = {AAI28649526}
}

@inproceedings{10.1145/1028664.1028677,
author = {Bhattacharrya, Arnab and Fuhrer, Robert},
title = {Smell detection for eclipse},
year = {2004},
isbn = {1581138334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1028664.1028677},
doi = {10.1145/1028664.1028677},
abstract = {Smells are architectural, rather than functional, flaws in software that tend to reduce maintainability, extensibility, modularity, testability, or other software quality measures. Common smells include overly-long method bodies, message chains, parallel inheritance, use of switch statements to implement polymorphic behavior, data clumps, overly specific variable types, and duplicated code. Code duplication in particular is a well-known source of maintenance problems. Because smells do not represent functional flaws, they can be remediated by behavior-preserving transformations (i.e. refactorings).We will demonstrate a nascent extensible smell detection framework for Java, implemented for the Eclipse IDE (www.eclipse.org). Our framework provides a simple Eclipse extension point for defining smell detectors, along with several basic smell detectors, including a reasonably efficient code duplication detector. Several of our smell detectors feature Eclipse quick-fix refactorings to automatically remediate smells upon the user's request. This transforms the tool from a mere problem indicator into a tool that actively assists developers in improving their code. We believe that such a facility has significant potential in educational settings, in directing a student's attention to software engineering principles while coding.Our demonstration will consist of two parts. First, we'll demonstrate some of our smell detectors on realistic source code bases (including Eclipse itself), along with their corresponding quick-fix support. Second, we'll walk through the implementation of a very simple smell detector within our framework, along with a very simple remediation.},
booktitle = {Companion to the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications},
pages = {22},
numpages = {1},
keywords = {code smells, refactoring, software engineering, software quality},
location = {Vancouver, BC, CANADA},
series = {OOPSLA '04}
}

@article{10.1145/181505.181515,
author = {Nelson, Wayne A.},
title = {Analyzing user interactions with hypermedia systems},
year = {1994},
issue_date = {Feb. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {0097-8930},
url = {https://doi.org/10.1145/181505.181515},
doi = {10.1145/181505.181515},
abstract = {The unprecedented freedom for users to control the scope and sequence of their interactions with hypermedia systems presents many challenges to those who design and study these systems in educational settings. Early efforts to develop hypermedia systems revealed that the node-link structure of such systems is both advantageous and problematic (Conklin, 1987). When users have the freedom to follow any of a multitude of link permutations, disorientation often results. Further, without appropriate training, novice users do not possess the strategies necessary for effective "browsing" of large hypermedia documents (Duffy &amp; Knuth, 1991). It has also been noted that the purpose for using the system or the task that the users are engaged in can influence patterns of interaction with hypermedia systems (Nelson, 1991). Many designers, therefore, advocate that features such as visual maps, database search facilities and guided tours be included in hypermedia systems to alleviate some of these problems (e.g. Hammond, 1989; Laurel, 1990, 1991).With the emergence of hypermedia systems as a major architecture for educational and other information-oriented software comes the related problem of how to document and analyze user interactions with such systems for the purposes of research and evaluation. There are a variety of interface design strategies that impact on how a system performs and should be evaluated. Many hypermedia systems to date have employed a "browsing" interface, but alternative approaches are also emerging (Nelson &amp; Palumbo, 1992). Regardless of the type of interface, many questions can be generated when studying the interactions of users with hypermedia systems. For example, how many users chose to follow a particular link, and why was one link chosen over another? How does the choice of one link affect choices of subsequent links? When are graphic images, animations and video segments accessed? What user tasks are appropriate for guiding interaction with the system? What kinds of strategies do users develop while working with hypermedia systems? These and many other questions need answers when designing, developing and evaluating hypermedia applications for education and other settings, and provide the focus for this short article.A wealth of user interaction data can be easily collected within many hypermedia development environments in order to study aspects of the interface, including the nature of user navigation patterns, the time spent at each node and the use of help and orienting facilities. The data can represent the paths a user follows through the system, and the choices made at each node in the system. The problem is that because of the nature of this data, traditional methods of analysis such as surveys or pretest-posttest designs, are not particularly effective for determining usability or comparing alternative interface designs. Researchers have had to develop new techniques for analyzing patterns of user interaction in order to evaluate the design and effectiveness of hypermedia systems (Misanchuk &amp; Schwier, 1992). There is a need to categorize and compare groups of users in order to compare the effectiveness of alternative system features, as well as describing characteristics of interaction by individual users within the same system.},
journal = {SIGGRAPH Comput. Graph.},
month = {feb},
pages = {43–45},
numpages = {3}
}

@article{10.5555/2460156.2460195,
author = {Purcell, Michelle and Ellis, Heidi J. C. and Hislop, Gregory W.},
title = {An approach for evaluating open source projects for student participation},
year = {2013},
issue_date = {June 2013},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {28},
number = {6},
issn = {1937-4771},
abstract = {Student participation in free and open source software (FOSS) has potential to improve student learning in computing majors. Participation can include contributing bug fixes, testing, writing documentation and developing new features[1]. These opportunities enable students to learn in a more authentic environment developing technical skills as well as teamwork and communication skills. However learning curves related to a large complex code base, use of development tools, understanding of community interactions, and scheduling of class deliverables with the FOSS calendar are only a few of the hurdles to education inherent in learning in a FOSS environment.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {199–200},
numpages = {2}
}

@article{10.1145/1131322.1131323,
author = {Nieh, Jason and Vaill, Chris},
title = {Experiences teaching operating systems using virtual platforms and Linux},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0163-5980},
url = {https://doi.org/10.1145/1131322.1131323},
doi = {10.1145/1131322.1131323},
abstract = {Operating system courses teach students much more when they provide hands-on kernel-level project experience with a real operating system. However, enabling a large class of students to do kernel development can be difficult. To address this problem, we created a virtual kernel development environment in which operating systems can be developed, debugged, and rebooted in a shared computer facility without affecting other users. Using virtual machines and remote display technology, our virtual kernel development laboratory enables even distance learning students at remote locations to participate in kernel development projects with on-campus students. We have successfully deployed and used our virtual kernel development environment together with the open-source Linux kernel to provide kernel-level project experiences for over nine hundred students in the introductory operating system course at Columbia University.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {apr},
pages = {100–104},
numpages = {5},
keywords = {computer science education, open-source software, operating systems, virtual machines, virtualization}
}

@inproceedings{10.1145/3568294.3580102,
author = {De la Rosa Gutierrez, Jose Pablo and S\o{}rensen, Anders Stengaard},
title = {PLATYPUS: An Environment for End-User Development of Robot-Assisted Physical Training},
year = {2023},
isbn = {9781450399708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568294.3580102},
doi = {10.1145/3568294.3580102},
abstract = {When robots are used for physical therapy, programming becomes too important to be left to programmers. Developing programs for training robots is time-consuming and requires expertise within multiple engineering domains, combined with physical training, therapy, and human interaction competencies. In this paper, we present Platypus: an end-user development environment that encompasses the design and execution of custom activities for robot-assisted physical training. The current version ships a set of plugins for Eclipse's IDE and uses a block-based visual language to specify the robot's behaviors at a high abstraction level, which are translated into the low-level code specifications followed by the robot. As a use case, we present its implementation on RoboTrainer, a modular, rope-based pulling device for training at home. While user tests suggest that the platform has the potential to reduce the technical obstacles for building custom training scenarios, informational and design learning barriers were revealed during the tests.},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {342–346},
numpages = {5},
keywords = {assistive technology, end-user development, human-computer interaction, robot-assisted training},
location = {<conf-loc>, <city>Stockholm</city>, <country>Sweden</country>, </conf-loc>},
series = {HRI '23}
}

@inproceedings{10.1145/1047344.1047508,
author = {Nieh, Jason and Vaill, Chris},
title = {Experiences teaching operating systems using virtual platforms and linux},
year = {2005},
isbn = {1581139977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1047344.1047508},
doi = {10.1145/1047344.1047508},
abstract = {Operating system courses teach students much more when they provide hands-on kernel-level project experience with a real operating system. However, enabling a large class of students to do kernel development can be difficult. To address this problem, we created a virtual kernel development environment in which operating systems can be developed, debugged, and rebooted in a shared computer facility without affecting other users. Using virtual machines and remote display technology, our virtual kernel development laboratory enables even distance learning students at remote locations to participate in kernel development projects with on-campus students. We have successfully deployed and used our virtual kernel development environment together with the open-source Linux kernel to provide kernel-level project experiences for over nine hundred students in the introductory operating system course at Columbia University.},
booktitle = {Proceedings of the 36th SIGCSE Technical Symposium on Computer Science Education},
pages = {520–524},
numpages = {5},
keywords = {computer science education, open-source software, operating systems, virtual machines, virtualization},
location = {St. Louis, Missouri, USA},
series = {SIGCSE '05}
}

@article{10.5555/2400161.2400168,
author = {Purcell, Michelle and Ellis, Heidi J. C. and Hislop, Gregory W.},
title = {An approach for evaluating open source projects for student participation},
year = {2013},
issue_date = {January 2013},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {28},
number = {3},
issn = {1937-4771},
abstract = {Student participation in free and open source software (FOSS) has potential to improve student learning in computing majors. Experiences contributing bug fixes, testing, writing documentation and developing new features can enable students to learn in a more authentic environment developing technical skills as well as teamwork and communication skills. However learning curves related to a large complex code base, use of development tools, understanding of community interactions, and scheduling of class deliverables with the FOSS calendar are only a few of the hurdles to education inherent in learning in a FOSS environment. One major roadblock to instructors desiring to support student involvement in FOSS projects is the difficulty in identifying FOSS projects suitable for student participation. This poster presents one attempt to address this problem by providing a framework of evaluation criteria that can be used to determine approachable FOSS projects for student involvement.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {31–32},
numpages = {2}
}

@inproceedings{10.1145/1384271.1384389,
author = {Gray, John and Harrison, Gill and Gorra, Andrea and Sheridan-Ross, Jakki and Finlay, Janet},
title = {A computer-based test to raise awareness of disability issues},
year = {2008},
isbn = {9781605580784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1384271.1384389},
doi = {10.1145/1384271.1384389},
abstract = {The purpose of this poster is to share our experience of a staff development tool that has been developed to raise awareness amongst staff about issues that students with disabilities might face. A computer-based test has been created as a training tool to raise awareness among university academic staff of some common experiences faced by people with visual, mobility, hearing and cognitive difficulties when using a computer [1]. The development team is based at Leeds Metropolitan University and is part of a UK centrally-funded "Centre for Excellence in Teaching and Learning" (CETL) [2], in collaboration with the Universities of Durham, Newcastle and Leeds. The Centre is devoted to promoting "Active Learning in Computing" (ALiC) [3] and is the only CETL within the Computer Science academic area. This test simulates experiences of disabled students who use computers and take computer-based tests, and provides advice and guidance to university teaching staff on how they may best cater for the needs of such students. The poster presents the reasons for creating such a tool in such a format, its structure and content.},
booktitle = {Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education},
pages = {347},
numpages = {1},
keywords = {computer assisted test, disability awareness, staff development},
location = {Madrid, Spain},
series = {ITiCSE '08}
}

@inproceedings{10.1109/AGILE.2010.14,
author = {Kajko-Mattsson, Mira and Azizyan, Gayane and Magarian, Miganoush Katrin},
title = {Classes of Distributed Agile Development Problems},
year = {2010},
isbn = {9780769541259},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/AGILE.2010.14},
doi = {10.1109/AGILE.2010.14},
abstract = {Little is known about problems encountered in distributed Agile development environments. There have been some case studies reporting on them. However, these studies have been mainly limited to the scope and context of only one or a few companies. As a result, we do not possess an overall picture of what types of problems and challenges the companies may encounter in distributed Agile environments. In this paper, we analyze twelve case studies from the existing literature, identify thirteen problems reported in them and their solutions, and we group the problems into six classes: Culture, Time Zone, Communication, Customer Collaboration, Trust, Training and Technical Issues.},
booktitle = {Proceedings of the 2010 Agile Conference},
pages = {51–58},
numpages = {8},
keywords = {communication, culture, customer availability, time zones, trust},
series = {AGILE '10}
}

@inproceedings{10.5555/525263.793947,
author = {Takvorian, Alexis and Maranian, Ken and Zack, David},
title = {Using the World Wide Web to Promote Software Engineering Education},
year = {1996},
isbn = {0818672498},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As part of Motorola's efforts to establish PowerPC systems as the platform of choice in the industry, Motorola's RISC Software department provides software development tools and operating systems support for the PowerPC architecture. The department has responded with enthusiasm to Motorola's corporate software initiatives that strive to increase even further the quality of our software products. The World Wide Web was found to be a useful tool in the group's software engineering education efforts. The Web has helped remove many obstacles encountered on the road to implementing software quality initiatives. This paper discusses how the World Wide Web technology helped bridge various levels of communication gaps and at the same time served as a catalyst for improving and distributing the group's software engineering education efforts.},
booktitle = {Proceedings of the 9th Conference on Software Engineering Education},
pages = {270},
series = {CSEE '96}
}

@inproceedings{10.1145/3334480.3382879,
author = {Schoop, Eldon and Huang, Forrest and Hartmann, Bj\"{o}rn},
title = {SCRAM: Simple Checks for Realtime Analysis of Model Training for Non-Expert ML Programmers},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382879},
doi = {10.1145/3334480.3382879},
abstract = {Many non-expert Machine Learning users wish to apply powerful deep learning models to their own domains but encounter hurdles in the opaque model tuning process. We introduce SCRAM, a tool which uses heuristics to detect potential error conditions in model output and suggests actionable steps and best practices to help such users tune their models. Inspired by metaphors from software engineering, SCRAM extends high-level deep learning development tools to interpret model metrics during training and produce human-readable error messages. We validate SCRAM through three author-created example scenarios with image and text datasets, and by collecting informal feedback from ML researchers with teaching experience. We finally reflect upon our feedback for the design of future ML debugging tools.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {debugging, interactive visualization, machine learning, tutorial systems},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI EA '20}
}

@inproceedings{10.1109/EEEE.2009.60,
author = {Qi, Chengming and Cui, Shoumei and Sun, Jianjing},
title = {On the Talents Cultivation and Evaluation Strategy},
year = {2009},
isbn = {9780769539072},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/EEEE.2009.60},
doi = {10.1109/EEEE.2009.60},
abstract = {Gardner's multiple intelligence theory provides a new thought for the colleges' cultivation of talents by its rich educational content. The multiple intelligence theory enlightens us that the intelligence structure of the creative talents is an integrated system where various intelligence essential factors are realigned, maintaining close ties with each other and make the mode of thinking diversified. In this paper, in view of the problems in the talents training of computer professional, we combine multiple intelligence theory with the characteristics of computer science to optimize instructional design, condense course content, create multi-intelligence development environment, implement effective teaching method and teaching means and adopt variety of evaluation methods to cultivate student's comprehensive ability.},
booktitle = {Proceedings of the 2009 International Conference on E-Learning, E-Business, Enterprise Information Systems, and E-Government},
pages = {129–132},
numpages = {4},
keywords = {Computer professional, Multiple intelligence theory, Talents training},
series = {EEEE '09}
}

@proceedings{10.1145/1117696,
title = {eclipse '05: Proceedings of the 2005 OOPSLA workshop on Eclipse technology eXchange},
year = {2005},
isbn = {1595933425},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {eTX and The Eclipse PhenomenonThe following sketches the history of the Eclipse project and
the eTX events.The Eclipse ProjectThe Eclipse Project began in April 1999 at IBM's OTI laboratory.
It was initially conceived as a successor product for the VisualAge
family of software development tools. VisualAge was a commercially
successful IDE, but it was also a closed environment built on
proprietary APIs. It did not integrate well with other vendors'
tools, and only the IBM/OTI team could enhance or extend the
product. Moreover, it was becoming apparent from customer
experience that more was required than a simple re-write of
VisualAge. In fact, there was growing demand for a tool integration
platform --- a programming environment that would provide kernel
IDE functionality, but also allow developers, third party vendors
and users to seamlessly add their own extensions, personalizations,
and enhancements.The Eclipse team set out to identify the essential kernel
concepts underlying the VisualAge product line (or any other IDE
for that matter). In effect, they wanted to strip out all of the
functionality within an IDE that was specific to a particular
programming language, development task, or programming model. The
hope was that there would be substantial residual function left
behind, that could then be restructured to form a content-neutral,
and programming language-neutral, foundation on which IDEs and
similar products could be built from components. It was a bold
venture, since there was no guarantee that anything practically
useful would result.What they discovered was Eclipse: a tool integration platform
together with a set of components (plugins, in the Eclipse
vernacular) that could be seamlessly assembled into a wide variety
of software development products. The Java Development Toolkit
(JDT) --- the Eclipse Java IDE --- became their proof-point. It was
built in parallel by a separate team, which operated independently
from the Eclipse Platform project. The JDT team had no special
privileges; they had to use the same APIs as any third party
product and were allowed no "back door" access to Platform kernel
functionality. The intent was that, despite these constraints, the
finished JDT should be indistinguishable from a purpose-built,
vertically integrated IDE product like VisualAge. This goal was
realized, the Eclipse Project was a success, and the Eclipse
Community was born.In the years since the Eclipse code base was released into open
source by IBM, its growth has been nothing short of spectacular.
Tens of thousands download the Eclipse SDK every week from over
fifty mirror sites around the globe. Thousands of Eclipse plugins
are now available from open source and commercial suppliers.
Software vendors are now shipping several hundred commercial
products based on Eclipse. Over 60 companies are members of the
Eclipse Foundation, which hosts Eclipse open source development.
The first Eclipse Developer Conference (EclipseCON 2004) was held
in February 2004 in Anaheim, California. Over 220 companies and
organizations from nearly 25 countries were represented. EclipseCON
2005, another success, was held in February 2005 in Burlingame,
California.Eclipse and Computer Science ResearchIt has been particularly interesting to see the uptake of
Eclipse within the research community. In retrospect one could
perhaps have anticipated this. Computing is, after all, an
empirical discipline --- ideas must be implemented to be validated.
For software researchers in particular, the computer becomes our
laboratory. We necessarily build on the work of those who have gone
before, and of course as time progresses our technology pyramids
keep getting higher. Complexity is our bane: the low-hanging fruit
were picked long ago, and most interesting problems are just not
simple. Consequently, experimentation usually requires complex
infrastructure, plumbing, as we often call it. Most researchers
spend far too much time building (and rebuilding, and fixing) this
plumbing, and far too little time actually developing new ideas.
Given the nature of research, there are seldom any applicable
standards for such infrastructure (these only come much later when
the research has matured into products). Consequently researchers
up to now have had to live and work in their own vertical towers,
sharing their ideas but only infrequently sharing code. The only
common programming platform among researchers was "emacs", and
while this continues to be very flexible, it lags far behind
industrial-scale IDEs in terms of functionality.But Eclipse changes this context. It provides a means to create
and share that necessary common infrastructure, particularly for
investigators in such areas as programming languages, tools, and
environments. Researchers can focus more of their time on their
real mission of innovation, and much less on the tedious plumbing
tasks. Moreover, Eclipse-based implementations are built from
commercial-quality components, resulting in robust demonstration
systems that make it much easier for researchers to publicize and
promote their work.What specifically does Eclipse offer researchers that makes it
so attractive? First, it is an extensible platform for integrating
components, which comes replete with a large number of commercial
quality components out of the box. It runs on nearly all operating
systems and GUI combinations, and is one of the few Java
implementations that actually realizes that language's "write once
run everywhere" potential (rather than the typical "write once test
everywhere" experience). Perhaps most importantly, it is available
in open source with a generous non-viral license. Finally, it has
tremendous visibility due to broad based industry support, which
includes the backing of such powerhouse firms as IBM, HP, SAP,
Intel, and many more.Eclipse and Computer Science EducationThere are numerous challenges in education these days such as
distance, limited resources and the recognized need to make
learning a personalized and active experience. Many educators are
consequently looking at how technology can address these challenges
and enhance learning in the classroom and beyond. For computer
science education, Eclipse has already been widely adopted as an
IDE to support programming. The advantages for some are that it is
free, platform independent and industrially relevant. But beyond
these obvious advantages, other researchers have recognized that
Eclipse provides an excellent infrastructure for developing
learning tools. These tools can leverage the wealth of technology
already present in the Eclipse community, as well as benefit from
integration with other learning tools developed by other
researchers and educators. The result of these multiple efforts is
the emergence of Eclipse as an effective and powerful platform to
support research in educational technologies and an improved
learning experience in many settings.The eclipse Technology eXchangeThat idea that Eclipse would provide exactly the rich, open and
robust platform that IT researchers needed was not initially an
obvious one, and so it needed to be promoted within the academic
community. IBM and eclipse.org set out to popularize these ideas by
hosting a series of workshops and birds-of-a-feather events at
various software research conferences. This ad hoc program
gradually evolved into the eclipse Technology eXchange (eTX)
workshops held in 2003, 2004 and 2005, the most recent of which
being held at OOPSLA 2005 in San Diego. These events provide a
forum for researchers who are using Eclipse to network and share
their experiences and their code. The foundation for a successful
eTX is a set of high quality, refereed presentations, which serve
to illustrate the breadth and vitality of the Eclipse research and
teaching communities. The paper presentations are combined with
lively discussions which will help set the stage for future
research and development using Eclipse.},
location = {San Diego, California}
}

@inproceedings{10.1145/1384271.1384315,
author = {Boyer, Kristy Elizabeth and Dwight, August A. and Fondren, R. Taylor and Vouk, Mladen A. and Lester, James C.},
title = {A development environment for distributed synchronous collaborative programming},
year = {2008},
isbn = {9781605580784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1384271.1384315},
doi = {10.1145/1384271.1384315},
abstract = {While collaborative approaches in the classroom have been shown to be highly beneficial for students of computer science, obstacles inherent in today's academic environment often prevent collocated collaborative approaches from being implemented. One solution to the collocation problem may lie with tools that facilitate distributed collaboration. This paper presents RIPPLE (Remote Interactive Pair Programming and Learning Environment), a development environment for distributed synchronous collaborative programming. RIPPLE is an open source software tool. Initial user tests demonstrate positive responses from students, and the potential for long term learning, motivation, and retention benefits is significant. In addition to its benefits for students, RIPPLE is a tool for computing education researchers who wish to collect data on collaborative programming.},
booktitle = {Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education},
pages = {158–162},
numpages = {5},
keywords = {distance learning, distributed collaboration, distributed pair programming, distributed tutoring, laboratory/active learning, programming environments},
location = {Madrid, Spain},
series = {ITiCSE '08}
}

@article{10.1080/07421222.2005.11045849,
author = {Chilton, Michael A. and Hardgrave, Bill C. and Armstrong, Deborah J.},
title = {Person-Job Cognitive Style Fit for Software Developers: The Effect on Strain and Performance},
year = {2005},
issue_date = {Number 2/Fall 2005},
publisher = {M. E. Sharpe, Inc.},
address = {USA},
volume = {22},
number = {2},
issn = {0742-1222},
url = {https://doi.org/10.1080/07421222.2005.11045849},
doi = {10.1080/07421222.2005.11045849},
abstract = {Software developers face a constant barrage of innovations designed to improve the development environment. Yet stress/strain among software developers has been steadily increasing and is at an all-time high, while their productivity is often questioned. Why, if these innovations are meant to improve the environment, are developers more stressed and less productive than they should be? Using a combination of cognitive style and person-environment fit theories as the theoretical lens, this study examines one potential source of stress/strain and productivity impediment among software developers. Specifically, this paper examines the fit between the preferred cognitive style of a software developer and his or her perception of the cognitive style required by the job environment, and the effect of that fit on stress/strain and performance. Data collected from a field study of 123 (object-oriented) software developers suggest that performance decreases and stress increases as this gap between cognitive styles becomes wider. Using surface response methodology, the precise fit relationship is modeled. The interaction of the developer and the environment provides explanatory power above and beyond either of the factors separately, suggesting that studies examining strain and performance of developers should explicitly consider and measure the cognitive style fit between the software developer and the software development environment. In practice, managers can use the results to help recognize misfit, its consequences, and the appropriate interventions (such as training or person/task matching).},
journal = {J. Manage. Inf. Syst.},
month = {nov},
pages = {193–226},
numpages = {34},
keywords = {Adaptioninnovation Theory, Personjob Fit, Software Developers, Software Development, Strain}
}

@inproceedings{10.5555/783106.783135,
author = {Truong, Nghi and Bancroft, Peter and Roe, Paul},
title = {A web based environment for learning to program},
year = {2003},
isbn = {0909925941},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {The purpose of this paper is to describe in detail the current development status of the innovative Environment for Learning to Program (ELP) which provides an interactive web-based environment for teaching programming to the first year Information Technology students at Queensland University of Technology (QUT). ELP allows students to program at the early stages of their course without the need to familiarize themselves with a program development environment. Most importantly, it eliminates all the difficulties associated with installing and running a Java compiler. Using ELP, students learn and develop their problem solving skills by working with program template exercises on the web. ELP provides a learning environment which meets the diverse needs of students.},
booktitle = {Proceedings of the 26th Australasian Computer Science Conference - Volume 16},
pages = {255–264},
numpages = {10},
keywords = {Java, XML, computer programming, online learning, tutoring system, web},
location = {<conf-loc>, <city>Adelaide</city>, <country>Australia</country>, </conf-loc>},
series = {ACSC '03}
}

@article{10.3103/S0005105518050060,
author = {Vasilieva, E. V.},
title = {Developing the Creative Abilities and Competencies of Future Digital Professionals},
year = {2018},
issue_date = {September 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {52},
number = {5},
issn = {0005-1055},
url = {https://doi.org/10.3103/S0005105518050060},
doi = {10.3103/S0005105518050060},
abstract = {This article describes the development of recommendations for using the project thinking approach in training and university education of future digital economy specialists. An overview of the key design thinking methodologies is presented. Recommendations for implementing the design thinking approach in academic disciplines are given through an example of a step-by-step description of a specific case. The design thinking approach aims to develop a person's creative abilities through empirical rules and experience, emotional intelligence and recognizing the value of other people's opinions. The article describes the methods used in design thinking: visualization tools (empathy, customer journey, and stakeholder maps), Customer Development tools, guerrilla ethnography, POV articulating, and rapid prototyping.The article also presents the primary tools used in design workshops for developing innovative ideas and adaptive problem solving. A case in formulating design thinking, titled "How can one improve the impression left on the students of professional refresher courses?" is detailed and accompanied by comments on the solutions the participants of presented design workshops.},
journal = {Autom. Doc. Math. Linguist.},
month = {sep},
pages = {248–256},
numpages = {9},
keywords = {IT education, competency-based learning, design thinking, digital competence, digital economy, information technology, innovation, soft skills}
}

@inproceedings{10.1145/2538862.2538970,
author = {Exter, Marisa},
title = {Comparing educational experiences and on-the-job needs of educational software designers},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2538970},
doi = {10.1145/2538862.2538970},
abstract = {This paper reports on part of the findings of a mixed-methods study which explored the educational experiences of Computing Professionals who design and develop educational software. A particular focus is given on the gaps professionals perceive between what was covered in their formal (university) education and the skills and knowledge that have been most important to them in their professional roles. Discrepancies were found particularly in areas related to practical skills (such as testing, maintaining code over time, use of source code control and development tools), communication, critical thinking and problem solving, and strategies used to continue learning on-the-job. Participant suggestions for improving university programs focused largely on the use of large scale, complex, authentic projects of significant duration. The author recommends further consideration be given to explicitly teaching the type of self-learning skills and strategies used by experienced professionals.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {355–360},
numpages = {6},
keywords = {computer science education, computing education, curriculum, informal education, non-formal education, pedagogy},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1109/FIE43999.2019.9028606,
author = {Font\~{a}o, Awdren and Gadelha, Bruno and J\'{u}nior, Alberto Castro},
title = {Balancing Theory and Practice in Software Engineering Education – A PBL, toolset based approach},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE43999.2019.9028606},
doi = {10.1109/FIE43999.2019.9028606},
abstract = {This paper presents an interdisciplinary approach designed to join theoretical foundations required in software engineering with practice required for professional performance in software industry. Although this is a recurring issue on software engineering education, students still have difficulties in dealing with software development environments that are usually more complex than any software projects they have worked at university courses. We developed an approach aligned with market demands and theoretical requirements in software engineering education that incorporates principles of Project Based Learning (PBL) and Agile Methods. It aims to support development of student technical and soft skills in three different, but related, disciplines: Software Engineering Practice, Database Practice and Software Project Management. We describe a set of seven recommendations on how we used largely adopted tools in software development industry in order to develop student technical and soft skills. We describe two case studies: (1) an exploratory case study with three professors and 24 students involving multiple projects; and (2) a confirmatory case study with three professors and 15 students involving a large-scale project. As a result, we point seven recommendations for tool adoption on capstone courses based on lessons learned.},
booktitle = {2019 IEEE Frontiers in Education Conference (FIE)},
pages = {1–8},
numpages = {8},
location = {Covington, KY, USA}
}

@phdthesis{10.5555/AAI28778306,
author = {Liu, Xiao and Beth, Rosson, Mary},
advisor = {Dinghao, Wu,},
title = {Neural Program Synthesis for Compiler Fuzzing},
year = {2020},
isbn = {9798535592121},
publisher = {The Pennsylvania State University},
abstract = {Compilers are among the most fundamental programming tools for building software. However, production compilers remain buggy. GNU compiler collection (GCC), as a long-lasting software released in 1987, provided as a standard compiler for most Unix-like operating systems, has caught over 3,410 bugs from the day they were created. Fuzzing is often leveraged for stress testing purposes with newly-generated, or mutated inputs to find new security vulnerabilities. In our study, we propose a grammar-based compiler fuzzing framework called DᴇᴇᴘFᴜᴢᴢ that continuously synthesizes well-formed C programs to trigger internal compiler errors or "bugs", as they are commonly called. In this framework, we are interested in how to apply generative deep neural networks (DNNs), such as the sequence-to-sequence model, to synthesize well-formed C programs based on training through syntax-correct programs. We are also interested in how to synthesize programs using a novel form of reinforcement learning, where the model becomes its teacher to start with a random neural network with no training data and trains itself through self-play. We will use a synthesized set of new C programs to fuzz off-the-shelf C compilers, e.g., GCC and Clang/LLVM. This thesis describes our analysis of neural program synthesis for compiler fuzzing in three steps.First, we conduct a first-step study by implementing DᴇᴇᴘFᴜᴢᴢ that deploys a sequence-to-sequence model to synthesize C programs. We have performed a detailed case study on analyzing the pass rate of generating well-formed programs and achieving the goal of fuzz testing, which requires a certain degree of variation. In general, DᴇᴇᴘFᴜᴢᴢ generated 82.63% syntax valid programs and improved the testing efficacy with regards to line, function, and branch coverage. It identified previously unknown bugs, and 8 of them were confirmed by the GCC developers.Second, for the cases when we could not get any or enough data to train a model for representing the grammar, we build a reinforcement learning framework for program synthesis and apply it to the BF programming language. With no training data set required, the model is initialized with random weights at the very beginning, and it evolves with environment rewards provided by the target compiler being tested. During the performance of the learning iterations, the neural network model gradually learns how to construct valid and diverse programs to improve testing efficacies under four different reward functions that we defined. We implemented the proposed method into a prototyping tool called AʟᴘʜᴀPʀᴏɢ. We performed an in-depth diversity analysis of the generated programs that explains the improved testing coverage of a target compiler being tested. We reported two important bugs for this production compiler and they were confirmed and addressed by the project owner.Third, we extend the framework to synthesize C programs, which is more challenging in terms of state space. We propose an automatic code mutation framework called FᴜᴢᴢBᴏᴏsᴛ that is based on deep reinforcement learning. By adopting testing coverage information collected at runtime as the reward, the fuzzing agent learns to fuzz a seed program that achieves an overall goal of testing coverage improvement. We implemented this new approach, and preliminary evidence showed that reinforcement fuzzing can outperform baseline random fuzzing on production compilers. It also showed that a pre-trained model can boost the fuzzing process for seed programs with similar patterns.This thesis solves the problem of using the DNN to synthesize new programs for compiler fuzz testing. Specifically, the proposed framework is able to handle compilers of different programming languages. Accordingly, DᴇᴇᴘFᴜᴢᴢ and FᴜᴢᴢBᴏᴏsᴛ are designed for the C compiler testing, and AʟᴘʜᴀPʀᴏɢ is designed for the BF language compiler testing. Additionally, the generative neural networks for program synthesis can be trained with or without training data. Moreover, the model in DᴇᴇᴘFᴜᴢᴢ is trained based on training data but AʟᴘʜᴀPʀᴏɢ and FᴜᴢᴢBᴏᴏsᴛ rely on reinforcement learning, which requires no training samples. We built prototyping tools for each study and applied them for practical use. Their effectiveness was evaluated, and they caught real bugs in off-the-shelf compilers.},
note = {AAI28778306}
}

@inproceedings{10.1145/305786.305828,
author = {McCracken, Michael and Waters, Robert},
title = {Why? When an otherwise successful intervention fails},
year = {1999},
isbn = {1581130872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/305786.305828},
doi = {10.1145/305786.305828},
abstract = {Problem-Based Learning (PBL) has been an effective technique in developing self-directed learning and problem-solving skills in students --- especially in the medical school environment. This paper looks at some preliminary results of an ethnographic study of students in a software development environment trying to use PBL. Our findings indicate that students need explicit training in group dynamics, students tend to rely excessively on existing knowledge, and they focus almost solely on product-related issues versus process-related ones. We then present some suggested improvements and future planned research.},
booktitle = {Proceedings of the 4th Annual SIGCSE/SIGCUE ITiCSE Conference on Innovation and Technology in Computer Science Education},
pages = {9–12},
numpages = {4},
keywords = {Computer Science Education, Information Systems Education, Problem Based Learning},
location = {Cracow, Poland},
series = {ITiCSE '99}
}

@inproceedings{10.1145/1067445.1067452,
author = {Truong, Nghi and Bancroft, Peter and Roe, Paul},
title = {Learning to program through the web},
year = {2005},
isbn = {1595930248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1067445.1067452},
doi = {10.1145/1067445.1067452},
abstract = {Computer-based tutoring systems which assist students in solving introductory programming problems have significant potential for improving the quality of programming education and reducing the instructor's work load. The innovative Environment for Learning to Program (ELP) provides an interactive web-based environment for teaching programming to first year Information Technology students at Queensland University of Technology (QUT). ELP allows students to undertake programming exercises by "filling in the gaps" of a partial computer program presented in a web page and to receive guidance in getting their programs to compile and run. Feedback on quality and correctness is provided through a program analysis framework. Students are given the opportunity to produce working programs at the early stages of their course without the need to familiarize themselves with a complex program development environment.},
booktitle = {Proceedings of the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education},
pages = {9–13},
numpages = {5},
keywords = {computer programming, feedback, flexible delivery, online learning, tutoring system, web},
location = {Caparica, Portugal},
series = {ITiCSE '05}
}

@inproceedings{10.1145/1113549.1113559,
author = {Bisiani, R.},
title = {The role of simulation in the development of task-oriented computer architectures},
year = {1979},
isbn = {0897910079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1113549.1113559},
doi = {10.1145/1113549.1113559},
abstract = {With the recent improvements in the technology of inlegrated circuits there has been increasing interest in task-oriented computer architectures. At Carnegie-Mellon University we have built a multi-microprocessor architecture tailored to the execution of the Harpy speech understanding system (Harpy Machine). While doing this, we have been confronted with the non-trivial problem of developing software for a lask-oriented multiprocessor. This paper is concerned with one approach to the problem of software development: the use of an ad-hoc simulator as a cheap software development tool (as opposed to the typical use of a simulator as a design evaluation tool). The experience made with the simulator in the development of the software for the Harpy Machine is also discussed.},
booktitle = {Proceedings of the Second Symposium on Small Systems},
pages = {69–74},
numpages = {6},
keywords = {debugging, microprocessors, multiprocessor systems, simulation, software development, speech recognition systems, task-oriented computer architecture},
location = {Dallas, Texas},
series = {SIGSMALL/PC}
}

@article{10.1155/2022/4127924,
author = {Fu, Qi and Bin, Sheng},
title = {Dynamic Modeling of Influencing Factors and Change Trend of College Students' Mental Health Based on Big Data},
year = {2022},
issue_date = {2022},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2022},
issn = {1058-9244},
url = {https://doi.org/10.1155/2022/4127924},
doi = {10.1155/2022/4127924},
abstract = {With the great changes of social development environment and the lack of psychological preparation of many students, many college students have many psychological problems in the process of adapting to the environment. If the problem cannot be solved in time, it will seriously affect their future adaptation and mental health. In order to solve the psychological problems of college students, the networked dynamic model constructed by using big data modeling technology can realize the system functions required by users and model the model accordingly. The main research direction of this paper is to study the influencing factors and change trend of college students' mental health under the background of big data. In the network-oriented scenario, the missing value of the data is supplemented, and the sequence prediction technology is added to the data. Finally, by adding corresponding data samples to the improved big data dynamic model, the results of the addition completion method and data sequence prediction technology are analyzed.},
journal = {Sci. Program.},
month = {jan},
numpages = {7}
}

@article{10.1007/s10639-010-9139-3,
author = {Blake, Ben},
title = {BLAKE: A language designed for Programming I},
year = {2010},
issue_date = {December  2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {4},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-010-9139-3},
doi = {10.1007/s10639-010-9139-3},
abstract = {The process of comprehending a problem, strategically developing a solution and translating the solution into an algorithm is arguably the single most important series of skills acquired during the education of an undergraduate computer science or information technology major. With this in mind, much care should be taken when choosing a programming language to deploy in the first University programming course. BLAKE, Beginners Language for Acquiring Key programming Essentials, is designed specifically for use in a Programming I class. BLAKE aids in enforcing fundamental object-oriented practices while simultaneously facilitating the transition to subsequent programming languages. BLAKE's major features include; consistent parameter passing, single inheritance, non-redundant control structures, a simple development environment, and hardware independent data types. The syntax remains relatively small while still facilitating a straightforward transition to industry standard programming languages.},
journal = {Education and Information Technologies},
month = {dec},
pages = {277–291},
numpages = {15},
keywords = {Curriculum, Grammar, Object oriented, Programming language, Syntax}
}

@inproceedings{10.1109/ICESS.2009.90,
author = {Zhu, Angela Yun and Inoue, Jun and Peralta, Marisa Linnea and Taha, Walid and O'Malley, Marcia K. and Powell, Dane},
title = {Implementing Haptic Feedback Environments from High-Level Descriptions},
year = {2009},
isbn = {9780769536781},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICESS.2009.90},
doi = {10.1109/ICESS.2009.90},
abstract = {Haptic feedback can be a critical component of virtual environments used in cognitive research, rehabilitation, military training, and entertainment. A limiting factor in the innovation and the acceptance of virtual environments with haptic feedback is the time and cost required to build them. This paper presents a development environment called iAcumen that supports a new approach for programming such systems. This approach allows the developer to directly express physical equations describing the underlying dynamics. By raising the level of abstraction for the developer, we avoid many of the problems that limit the effectiveness of traditional approaches.},
booktitle = {Proceedings of the 2009 International Conference on Embedded Software and Systems},
pages = {482–489},
numpages = {8},
keywords = {haptic feedback systems, simulation, virtual environments},
series = {ICESS '09}
}

@inproceedings{10.1145/2157136.2157311,
author = {Kurtz, Barry L. and Fenwick, James B. and Meznar, Philip},
title = {Developing microlabs using Google web toolkit},
year = {2012},
isbn = {9781450310987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2157136.2157311},
doi = {10.1145/2157136.2157311},
abstract = {Closed labs have become a common feature in computer science education because they provide hands-on experience in a supervised setting. Microlabs extend this approach into the lecture format with very short hands-on activities in the "middle of the lecture." The programming microlab approach was developed for a distributed computing course that integrated all levels of parallelism (multicore, cluster, and grid). Since that time we have developed logical microlabs where students solve conceptual problems that do not involve programming. These are integrated into a Microlab Learning Cycle. We want our microlabs to be usable with a wide variety of computing devices, including tablets. After experimenting with different development environments we have adopted the Google Web Toolkit (GWT). After presenting the current status of our activities, we discuss future directions for microlab development. This work is supported, in part, by three National Science Foundation grants.},
booktitle = {Proceedings of the 43rd ACM Technical Symposium on Computer Science Education},
pages = {607–612},
numpages = {6},
keywords = {active learning, automated grading, web development},
location = {Raleigh, North Carolina, USA},
series = {SIGCSE '12}
}

@inproceedings{10.5555/1562524.1562569,
author = {Johnson, W. Lewis and Vilhjalmsson, Hannes and Marsella, Stacy},
title = {Serious Games for Language Learning: How Much Game, How Much AI?},
year = {2005},
isbn = {1586035304},
publisher = {IOS Press},
address = {NLD},
abstract = {Modern computer games show potential not just for engaging and entertaining users, but also in promoting learning. Game designers employ a range of techniques to promote long-term user engagement and motivation. These techniques are increasingly being employed in so-called serious games, games that have non-entertainment purposes such as education or training. Although such games share the goal of AIED of promoting deep learner engagement with subject matter, the techniques employed are very different. Can AIED technologies complement and enhance serious game design techniques, or does good serious game design render AIED techniques superfluous? This paper explores these questions in the context of the Tactical Language Training System (TLTS), a program that supports rapid acquisition of foreign language and cultural skills. The TLTS combines game design principles and game development tools with learner modelling, pedagogical agents, and pedagogical dramas. Learners carry out missions in a simulated game world, interacting with non-player characters. A virtual aide assists the learners if they run into difficulties, and gives performance feedback in the context of preparatory exercises. Artificial intelligence plays a key role in controlling the behaviour of the non-player characters in the game; intelligent tutoring provides supplementary scaffolding.},
booktitle = {Proceedings of the 2005 Conference on Artificial Intelligence in Education: Supporting Learning through Intelligent and Socially Informed Technology},
pages = {306–313},
numpages = {8}
}

@inproceedings{10.1109/ICETET.2011.37,
author = {G\'{a}ti, Jozsef and K\'{a}rty\'{a}s, Gyula},
title = {Computer System Support for Higher Education Programs in Engineering},
year = {2011},
isbn = {9780769545615},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICETET.2011.37},
doi = {10.1109/ICETET.2011.37},
abstract = {Unique affect of computer and information technology development on education in engineering is that both higher education courses and product development environments can utilize high performance methods, tools, and systems. Product development relies upon sophisticated product description and simulations. At the same time, education of engineers involves teaching competences those are also applied in product modeling and simulation. It is evident, that connection of computer systems in product development and higher education could bring a real integration of academic system with field practice. A former result in computer modeling of higher education courses was analyzed by the authors regarding issues for application at the every day practice of engineering education. This paper introduces new results in possibilities for the support of higher education programs considering recent advances in higher education and engineering modeling processes. Paper starts with an outline of supports for education programs in a computer system. Following this, teaching, projects, and problem solving are discussed in education program. Next, course description, elements of higher education process, and main connections of industrial instruction are explained in a practice oriented context. Finally, integrating product modeling with course modeling is sketched and concluded.},
booktitle = {Proceedings of the 2011 Fourth International Conference on Emerging Trends in Engineering &amp; Technology},
pages = {61–65},
numpages = {5},
keywords = {computer system assisted education, course modeling, higher education in engineering},
series = {ICETET '11}
}

@inproceedings{10.1145/3463274.3463806,
author = {Shrestha, Sohil Lal and Csallner, Christoph},
title = {SLGPT: Using Transfer Learning to Directly Generate Simulink Model Files and Find Bugs in the Simulink Toolchain},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463806},
doi = {10.1145/3463274.3463806},
abstract = {Finding bugs in a commercial cyber-physical system (CPS) development tool such as Simulink is hard as its codebase contains millions of lines of code and complete formal language specifications are not available. While deep learning techniques promise to learn such language specifications from sample models, deep learning needs a large number of training data to work well. SLGPT addresses this problem by using transfer learning to leverage the powerful Generative Pre-trained Transformer 2 (GPT-2) model, which has been pre-trained on a large set of training data. SLGPT adapts GPT-2 to Simulink with both randomly generated models and models mined from open-source repositories. SLGPT produced Simulink models that are both more similar to open-source models than its closest competitor, DeepFuzzSL, and found a super-set of the Simulink development toolchain bugs found by DeepFuzzSL.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {260–265},
numpages = {6},
keywords = {Cyber-physical system development, GPT-2, Simulink, deep learning, programming language modeling, tool chain bugs},
location = {Trondheim, Norway},
series = {EASE '21}
}

@article{10.2307/248753,
author = {Naumann, Justus D. and Palvia, Shailendra},
title = {A selection model for systems development tools},
year = {1982},
issue_date = {March 1982},
publisher = {Society for Information Management and The Management Information Systems Research Center},
address = {USA},
volume = {6},
number = {1},
issn = {0276-7783},
url = {https://doi.org/10.2307/248753},
doi = {10.2307/248753},
abstract = {Selecting from the many currently available systems development methodologies (SDMs) and development techniques is a difficult problem with economic, technical, and behavioral implications. A quantitative approach to the selection problem is represented.The selection model begins with a definition of a superset of functions expected of a systems development tool. Functions are then weighted, using a Delphi approach to achieve acceptable valuations among system managers. Next, each approach under consideration is evaluated with respect to each function desired. After scores are computed for each methodology, economic and qualitative aspects such as training availability and cost can be used to differentiate the highest ranked alternatives.The four-person MBA project team from the Graduate School of Management at the University of Minnesota, with the guidance from authors, applied the model to a methodology selection problem. In addition to producing a quantitative ranking of competing methodologies, the approach described furthered understanding of the functions to be performed by the methodologies being considered. It also gained acceptance, admittedly reluctant, of the recommended methodology from managers who strongly advocated their own favorites.},
journal = {MIS Q.},
month = {mar},
pages = {39–48},
numpages = {10},
keywords = {SDM, management, methodology, standards, systems development}
}

@inproceedings{10.1007/978-3-540-85033-5_46,
author = {Wang, Xiaodan and Fang, Fang and Fan, Lei},
title = {Ontology-Based Description of Learning Object},
year = {2008},
isbn = {9783540850328},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-85033-5_46},
doi = {10.1007/978-3-540-85033-5_46},
abstract = {With the development of information technologies many researchers and instructors are interested in the educational resources on the Web. All the resources are described as Learning Object which focuses on reusability and automation. Learning Object is a central notion of the majority of current researches to Web-based education, and many institutions devote themselves to research the standardization of learning object. After analyzing Learning Object's definition and metadata the authors emphasize that the metadata alone is not enough because it is lack of semantic and reasoning capability. Semantic Web is put forward to solve the semantic problems in current Web. Ontology is the core concept and technology in this framework. In order to reuse and share learning object better, the ontology-based description of learning object is expatiated, meanwhile this paper provides a learning object ontology with the development tool Prot\'{e}g\'{e}. At last a shareable model in Semantic Web is mentioned.},
booktitle = {Proceedings of the 7th International Conference on Advances in Web Based Learning},
pages = {468–476},
numpages = {9},
keywords = {Learning Object, Ontology, Semantic Web, share},
location = {Jinhua, China},
series = {ICWL '08}
}

@inproceedings{10.1145/3308557.3308666,
author = {Park, Yoomi and Lim, Eun-Ji and Ahn, Shinyoung and Choi, Wan and Kim, Taewoo},
title = {DL-dashboard: user-friendly deep learning model development environment},
year = {2019},
isbn = {9781450366731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308557.3308666},
doi = {10.1145/3308557.3308666},
abstract = {In recent years, deep learning has contributed to a big step forward in artificial intelligence, so that deep learning models have been created extensively in a variety of areas. However, development of deep learning model requires high implementation skills as well as domain knowledge. Additionally, finding the best model is a process of a lot of trial-and-error for developers. To alleviate the developers' difficulties, we have developed a deep learning model development environment called DL-Dashboard that allows developers can create new models easily and quickly by drag-and-dropping built-in layer component and can train the models by selecting one of the suggested training options without much deep learning experience. We explain design principles and implementation of DL-Dashboard system and show how developers can create and train models user-friendly on it.},
booktitle = {Companion Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {73–74},
numpages = {2},
keywords = {deep learning, deep learning model creation, user interface},
location = {<conf-loc>, <city>Marina del Ray</city>, <state>California</state>, </conf-loc>},
series = {IUI '19 Companion}
}

@article{10.1155/2022/7703327,
author = {Chen, Sijing and Zhu, Rukui and Huang, Jinshun and Bin, Sheng},
title = {Construction and Application of University Education Management Model Based on Intelligent Programming Analysis Method},
year = {2022},
issue_date = {2022},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2022},
issn = {1058-9244},
url = {https://doi.org/10.1155/2022/7703327},
doi = {10.1155/2022/7703327},
abstract = {The intelligent programming analysis system based on Internet has attracted more and more attention in the field of education management. The problem of how to flexibly master the application capabilities of intelligent programming and development tools, debugging and optimizing programs, and project deployment is becoming more and more serious. Based on the intelligent programming analysis method, this paper systematically introduces the research and design process of education management model and designs modules such as distributed parallel PhpDig, information analyzer, and information resource database. The system adopts client/middleware/server (C/M/S) three-tier architecture planning and design. The server-side carries the resource manager, the middleware carries the intelligent work tasks (information analyzer, resource collection agent), and the client implements user interaction and data representation solves the storage load problem of a large amount of data. The experimental results show that the semantic processing of search object attributes can improve the retrieval performance, and the retrieval rate and tolerance factor reach 87.6% and 0.041, respectively, which effectively promotes the integration of the analysis data on the chain level and the education system.},
journal = {Sci. Program.},
month = {jan},
numpages = {14}
}

@inproceedings{10.1145/3209635.3209643,
author = {Hadfield, Steven M. and Raynor, Justin T. and Sievers, Matthew D.},
title = {Engaging Secondary and Post-Secondary Students to Learn and Explore Programming Using a Theme-Based Curriculum and the Sphero SPRK+ Robot},
year = {2018},
isbn = {9781450358057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209635.3209643},
doi = {10.1145/3209635.3209643},
abstract = {The recent emergence of durable, low cost, and highly capable robots on the commercial market provides opportunity for engaging and highly motivational new curricula to teach computer programming and problem solving principles such as those typically found in a CS 0.5 course and those implementing the College Board's CS Principles curriculum framework. This paper documents a seventeen lesson curriculum based on the Sphero SPRK+ robot, the Sphero Edu development environment, and a motivating theme based upon the 2015 movie, The Martian. Along with the curriculum itself, discussion includes an experience report with a pilot run of the curriculum with two small sections of a freshman-level introduction to computing course aligned with the College Board's CS Principles curriculum framework. Initial results indicate great potential for highly engaging and effective pedagogy based on this approach. The results also reveal some practical challenges with implementing similar approaches based upon current and near-term technologies.},
booktitle = {Proceedings of the 23rd Western Canadian Conference on Computing Education},
articleno = {2},
numpages = {6},
keywords = {Blockly, CS Principles, Introduction to Programming, JavaScript, Robotics, Sphero SPRK+, Theme-Based Learning Activities},
location = {Victoria, BC, Canada},
series = {WCCCE '18}
}

@article{10.1109/72.712157,
author = {Meneganti, M. and Saviello, F. S. and Tagliaferri, R.},
title = {Fuzzy neural networks for classification and detection of anomalies},
year = {1998},
issue_date = {September 1998},
publisher = {IEEE Press},
volume = {9},
number = {5},
issn = {1045-9227},
url = {https://doi.org/10.1109/72.712157},
doi = {10.1109/72.712157},
abstract = {A new learning algorithm for the Simpson fuzzy min-max neural network is presented. It overcomes some undesired properties of the Simpson model. Our new algorithm improves the network performance; the classification result does not depend on the presentation order of the patterns in the training set, and at each step, the classification error in the training set cannot increase. The new neural model is particularly useful in classification problems. Tests were executed on three different classification problems: 1) with two-dimensional synthetic data; 2) with realistic data generated by a simulator to find anomalies in the cooling system of a blast furnace; and 3) with real data for industrial diagnosis. The experiments were made following some recent evaluation criteria known in the literature and by using Microsoft Visual C++ development environment on personal computers},
journal = {Trans. Neur. Netw.},
month = {sep},
pages = {848–861},
numpages = {14}
}

@article{10.1155/2022/3414935,
author = {Feng, Peilu and Wu, Qi and Zhu, Fusheng},
title = {Digital Teaching Management System Based on Deep Learning of Internet of Things},
year = {2022},
issue_date = {2022},
publisher = {IOS Press},
address = {NLD},
volume = {2022},
issn = {1574-017X},
url = {https://doi.org/10.1155/2022/3414935},
doi = {10.1155/2022/3414935},
abstract = {In order to solve a series of problems similar to the repetitive construction of resources and low degree of resource sharing in cruciform teaching, this paper studies the digital disarming management. Based on the in-depth analysis of the actual needs of the digital teaching resource service system and the key problems in the system, taking the resources and business process as the starting point, based on Java Web related technology, combined with the related processes of resources and business processing, this paper designs the digital teaching resource system of colleges and universities. The system has the functions of resource digitization, process management, and interaction with other platforms. The system database platform and operation platform are built; the development environment is configured; and the user login service function, data conversion service function, process management service function, and system management service function are completed. The function is tested by unit test, and the performance is tested and analyzed according to user requirements and design objectives. The system adopts Java web development technology, uses S2SH framework as the development basis, and takes Oracle database as the data storage platform through Tomcat6.0 web server for program publishing. Through system testing and analysis, the software can operate normally and achieve the expected functions and can be put into use.},
journal = {Mob. Inf. Syst.},
month = {jan},
numpages = {11}
}

@inproceedings{10.5555/1323159.1323205,
author = {Derrick, E. Joseph and Htay, Maung and Vaccare, Carmel},
title = {EVA (extended video application): developing tablet PC software for multi-disciplinary mobile e-learning},
year = {2007},
publisher = {ACTA Press},
address = {USA},
abstract = {The use of video within clinical courses at Radford University (RU) is currently limited by labor-intensive, tape-based equipment for organizing and accessing specific scenes and events. Our faculty has successfully applied Tablet PCs and Microsoft OneNote® in courses to capture and analyze instructional sessions, but these existing systems have limited capabilities. For example, OneNote currently only supports synchronous live feeds of web cameras with moderate video fidelity. However, prerecorded asynchronous sessions fed from DVD players or saved digital movie files of sessions at remote locations are all primary sources of observation data. The overall goal of this project is to design, implement, and evaluate a Tablet PC-based software application called EVA (Extended Video Application) to overcome these limitations and substantially improve the effectiveness of specific clinical learning experiences in education, counseling, and social work courses. This paper gives the background, motivation, and an overview of the project, reports on project status, describes the software development environment, and discusses the technical difficulties and issues surrounding the design and implementation components. The paper also covers future work (which includes a rigorous evaluation component), expected outcomes, and conclusions.},
booktitle = {Proceedings of the Sixth Conference on IASTED International Conference Web-Based Education - Volume 2},
pages = {258–263},
numpages = {6},
keywords = {assessment, mobile e-learning, software development, tablet},
location = {Chamonix, France},
series = {WBED'07}
}

@article{10.1109/3477.740163,
author = {Wu, Tzu-Ping and Chen, Shyi-Ming},
title = {A new method for constructing membership functions and fuzzy rules from training examples},
year = {1999},
issue_date = {February 1999},
publisher = {IEEE Press},
volume = {29},
number = {1},
issn = {1083-4419},
url = {https://doi.org/10.1109/3477.740163},
doi = {10.1109/3477.740163},
abstract = {To extract knowledge from a set of numerical data and build up a rule-based system is an important research topic in knowledge acquisition and expert systems. In recent years, many fuzzy systems that automatically generate fuzzy rules from numerical data have been proposed. In this paper, we propose a new fuzzy learning algorithm based on the α-cuts of equivalence relations and the α-cuts of fuzzy sets to construct the membership functions of the input variables and the output variables of fuzzy rules and to induce the fuzzy rules from the numerical training data set. Based on the proposed fuzzy learning algorithm, we also implemented a program on a Pentium PC using the MATLAB development tool to deal with the Iris data classification problem. The experimental results show that the proposed fuzzy learning algorithm has a higher average classification ratio and can generate fewer rules than the existing algorithm},
journal = {Trans. Sys. Man Cyber. Part B},
month = {feb},
pages = {25–40},
numpages = {16}
}

@article{10.1007/s10055-021-00550-1,
author = {VanHorn, Kevin and \c{C}obano\u{g}lu, Murat Can},
title = {Democratizing AI in biomedical image classification using virtual reality},
year = {2022},
issue_date = {Mar 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {1},
issn = {1359-4338},
url = {https://doi.org/10.1007/s10055-021-00550-1},
doi = {10.1007/s10055-021-00550-1},
abstract = {Artificial intelligence models can produce powerful predictive computer vision tools for healthcare. However, their development simultaneously requires computational skill as well as biomedical expertise. This barrier often impedes the wider utilization of AI in professional environments since biomedical experts often lack software development skills. We present the first development environment where a user with no prior training can build near-expert level convolutional neural network classifiers on real-world datasets. Our key contribution is a simplified environment in virtual reality where the user can build, compute, and critique a model. Through a controlled user study, we show that our software enables biomedical researchers and healthcare professionals with no AI development experience to build AI models with near-expert performance. We conclude that the potential role for AI in the biomedical domain can be realized more effectively by making its development more intuitive for non-technical domain experts using novel modes of interaction.},
journal = {Virtual Real.},
month = {mar},
pages = {159–171},
numpages = {13},
keywords = {Deep learning, Machine learning, Neural nets, Virtual reality, Visualization}
}

@inproceedings{10.1109/IE.2014.34,
author = {R\'{\i}os, Anasol Pe\~{n}a and Callaghan, Vic and Gardner, Michael and Alhaddad, Mohammed J.},
title = {Using Mixed-Reality to Develop Smart Environments},
year = {2014},
isbn = {9781479929474},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IE.2014.34},
doi = {10.1109/IE.2014.34},
abstract = {Smart homes, smart cars, smart classrooms are now a reality as the world becomes increasingly interconnected by ubiquitous computing technology. The next step is to interconnect such environments, however there are a number of significant barriers to advancing research in this area, most notably the lack of available environments, standards and tools etc. A possible solution is the use of simulated spaces, nevertheless as realistic as strive to make them, they are, at best, only approximations to the real spaces, with important differences such as utilising idealised rather than noisy sensor data. In this respect, an improvement to simulation is emulation, which uses specially adapted physical components to imitate real systems and environments. In this paper we present our work-in-progress towards the creation of a development tool for intelligent environments based on the interconnection of simulated, emulated and real intelligent spaces using a distributed model of mixed reality. To do so, we propose the use of physical/virtual components (xReality objects) able to be combined through a 3D graphical user interface, sharing real-time information. We present three scenarios of interconnected real and emulated spaces, used for education, achieving integration between real and virtual worlds.},
booktitle = {Proceedings of the 2014 International Conference on Intelligent Environments},
pages = {182–189},
numpages = {8},
keywords = {HCI, blended reality, hyperreality, intelligent environments, interreality, mixed reality, ubiquitous virtual reality},
series = {IE '14}
}

@inproceedings{10.1145/98894.99124,
author = {Pratt, Lorien and Cebulka, Kathleen D. and Clitherow, Peter},
title = {Residual speech signal compression: an experiment in the practical application of neural network technology},
year = {1990},
isbn = {0897913728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/98894.99124},
doi = {10.1145/98894.99124},
abstract = {Neural networks are a popular area of research today. However, neural network algorithms have only recently proven valuable to application problems. This paper seeks to aid in the process of transferring neural network technology from research to a development environment by describing our experience in applying this technology.The application studied here is Speaker Identity Verification (SIV), which is the task of verifying a speaker's identity by comparing the speaker's voice pattern to a stored template.In this paper, we describe the application of the back-propagation neural network algorithm to one aspect of the SIV problem, called Residual Compression (RC). The RC problem is to extract useful features from a part of the speech signal that was not utilized by previous SIV systems. Here, we describe a neural network architecture, pre-processing algorithm, training methodology, and empirical results for this problem. We also present a few guidelines for the use of neural networks in applied settings.},
booktitle = {Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2},
pages = {1063–1072},
numpages = {10},
location = {Charleston, South Carolina, USA},
series = {IEA/AIE '90}
}

@inproceedings{10.1145/2538862.2544310,
author = {Ureel II, Leo C. and Earnest, John and Wallace, Charles},
title = {Copper country programmers: a novel curriculum for beginning programmers in middle and high school (abstract only)},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2544310},
doi = {10.1145/2538862.2544310},
abstract = {With the continued and alarming lack of involvement in computing among college students, attention has recently focused on engaging students at the middle and high school levels. Our "Copper Country Programmers" club began as a community outreach program to fill a gap left by the elimination of the computing curriculum in the local school district. In our club, university faculty and students provide curriculum, tools, and classroom tutoring for young novice programmers across the school district. Our curriculum teaches programming through a series of exercises involving HTML, BASIC, LOGO, and Processing (a graphics-oriented Java variant.) Contrary to some other approaches, we present students initially with a low level, bare bones programming model and development environment, then gradually increase the functionality and complexity. We find that students readily grasp the simple, transparent initial model, then experience first-hand the motivations for adding functionality (and complexity). Our cross-curricular programming topics include graphic design, interactive fiction, computer generated poetry, mathematical simulation, computational geometry, game physics, computer art, artificial intelligence, video game development, and critical thinking. We provide examples of our curriculum, student work, problems encountered, and how they were resolved.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {722–723},
numpages = {2},
keywords = {K-12 computer science curriculum, K-12 instruction, community outreach},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/3524383.3533247,
author = {Zhao, Huimin},
title = {Design of English Teaching Interactive System Based on Artificial Intelligence},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3533247},
doi = {10.1145/3524383.3533247},
abstract = {By exploring the characteristic of interaction of network teaching, we proposed to develop an English teaching interaction system based on artificial intelligence, which can solve the problems in current English teaching class. We analyzed the functional requirements of the system corresponding to different user. By buiding Ubuntu operating system as the computing platform and exploring the object-oriented dynamic teaching environment provided by artificial intelligence platform, we build the LAMP development environment with MySQL database and PHP script language. We constructed a three-tier separated architecture system by using B/S mode, and refined the design and implementation of each module function through the secondary development of platform module plug-ins. To exhibit the advantages of the artificial intelligence system, we take college English teaching as an example to illustrate its functions. The functions of curriculum creation, teaching resources and activity design can be realized in the system, which verifies the effectiveness of the artificial intelligence system in interactive teaching and learning.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {451–454},
numpages = {4},
keywords = {Artificial intelligence, B/S architecture, English teaching, LAMP development environment},
location = {Shanghai, China},
series = {ICBDE '22}
}

@phdthesis{10.5555/1104364,
author = {Dyck, Randall and Neiman, Amiram},
title = {A comparison of student perceptions of complexity for traditional and object-oriented programs},
year = {2005},
isbn = {0542155397},
publisher = {Northcentral University},
abstract = {Scope of study. This dissertation investigated differences in students' perceptions of the relative perceived complexity of traditionally developed software programs versus object-oriented developed software programs. Previous research and students at two different academic institutions were surveyed to gain insights for developing a framework by which different programming paradigms may be compared for complexity. Findings and conclusions. This dissertation discovered that the surveyed students' perceptions of programs' relative complexity increase when object-oriented technologies are used to solve a given business problem instead of traditional programming technologies. It will be of interest to faculty and administrators at technical institutes who want to have a greater understanding of student perceptions of complexity in different programming paradigms. It identified critical success factors, such as formatting and input/output commands, that can impact perceived program complexity. The findings also indicated a need for more efficient development tools, better training for existing tools, and more consistent design approaches.},
note = {AAI3176270}
}

@article{10.5555/614651.614726,
author = {M\"{u}hlh\"{a}user, Max and Gecsei, Jan},
title = {Services, Frameworks, and Paradigms for Distributed Multimedia Applications},
year = {1996},
issue_date = {September 1996},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {3},
number = {3},
issn = {1070-986X},
abstract = {The development of distributed multimedia applications is supported by an increasing number of services. While such services pave the way toward sophisticated multimedia support even in distributed systems, using them still makes the task of developers quite tedious. This is because several inconsistent services have to be interfaced in order to reflect different aspects. As a way to alleviate this problem, we make the case for an encompassing framework in which all services would be offered under a unifying paradigm. First we give an overview of existing multimedia services with a focus on distribution, extracting the requirements imposed on multimedia extensions to general frameworks as a set of so-called abstractions. Known development environments for distributed applications are obvious candidates for such encompassing frameworks. We review these based on four popular paradigms: client-server/remote procedure call, object-orientation, hypermedia, and open documents. We also investigate possible multimedia extensions and discuss the "expressive power" of the paradigms. In conclusion, we propose steps towards an encompassing framework based on a hybrid object/hypermedia paradigm. Readers may contact M\"{u}hlh\"{a}user at the University of Linz, Altenbergerstrasse 69, A-4040 Linz, Austria, e-mail: max@tk.uni-linz.ac.at},
journal = {IEEE MultiMedia},
month = {sep},
pages = {48–61},
numpages = {14}
}

@article{10.1016/S1045-926X(06)80007-8,
author = {Rasure, John R. and Williams, Carla S.},
title = {An integrated data flow visual language and software development environment},
year = {1991},
issue_date = {September, 1991},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {2},
number = {3},
issn = {1045-926X},
url = {https://doi.org/10.1016/S1045-926X(06)80007-8},
doi = {10.1016/S1045-926X(06)80007-8},
abstract = {The current generation of data flow based visual programming systems is all too often limited in application. It is our contention that data flow visual languages, to be more widely accepted for solving a broad range of problems, need to be more general in their syntax, semantics, translation schemes, computational model, execution methods and scheduling. These capabilities should be accompanied by a development environment that facilitates information processing extensions needed by the user to solve a wide range of application-specific problems. This paper addresses these issues by describing and critiquing the Khoros system implemented by the University of New Mexico, Khoros Group. The Khoros infrastructure consists of several layers of interacting subsystems. A user interface development system (UIDS) combines a high-level user interface specification with methods of software development that are embedded in a code generation tool set. The UIDS is used to create, install and maintain the fundamental operators for cantata, the visual programming language component of Khoros.},
journal = {J. Vis. Lang. Comput.},
month = {sep},
pages = {217–246},
numpages = {30}
}

@article{10.1155/2021/6259995,
author = {Gao, Jie and Su, Jian},
title = {Research on Machine Learning-Based Error Correction Algorithm for Spoken French},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/6259995},
doi = {10.1155/2021/6259995},
abstract = {In order to overcome the problems of low error capture accuracy and long response time of traditional spoken French error correction algorithms, this study designed a French spoken error correction algorithm based on machine learning. Based on the construction of the French spoken pronunciation signal model, the algorithm analyzes the spectral features of French spoken pronunciation and then selects and classifies the features and captures the abnormal pronunciation signals. Based on this, the machine learning network architecture and the training process of the machine learning network are designed, and the operation structure of the algorithm, the algorithm program, the algorithm development environment, and the identification of oral errors are designed to complete the correction of oral French errors. Experimental results show that the proposed algorithm has high error capture accuracy and short response time, which prove its high efficiency and timeliness.},
journal = {Sec. and Commun. Netw.},
month = {jan},
numpages = {10}
}

@inproceedings{10.1145/3407982.3408003,
author = {Petkov, Emiliyan and Angelov, Vladislav},
title = {Virtual Reality Training System for Specialists Who Operate on High-Voltage Switchgears in an Oil Plant in Russia},
year = {2020},
isbn = {9781450377683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407982.3408003},
doi = {10.1145/3407982.3408003},
abstract = {Operating on high voltage switchgears in oil plants in Russia as well as anywhere else in the world requires well-trained operative personnel. To improve the training of the workers in such plants computer-based logical simulators are developed usually. However, they are not sufficient enough in training because the trainees learn theoretically and not practically. Training specialists from different areas of industry using virtual reality environments where participants learn practically is up-to-date topic and a sphere for top-of-the-art research and development. This article examines the creation of a virtual reality training system for personnel from oil plants and represents the requirements for the training environment, the model of the system and the selection of a specific virtual reality technology and development environment. The definitions of the problem, the research and the development of the system have been entrusted to the authors of this paper by CS Construction Solutions Ltd., UK.},
booktitle = {Proceedings of the 21st International Conference on Computer Systems and Technologies},
pages = {266–269},
numpages = {4},
keywords = {Industry 4.0, Virtual reality, oil, plant, switchgears, training},
location = {Ruse, Bulgaria},
series = {CompSysTech '20}
}

@inproceedings{10.1109/SSDM.2000.869796,
author = {Balovnev, Oleg and Breunig, Martin and Cremers, Armin B. and Shumilov, Serge},
title = {Extending GeoToolKit to Access Distributed Spatial Data and Operations},
year = {2000},
isbn = {0769506860},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SSDM.2000.869796},
doi = {10.1109/SSDM.2000.869796},
abstract = {The integration of extremely heterogeneous geo-scientific data is a challenging problem. To provide a non-redundant maintenance and cooperative utilization of data by different applications developed within the interdisciplinary geo-scientific research center at the University of Bonn we elaborated a technology based on GeoToolKit - a component-based software development tool. In the paper, we present GeoToolKit extensions required to deal with distributed heterogeneous spatial data sources. They involve metadata-level structures and methods that encourage a convenient navigation in multiple distributed data sources and smart assembling of appropriate data into local task-specific configurations.The proposed extensions can be beneficial for the development of special-purpose applications and rather general data browsers capable to deal with a wide range of geo-scientific data. In addition, they are likely to be helpful in the creation of geo-scientific warehouses and in the development of exploratory applications, which usually need the access to distributed data sources. Finally, CORBA-based architectural extensions of GeoToolKit to provide data and operation transfer are presented and evaluated with real geo-scientific applications.},
booktitle = {Proceedings of the 12th International Conference on Scientific and Statistical Database Management},
pages = {259},
series = {SSDBM '00}
}

@inproceedings{10.1145/2089155.2089157,
author = {Myers, Brad A.},
title = {Inherent vs. accidental vs. intentional difficulties in programming},
year = {2011},
isbn = {9781450310246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2089155.2089157},
doi = {10.1145/2089155.2089157},
abstract = {It is the conventional wisdom that some aspects of programming are difficult to learn, and some aspects are error-prone even for experts. Is it possible to separate what is inherently difficult, and therefore most appropriately dealt with through education, versus what is just accidentally difficult, so a new design for a language or development environment might be able to "fix" the problem? And are there aspects that a designer makes difficult "on purpose"? For example, compare recursion, the syntax for switch statements in C, and how unification works in Prolog, respectively. It is not clear that the conventional wisdom on this topic can be trusted. For example, whereas most argue that concurrency is inherently difficult, the creators of the Alice language argue that they have found a way to make it understandable to novices. This talk will explore some HCI research on this topic, and approaches for identifying the differences.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN Workshop on Evaluation and Usability of Programming Languages and Tools},
pages = {1–2},
numpages = {2},
keywords = {empirical studies of programmers, novice programming, usability of ides},
location = {Portland, Oregon, USA},
series = {PLATEAU '11}
}

@inproceedings{10.1007/978-3-642-29737-3_28,
author = {Forsell, Martti and Tr\"{a}ff, Jesper Larsson},
title = {HPPC 2010: 5th workshop on highly parallel processing on a chip},
year = {2011},
isbn = {9783642297366},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-29737-3_28},
doi = {10.1007/978-3-642-29737-3_28},
abstract = {Despite the processor industry having more or less successfully invested already 10 years to develop better and increasingly parallel multicore architectures, both software community and educational institutions appear still to rely on the sequential computing paradigm as the primary mechanism for expressing the (very often originally inherently parallel) functionality, especially in the arena of general purpose computing. In that respect, parallel programming has remained a hobby of highly educated specialists and is still too often being considered as too difficult for the average programmer. Excuses are various: lack of education, lack of suitable easy-to-use tools, too architecture-dependent mechanisms, huge existing base of sequential legacy code, steep learning curves, and inefficient architectures. It is important for the scientific community to analyze the situation and understand whether the problem is with hardware architectures, software development tools and practices, or both. Although we would be tempted to answer this question (and actually try to do so elsewhere), there is strong need for wider academic discussion on these topics and presentation of research results in scientific workshops and conferences.},
booktitle = {Proceedings of the 2011 International Conference on Parallel Processing},
pages = {245–247},
numpages = {3},
location = {Bordeaux, France},
series = {Euro-Par'11}
}

@inproceedings{10.1109/HICSS.2005.61,
author = {Reed, N. E.},
title = {A User Controlled Approach to Adjustable Autonomy},
year = {2005},
isbn = {07695226889},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2005.61},
doi = {10.1109/HICSS.2005.61},
abstract = {This paper describes a framework for collaboration between a user and a multi-agent system to achieve adjustable autonomy. Adjustable autonomy (AA) is when the levels of autonomy of the agent system - its control over its reasoning - changes during execution due to interaction with a user or other systems. We describe a prototype agent development environment that allows users flexible on-line control over an otherwise completely autonomous agent system. AA can improve productivity during system design, allow earlier deployment, and create a more flexible system. AA can reduce the load on instructors when used in training situations, and allow the user to aid the system when faced with unforseen situations or problems. Examples are given of AA in simulated pilots for fighter aircraft. We found that the AA - the user's ability to modify the behavior of the agents during execution, resulted in a better, more flexible system.},
booktitle = {Proceedings of the Proceedings of the 38th Annual Hawaii International Conference on System Sciences - Volume 09},
pages = {295.2},
series = {HICSS '05}
}

@article{10.1109/TE.2005.850709,
author = {Del Corso, D. and Ovcin, E. and Morrone, G.},
title = {A teacher friendly environment to foster learner-centered customization in the development of interactive educational packages},
year = {2005},
issue_date = {November 2005},
publisher = {IEEE Press},
volume = {48},
number = {4},
issn = {0018-9359},
url = {https://doi.org/10.1109/TE.2005.850709},
doi = {10.1109/TE.2005.850709},
abstract = {A good teacher is able to customize the lesson to fit the requirements and needs of the learners he or she has in the classroom. This process becomes difficult and expensive in open and distance education, where customization means availability of similar contents, presented in diversified styles. A methodology and the tools to tackle the problem by using automated course compilation have been developed in the 3DE project (Design, Development, and Delivery-Electronic Environment for Educational Multimedia). The work on course customization showed the key role of the authoring process and the related problems. This paper summarizes the methodology and describes the development environment designed to assist authors in the creation of customized educational material. The environment seeks to help teachers/authors understand the relations among pedagogical and technical aspects and provides instructions, guidelines, and assistance for the development of learning-styles-aware material. The paper focuses on the author interface of the environment with details on the pedagogical framework, the authors' guide, the classification guide, and the metadata tool.},
journal = {IEEE Trans. on Educ.},
month = {nov},
pages = {574–579},
numpages = {6},
keywords = {Authoring guide, automated course construction, educational effectiveness, interoperability, learning styles (LSs), personalized learning, reuse}
}

@book{10.5555/3385370,
author = {Ramachandran, Muthu and Mahmood, Zaigham},
title = {Software Engineering in the Era of Cloud Computing},
year = {2020},
isbn = {3030336239},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {This book focuses on the development and implementation of cloud-based, complex software that allows parallelism, fast processing, and real-time connectivity. Software engineering (SE) is the design, development, testing, and implementation of software applications, and this discipline is as well developed as the practice is well established whereas the Cloud Software Engineering (CSE) is the design, development, testing, and continuous delivery of service-oriented software systems and applications (Software as a Service Paradigm). However, with the emergence of the highly attractive cloud computing (CC) paradigm, the tools and techniques for SE are changing. CC provides the latest software development environments and the necessary platforms relatively easily and inexpensively. It also allows the provision of software applications equally easily and on a pay-as-you-go basis. Business requirements for the use of software are also changing and there is a need for applications in big data analytics, parallel computing, AI, natural language processing, and biometrics, etc. These require huge amounts of computing power and sophisticated data management mechanisms, as well as device connectivity for Internet of Things (IoT) environments. In terms of hardware, software, communication, and storage, CC is highly attractive for developing complex software that is rapidly becoming essential for all sectors of life, including commerce, health, education, and transportation. The book fills a gap in the SE literature by providing scientific contributions from researchers and practitioners, focusing on frameworks, methodologies, applications, benefits and inherent challenges/barriers to engineering software using the CC paradigm.}
}

@inproceedings{10.1109/ITSC.2019.8917398,
author = {Elmadawi, Khaled and Abdelrazek, Moemen and Elsobky, Mohamed and Eraqi, Hesham M. and Zahran, Mohamed},
title = {End-to-end sensor modeling for LiDAR Point Cloud},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC.2019.8917398},
doi = {10.1109/ITSC.2019.8917398},
abstract = {Advanced sensors are a key to enable self-driving cars technology. Laser scanner sensors (LiDAR, Light Detection And Ranging) became a fundamental choice due to its long-range and robustness to low light driving conditions. The problem of designing a control software for self-driving cars is a complex task to explicitly formulate in rule-based systems, thus recent approaches rely on machine learning that can learn those rules from data. The major problem with such approaches is that the amount of training data required for generalizing a machine learning model is big, and on the other hand LiDAR data annotation is very costly compared to other car sensors. An accurate LiDAR sensor model can cope with such problem. Moreover, its value goes beyond this because existing LiDAR development, validation, and evaluation platforms and processes are very costly, and virtual testing and development environments are still immature in terms of physical properties representation.In this work we propose a novel Deep Learning-based LiDAR sensor model. This method models the sensor echos, using a Deep Neural Network to model echo pulse widths learned from real data using Polar Grid Maps (PGM). We benchmark our model performance against comprehensive real sensor data and very promising results are achieved that sets a baseline for future works.},
booktitle = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
pages = {1619–1624},
numpages = {6},
location = {Auckland, New Zealand}
}

@inproceedings{10.1109/DSD.2006.80,
author = {Cowell, Michael and Postula, Adam},
title = {Rachael SPARC: An Open Source 32-bit Microprocessor Core for SoCs},
year = {2006},
isbn = {0769526098},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DSD.2006.80},
doi = {10.1109/DSD.2006.80},
abstract = {SoC design methodology is totally dependent on the availability of reliable, easily interfaced and well supported IP cores. Complex cores such as microprocessors can be very expensive if licenced from commercial providers. Research projects, and even small companies, look for open source cores despite the problems associated with this source. The Rachael embedded processor discussed in this paper is a result of an open source initiative, supported both by a commercial design firm and a university. The processor is based on the proven SPARC architecture and was developed in Verilog with systematic methodology as used in industry. Rachael has a flexible memory architecture and is interfaced to the AMBA on chip bus. It is supported by a suite of development tools and is made available as an open source core. Rachel was extensively tested on Virtex4, an earlier version was used in a commercial chip, and a full version is to be fabricated. We discuss architectural issues and trade-offs in the design of Rachael, present its architecture, and analyse performance factors. The Verilog pre-processor developed for the project is briefly introduced. The open source project is presented and analysed from both the university and industry perspectives. Rachael runs at speed on Xilinx's ML401 board and it can be demonstrated executing various software applications.},
booktitle = {Proceedings of the 9th EUROMICRO Conference on Digital System Design},
pages = {415–422},
numpages = {8},
series = {DSD '06}
}

@inproceedings{10.5555/1565421.1565426,
author = {Sehring, Hans-Werner and Bossung, Sebastian and Hupe, Patrick and Skusa, Michael and Schmidt, Joachim W.},
title = {Pattern Repositories for Software Engineering Education},
year = {2007},
isbn = {9781586037154},
publisher = {IOS Press},
address = {NLD},
abstract = {Modern software engineering attacks its complexity problems by applying well-understood development principles. In particular, the systematic adoption of design patterns caused a significant improvement of software engineering and is one of the most effective remedies for what was formerly called the software crises. Design patterns and their utilization constitute an increasing body of knowledge in software engineering. Due to their regular structure, their orthogonal applicability and the availability of meaningful examples design patterns can serve as an excellent set of use cases for organizational memories, for software development tools and for e-learning environments.Patterns are defined and described on two levels [1]: by real-world examples---e.g., textual or graphical content on their principles, best practices, structure diagrams, code etc.---and by conceptual models---e.g., on categories of application problems, software solutions, deployment consequences etc. This intrinsically dualistic nature of patterns makes them good candidates for conceptual content management (CCM). In this paper we report on the application of the CCM approach to a repository for teaching and training in pattern-based software design as well as for the support of the corresponding e-learning processes.},
booktitle = {Proceedings of the 2007 Conference on Databases and Information Systems IV: Selected Papers from the Seventh International Baltic Conference DB&amp;IS'2006},
pages = {40–54},
numpages = {15},
keywords = {Conceptual Modeling, Content Management, Design Patterns, E-Learning}
}

@inproceedings{10.1145/800225.806822,
author = {Purtilo, James},
title = {Polylith: An environment to support management of tool interfaces},
year = {1985},
isbn = {0897911652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800225.806822},
doi = {10.1145/800225.806822},
abstract = {Polylith is the name of a set of enhanced execution time system services along with development tools and an interfacing methodology.1 As a system, Polylith supports the reliable union of many component tools, addressing the problems of data interchange and synchronization between these tools. It facilitates reuse of code, and promotes the notion that construction of large programs should be viewed instead as orchestration of services. The Polylith is visible as a grammar in which instances of environments2 are precisely and rapidly specified; it is, through compilation and execution of assertions in that language, a medium through which many programs and tools can be united with impunity.This paper presents an overview of the Polylith architecture, along with some brief remarks on the requirements analysis leading to Project Polylith at the University of Illinois. Section 2 presents this architecture, summarizing language and data transformation issues. Simple examples are included. Section 3 introduces one particular instance of an environment specified within Polylith called Minion. It is presented as an extended example, showing how the Polylith is utilized to construct an enthusiastic assistant for mathematical problem solving. The closing section contains some evaluation of how Polylith affects the task of environment development.},
booktitle = {Proceedings of the ACM SIGPLAN 85 Symposium on Language Issues in Programming Environments},
pages = {12–18},
numpages = {7},
location = {Seattle, Washington, USA},
series = {SLIPE '85}
}

@book{10.5555/2771337,
author = {Ray, John},
title = {iOS 8 Application Development in 24 Hours, Sams Teach Yourself},
year = {2015},
isbn = {0672337231},
publisher = {Sams publishing},
edition = {6th},
abstract = {In just 24 sessions of one hour each, learn how to build powerful applications for todays hottest handheld devices: the iPhone and iPad! Using this books straightforward, step-by-step approach, youll master every skill and technology you need, from setting up your iOS development environment to building great user interfaces, sensing motion to writing multitasking applications. Each lesson builds on what youve already learned, giving you a rock-solid foundation for real-world success! Step-by-step instructions carefully walk you through the most common iOS development tasks. Quizzes and Exercises help you test your knowledge. By the Way notes present interesting information related to the discussion. Did You Know? tips show you easier ways to perform tasks. Watch Out! cautions alert you to possible problems and give you advice on how to avoid them. John Ray is currently serving as the Director of the Office of Research Information Systems at the Ohio State University. His many books include Using TCP/IP: Special Edition, Maximum Mac OS X Security, Mac OS X Unleashed, Teach Yourself Dreamweaver MX in 21 Days, and Sams Teach Yourself iOS 7 Application Development in 24 Hours. Printed in full colorfigures and code appear as they do in Xcode Covers iOS 8 and up Learn to navigate the Xcode 6.x development environment Prepare your system and iDevice for efficient development Get started quickly with Apples new language: Swift Test code using the new iOS Playground Understand the Model-View-Controller (MVC) development pattern Visually design and code interfaces using Xcode Storyboards, Segues, Exits, Image Slicing, and the iOS Object Library Use Auto Layout and Size Classes to adapt to different screen sizes and orientations Build advanced UIs with Tables, Split Views, Navigation Controllers, and more Read and write preferences and data, and create System Settings plug-ins Use the iOS media playback and recording capabilities Take photos and manipulate graphics with Core Image Sense motion, orientation, and location with the accelerometer, gyroscope, and GPS Integrate online services using Twitter, Facebook, Email, Web Views, and Apple Maps Create universal applications that run on both the iPhone and iPad Write background-aware multitasking applications Trace, debug, and monitor your applications as they run}
}

@inproceedings{10.1145/2664243.2664254,
author = {Oliveira, Daniela and Rosenthal, Marissa and Morin, Nicole and Yeh, Kuo-Chuan and Cappos, Justin and Zhuang, Yanyan},
title = {It's the psychology stupid: how heuristics explain software vulnerabilities and how priming can illuminate developer's blind spots},
year = {2014},
isbn = {9781450330053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2664243.2664254},
doi = {10.1145/2664243.2664254},
abstract = {Despite the security community's emphasis on the importance of building secure software, the number of new vulnerabilities found in our systems is increasing. In addition, vulnerabilities that have been studied for years are still commonly reported in vulnerability databases. This paper investigates a new hypothesis that software vulnerabilities are blind spots in developer's heuristic-based decision-making processes. Heuristics are simple computational models to solve problems without considering all the information available. They are an adaptive response to our short working memory because they require less cognitive effort. Our hypothesis is that as software vulnerabilities represent corner cases that exercise unusual information flows, they tend to be left out from the repertoire of heuristics used by developers during their programming tasks.To validate this hypothesis we conducted a study with 47 developers using psychological manipulation. In this study each developer worked for approximately one hour on six vulnerable programming scenarios. The sessions progressed from providing no information about the possibility of vulnerabilities, to priming developers about unexpected results, and explicitly mentioning the existence of vulnerabilities in the code. The results show that (i) security is not a priority in software development environments, (ii) security is not part of developer's mindset while coding, (iii) developers assume common cases for their code, (iv) security thinking requires cognitive effort, (v) security education helps, but developers can have difficulties correlating a particular learned vulnerability or security information with their current working task, and (vi) priming or explicitly cueing about vulnerabilities on-the-spot is a powerful mechanism to make developers aware about potential vulnerabilities.},
booktitle = {Proceedings of the 30th Annual Computer Security Applications Conference},
pages = {296–305},
numpages = {10},
location = {New Orleans, Louisiana, USA},
series = {ACSAC '14}
}

@inproceedings{10.5555/795698.798373,
author = {McGuire, Eugene G.},
title = {Initial Effects of Software Process Improvement on an Experienced Software Development Team},
year = {1996},
isbn = {0818673249},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper discusses the initial stages of a long-term case study designed to examine the efforts of an experienced software development team in moving to a more process-driven software development environment. This team has previously produced software that has met functional, schedule, and cost criteria as specified by their customers. The software development manager, as directed by an organizational mandate, has initiated efforts to move this team into a more structured, formalism-based development and testing environment. The Capability Maturity Model (CMM) developed by the Software Engineering Institute (SEI) is being used as the model for this effort. The group dynamic and team development aspects of this effort are being carefully monitored to determine possible sources of resistance to change and to develop intervention "just-in-time" training sessions that can address identified problem areas, particularly those that may directly affect productivity, quality, and schedule. This paper discusses initial findings in this area and addresses them within the CMM framework.},
booktitle = {Proceedings of the 29th Hawaii International Conference on System Sciences Volume 1: Software Technology and Architecture},
pages = {713},
series = {HICSS '96}
}

@inproceedings{10.5555/1565321.1565322,
title = {Front Matter},
year = {2006},
isbn = {1586036734},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for the new economy and science. It creates new markets and new directions for a more reliable, flexible, and robust society. It empowers the exploration of our world in ever more depth. However, software often falls short behind our expectations. Current software methodologies, tools, and techniques remain expensive and not yet reliable for a highly changeable and evolutionary market. Many approaches have been proven only as case-by-case oriented methods.This book presents a number of new trends and theories in the direction in which we believe software science and engineering may develop to transform the role of software and science in tomorrow's information society.This book is an attempt to capture the essence of a new state of art in software science and its supporting technology. The book also aims at identifying the challenges such a technology has to master. It contains papers accepted at the Fifth International Conference on New Trends in software Methodology Tools, and Techniques, V, (SoMeT_06) held in Quebec, Canada, from 25th to 27th October 2006, (url: www.somet.soft.iwate-pu.ac.jp/somet_06). This workshop had brought together researchers and practitioners to share their original research results and practical development experiences in software science, and its related new challenging technology.One of the important issues addressed in this book is software security tools and techniques. Another example we challenge in this conference is, Lyee methodology as a new Japanese emerged software methodology that has been patented in several countries in Europe, Asia, and America. But it is still in its early stage of emerging as a new software style. This book and the series it continues will also contribute to elaborate on such new trends and related academic research studies and development.A major goal of this book was to gather scholars from the international research community to discuss and share research experiences on new software methodologies, and formal techniques. The book also investigated other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. These are essential for developing a variety of information systems research projects and to assess the practical impact on real-world software problems.For an outline of the past series of related events that contributed to this publication are SoMeT_02 that was held October 3--5, 2002, in Sorbonne, Paris, France. SoMeT_03 was held in Stockholm, Sweden, SoMeT_04 held in Leipzig, Germany, SoMeT_05 held in Tokyo, Japan and this publication covers SoMeT_06; held in Quebec, Canada. These events, also initiate the forthcoming SoMeT_07, to be organized in Rome, Italy in November 2007, (url: www.somet.soft.iwate-pu.ac.jp/somet_07/).This book participates to provide an opportunity for exchanging ideas and experiences in the field of software technology opening up new avenues for software development, methodologies, tools, and techniques, especially, software security and program coding diagnosis and related software maintenance techniques aspects.The Lyee framework for example, captures the essence of the innovations, controversies, challenges and possible solutions of the software industry. This world wide patented software approach was born and enriched from experience, and it is time, and again through SoMeT_06 to try to let it stimulate the academic research on software engineering attempting to close the gap so far existed between theory and practice. We believe that this book creates an opportunity for us in the software science community to think about where we are today and where we are going.The book is a collection of 28 carefully reviewed best-selected papers by the reviewing committee.This book covers the following areas:• Software engineering aspects on software security, programs diagnosis and maintenance• Static and dynamic analysis on Lyee-oriented software performance model• Software security aspects on Java mobile code, and networking• Practical artefact on software security, software validation and diagnosis• Software optimization and formal methods• Requirement engineering and requirement elicitation• Software methodologies and Lyee oriented software techniques• Automatic software generation, reuse, and legacy systems• Software quality and process assessment• Intelligent software systems and evolution• End-user requirement engineering and programming environment• Ontology and philosophical aspects on software engineering• Business software models and other kinds of software application models, based on Lyee theoryAll papers published in this book are carefully reviewed selected by the SOMET international reviewing committee. Each paper has been reviewed by three and up to four reviewers and has been revised based on the review reports. The papers were reviewed on the basis of technical soundness, relevance, originality, significance, and clarity.This book outcome is also a collective effort from all the Lyee International project collaborators and industrial supporters. We also, gratefully thank Iwate Prefectural University, University of Laval, Catena Co., ISD, Ltd, SANGIKYO co., and others for their overwhelming support, on this work. We specially, are thankful to the review committee and others who participated in the review of all submitted papers and thanks also for the hot discussion we have had on the reviews evaluation meetings that selected those contributed in book.This outcome is another milestone in mastering new challenges on software and its new promising technology, within SoMeT's consecutive events. Also, it gives the reader new insights, inspiration and concrete material to elaborate and study this new technology.Also, at last and not least, we would like to thank and acknowledge the support of the University of Leipzig, Telematik and e-Business group for allowing us to use the Paperdyne System as a conference-supporting tool during all the phases on review transactions.The Editors},
booktitle = {Proceedings of the 2006 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Fifth SoMeT_06},
pages = {i–xiv}
}

@inproceedings{10.1145/3442536.3442546,
author = {Weng, Ting-Sheng and Li, Chien-Kuo and Hsu, Meng-Hui},
title = {Development of Robotic Quiz Games for Self-Regulated Learning of Primary School Children},
year = {2021},
isbn = {9781450388832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442536.3442546},
doi = {10.1145/3442536.3442546},
abstract = {The progressive development of information technology has provided multiple learning modes. The rich content and innovative applications available allow pupils to improve their skills through self-regulated learning (SRL), which has become an important education goal. Intelligent robots can be used in a wide range of applications, from programmed movements for learning activities, to the combination of artificial intelligence and sensor technology for human life and education. A robot's dynamic and interesting interface is more suitable for children's self-regulated learning. This study used a Zenbo robot as the development tool and Zenbo Scratch platform programming to develop an AI robot math quiz game for primary school students. Two elementary school math teachers, and a parent and a 5th grade primary school student were involved in the development of the game. This study used the parent's and student's continuous interaction with the robot to adjust the code and achieve the best human-computer interaction in robotic mathematics problem solving. Moreover, this study developed a companion robot for a math quiz game, which can be used for reviewing what has been learned in class. The robot can be used for self-regulated learning by young children to increase student learning outcome.},
booktitle = {Proceedings of the 2020 3rd Artificial Intelligence and Cloud Computing Conference},
pages = {58–62},
numpages = {5},
keywords = {AI robot, Perceived usefulness, Perceived enjoyment, Robotic quiz games, Self-regulated learning},
location = {Kyoto, Japan},
series = {AICCC '20}
}

@inproceedings{10.5555/1659308.1659309,
title = {Front Matter},
year = {2009},
isbn = {9781607500490},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for science and the new economy. It creates new markets and new directions for a more reliable, flexible and robust society. It empowers the exploration of our world in ever more depth. However, software often falls short of our expectations. Current software methodologies, tools, and techniques remain expensive and are not yet sufficiently reliable for a constantly changing and evolving market, and many promising approaches have proved to be no more than case-by-case oriented methods.This book explores new trends and theories which illuminate the direction of developments in this field, developments which we believe will lead to a transformation of the role of software and science integration in tomorrow's global information society. By discussing issues ranging from research practices and techniques and methodologies, to proposing and reporting solutions needed for global world business, it offers an opportunity for the software science community to think about where we are today and where we are going.The book aims to capture the essence of a new state of the art in software science and its supporting technology, and to identify the challenges that such a technology will have to master. It contains extensively reviewed papers presented at the eighth International Conference on New Trends in software Methodology Tools, and Techniques, (SoMeT_09) held in Prague, Czech Republic, with the collaboration of Czech Technical University, from September 23rd to 25th 2009. (http://www.action-m.com/ somet_2009/).This conference brought together researchers and practitioners to share their original research results and practical development experience in software science and related new technologies.This volume participates in the conference the SoMeT series Previous related events that contributed to this publication are: SoMeT_02 (the Sorbonne, Paris, October 3rd to 5th 2002); SoMeT_03 (Stockholm, Sweden); SoMeT_04 (Leipzig, Germany); SoMeT_05 (Tokyo, Japan); SoMeT_06 (Quebec, Canada); SoMeT_07 (Rome, Italy); SoMeT_08 (Sharjah, UAE) and SoMeT_09 (Prague, Czech Republic). This series of conferences will be continued with SoMeT 10 in Japan from September 29th to October 1st 2010, of which it forms a part, by providing an opportunity for exchanging ideas and experiences in the field of software technology; opening up new avenues for software development, methodologies, tools, and techniques, especially with regard to software security and programme coding diagnosis and aspects of related software maintenance techniques. The emphasis has been placed on human-centric software methodologies, end-user development techniques, and emotional reasoning, for an optimally harmonised performance between the design tool and the user.This book, and the series it forms part of, will continue to contribute to and elaborate on new trends and related academic research studies and developments in SoMeT_2010 in Japan.A major goal of this work was to assemble the work of scholars from the international research community to discuss and share research experiences of new software methodologies and techniques. One of the important issues addressed is the handling of cognitive issues in software development to adapt it to the user's mental state. Tools and techniques related to this aspect form part of the contribution to this book. Another subject raised at the conference was intelligent software design in software security and programme conversions. The book also investigates other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. This is essential for a comprehensive overview of information systems and research projects, and to assess their practical impact on real-world software problems. This represents another milestone in mastering the new challenges of software and its promising technology, addressed by the SoMeT conferences, and provides the reader with new insights, inspiration and concrete material to further the study of this new technology.The book is a collection of 41 carefully refereed papers selected by the reviewing committee and covering:• Software engineering aspects of software security programmes, diagnosis and maintenance• Static and dynamic analysis of software performance models• Software security aspects and networking• Practical artefacts of software security, software validation and diagnosis• Software optimization and formal methods• Requirement engineering and requirement elicitation• Software methodologies and related techniques• Automatic software generation, re-coding and legacy systems• Software quality and process assessment• Intelligent software systems and evolution• End-user requirement engineering, programming environment for Web applications• Ontology and philosophical aspects on software engineering• Cognitive Software and human behavioural analysis in software design.All papers published in this book have been carefully reviewed, on the basis of technical soundness, relevance, originality, significance, and clarity, by up to four reviewers. They were then revised on the of the review reports before being selected by the SoMeT international reviewing committee.This book is the result of a collective effort from many industrial partners and colleagues throughout the world. We would like to thank Iwate Prefectural University, in particular the President, Prof. Yoshihisa Nakamura; SANGIKYO Co., especially the president Mr. M. Sengoku; the Czech Technical University of Prague (Czech Republic); ARISES of Iwate Prefectural University and all the others who have contributed their invaluable support to this work. Most especially, we thank the reviewing committee and all those who participated in the rigorous reviewing process and the lively discussion and evaluation meetings which led to the selected papers which appear in this book. Last and not least, we would also like to thank the Microsoft Conference Management Tool team for their expert guidance on the use of the Microsoft CMT System as a conference-support tool during all the phases of SoMeT.The Editors},
booktitle = {Proceedings of the 2009 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Eighth SoMeT_09},
pages = {i–xiv}
}

@inproceedings{10.1145/3626202.3637565,
author = {Xu, Zhenyu and Yu, Miaoxiang and Cai, Jillian and Gafsi, Saddam and Ryckman, Judson Douglas and Yang, Qing and Wei, Tao},
title = {An FPGA-Enabled Framework for Rapid Automated Design of Photonic Integrated Circuits},
year = {2024},
isbn = {9798400704185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626202.3637565},
doi = {10.1145/3626202.3637565},
abstract = {This paper introduces an FPGA-enabled framework to accelerate the automated design process for Photonic Integrated Circuit (PIC) devices. PICs are foreseen as a foundation for the next-generation semiconductors. However, the complexity of PIC design presents considerable challenges. Machine Learning (ML) techniques have shown promise in the realm of PIC design. The primary hurdle, however, is the extended training duration, solely constrained by the slow electromagnetic (EM) Finite-Difference Time-Domain (FDTD) solver. We propose a fast framework with a dedicated FPGA FDTD accelerator tailor-designed to speed up the PIC simulation. Benchmarking was carried out against commercial tools, with the single-FPGA accelerator outperforming both a multicore CPU and a GPU cluster. We taped out and evaluated the PIC devices designed through the proposed framework, and the experimental outcomes aligned. This demonstrates the full design circle, showcasing that the proposed framework enabled by FPGA breaks the current bottleneck in this domain. This study was conducted entirely on a commercial cloud platform (AWS), leveraging CPUs, FPGAs, and GPUs, with FPGA programming efficiently executed using High-Level Synthesis (HLS) and the Xilinx Runtime (XRT). The FPGA, along with its modern development tools, is seamlessly integrated into a heterogeneous computing platform, showcasing the accessible and practical nature of this approach. Our findings show the exciting possibility that ML-based physical design could be notably sped up enabled by FPGAs in a cloud-hosted heterogeneous cluster as a service.},
booktitle = {Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {119–129},
numpages = {11},
keywords = {cloud computing, finite difference time domain, fpga, heterogeneous computing, machine learning, photonic integrated circuits},
location = {<conf-loc>, <city>Monterey</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {FPGA '24}
}

@inproceedings{10.1109/ESEM.2017.37,
author = {Fronza, Ilenia and Wang, Xiaofeng},
title = {Towards an approach to prevent social loafing in software development teams},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.37},
doi = {10.1109/ESEM.2017.37},
abstract = {[Background] A high-functioning team is a decisive factor for a successful software development project. However building such a team is not easy. Among many issues and obstacles encountered by teams, social loafing is a common but difficult one to tackle.[Aim] We intend to construct an approach to effectively prevent social loafing behaviors in software development teams.[Method] We built one social loafing prevention approach based on existing literature and survey instruments. It has been applied in an educational context with 2nd-year computer science students working on software development projects in teams.[Results] The approach starts with increasing team members' awareness of social loafing. Team Expectations Agreement (TEA) is then used to help the team to write down the terms that explicitly prevent social loafing. During the project, a small survey instrument is used to track regularly if the specified terms are followed by the team members. At the end of a period, the presence/absence of social loafing is assessed by the team using another short survey. How to interpret the results of the surveys is explained as part of the presented approach.[Conclusions] This approach has potential to improve teamwork skills of students, which is not adequately addressed in higher education programs. Meanwhile it can be adapted in professional software development environments to prevent social loafing and improve teamwork. The next step of our study will be using the collected data to evaluate the proposed approach, and formulating a set of recommendations to use the approach in the professional software development context.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {241–246},
numpages = {6},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/2839509.2850566,
author = {Hadfield, Steven M. and Coulston, Christopher S. and Hadfield, Marissa G. and Warner, Lillian B.},
title = {Adventures in K-5 STEM Outreach Using the NAO Robot (Abstract Only)},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2850566},
doi = {10.1145/2839509.2850566},
abstract = {The humanoid NAO robot continues to win both hearts and imaginations with its lifelike appearance and behaviors. Its consistent growth in popularity, an increasing wealth of free behaviors, and the intuitive Choregraphe development environment provide educators and developers with exceptional opportunities to motivate interest in STEM disciplines and breach impediments such as perceptions that such technology is simply too hard. In this poster, the authors discuss a variety of experimental uses of the NAO robots for K-5 STEM Outreach. Initial development and demonstrations focused on generating enthusiasm for both robotics and programming. Dancing and exercising behaviors from Notre Dame University's F.U.N. Lab and Aldebaran Robots easily integrated into voice-controlled Choregraphe demonstration scripts. Faculty and undergraduate students directed this enthusiasm to motivate engagement in Hour of Code programming activities. The team also utilized the NAO robots in small group settings within a K-5 Response To Intervention (RTI) program where demonstrations were followed by having the children interactively experiment with the robots stimulating imagination, creativity, curiosity, and problem solving skills as well as confidence and self-esteem. Next the RTI children actually programmed the robots using a story-based methodology and the powerful while intuitive building block programming constructs of Choregraphe. The team's on-going development efforts focus on expanding the repertoire of available behaviors to include interactive math games and foreign language educational dialogs. Results from use of these new behaviors will be presented at the SIGCSE Symposium. The team is also working to measure attitudinal, conceptual understanding, and math and language skills improvements.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {697},
numpages = {1},
keywords = {k-5, nao, outreach, robots, rti, stem},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3573051.3596180,
author = {Elhayany, Mohamed and Meinel, Christoph},
title = {Towards Automated Code Assessment with OpenJupyter in MOOCs},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596180},
doi = {10.1145/3573051.3596180},
abstract = {The popularity of Massive Open Online Courses (MOOCs) as a means of delivering education to large numbers of students has been growing steadily over the last decade. As technology improves, more educational content is becoming readily available to the public. JupyterLab, an open-source web-based interactive development environment (IDE), is also becoming increasingly popular in education, however, it is so far primarily used in small classroom settings. JupyterLab can provide a more interactive, hands-on, and collaborative learning experience for students in MOOCs, and it is highly customizable and can be accessed from anywhere. To capitalize on these benefits, we have developed OpenJupyter, which integrates JupyterLab at scale with MOOCs, enhancing the student learning experience and providing hands-on exercises for data science courses, making them more interactive and engaging. While MOOCs provide access to education for a large number of students, one of the significant challenges is providing effective and timely feedback to learners.&nbsp;OpenJupyter includes an auto-assessment capability that addresses this problem in MOOCs by automating the evaluation process and providing feedback to learners in a timely manner. In this paper, we provide an overview of the architecture of OpenJupyter, its scalability in the context of MOOCs, and its effectiveness in addressing the auto-assessment challenge. We also discuss the Advantages and limitations associated with using OpenJupyter in a MOOC context and provide a reference for educators and researchers who wish to implement similar tools. Our efforts aim to foster an open educational environment in the field of programming by providing learners with an interactive learning tool and a streamlined technical setup, allowing them to acquire and test their knowledge at their own pace.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {321–325},
numpages = {5},
keywords = {JupyterLab, MOOC, auto-assessment, openjupyter, programming},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1109/ISISE.2009.53,
author = {Rao, Jinjun and Gao, Tongyue and Gong, Zhenbang and Jiang, Zhen},
title = {Low Cost Hand Gesture Learning and Recognition System Based on Hidden Markov Model},
year = {2009},
isbn = {9780769539911},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISISE.2009.53},
doi = {10.1109/ISISE.2009.53},
abstract = {Focusing on recognizing some typical gestures based on 3-axis MEMS accelerometer to interact with an application of human-machine interactive game, the hand gesture problem is analyzed firstly, then theory basis of HMM is introduced. In order to obtain training gesture data for HMM, and also to provide a hardware basis for gesture recognition, a low cost data acquisition system hardware is researched and designed. The system can get the acceleration data of user’s gesture and transmit them wirelessly to a personal computer. In the hand gesture recognition approach, k-mean algorithms are applied to cluster and abstract the vector data from sensor. And then the quantized vectors are put into a hidden Markov model to learn and recognize user’s gestures. Finally the gesture recognition library is implemented in C# development environment, and is utilized in a human-machine interactive game application. The results show that the typical gesture emerging in the game can be identified in a high rate, and the user can experience more interest and interaction.},
booktitle = {Proceedings of the 2009 Second International Symposium on Information Science and Engineering},
pages = {433–438},
numpages = {6},
keywords = {Hidden markov model, accelerometer, gesture recognition, virtual reality},
series = {ISISE '09}
}

@inproceedings{10.1145/3512290.3528700,
author = {Sobania, Dominik and Briesch, Martin and Rothlauf, Franz},
title = {Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528700},
doi = {10.1145/3512290.3528700},
abstract = {GitHub Copilot, an extension for the Visual Studio Code development environment powered by the large-scale language model Codex, makes automatic program synthesis available for software developers. This model has been extensively studied in the field of deep learning, however, a comparison to genetic programming, which is also known for its performance in automatic program synthesis, has not yet been carried out. In this paper, we evaluate GitHub Copilot on standard program synthesis benchmark problems and compare the achieved results with those from the genetic programming literature. In addition, we discuss the performance of both approaches. We find that the performance of the two approaches on the benchmark problems is quite similar, however, in comparison to GitHub Copilot, the program synthesis approaches based on genetic programming are not yet mature enough to support programmers in practical software development. Genetic programming usually needs a huge amount of expensive hand-labeled training cases and takes too much time to generate solutions. Furthermore, source code generated by genetic programming approaches is often bloated and difficult to understand. For future work on program synthesis with genetic programming, we suggest researchers to focus on improving the execution time, readability, and usability.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1019–1027},
numpages = {9},
keywords = {GitHub copilot, codex, genetic programming, large-scale language models, program synthesis, software engineering},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@article{10.1109/13.804563,
author = {Ivins, J. R. and Holland, R.},
title = {Reflections on the operation of large multidisciplinary projects in engineering, design and design management},
year = {1999},
issue_date = {November 1999},
publisher = {IEEE Press},
volume = {42},
number = {4},
issn = {0018-9359},
url = {https://doi.org/10.1109/13.804563},
doi = {10.1109/13.804563},
abstract = {Multi-disciplinary product design teams are now an accepted project development tool in industry. Many advantages cited are: rapid prototyping, cost reduction and the design of a more marketable product. While multi-disciplinary teams are common in such environments, it is more problematical to offer students in higher education experience of working in such a team. A variety of difficulties are encountered by educators wishing to provide such an experience for their students, ranging from cultural through logistical to the level of intellectual rigour required and the assessment methodologies used. This paper describes the research methodologies and results of an investigation into the operation of a large multi-disciplinary team project conducted by the authors over a period of two years to two sets of undergraduates, each consisting of over one hundred and eighty full-time students drawn from a number of engineering, design and design management disciplines. An action research programme to investigate the group experience and attitudes was undertaken and the principal findings are presented and briefly debated. The findings support the view that the exercise provided a wide range of tangible and intangible benefits. This enables the authors to propose a conceptual model of large multi-disciplinary team working which may be used by other educators as a basis for developing future project modules within their host institution's environment. Finally, the benefits gained, as shown by previous research, for students and staff from such projects are summarised},
journal = {IEEE Trans. on Educ.},
month = {nov},
pages = {9 pp.}
}

@inproceedings{10.1145/504450.504460,
author = {Marzullo, Keith and Ogg, Michael and Ricciardi, Aleta and Amoroso, Alessandro and Calkins, F. Andrew and Rothfus, Eric},
title = {NILE: wide-area computing for high energy physics},
year = {1996},
isbn = {9781450373395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/504450.504460},
doi = {10.1145/504450.504460},
abstract = {The CLEO project [2], centered at Cornell University, is a large-scale high energy physics project. The goals of the project arise from an esoteric question---why is there apparently so little antimatter in the universe?---and the computational problems that arise in trying to answer this question are quite challenging.To answer this question, the CESR storage ring at Cornell is used to generate a beam of electrons directed at an equally strong beam of positrons. These two beams meet inside a detector that is embedded in a magnetic field and is equipped with sensors. The collisions of electrons and positrons generate several secondary subatomic particles. Each collision is called an event and is sensed by detecting charged particles (via the ionization they produce in a drift chamber) and neutral particles (in the case of photons, via their deposition of energy in a crystal calorimeter), as well as by other specialized detector elements. Most events are ignored, but some are recorded in what is called raw data (typically 8Kbytes per event). Offline, a second program called pass2 computes, for each event, the physical properties of the particles, such as their momenta, masses, and charges. This compute-bound program produces a new set of records describing the events (now typically 20Kbytes per event). Finally, a third program reads these events, and produces a lossily-compressed version of only certain frequently-accessed fields, written in what is called roar format (typically 2Kbytes per event).The physicists analyze this data with programs that are, for the most part, embarrassingly parallel and I/O limited. Such programs typically compute a result based on a projection of a selection of a large number of events, where the result is insensitive to the order in which the events are processed. For example, a program may construct histograms, or compute statistics, or cull the raw data for physical inspection. The projection is either the complete pass2 record or (much more often) the smaller roar record, and the selection is done in an ad-hoc manner by the program itself.Other programs are run as well. For example, a Monte Carlo simulation of the experiment is also run (called monte carlo) in order to correct the data for detector acceptance and inefficiencies, as well as testing aspects of the model used to interpret the data. This program is compute bound. Another important example is called recompress. Roughly every two years, improvements in detector calibration and reconstruction algorithms make it worthwhile to recompute more accurate pass2 data (and hence, more accurate roar data) from all of the raw data. This program is compute-bound (it currently requires 24 200-MIP workstations running flat out for three months) and so must be carefully worked into the schedule so that it does not seriously impact the ongoing operations.Making this more concrete, the current experiment generates approximately 1 terabyte of event data a year. Only recent roar data can be kept on disk; all other data must reside on tape. The data processing demands consume approximately 12,000 SPECint92 cycles a year. Improvements in the performance of CESR and the sensitivity of the detector will cause both of these values to go up by a factor of ten in the next few years, which will correspondingly increase the storage and computational needs by a factor of ten.The CLEO project prides itself on being able to do big science on a tight budget, and so the programming environment that the CLEO project provides for researchers is innovative but somewhat primitive. Jobs that access the entire data set can take days to complete. To circumvent limited access to tape, the network, or compute resources close to the central disk, physicists often do preliminary selections and projections (called skims) to create private disk data sets of events for further local analysis. Limited resources usually exact a high human price for resource and job management and ironically, can sometimes lead to inefficiencies. Given the increase in data storage, data retrieval, and computational needs, it has become clear that the CLEO physicists require a better distributed environment in which to do their work.Hence, an NSF-funded National Challenge project was started with participants from both high energy physics, distributed computing, and data storage, in order to provide a better environment for the CLEO experiment. The goals of this project, called NILE [7], are:Finally, the CLEO necessity of building on a budget carries over to NILE. There are some more expensive resources, such as ATM switches and tape silos, that it will be necessary to use. However, as far as possible we are using commodity equipment, and free or inexpensive software whenever possible. For example, one of our principal development platforms is Pentium-based PCs, interconnected with 100 Mbps Ethernet, running Linux and the GNU suite of tools.},
booktitle = {Proceedings of the 7th Workshop on ACM SIGOPS European Workshop: Systems Support for Worldwide Applications},
pages = {49–54},
numpages = {6},
location = {Connemara, Ireland},
series = {EW 7}
}

@inproceedings{10.1145/3287324.3293709,
author = {Sankaranarayanan, Sreecharan},
title = {Online Mob Programming: Effective Collaborative Project-Based Learning},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293709},
doi = {10.1145/3287324.3293709},
abstract = {This work presents a new paradigm for collaborative project-based computer science education called Online Mob Programming (OMP). OMP is adapted from the industrial practice of Mob Programming, where groups of developers work on the same problem, at the same time, in the same place. OMP was designed and implemented as a technique where a group of 4-6 students collaborate online through a structured process for solving programming tasks. In OMP, students rotate through clearly defined roles to collectively contribute towards a solution to a programming challenge. These roles require students to brainstorm potential solutions, decide on a path forward, and implement the correct course of action respectively. OMP was investigated in the context of a 6-week free online course on Cloud Computing. During the course, students participated in four intelligent conversational agent-coordinated OMP sessions. By instrumenting the online development environment, all student code revisions and chat logs were collected in addition to qualitative data from questionnaires. Analyses show evidence of success in terms of students following the structure of OMP and further investigations into differences in mob behavior based on the size, and problem outcome provide pedagogically valuable insights and a path toward building OMP into the computer science education curriculum.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1296},
numpages = {1},
keywords = {collaborative learning, computer-supported collaborative learning, mob programming, online mob programming, project-based learning},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@phdthesis{10.5555/927446,
author = {Hung, Shih-Hao and Davidson, Edward S.},
title = {Optimizing parallel applications},
year = {1998},
isbn = {0591944391},
publisher = {University of Michigan},
address = {USA},
abstract = {While parallel computing offers an attractive perspective for the future, developing efficient parallel applications today is a labor-intensive process that requires an intimate knowledge of the machines, the applications, and many subtle machine-application interactions. Optimizing applications so that they can achieve their full potential on parallel machines is often beyond the programmer's or the compiler's ability; furthermore its complexity will not be reduced with the increasingly complex computer architectures of the foreseeable future. In this dissertation, we discuss how application performance can be optimized systematically. We show how insights regarding machine-application pairs and the weaknesses in their delivered performance can be derived by characterizing the machine, the application, and the machine-application interactions. We describe a general performance tuning scheme that can be used for selecting and applying a broad range of performance tuning actions to solve major performance problems in a structured sequence of steps, and discuss the interrelationship among and between performance problems and performance tuning actions. To guide programmers in performance tuning, we developed a goal-directed performance tuning methodology that employs hierarchical performance bounds to characterize the delivered performance quantitatively and explain where potential performance is lost. To reduce the complexity of performance tuning, we developed an innovative performance modeling scheme to quickly derive machine-application interactions from abstract representations of the machine and application of interest.Collectively, this dissertation unifies a range of research work done within the Parallel Performance Project at the University of Michigan over the past seven years and significantly improves the state-of-the-art in parallel application development environments.},
note = {AAI9840559}
}

@phdthesis{10.5555/1834920,
author = {Fyllingness, Jennifer Lynne},
advisor = {Gardiner, John Jacob},
title = {Internet as a training tool in small tourism and hospitality businesses in norway},
year = {2009},
isbn = {9781109465778},
publisher = {Seattle University},
abstract = {Training and development are necessary components to insure the success of small businesses in the changing global economy. Previous small- and medium-size enterprise (SME) research has shown that the owners and employees of small businesses receive less formal education than employees at larger firms due to high costs, inconvenient locations, inflexible schedules, lack of human resources, and owner attitudes. Technology and the advancement of internet-based learning may be a solution to many of these potential barriers. This descriptive study examines the current adoption and attitudes towards the Internet as a formal and informal training and development tool by small tourism and hospitality businesses (STHB) in Northern Norway. The study provides an introduction to SMEs and the attitudes and barriers they face in pursuing training, the advantages and disadvantages of internet-based training and a discussion of formal versus informal training methods followed by a description of the methodology and results. For this study a web-based survey link was e-mailed to 615 STHBs in Northern Norway in June and July 2008. The findings of this research have significance in the context of STHBs in Northern Norway, but given the low response rate of 24.2% (N=149), cannot be generalized to other contexts. The researcher found that the majority of small tourism and hospitality owner-managers who had participated in training in the last 2 years perceived a variety of positive outcomes and that those owner-managers who had previously participated in internet-based training had a positive experience. Overall, the owner-managers had a positive view of information communication technologies (ICTs) and internet-based training and the majority indicated they were interested in utilizing the internet for training and development. The researcher also found that the STHBs utilized information communication technologies within their businesses for a variety of informal educational purposes including e-mail, internet searches, word processing, billing/checking, marketing/advertising and e-commerce. The least used of the ICT options provided were social networking programs. Overall, the researcher found that small tourism and hospitality businesses in Northern Norway were interested in internet-based training opportunities if the classes offered are relevant for their business and relatively inexpensive.},
note = {AAI3383804}
}

@inproceedings{10.5555/1566971.1566972,
title = {Front Matter},
year = {2007},
isbn = {9781586037949},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for the new economy and science. It creates new markets and new directions for a more reliable, flexible, and robust society. It empowers the exploration of our world in ever more depth. However, software often falls short of our expectations. Current software methodologies, tools, and techniques remain expensive and not yet reliable enough for a highly changeable and evolutionary market. Many approaches have been proven only as case-by-case oriented methods.This bookas part of SOMET seriescontributes on new trendstheories in the direction in which we believe software science and engineering may develop to transform the role of software and science integration in tomorrow's global information society.This book is an attempt to capture the essence on a new state of art in software science and its supporting technology. The book also aims at identifying the challenges such a technology has to master. It contains highly extensively reviewed papers lectured at the Sixth International Conference on New Trends in Software Methodology Tools, and Techniques, V, (SoMeT_07) held in Rome (CNR), Italy, from 7 to 9 November 2007, (http://www.somet.soft.iwate-pu.ac.jp/somet_07). This conference brought together researchers and practitioners to share their original research results and practical development experiences in software science, and its related new challenging technology.One of the important issues addressed by this book is software development security tools and techniques. Another example we challenge in this conference is intelligent software design from the human aspect and the technology aspect. This book and the series it continues will also elaborate on such new trends and related academic research studies and development.A major goal was to gather scholars from the international research community to discuss and share research experiences on new software methodologies, and formal techniques. The book also investigates other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. These are essential for developing a variety of information systems research projects and to assess the practical impact on real-world software problems.For an outline of the past series of related events that contributed to this publication, SoMeT_02 was held on October 3--5, 2002, in Sorbonne, Paris, France.SoMeT_03 was held in Stockholm, Sweden, SoMeT_04 in Leipzig, Germany, SoMeT_05 in Tokyo, Japan, SoMeT_06 in Quebec, Canada, and most recently SoMeT_07 held in Rome, Italy. These events also initiate a future event to be organized in October 2008, (http://www.somet.soft.iwate-pu.ac.jp/somet_08/).This book provides an opportunity for exchanging ideas and experiences in the field of software technology, opening up new avenues for software development, methodologies, tools, and techniques, especially, software security and program coding diagnosis and related software maintenance techniques aspects. Also, we have emphasized human centric software methodologies, end-user development techniques, and human emotional reasoning for best performance harmony between the design tool and the user.Issues discussed are research practices, techniques and methodologies proposing and reporting solutions needed for global world business. We believe that this book creates an opportunity for us in the software science community to think about where we are today and where we are going.The book is a collection of 33 carefully reviewed best-selected papers by the reviewing committee.The areas covered are:• Software engineering aspects on software security, programs diagnosis and maintenance• Static and dynamic analysis on Lyee-oriented software performance model• Software security aspects on Java mobile code, and networking• Practical artefact on software security, software validation and diagnosis• Software optimization and formal methods• Requirement engineering and requirement elicitation• Software methodologies and Lyee oriented software techniques• Automatic software generation, reuse, and legacy systems• Software quality and process assessment• Intelligent software systems and evolution• End-user requirement engineering and programming environment• Ontology and philosophical aspects on software engineering• Business software models and other kinds of software application models, based on Lyee theoryAll papers published in this book have been carefully reviewed and selected by the SOMET international reviewing committee. Each paper has been reviewed by between three and six reviewers and has been revised based on the review reports. The papers were reviewed on the basis of technical soundness, relevance, originality, significance, and clarity.This book outcome is also a collective effort from many industrial partners and colleagues from around the world. We also gratefully thank Iwate Prefectural University, especially its President Prof. Makoto Taniguchi, CNR Rome, Italy, Catena Co., SANGIKYO Co., ARISES and others for their overwhelming support of this work. We especially are thankful to the reviewing committee and others who participated in the hard effective review of all submitted papers and thanks also for the hot discussions we have had at the review evaluation meetings which selected the contributions in this book.This outcome is another milestone in mastering new challenges in software and its new promising technology, within SoMeT's consecutive events. It also gives the reader new insights, inspiration and concrete material to elaborate and study this new technology.Last but not least, we would like to thank and acknowledge the Microsoft Conference Management Tool team for the support it has provided on the use of Microsoft CMT System as a conference-supporting tool during all the phases of the SOMET transactions.The Editors},
booktitle = {Proceedings of the 2007 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Sixth SoMeT_07},
pages = {i–xiii}
}

@phdthesis{10.5555/1467771,
author = {Wee, Sewook},
advisor = {Kozyrakis, Christos},
title = {Atlas: software development environment for hardware transactional memory},
year = {2008},
isbn = {9780549622680},
publisher = {Stanford University},
address = {Stanford, CA, USA},
abstract = {Multi-cores are already available on today's personal computers, and parallel programming is the key to utilizing their scalable performance. However, writing a fast and correct parallel program is still difficult because multiple threads run on the shared data; thus, programmers should synchronize them properly. To address this difficulty, Transactional Memory (TM) has been proposed as an alternative to conventional lock-based synchronization. TM can be implemented in a variety of ways; software TM (STM) is attractive because it runs on off-the-shelf hardware without modification, whereas hardware TM (HTM) performs much better and provides correct and predictable results. This research is built upon the Transactional Coherence and Consistency architecture (TCC), an HTM architecture developed at Stanford University. Moreover, unlike other proposals, TCC uses TM mechanisms to replace conventional MESI (Modified-Exclusive-Shared-Invalid) protocol, having all user code executes within transactions—i.e. all transactions, all the time. To develop parallel applications that fully utilize TM's capability, a complete software development environment is necessary. The software environment includes programming languages, an operating system, and performance and functionality debugging tools. This thesis presents a software development environment, referred to as ATLAS; it addresses the challenges of the latter two issues, the operating system and the productivity tools, on the full-system prototype of the TCC architecture. Running an operating system on an HTM system faces many challenges: it requires a communication mechanism between the user thread and the operating system that does not compromise the atomicity and isolation of transactions; It also requires a mechanism to handle irrevocable operations, such as I/O, and external actions, such as interrupts. ATLAS addresses these issues by dedicating a CPU to run the operating system (OS CPU). The remaining CPUs run a proxy kernel that handles the interactions of the applications with the operating system using a separate communication channel. This thesis describes the implementation of OS functionality using this approach and demonstrates that it scales efficiently to multi-core systems with 32 processors. ATLAS builds upon TM resources to provide three functional and performance debugging tools for parallel programming. The first tool, ReplayT, provides the deterministic replay of multithreaded applications; it tracks execution at the granularity of transactions to reduce both the time and space overhead of logging thread interactions. The second tool, AVIO-TM, detects atomicity violation bugs in transactional memory programs. It extends, simplifies, and accelerates the proposed AVIO mechanism. The third tool, TAPE, is a light-weight runtime performance bottleneck monitor that identifies the performance bottlenecks of TM applications with the detailed information that is needed to optimize the applications. TAPE builds upon TM hardware that continuously monitors all memory accesses in the user code.},
note = {AAI3313683}
}

@inproceedings{10.1145/2462476.2465611,
author = {Dodds, Zachary and Erlinger, Michael},
title = {MyCS: building a middle-years CS curriculum},
year = {2013},
isbn = {9781450320788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462476.2465611},
doi = {10.1145/2462476.2465611},
abstract = {In this poster we present a CS curriculum aimed at "middle-years" students. In the U.S., this corresponds roughly to middle school and early high-school, i.e., students aged 11-15. This MyCS curriculum provides a hands-on introduction to computer science through six distinct modules. Three programming modules use the Scratch environment to build computational sophistication and procedural intuition within an inviting development environment. Alternating with Scratch are three problem-solving modules whose activities deepen awareness and facility with computation through novel and adapted exercises. By carefully integrating novel activities with a targeted subset of the excellent resources from ECS, AP CS Principles, webdev tools, and CS unplugged, MyCS enables middle-school teachers without prior CS training to offer an 18-week course after only one week of summertime professional development. More than 25 classrooms and 900 students have participated in MyCS's pilot years within the Pomona, CA and Lihue, HI school districts. 2013-15 will see an expansion of its workshops, academic-year classes, and program evaluation. This poster shares our development and deployment of MyCS to date, and it invites feedback from the experiences of other institutions, worldwide, in their efforts to foster adolescents' computational identities.},
booktitle = {Proceedings of the 18th ACM Conference on Innovation and Technology in Computer Science Education},
pages = {330},
numpages = {1},
keywords = {middle-school computer science},
location = {Canterbury, England, UK},
series = {ITiCSE '13}
}

@inproceedings{10.5555/1416502.1416531,
author = {Hashiura, Hiroaki and Kuwabara, Toru and Qiu, Yumei and Yamashita, Koutarou and Ishikawa, Tatsuya and Shirakawa, Kiyomi and Komiya, Seiichi},
title = {A system for supporting group exercise in software development with facilities to create an optimal plan of student grouping and team formation of each group},
year = {2008},
isbn = {9789606766428},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {In the recent years, software development has become larger in scale and more complicated. Furthermore, development with faster delivery and lower cost is required, thus the software development environment is becoming more and more critical. Therefore, companies are seeking high potential students in universities who have a practical sense. The information science and engineering department of Shibaura Institute of Technology provides group exercise classes that adopt a more practical approach to software development such that students can obtain the knowledge and skills necessary for software development (strictly it is a team exercise in the sense that each member of the group shares the tasks for software development). However, as the class hours assigned for learning software engineering are not sufficient, EtUDE (Environment for Ultimate software Development Exercise), the support system that learners can collectively practice at any time anywhere was developed, and introduced to the exercise classes. However, as the skills of software development differ from one student to another, the problem that some teams are not able to accomplish the assignment by the deadline often occurs. Therefore, the authors developed EtUDE/GO, which automatically generates the best plan for team formation. Optimizing team formation with this system, we can solve the problem attributable to difference in students' software development skills. This paper discusses the system overview of EtUDE/GO as well as the consequence of applying the system.},
booktitle = {Proceedings of the 7th WSEAS International Conference on Software Engineering, Parallel and Distributed Systems},
pages = {149–157},
numpages = {9},
keywords = {exercise for software development, exercises in units of groups, genetic algorithm, optimizing project team formation, web application system},
location = {Cambridge, UK},
series = {SEPADS'08}
}

@inproceedings{10.5555/1563296.1563297,
title = {Front Matter},
year = {2005},
isbn = {1586035568},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for the new economy and science. It creates new markets and new directions for a more reliable, flexible, and robust society. It empowers the exploration of our world in ever more depth.However, software often falls short of our expectations. Current software methodologies, tools, and techniques remain expensive and not yet reliable for a highly changeable and evolutionary market. Many approaches have been proven only as case-by-case oriented methods.This book presents a number of new trends and theories in the direction in which we believe software science and engineering may develop to transform the role of software and science in tomorrow's information society.This book is an attempt to capture the essence of a new state of art in software science and its supporting technology. The book also aims at identifying the challenges such a technology has to master. It contains papers accepted at the fourth International Conference on New Trends in Software Methodologies Tools and Techniques, IV, (SoMeT_05) held in Tokyo, Japan, from 28th to 30th September 2005, (http://www.somet.soft.iwate-pu.ac.jp/somet_05). This workshop brought together researchers and practitioners to share their original research results and practical development experiences in software science, and its related new challenging technology.One example we challenge in this conference is Lyee methodology --a newly emerged Japanese software methodology that has been patented in several countries in Europe, Asia, and America, but which is still at an early stage of emerging as a new software style. This conference and the series it continues will also contribute to elaborate on such new trends and related academic research studies and development.A major goal of this international conference was to gather scholars from the international research community to discuss and share research experiences on new software methodologies, and formal techniques. The conference also investigated other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. These are essential for developing a variety of information systems research projects and to assess the practical impact on real-world software problems.SoMeT_02 was held on October 3--5, 2002, in Sorbonne, Paris, France, SoMeT_03 in Stockholm, Sweden, SoMeT_04 in Leipzig, Germany and the conference that these proceedings cover, SoMeT_05, was held in Tokyo, Japan. These events initiate a forthcoming series that will include the 5th conference, SoMeT_W06, to be organized in Quebec, Canada on September 2006 (http://www.somet.soft.iwate-pu.ac.jp/somet_06/).This book is also in part a means for presenting few selected parts of the results of the Lyee International research project (http://www.lyee-project.soft.iwate-pu.ac.jp), which aims at the exploration and development of novel software engineering methods and software generation tools based on the Lyee framework. This project was sponsored by Catena and other major Japanese enterprises in the area of software methodologies and technologies.This book participates to provide an opportunity for exchanging ideas and experiences in the field of software technology, opening up new avenues for software development, methodologies, tools, and techniques.The Lyee framework for example, captures the essence of the innovations, controversies, challenges and possible solutions of the software industry. This world wide patented software approach was born and enriched from experience, and it is time, and again through SoMeT_W05 to try to let it stimulate the academic research on software engineering, attempting to close the gap that has so far existed between theory and practice. We believe that this book creates an opportunity for us in the software science community to think about where we are today and where we are going.The book is a collection of 26 carefully reviewed best-selected papers by the reviewing committee.The areas covered in the book are: --Requirement engineering and requirement elicitation, and its tools; --Software methodologies and tools for robust, reliable, non-fragile software design; --Lyee oriented software techniques, and its legacy systems; --Automatic software generation versus reuse, and legacy systems, source code analysis and manipulation; --Software quality and process assessment; --Intelligent software systems design, and software evolution techniques; --Software optimization and formal methods; --Static and dynamic analysis on software performance model, and software maintenance; --End-user programming environment, User-centered Adoption-Centric Reengineering techniques; --Ontology, cognitive models and philosophical aspects of software design; --Software design through interaction, and precognitive software techniques for interactive software entertainment applications; --Business oriented software application models; --Software Engineering models, and formal techniques for software representation, software testing and validation; --Aspect oriented programming; --Other software engineering disciplines.All papers published in this book are carefully reviewed and selected by SOMET international program committee. Each paper has been reviewed by three or four reviewers and has been revised based on the review reports. The papers were reviewed on the basis of technical soundness, relevance, originality, significance, and clarity. The acceptance rate for the papers listed in this book is 48% in this year SoMeT.This book was made possible by the collective efforts of all the Lyee International project collaborators and other people and supporters. We gratefully thank Iwate Prefectural University, University of Laval, Catena Co., ISD, Ltd, SANGIKYO co., and others for their overwhelming support. We are especially thankful to the program committee and others who participated in the review of all submitted papers and thanks also for the hot discussion we have had on the PC meetings to select the papers listed in this book.This book is another milestone in mastering new challenges on software and its new promising technology, within the SoMeT framework and others. Also, it gives the reader new insights, inspiration and concrete material to elaborate and study this new technology.We would also like to thank and acknowledge the support of the University of Leipzig, Telematik and e-Business group for allowing us to use the Paperdyne System as a conference-supporting tool during all the phases on this transaction.The Editors},
booktitle = {Proceedings of the 2005 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the Fourth SoMeT_W05},
pages = {i–xiii}
}

@inproceedings{10.1145/3456887.3457496,
author = {Zhang, Rongfeng},
title = {Design of Integrated Courseware Development Environment for Computer Education Based on Online Evaluation System},
year = {2021},
isbn = {9781450389969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456887.3457496},
doi = {10.1145/3456887.3457496},
abstract = {Distance education based on computer has become the main part of modern education in our country, which has contributed to the popularization of higher education and the improvement of teaching quality. However, at the same time, there are still many deficiencies in the opening of the existing computer education system courseware. Therefore, this paper puts forward a research on the design of integrated courseware development environment for computer education based on online evaluation system. This paper makes a detailed analysis of the classification and selection of computer education courseware, and points out the main problems existing in the existing courseware mode. In view of these shortcomings, according to the characteristics of computer education courseware, this paper puts forward the optimization design of online evaluation system. In this paper, the development ideas and objectives of the integrated courseware system are further studied. SCORM content aggregation model is used to effectively manage the micro teaching units in the courseware. Especially after the establishment of the classroom management module, the computer distance education becomes more flexible. In the application of courseware, it can be combined according to the needs, which greatly expands the computer education department the practicability of the system. The analysis shows that the computer integrated courseware management scheme based on online evaluation system not only improves the courseware editing and demonstration ability of the computer education system, but also greatly improves the comprehensive performance of the system. The teaching effect can be evaluated in real time, so that the students' learning efficiency is higher.},
booktitle = {2021 2nd International Conference on Computers, Information Processing and Advanced Education},
pages = {1231–1234},
numpages = {4},
keywords = {Computer Courseware, Computer Networks, Distance Education, Online Evaluation System},
location = {Ottawa, ON, Canada},
series = {CIPAE 2021}
}

@book{10.5555/1611309,
author = {Browne, Paul},
title = {JBoss Drools Business Rules},
year = {2009},
isbn = {1847196063},
publisher = {Packt Publishing},
abstract = {Capture, automate, and reuse your business processes in a clear English language that your computer can understand. An easy-to-understand JBoss Drools business rules tutorial for non-programmers Automate your business processes such as order processing, supply management, staff activity, and more Prototype, test, and implement workflows by themselves using business rules that are simple statements written in an English-like language Discover advanced features of Drools to write clear business rules that execute quickly For confident users of Excel or other business software, this book is everything you need to learn JBoss Drools business rules and successfully automate your business. In Detail In business, a lot of actions are trigged by rules: "Order more ice cream when the stock is below 100 units and temperature is above 25 C", "Approve credit card application when the credit background check is OK, past relationship with the customer is profitable, and identity is confirmed", and so on. Traditional computer programming languages make it difficult to translate this "natural language" into a software program. But JBoss Rules (also known as Drools) enables anybody with basic IT skills and an understanding of the business to turn statements such as these into running computer code. This book will teach you to specify business rules using JBoss Drools, and then put them into action in your business. You will be able to create rules that trigger actions and decisions, based on data that comes from a variety of sources and departments right across your business. Regardless of the size of your business, you can make your processes more effective and manageable by adopting JBoss Rules. Banks use business rules to process your mortgage (home loan) application, and to manage the process through each step (initial indication of amount available, actual application, approval of the total according to strict rules regarding the amount of income, house value, previous repayment record, swapping title deeds, and so on). Countries such as Australia apply business rules to visa applications (when you want to go and live there)--you get points for your age, whether you have a degree or masters, your occupation, any family members in the country, and a variety of other factors. Supermarkets apply business rules to what stock they should have on their shelves and where--this depends upon analyzing factors such as how much shelf space there is, what location the supermarket is in, what people have bought the week before, the weather forecast for next week (for example, ice cream in hot weather), and what discounts the manufacturers are giving. This book shows how you can use similar rules and processes in your business or organization. It begins with a detailed, clear explanation of business rules and how JBoss Rules supports them. You will then see how to install and get to grips with the essential software required to use JBoss Rules. Once you have mastered the basic tools, you will learn how to build practical and effective of the business rule systems. The book provides clear explanations of business rule jargon. You will learn how to work with Decision Tables, Domain-Specifi c Languages (DSL)s, the Guvnor and JBoss Integrated Development Environment (IDE), workflow and much more. By the end of the book you will know exactly how to harness the power of JBoss Rules in your business. What you will learn from this book? Understand the basics of business rules and JBoss rules with minimal effortInstall the required software easily and learn to use the Guvnor, which is a user-friendly web editor that's also powerful enough to test our rules as we write themLearn to write sophisticated rules and import the fact model into the Guvnor and then build a guided rule around it, which makes your web pages a lot clearerGain complete knowledge of what we can do with the Guvnor rule editor, and then use the JBoss IDE as an even more powerful way of writing rules, and automate processes for discounts, orders, sales, and moreKnow the structure of the rule file through the example of a shipping schedule, which will help you with your own shipping scheduleTest your rules not only in the Guvnor, but also using FIT for rule testing against requirements documents; run unit tests using JUnit for error-free rules and interruption-free servicesSpecifically, non-developers can work with Excel spreadsheets as a fact model to develop business processes without learning any other new technologyWork with DSLs (Domain-Specific Languages) and rule flow to make writing rules easy; which makes staff training quicker and your working life easierDeploy your business rules to the real world, which completes your project successfully, and combine this into a web project using the framework of your choice to provide better servicesBenefit from concepts such as truth maintenance, conflict resolution, pattern matching rules agenda, and the Rete algorithm to provide advanced and faster business systems so that staff efficiency is maximized Approach This book takes a practical approach, with step-by-step instructions. It doesn't hesitate to talk about the technologies, but takes time to explain them (to an Excel power-user level). There is a good use of graphics and code where necessary. Who this book is written for? If you are a business analyst - somebody involved with enterprise IT but at a high level, understanding problems and planning solutions, rather than coding in-depth implementations - then this book is for you. If you are a business user who needs to write rules, or a technical person who needs to support rules, this book is for you. If you are looking for an introduction to rule engine technology, this book will satisfy your needs. If you are a business user and want to write rules using Guvnor/JBoss IDE, this book will be suitable for you. This book will also suit your need if you are a business user and want to understand what Drools can do and how it works, but would rather leave the implementation to a developer.}
}

@techreport{10.5555/903388,
author = {Hartson, H. R and Hix, Deborah and Kraly, Thomas M.},
title = {Developing Human-Computer Interface Models and Representation Techniques(Dialogue Management as an Integral Part of Software Engineering)},
year = {1987},
publisher = {Virginia Polytechnic Institute &amp; State University},
address = {USA},
abstract = {The Dialogue Management Project at Virginia Tech is studying the poorly understood problem of human-computer dialogue development. This problem often leads to low usability in human-computer dialogues. The Dialogue Management Project approaches solutions to low usability in interfaces by addressing human-computer dialogue development as an integral and equal part of the total system development process. This project consists of two rather distinct, but dependent, parts. One is development of concepts for dialogue management, and the other is implementation of a dialogue management system (DMS) to evaluate these concepts. The goal of this paper is to describe our approach to the development of two of these conceptual aspects and how we oriented those toward the needs of practical implementation. The two conceptual aspects are (a) a structural, descriptive model of human-computer interaction, and (b) Techniques for representing both the behavioral (end-user''s) view and the constructional (developer''s) view of dialogue. The approach to their development was a technology transfer process that was part of a two-year university/industry research liaison between the Dialogue Management Project and IBM Federal Systems Division (FSD), now called Systems Integration Division. Part of this liaison was aimed at moving our research ideas and results into a real-world dialogue development environment. Following presentation of the technical problems and solutions, the paper concludes with a discussion of results of our liaison and by raising and addressing some questions of mutual interest that arose during our cooperative interaction.}
}

@inproceedings{10.5555/1860875.1860876,
title = {Front Matter},
year = {2010},
isbn = {9781607506287},
publisher = {IOS Press},
address = {NLD},
abstract = {Software is the essential enabler for science and the new economy. It creates new markets and new directions for a more reliable, flexible and robust society. It empowers the exploration of our world in ever more depth. However, software often falls short of our expectations. Current software methodologies, tools, and techniques remain expensive and are not yet sufficiently reliable for a constantly changing and evolving market, and many promising approaches have proved to be no more than case-by-case oriented methods.This book explores new trends and theories which illuminate the direction of developments in this field, developments which we believe will lead to a transformation of the role of software and science integration in tomorrow's global information society. By discussing issues ranging from research practices and techniques and methodologies, to proposing and reporting solutions needed for global world business, it offers an opportunity for the software science community to think about where we are today and where we are going.The book aims to capture the essence of a new state of the art in software science and its supporting technology, and to identify the challenges that such a technology will have to master. It contains extensively reviewed papers presented at the ninth International Conference on New Trends in software Methodology Tools, and Techniques, (SoMeT_10) held in Yokohama, Japan, with the collaboration of SANGIKYO Co., from September 29th to October 1st 2009. (http://www.somet.somet.iwate-pu.ac.jp/somet_10/).This conference brought together researchers and practitioners to share their original research results and practical development experience in software science and related new technologies.This volume participates in the conference and the SoMeT series Previous related events that contributed to this publication are: SoMeT_02 (the Sorbonne, Paris, 2002); SoMeT_03 (Stockholm, Sweden, 2003); SoMeT_04 (Leipzig, Germany, 2004); SoMeT_05 (Tokyo, Japan, 2005); SoMeT_06 (Quebec, Canada, 2006); SoMeT_07 (Rome, Italy, 2007); SoMeT_08 (Sharjah, UAE, 2008); SoMeT_09 (Prague, Czech Republic, 2009) and SoMeT_10 (Yokohama, Japan, 2010). of which it forms a part, by providing an opportunity for exchanging ideas and experiences in the field of software technology; opening up new avenues for software development, methodologies, tools, and techniques, especially with regard to software security and programme coding diagnosis and aspects of related software maintenance techniques. The emphasis has been placed on human-centric software methodologies, end-user development techniques, and emotional reasoning, for an optimally harmonised performance between the design tool and the user.This book, and the series it forms part of, will continue to contribute to and elaborate on new trends and related academic research studies and developments in SoMeT_2011 in Germany.A major goal of this work was to assemble the work of scholars from the international research community to discuss and share research experiences of new software methodologies and techniques. One of the important issues addressed is the handling of cognitive issues in software development to adapt it to the user's mental state. Tools and techniques related to this aspect form part of the contribution to this book. Another subject raised at the conference was intelligent software design in software security and programme conversions. The book also investigates other comparable theories and practices in software science, including emerging technologies, from their computational foundations in terms of models, methodologies, and tools. This is essential for a comprehensive overview of information systems and research projects, and to assess their practical impact on real-world software problems. This represents another milestone in mastering the new challenges of software and its promising technology, addressed by the SoMeT conferences, and provides the reader with new insights, inspiration and concrete material to further the study of this new technology.The book is a collection of 30 carefully refereed papers selected by the reviewing committee and covering:• Software engineering aspects of software security programmes, diagnosis and maintenance• Static and dynamic analysis of software performance models• Software security aspects and networking• Agile software and lean methods• Practical artefacts of software security, software validation and diagnosis• Software optimization and formal methods• Requirement engineering and requirement elicitation• Software methodologies and related techniques• Automatic software generation, re-coding and legacy systems• Software quality and process assessment• Intelligent software systems design and evolution• Artificial Intelligence Techniques on Software Engineering, and Requirement Engineering• End-user requirement engineering, programming environment for Web applications• Ontology, cognitive models and philosophical aspects on software design• Business oriented software application models• Model Driven Development (DVD), code centric to model centric software engineering• Cognitive Software and human behavioural analysis in software design.All papers published in this book have been carefully reviewed, on the basis of technical soundness, relevance, originality, significance, and clarity, by up to four reviewers. They were then revised on the basis of the review reports before being selected by the SoMeT_10 international reviewing committee.This book is the result of a collective effort from many industrial partners and colleagues throughout the world. I would like acknowledge my gratitude to JSPS (Japan Society for the Promotion of Science) and SANGIKYO Co., for the sponsorship and support. Also, my thanks go to Iwate Prefectural University, ARISES, and all the others who have contributed their invaluable support to this work. Most especially, I thank the reviewing committee and all those who participated in the rigorous reviewing process and the lively discussion and evaluation meetings which led to the selected papers which appear in this book. Last and not least, I would also like to thank the Microsoft Conference Management Tool team for their expert guidance on the use of the Microsoft CMT System as a conference-support tool during all the phases of SoMeT_10.The editor},
booktitle = {Proceedings of the 2010 Conference on New Trends in Software Methodologies, Tools and Techniques: Proceedings of the 9th SoMeT_10},
pages = {i–xiii}
}

@phdthesis{10.5555/921884,
author = {Bevill, Douglas A.},
advisor = {Childs, John},
title = {An investigation into the efficacy of transaction shells as a computer-based instructional design and delivery tool for an instructional designer, instructor, and end user},
year = {1995},
publisher = {Wayne State University},
address = {USA},
abstract = {The development of computer based instruction (CBI) is a costly labor intensive endeavor which is considered a serious obstacle to the widespread use of CBI. Transaction shells, a CBI authoring and delivery tool, represents one attempt at finding a solution to this problem. The specific purposes of this study were to investigate: (a) The efficacy of transaction shells as an authoring environment and instructional paradigm for an instructional developer, (b) the efficacy of a transaction shell based instructional product for an instructor as a user of the development tool or owner of the instructional lesson, and (c) the efficacy of a transaction shell based instructional lesson for the student or user of that lesson. The samples for this study involved a graduate student in Instructional Technology as an instructional developer, an instructor in the School of Engineering, and users of CBI drawn from the graduate population in Instructional Technology all at Wayne State University in Detroit, Michigan.The results of the study include: (1) Transaction shells as represented in this version of ID Expert contain enactment problems that need to be addressed. (2) The transaction shell authoring approach is different from other authoring programs and the commercial version of this software should be provided with clear and concise directions. (3) The educational environment will be slow in feeling the impact of this authoring tool due to hardware requirements. (4) Transaction shells provide an effective and efficient instructional development review process. (5) The transaction shell automated instructional delivery paradigm allows the developer to concentrate on the instructional content and not the instruction. (6) Transaction shells can be used as a presentation program. (7) Transaction shells appear to be efficient and effective for instructional delivery. (8) The use of multimedia resources within transaction shells should be determined by motivational concerns identified with specific audiences or where the instructional outcomes are not being accomplished.Recommendations were made for additional research that would build on the findings of this study and implications were made for the field of instructional design and automated authoring tools.},
note = {AAI9530525}
}

@phdthesis{10.5555/924952,
author = {Popovich, Steven S.},
advisor = {Kaiser, Gail E.},
title = {An architecture for extensible workflow process servers},
year = {1997},
isbn = {0591326434},
publisher = {Columbia University},
address = {USA},
abstract = {Process-centered environments (PCEs) are viewed by many as a way around the "software crisis"; they formalize development methodologies in enactable process modeling languages (PMLs) that can be "executed" in order to provide various assistance modes to developers. These include, e.g. maintaining consistency, monitoring compliance with and/or actually enforcing the process, automation of routine activities, and guiding developers through the process. Research has produced a variety of PCEs, each with its own PML, providing one or more assistance modes. However, no consensus has formed on such issues as the appropriate set of assistance modes, or the "best" software process formalism; we doubt that there is a single "best" PML. Thus, for a development organization wishing to move to a process-based environment, there is a problem of competing formalisms--choosing an appropriate PCE, with its associated PML.Problems can also arise in migrating to a PCE. Integrating the organization's development tools into the PCE is generally simple, but the process also must be "coded up" in the PML, and the existing code "immigrated" into the PCE. This can present technical difficulties, but there are also serious organizational problems, notably training the developers, managers, etc., who know only the old legacy environment, to use the new PCE.We address both problems: Competing formalisms, by translating high-level PMLs into a rule-based "process assembly language" (PAL), and legacy environments, by means of process servers that provide process enactment facilities to an existing environment, rather than taking over top-level control, as PCEs generally do. We present A scMBER, a process engine with extensible PAL syntax and semantics tailorable to any desired assistance modes, as a kernel for process servers supporting a wide range of PMLs. New process enactment directives may be added to the syntax of the PAL, adding new assistance modes to A scMBER. The process engine is parameterized by callbacks to mediator code in order to implement any new directives and to modify the enactment behavior. A scMBER's mediators also facilitate integration with a wide variety of environment architectures.},
note = {AAI9723838}
}

@book{10.5555/1076985,
author = {Berg, Clifford},
title = {High-Assurance Design: Architecting Secure and Reliable Enterprise Applications},
year = {2005},
isbn = {0321375777},
publisher = {Addison-Wesley Professional},
abstract = {How to Design for Software Reliability, Security, and MaintainabilityMany enterprises unfortunately depend on software that is insecure, unreliable, and fragile. They compensate by investing heavily in workarounds and maintenance, and by employing hordes of "gurus" to manage their systems' flaws. This must change. And it can. In this book, respected software architect Clifford J. Berg shows how to design high-assurance applications-applications with proven, built-in reliability, security, manageability, and maintainability.High-Assurance Design presents basic design principles and patterns that can be used in any contemporary development environment and satisfy the business demand for agility, responsiveness, and low cost. Berg draws on real-world experience, focusing heavily on the activities and relationships associated with building superior software in a mainstream business environment. Practicing architects, lead designers, and technical managers will benefit from the coverage of the entire software lifecycle, showing how to: Understand and avoid the problems that lead to unreliable, insecure software Refocus design and development resources to improve software Identify project risks and plan for assurable designs Obtain the requirements needed to deliver high assurance Design application systems that meet the identified requirements Verify that the design satisfies these requirements Plan and design tests for reliability and security Integrate security design, reliability design, and application design into one coherent set of processes Incorporate these concerns into any software development methodology© Copyright Pearson Education. All rights reserved.}
}

@book{10.5555/548919,
author = {Li, Sing and Economopoulos, Panos},
title = {Professional Visual C++ Activex Intranet Programming},
year = {1997},
isbn = {1861000375},
publisher = {Wrox Press Ltd.},
address = {GBR},
abstract = {From the Publisher: Long Live the Information Superhighway! Wait a minute! I heard some loud heckling from the back row. "I thought this is a 'Microsoft ActiveX' technical programming book !" It is. The point is, there are two ways you can show a technology: either in dry, boring isolation, talking about the nuances without reference to the problems the technology is supposed to solve, or as we've attempted to do, applying the technology to a real-world situation. We've chosen to show you ActiveX programming by applying this technology to a large growth area, intranets. As you'll soon see, the ActiveX family of technologies is a very broad family covering the gamut of Internet-intranet, client-server and distributing computing solutions. It has to be this way, since ActiveX represents Microsoft's entire investment into the Internet-intranet (and object based distributed computing) race. Within this book, ActiveX will be used as a vehicle to explore many of the concepts and techniques involved in intranet construction. In many cases, the concepts and approaches explored are generally applicable to your practice whether it's Microsoft-, UNIX-, or even Netscape-centric. What's Covered in this Book Just before the technical reader, fluent with Visual C++, decides to return this book to the bookstore for a refund, I must say that this is by no means a 'lame theoretical treatise'. We turn on the power-throttle, and shine our high beams on the core ActiveX technologies once we've reached Chapter 2. In fact, a discussion of what intranets are about has been relegated to Appendix A, simply because you don't actually need to know about them to gain an understanding of ActiveX. From there we dive into the depths of the Microsoft's Component Object Model (COM) which is fundamental to all of Microsoft's ActiveX technology. Covering the basics, we'll be taking a view that reduces this complex topic to simple programming practices that we're fully familiar with. From there, we examine the concept of COM aggregation and show how it further enhances code reuse and provides a powerful mechanism for COM. In Chapter 3, we take our understanding of COM and put it into practice by writing an ActiveX control from scratch, using just raw C++. Here, we'll become intimate with the complete anatomy of a simple ActiveX control. When we move on to more powerful libraries and code generation wizards, this basic understanding will enable us to adapt and troubleshoot more effectively. In this chapter, we encounter many essential COM interfaces through actual hands-on programming; we'll also get acquainted with some indispensable COM programming tools such as the MIDL compiler and the Object Viewer utility. To handle some more complex problems without coding forever, we'll take a look at programming libraries to simplify the COM object programming task (ActiveX controls, to be precise). We'll explore how to code powerful, yet super efficient and tiny COM objects using the ActiveX Template Library (ATL) 2.1, and we will spend some time explaining many of the new COM interfaces, and show how ATL makes everything simple. Also in this chapter, we'll be learning about the threading models supported by COM objects and the different types of COM servers that can be created. Using ATL to create ActiveX controls is the focus of Chapter 4. Chapters 2, 3 and 4 give us enough background into understanding what ActiveX provides for the intranet development environment. We'll understand how ActiveX controls can be fundamental building blocks (actually software components) in both client- and server-based programming. We'll make excursions into the ActiveX controls (OCX ) specifications in Chapter 5, covering the differences between the OLE Control specification (for Visual controls) and the new OC96 specification (for ActiveX controls). We'll actually be designing an Events Calendar control. This control will display currently active events (for the month) from different company departments for easy and straightforward access. The distributed 'live update' nature of this control eliminates the need for consolidating events information in a centralized database. In Chapters 6 and 7 we put our design into code. Using Visual C++ 5.0 and MFC 4.21, we'll be building the actual Events Calendar intranet control. The control class and custom wizard provided by MFC greatly simplify much of the development. We'll also be building two additional 'back-end' ActiveX controls using ATL 2.1 to do data processing for the Visual Calendar control. Finally, we will test the controls and show that the Calendar control is a bona fide ActiveX control that can be hosted within containers such as Visual Basic 5.0, Internet Explorer 3.0, and FrontPage 97. In Chapter 8, we shift into the highest gear and attempt to put the Calendar control through its paces by using DCOM to run the front-end and back-end ActiveX controls across three separate machines. Along the way, we'll learn a lot about DCOM and how it enables true distributed computing. We'll also be examining the difficult problem of ActiveX control code installation and revision control, and see how the Internet Explorer 3.0 provides us with a ready-made solution to the problem. As part of the installation solution, we'll develop a small program to download controls from remote sites. After the intensive programming in Chapter 8, we shift our focus to a hot intranet issue in Chapter 9: security. We'll examine the topic by drawing a parallel to the Windows NT security model which is fundamental to all other security mechanisms built upon it. We'll learn about the various security, authentication and encryption APIs and COM interfaces available to intranet application developers. Special attention will be paid to DCOM related security issues and how arbitrary distributed objects may be prevented or allowed to execute on certain machines. We conclude our coverage in Chapter 10 by casting aside our overly enthusiastic attitude and examine some real and hard-to-tackle ActiveX and intranet deployment issues, suggesting potential solutions wherever they are available. We'll cover a lot of ground in the following pages. I hope your journey into the exciting world of ActiveX will be as pleasant, productive, and profitable for you as it has been for us. What You Need to Use This Book To use this book you need Visual C++ 5.0, and the latest version of Microsoft's best-selling C++ compiler. This version is 32-bit only, so you'll need to install it on Windows 95, Windows NT 3.51 or NT 4, which means a 486 CPU or better and a minimum 16Mb of memory. For Visual C++, you'll need quite a lot of hard disk space - a typical installation is 170 Mbytes. You can do a minimal installation which takes up around 40 Mbytes, but this will mean longer compile times as the CD-Rom will be utilized more often. Some of the later chapters require you to have access to a network and a second computer to test the code correctly. You'll also need to have DCOM for Windows 95 (information on obtaining this is given in Chapter 8) or Windows NT 4.0. |AUTHORBIO: Bitten by the microcomputer bug since 1978, Sing has grown up with the microprocessor age. His first personal computer was a $99 do-it-yourself Netronics COSMIC ELF computer with 256 bytes of memory, mail ordered from the back pages of Popular Electronics magazine. Currently, Sing is an active author, consultant, and entrepreneur. He has written for popular technical journals and is the creator of the "Internet Global Phone", one of the very first Internet phones available. His wide-ranging consulting expertise spans Internet and Intranet systems design, distributed architectures, digital convergence, embedded systems, real-time technologies, and cross platform software design. Recently, he has completed an assignment with Nortel Multimedia Labs working in Computer Telephony Integration, and Advanced Callcenter Management products. Sing is a founder of microWonders, an emerging company specializing in products to fulfill the ubiquitous "computing anywhere" vision. Other titles by this author: ATL Programmer's Resource Kit Professional COM Applications with ATL Professional IE4 Programming Visual C++ 4 Master Class|AUTHORBIO: Panos Economopoulos has been the architect, designer and leader for implementations of a number of complex and successful distributed computer systems. Currently, he is Manager of Research and Development at Telesis North. Here, he designed the OnAir series of mobile client-server products that provide efficient and robust remote access to BackOffice servers over a variety of satellite and other wireless networks. He has extensive experience as a consultant to the Industry and has developed, and taught, a variety of courses both at University undergrad level and for mature developers. He's also carried out advanced research at the University of Toronto - results of which have been published in several research journals. Other titles by this author ATL Programmer's Resource Kit, Professional COM Applications with ATL|}
}

@inproceedings{10.1145/3304221.3319751,
author = {Kennedy, Cazembe and Kraemer, Eileen T.},
title = {Qualitative Observations of Student Reasoning: Coding in the Wild},
year = {2019},
isbn = {9781450368957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3304221.3319751},
doi = {10.1145/3304221.3319751},
abstract = {Understanding student thinking and identifying student misconceptions are important precursors to developing high quality pedagogical materials and approaches. Prior work has used conceptual assessment surveys and task-based interviews to pursue this knowledge, typically having students evaluate existing code or predict or explain code behavior. However, features of student thinking captured under these conditions may differ from features of student thinking that occurs "in the wild." We present the results of a study conducted with 10 introductory CS students at a large, engineering-focused US university. In this work, students were asked to "Please, think aloud" as they interacted in a development environment, attempting to reverse engineer a solution to a defined task based on a provided executable. We captured and analyzed video recordings of the screen and audio recordings of their utterances to characterize their actions, current task, the relevant CS concept involved, current problem-solving phase, and expressed level of certainty. We also took note of the nature of their uncertainties and if and how these uncertainties were resolved. We found that students showed uncertainty regardless of success at task completion. Students who were successful at the task engaged in live experimentation to address these uncertainties in their thinking and coding. However, the ability to produce a program that behaved correctly did not guarantee that the students fully grasped the underlying concepts: some students submitted erroneous code that coincidentally worked, while expressing doubts about the correctness of their implementation.},
booktitle = {Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {224–230},
numpages = {7},
keywords = {computing education, cs1, cs2, introductory programming, misconceptions, pedagogical content knowledge, qualitative analysis},
location = {Aberdeen, Scotland Uk},
series = {ITiCSE '19}
}

@article{10.5555/1060033.1060035,
author = {Tedder, Maurice and Chamulak, David and Chen, Li-Ping and Nair, Santosh and Shvartsman, Andrey and Tseng, I. and Chung, Chan-Jin},
title = {An affordable modular mobile robotic platform with fuzzy logic control and evolutionary artificial neural networks},
year = {2004},
issue_date = {August 2004},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {21},
number = {8},
issn = {0741-2223},
abstract = {Autonomous robotics projects encompass the rich nature of integrated systems that includes mechanical, electrical, and computational software components. The availability of smaller and cheaper hardware components has helped make possible a new dimension in operational autonomy. This paper describes a mobile robotic platform consisting of several integrated modules including a laptop computer that serves as the main control module, microcontroller-based motion control module, a vision processing module, a sensor interface module, and a navigation module. The laptop computer module contains the main software development environment with a user interface to access and control all other modules. Programming language independence is achieved by using standard input/output computer interfaces including RS-232 serial port, USB, networking, audio input and output, and parallel port devices. However, with the same hardware technology available to all, the distinguishing factor in most cases for intelligent systems becomes the software design. The software for autonomous robots must intelligently control the hardware so that it functions in unstructured, dynamic, and uncertain environments while maintaining an autonomous adaptability. This paper describes how we introduced fuzzy logic control to one robot platform in order to solve the 2003 Intelligent Ground Vehicle Competition (IGVC) Autonomous Challenge problem. This paper also describes the introduction of hybrid software design that utilizes Fuzzy Evolutionary Artificial Neural Network techniques. In this design, rather than using a control program that is directly coded, the robot's artificial neural net is first trained with a training data set using evolutionary optimization techniques to adjust weight values between neurons. The trained neural network with a weight average defuzzification method was able to make correct decisions to unseen vision patterns for the IGVC Autonomous Challenge. A comparison of the Lawrence Technological University robot designs and the design of the other competing schools shows that our platforms were the most affordable robot systems to use as tools for computer science and engineering education. © 2004 Wiley Periodicals, Inc.},
journal = {J. Robot. Syst.},
month = {aug},
pages = {419–428},
numpages = {10}
}

@inproceedings{10.1145/3287324.3293750,
author = {Leelanupab, Teerapong and Meephruek, Tiwipab},
title = {CodeBuddy (Collaborative Software Development Environment): In- and Out-Class Practice for Remote Pair-Programming with Monitoring Coding Students' Progress},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293750},
doi = {10.1145/3287324.3293750},
abstract = {Pair-programming is an Agile technique in Extreme Programming (XP) where traditionally two programmers need to be collocated and work together at one workstation. Previous research has shown that pair-programming is very beneficial in software engineering education. However, learning and practicing pair-programming are mostly limited in a class where students can only learn to collaboratively program with another student in controlled or laboratory settings. Although nowadays there exist some collaborative tools, such as CodePilot, Google Colaboratory and Git, they are not specifically pair-programming-oriented. This impedes a pairing's ability to discuss effective strategies in problem solving, to form productive or mutually learning pairs, and to predict pair compatibility. To encourage students in out-class practice of pair-programming, we present a demonstration of a novel web-based software development environment, called CodeBuddy, for remote pair-programming. CodeBuddy provides instructors and students with several features for managing laboratory classes and practicing pair-programming. Examples of CodeBuddy's features include: coding screen mirroring between a pair, output terminal to show compiled results, face-to-face like communication channels (i.e., video calling and instant text messaging), automatic and manual role switching, code quality analysis for monitoring coding students' progress and recommending a pair with targeted pairing goals, implicit code reviews using face detection for tracking a reviewer's engagement, line-by-line code commenting, etc. The demonstration consists of a walkthrough of two use-case scenarios: an instructor assigns a problem-solving task and two students remotely work together in a pair using CodeBuddy on two different workstations to solve it.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1290},
numpages = {1},
keywords = {agile software development, pair programming, remote collaboration and learning, software engineering},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@phdthesis{10.5555/221842,
author = {Jeong, Jong Sik},
title = {An intelligent computer-assisted rater trainer with fault tree analysis for isolating},
year = {1995},
publisher = {University of Alabama},
address = {USA},
abstract = {The training of raters is one practical way for enhancing the accuracy of performance ratings. Traditional methods for presenting the training, such as lecture, group discussion, and practice/feedback, are subject to certain weaknesses. A new ICART training method, which is an application of knowledge-based systems, was suggested to overcome the weaknesses of these traditional training methods. The objectives of this study were three-fold: (1) preparing a proposal for training raters, (2) developing the ICART system according to proposal guidelines, and (3) examining whether the developed ICART system may improve the accuracy of performance ratings.The fault tree analysis technique was used to effectively define the training content. During this analysis of the problem domain, biases that occurred in judgmental activities were considered as rating-error sources. Based on a questionnaire survey and factor analysis, relationships between rating errors and error sources were explored. The results of the fault tree analysis defined the scope and the order of training tasks.Based on the defined training content, the ICART system was developed as a prototype in accordance with formalities of intelligent tutoring systems. The system was composed of three functional modules: a domain-expert knowledge module, a teaching knowledge module, and a user-interface module. Production systems and hypertext techniques were utilized to represent knowledge for the modules in the system. The Exsys Professional shell was used as a development tool.The developed ICART system was tested in the situation of evaluating an individual performing a briefing. A total of twenty-eight ROTC cadets participated in two scheduled training sessions. The control group was trained by the traditional lecture and practice method; the experimental group was trained by interacting with the ICART system. Five accuracy components, elevation, differential elevation, stereotype accuracy, differential accuracy, and overall accuracy, were used as dependent variables for analyzing training effects. It was found that training by the ICART system improved accuracy components of differential elevation and overall accuracy.},
note = {UMI Order No. GAX94-29236}
}

@proceedings{10.1145/977091,
title = {CF '04: Proceedings of the 1st conference on Computing frontiers},
year = {2004},
isbn = {1581137419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizing committee, we welcome you to the First Conference on Computing Frontiers (CF'04). We selected Ischia as the conference site, for we felt it would create a pleasant forum for the meeting. We also wished to initiate a tradition of a comfortable and relaxing atmosphere which would be conducive to many informal discussions.The conference was created to allow the exploration of those technologies at the very edge of computing which would have the potential to substantially improve existing art. To this end, we have considered papers on theory, methods, technologies, and implementations concerned with innovations in computing paradigms, computational models, architectural paradigms, computer architectures, development environments, compilers, and operating environments.The organizing committee has strived to put together a stimulating scientific program consisting of regular papers and invited sessions on some promising and established areas of research. The technical program contains 36 papers, selected out of 57 submitted manuscripts through a careful peer review process and grouped in 13 sessions, spanning from architectures to processors, from networking to clusters, and from computational models to applications. The keynote address ("Quantum Parallelism and the Exact Simulation of Physical Systems") by Professor Dan Marinescu (University of Central Florida) will tackle quantum computing, one of the most promising technologies to break the performance barriers of conventional systems. Three special sessions have also been organized on "NOMADS (Networks of Mobile Adaptive Dependable Systems)" by Professor Miroslaw Malek (Humboldt-Universit\"{a}t zu Berlin), "Memory Wall" by Professor Mateo Valero (Universidad Politecnica de Catalunya), and "Reconfigurable Computing" by Professor J\"{u}rgen Becker (Universit\"{a}t Karlsruhe (TH)).We must emphasize that several people, all volunteers, have worked to make this Computing Frontiers conference a success. We would like to thank in particular all the members of the steering and program committees, the special sessions chairs, the publicity and publication chairs, the treasurer, the local arrangements chair, and all the authors and reviewers.Welcome to Ischia and to CF'04. We trust that you will enjoy the scientific as well the social program of the conference. Also, we hope you will consider submitting or attending Computing Frontiers next year and will tell all your colleagues what an informative and productive meeting this has been.},
location = {Ischia, Italy}
}

@inproceedings{10.5555/1864181.1864187,
author = {Mammino, Liliana},
title = {Plenary lecture 6: language aspects in science and technology education: novel approaches for new technologies},
year = {2010},
isbn = {9789604742028},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Language is the fundamental tool for the development of thought. It is therefore an essential tool for all the inquiry aspects in the sciences (identifying relationships between pieces of information, identifying investigation questions, formulating and verifying hypotheses, making inferences) and in the trains of thoughts leading from information to interpretation and ultimately to theory. It is thus extremely important that science students acquire sufficiently sophisticated levels of language-mastering to be able to use it for a real familiarization with the main aspects of doing science.In recent years, there is a growing concern about fast deterioration of the quality of language-mastering among the young generation, mostly as a result of the dominant use of communication technologies for which short, grammatically and logically unconnected sentences are viewed as the most suitable options. Such deterioration poses a threat to the development of science thoughts in future years, because of the risk of inadequacies in the ability to utilise the essential thought-development tool to its full power.The current presentation suggests that the development of language-mastering abilities up to the sophistication levels that are needed for the generation and communication of scientific information needs to become a relevant component of science and technology education. This requires the design of novel approaches, integrating the increasing utilization of new, computer-based, educational technologies with the development of language-mastering abilities. The design is challenging, because of the complexity of the language skills that are relevant within the sciences ? skills concerning the identification and expression of individual logical or method-related relationships (e.g., cause-effect, hypothesis-thesis, condition-consequence) and of comprehensive logical and interpretation frameworks. The presentation proposes and discusses some options, considering implementation pathways, feasibility assessments and expected impacts, on the basis of long experience with the analysis of language-related difficulties encountered by science students and of the interplays between language communication and other communications forms, like visualization.},
booktitle = {Proceedings of the 7th WSEAS International Conference on Engineering Education},
pages = {20},
numpages = {1},
location = {Corfu Island, Greece},
series = {EDUCATION'10}
}

@book{10.5555/515554,
author = {Columbus, Louis},
title = {The  Microsoft Windows XP Professional Handbook},
year = {2002},
isbn = {1584502193},
publisher = {Charles River Media, Inc.},
address = {USA},
abstract = {From the Publisher: Focusing on the needs of the advanced technical professional who is responsible for an entireseries of Windows NT, Windows2000 and Windows XP Professional, The Windows XP Professional Handbook is designed to be both a handy desk reference in addition to a content review for MCSE courses. This book provides readers with insights into how Microsoft's latest enterprise-based operating system solves the connectivity and development environment challenges that system administrators and advanced users face everyday. It also focuses on how to solve the tougher issues that arise in organizations running multiple operating systems. In addition to the expanded coverage of the most challenging aspects of keeping a workgroup going, this hands-on guide also includes short case studies as sidebars, illustrating key terms and concepts from the content covered in the chapter. KEY FEATURES - Contains information on upgrading from previous operating systems and installing popular software applications on Windows XP Professional - Focuses on the needs of technical professionals responsible for a series of Windows XP systems - Acts as a complete desk reference and resource for various certification topics - Provides insights on solving everyday problems with Windows XP Professional - Includes a CD-ROM with demos of tools and utilities to evaluate leading Windows XP applications prior to purchasing them Author Biography: Louis Columbus (Orange, CA) is Senior Manager, Workstation Marketing at Gateway Inc., and has written ten other books on computer topics, three focusing on the Windows XP Professional operating system. Louis' interests include writing articles and books on computer topics and teaching systems analysis, systems design and Windows NT and XP Professional and Server courses at a local university.}
}

@article{10.1023/A:1019069012307,
author = {Topcuoglu, Haluk and Hariri, Salim and Kim, Dongmin and Kim, Yoonhee and Bing, Xue and Ye, Baoqing and Ra, Ilkyeun and Valente, Jon},
title = {The design and evaluation of a virtual distributed computing environment},
year = {1998},
issue_date = {May 1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {1},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1023/A:1019069012307},
doi = {10.1023/A:1019069012307},
abstract = {Current advances in high-speed networks such as ATM and fiber-optics, and software technologies such as the JAVA programming language and WWW tools, have made network-based computing a cost-effective, high-performance distributed computing environment. Metacomputing, a special subset of network-based computing, is a well-integrated execution environment derived by combining diverse and distributed resources such as MPPs, workstations, mass storage, and databases that show a heterogeneous nature in terms of hardware, software, and organization. In this paper we present the Virtual Distributed Computing Environment (VDCE), a metacomputing environment currently being developed at Syracuse University. VDCE provides an efficient web-based approach for developing, evaluating, and visualizing large-scale distributed applications that are based on predefined task libraries on diverse platforms. The VDCE task libraries relieve end-users of tedious task implementations and also support reusability. The VDCE software architecture is described in terms of three modules: (a) the Application Editor, a user-friendly application development environment that generates the Application Flow Graph (AFG) of an application; (b) the Application Scheduler, which provides an efficient task-to-resource mapping of AFG; and (c) the VDCE Runtime System, which is responsible for running and managing application execution and for monitoring the VDCE resources. We present experimental results of an application execution on the VDCE prototype for evaluating the performance of different machine and network configurations. We also show how the VDCE can be used as a problem-solving environment on which large-scale, network-centric applications can be developed by a novice programmer rather than by an expert in low-level details of parallel programming languages.},
journal = {Cluster Computing},
month = {may},
pages = {81–93},
numpages = {13},
keywords = {Asynchronous Transfer Mode, Application Execution, Asynchronous Transfer Mode Network, Cluster Manager, Site Repository}
}

@inproceedings{10.5555/1416502.1416527,
author = {Hashiura, Hiroaki and Yamashita, Kotaro and Ishikawa, Tatsuya and Isozaki, Yuka and Komiya, Seiichi},
title = {A software development group exercise support environment, EtUDE: the system overview and the system evaluation through applying to classes},
year = {2008},
isbn = {9789606766428},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {In the recent years, software development has become larger in scale and more complicated. Furthermore, development with faster delivery and lower cost is required, thus the software development environment is becoming more and more complex. Accordingly, companies seek high potential students in universities who have a practical sense. The information engineering department of Shibaura Institute of Technology provides classes that adopt a more practical approach to software development so that students can obtain knowledge and skills necessary for software development. However, as class hours assigned for learning software engineering are not sufficient, a support system that enables students to practice outside the classroom or school is required. This support system should have work such that it enables each member belonging to the same group to collectively perform tasks just like in the classroom. In order to solve this problem, the authors developed EtUDE [1][2], the group exercise support environment for software development. Although group exercise helps to reduce the burden of instructing compared to individual exercise, it is difficult to offer instructions that meet the individual's needs. However, it is the goal of the exercise-based classes to accomplish the obtainment of knowledge and skills for software development according to each student's level. Therefore, the authors developed the group exercise support environment for software development, EtUDE which features various functions necessary for group exercise support as well as the function that detects learners who do not benefit from the group exercise and need individual instruction. With this, the software development exercise in more practical form will be available, and at the same time, it is made possible to acquire the knowledge and skills necessary for software development according to each student's level. This essay presents the overview of EtUDE system and the outcome of the application of the system.},
booktitle = {Proceedings of the 7th WSEAS International Conference on Software Engineering, Parallel and Distributed Systems},
pages = {124–131},
numpages = {8},
keywords = {software development environment, software development group exercises},
location = {Cambridge, UK},
series = {SEPADS'08}
}

@book{10.5555/1708084,
author = {Byeon, Ok-Hwan and Kwon, Jang Hyuk and Dunning, Thom and Cho, Kum Won and Savoy-Navarro, Aurore},
title = {Future Application and Middleware Technology on e-Science},
year = {2009},
isbn = {1441917187},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {As scientific research increasingly requires the efforts of globally diverse, multidisciplinary teams from different laboratories and organizations, there surfaces a need to facilitate this kind of collaborative work. These large-scale scientific initiatives, where information technology plays an important role, are termed e-Science; e-Science activities employ geographically distributed resources like high performance computing facilities, scientific instruments, databases, and high performance networks. Future Application and Middleware Technology on e-Science presents selected papers from the 2008 Korea e-Science All-Hands-Meeting (AHM 2008). Hosted by the Korea Institute of Science and Technology Information (KISTI), the Korea e-Science AHM was designed to bring together developers and users of e-Science applications and enabling information technologies from international and interdisciplinary research communities. The AHM 2008 conference served as a forum for engineers and scientists to present state-of-the-art research and product/tool developments, and to highlight related activities in all fields of e-Science. The following topics concerning e-Science are covered in this volume: e-Science applications in Physics, Astronomy, and Chemistry e-Science applications in Bio-medicine and Life science e-Science applications in Geo-science with remote sensing e-Science applications in Climates and Earth systems e-Science applications in Engineering (Aerospace, Ship, Automobile, and etc) Grid technologies (Computing, Data, VO, and etc) Collaborative science models and techniques Enabling technologies of Workflows and Web services Resource management and scheduling Problem solving environments Scientific data management Application development environments Robust and transparent middleware Programming paradigms and models Software engineering tools Community development and engagement User outreach, training, and documentation The works presented in this edited volume bring together cross-disciplinary information on e-Science in one cohesive source. This book is suitable for the professional audience composed of industry researchers and practitioners of e-Science. This volume is also suitable for advanced-level students in the field.}
}

@book{10.5555/517603,
author = {Dahnoun, Naim},
title = {Digital Signal Processing Implementation Using the TMS320C6000 DSP Platform},
year = {2000},
isbn = {0201619164},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
edition = {1st},
abstract = {From the Book: PREFACE: Preface Digital signal processing techniques are now so powerful that sometimes it is extremely difficult, if not impossible, for analogue signal processing to achieve the same or closer performance. Added to this, digital signal processors are very affordable and include good development tools and support. This is sufficient to explain the growing number of areas of application for DSP, including motor drives, communications, biomedical instrumentation and automotive applications. Having dealt for some time with undergraduate and postgraduate students, researchers and digital signal processor users in general, I have found that first-time users of DSP find a barrier obstructing them in progressing from theory to the full implementation of algorithms. When it comes to implementing an algorithm many questions arise, questions such as: Which processor to use - fixed or floating point Which manufacturer to choose Which application hardware to use How many I/O interfaces are needed and how fast should they be When these questions are answered, more questions arise regarding the implementation on the specific processor and hardware selected. In this book, use of the TMS320C6000 will be justified, and the hardware and complete implementation of selected algorithms will be dealt with in detail. Material used for the teaching of undergraduate and postgraduate students, along with laboratory experiments, are used to demonstrate and simplify the transition from theory to the full implementation on the TMS320C6201 processor. This book is divided into nine chapters. Chapters2and 3 are very important and it is advisable that they are well understood before progressing onto subsequent chapters. Chapter 1 Introduction This introductory chapter provides the reader with general knowledge on general-purpose DSP processors and also provides an up-to-date TMS320 roadmap showing the evolution of Texas Instruments' DSP chips in terms of processing power. Chapter 2 The TMS320C62xxlC67xx architecture The objective of this chapter is to provide a comprehensive description of the 'C6x architecture. This includes a detailed description of the Central Processing Unit (CPU) and program control along with an overview of the memory organisation, serial ports, boot function and internal timer. Chapter 3 Software development tools and TMS32OC6201 EVM overview This chapter is divided into three main parts. The first part describes the software development tools, the second part describes the Evaluation Module (EVM) and, finally, the third part describes the codec, and use of interrupts along with some useful programs for testing the TMS320C6201 EVM. Chapter 4 Software optimisation To introduce the need for code optimisation, this chapter starts by developing the concept of pipelining. Since the TMS320C62xx and the TMS320C67xx each have eight units, which are dedicated to different operations, and since different instructions can have different latencies, the programmer or the tools are left with the burden of scheduling the code. Backed by examples, this chapter explains the different techniques used to optimise DSP code on these processors. Chapter 5 Finite Impulse Response (FIR) filter implementation The purpose of this chapter is twofold. Primarily, it shows how to design an FIR filter and implement it on the TMS320C62xx processor, and secondly, it shows how to optimise the code as discussed in Chapter 4. This chapter discusses the interface between C and assembly, how to use intrinsics, and how to put into practice material that has been covered in the previous chapters. Chapter 6 Infinite Impulse Response (IIR) filter implementation This chapter introduces the IIR filters and describes two popular design methods, that is the bilinear and the impulse invariant methods. Step by step, this chapter shows the procedures necessary to implement typical IIR filters specified by their transfer functions. Finally, this chapter provides complete implementation of an IIR filter in C language, assembly and linear assembly, and shows how to interface C with linear assembly. Chapter 7 Adaptive filter implementation This chapter starts by introducing the need for an adaptive filter in communications. It then shows how to calculate the filter coefficients using the Mean Square Error (MSE) criterion, exposes the Least Mean Square (LMS) algorithm and, finally, shows how the LMS algorithm is implemented in both C and assembly. Chapter 8 Goertzel algorithm implementation This chapter deals with Dual Tone Multi-Frequency (DTMF) detection and provides a practical example of the Goertzel algorithm. This chapter also shows how to produce optimised code by the pen and paper method, describes linear assembly and demonstrates how to program the Direct Memory Access (DMA). Chapter 9 Implementation of the Discrete Cosine Transform This chapter starts by introducing the need for video compression to reduce the channel bandwidth requirement, then explains the Joint Photographic Experts Group (JPEG) image codec. This includes a detailed discussion and the implementation of the Discrete Cosine Transform (DCT) and Inverse Discrete Cosine Transform (IDCT) and concentrates on their optimisation. An explanation of the PC-DSP communication via the PCI bus is also provided. Software The accompanying CD includes all the programs used in this book. To help the reader in locating or viewing the files, an Index.htm file has been included. The files are in separate directories corresponding to each chapter. Some directories are further divided in sub-directories to separate different implementations. Batch files for compiling, assembling and linking these programs are included. All the files have been tested (the environment may need to be modified: see env.bat file). Software using Code Composer Studio environment is also provided. Software updates including code running on the TMS320C6211 DSK can be obtained from the Publisher. Acknowledgements As you can imagine, it is hard to produce any textbook on state-of-the-art technology, especially when the time factor is playing against you. However, with the first very encouraging comments from the five anonymous reviewers, my motivation for writing this book surged, and therefore I would like to thank them for their constructive comments. Due to the unfamiliarity with this processor, it was difficult to share ideas with other users. But with Tuan-Kiang Chiew, Kwee-Tong Heng and Michael Hart many problems were solved and many grey areas were clarified; I extend to them special thanks. I am indebted to Robert Owen, Hans Peter Blaettel, Gene Frantz, Neville Bulsara, Greg Peake, Helga and Graham Stevenson and Maria Ho of Texas Instruments for their encouragement, continuous help and support. I owe my thanks to Professor Barrie Jones, Professor David Evans, Dr. John Fothergill and Fernando Schlindwein from Leicester University for their encouragement, and Dr Anthony Brooms from Oxford University and Dr Mark Yoder from the Rose-Hulman Institute of Technology, USA, for reviewing the material. My thanks to all of my colleagues at the Department of Engineering at Bristol University and to all of our students, in particular Khaled, Fernando, Mohamed, Samir, Chris, Shirley and Julian. Also I would like to thank Cornelius Kellerhoff, European DSP Business Development Consultant, Paul Coulton, Communications Research Centre, Lancaster University, and Mariusz Jankowski, University of Southern Maine, for their valuable technical reviews. I thank my parents, family and friends for their support and encouragement. Finally, many thanks to Karen Sutherland, Julie Knight and all of the Pearson Education and Prentice Hall team who were very kind, supportive and encouraging. N. Dahnoun Naim.Dahnoun@Bristol.ac.uk}
}

@inproceedings{10.1109/ADCOM.2007.126,
author = {Buyya, Rajkumar},
title = {Tutorial 1: Utility-Oriented Grid Computing and the Gridbus Middleware},
year = {2007},
isbn = {0769530591},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ADCOM.2007.126},
doi = {10.1109/ADCOM.2007.126},
abstract = {Grid computing, one of the latest buzzwords in the ICT industry, is emerging as a new paradigm for Internet-based parallel and distributing computing. It enables the sharing, selection, and aggregation of geographically distributed autonomous resources, such as computers (PCs, servers, clusters, supercomputers), databases, and scientific instruments, for solving large-scale problems in science, engineering, and commerce. It leverages existing IT infrastructure to optimize compute resources and manage data and computing workloads. The developers of Grids and Grid applications need to address numerous challenges: security, heterogeneity, dynamicity, scalability, reliability, service creation and pricing, resource discovery, resource management, application dAbsecomposition and service composition, and qualify of services. A number of projects around the world are developing technologies that help address one or more of these challenges. To address some of these challenges, the Gridbus Project at the University of Melbourne has developed grid middleware technologies that (1) enable the creation of Utility Grids, which provide economic incentive for Grid service providers for sharing resources; and (2) support rapid development and optimal deployment of eScience and eBusiness applications on enterprise and global Grids. The components of Gridbus middleware are: Grid application development environment for rapid creation of distributed applications, Grid service broker and application scheduler, Grid workflow management engine, SLA (service-level agreements) based Scheduler for clusters, Web-services based Grid market directory (GMD), Grid accounting services, Gridscape for creation of dynamic and interactive resource monitoring portals, Portlets for creation of Grid portals that support web-based management of Grid applications execution, and GridSim toolkit for performance evaluation. In addition, Gridbus also includes a widely used .NET-based enterprise Grid technology and Grid web services framework to support the integration of both Windows and Unix-class resources for Grid computing.},
booktitle = {Proceedings of the 15th International Conference on Advanced Computing and Communications},
pages = {xxiii–xxiv},
series = {ADCOM '07}
}

@proceedings{10.1145/2567634,
title = {PPAA '14: Proceedings of the first workshop on Parallel programming for analytics applications},
year = {2014},
isbn = {9781450326544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {"Necessity is the mother of invention". That premise motivated the organizers of this workshop and the program committee to bring together communities that need vast amounts of computingresources for big analytics problems and the communities that can build computing platforms to solve those problems.It is our great pleasure to welcome you to the First Workshop on Parallel Programming for Analytics Applications (PPAA). Analytics applications are scaling rapidly in terms of the size and variety of data analyzed, the complexity of models explored and tested, and the number of analytics professionals or data scientists supported concurrently. At the same time hardware systems are embracing new technologies like on-chip and off-chip accelerators, vector extensions to instruction sets, and solid state disks. New programming methodologies and run-times to support them are emerging to facilitate the development of new analytics applications, and to leverage emerging systems. This workshop provides a forum for the applications community, runtime and development environment community and systems community to exchange the outlook for progress in each of these areas and exchange ideas on how to cross leverage the progress.We especially encourage attendees to attend the keynote and invited talk presentations. These valuable and insightful talks can and will guide us to a better understanding of the future: Future Directions in Analytic Applications, Dr. Edward J. Baranoski (currently Director of the Office of Smart Collection at Intelligence Advanced Research Projects Activity (IARPA) where the focus is on dramatically improving the value of collected data from all sources.)Cognitive Computing Journey, David Nahamoo (currently at IBM Research, focusing on cognitive computing.)Graphs &amp; Networks: Computing and Analytics at Lincoln Laboratory, Robert A. Bond (currently with IBM Lincoln Labs architectures, conducting research that support graph analytics as data sets scale to million-node graphs and beyond.)High-speed Graph Analytics with the Galois System, Dr. Keshav Pingali (Professor in the Department of Computer Science at the University of Texas at Austin, research is focused on programming languages and tools for multicore and manycore processors.)},
location = {Orlando, Florida, USA}
}

@book{10.5555/3294129,
author = {Zafar, Iffat and Tzanidou, Giounona and Burton, Richard and Patel, Nimesh and Araujo, Leonardo},
title = {Hands-On Convolutional Neural Networks with TensorFlow: Solve computer vision problems with modeling in TensorFlow and Python},
year = {2018},
isbn = {1789130336},
publisher = {Packt Publishing},
abstract = {Learn how to apply TensorFlow to a wide range of deep learning and Machine Learning problems with this practical guide on training CNNs for image classification, image recognition, object detection and many computer vision challenges. Key Features Learn the fundamentals of Convolutional Neural Networks Harness Python and Tensorflow to train CNNs Build scalable deep learning models that can process millions of items Book Description Convolutional Neural Networks (CNN) are one of the most popular architectures used in computer vision apps. This book is an introduction to CNNs through solving real-world problems in deep learning while teaching you their implementation in popular Python library - TensorFlow. By the end of the book, you will be training CNNs in no time! We start with an overview of popular machine learning and deep learning models, and then get you set up with a TensorFlow development environment. This environment is the basis for implementing and training deep learning models in later chapters. Then, you will use Convolutional Neural Networks to work on problems such as image classification, object detection, and semantic segmentation. After that, you will use transfer learning to see how these models can solve other deep learning problems. You will also get a taste of implementing generative models such as autoencoders and generative adversarial networks. Later on, you will see useful tips on machine learning best practices and troubleshooting. Finally, you will learn how to apply your models on large datasets of millions of images. What you will learn Train machine learning models with TensorFlow Create systems that can evolve and scale during their life cycle Use CNNs in image recognition and classification Use TensorFlow for building deep learning models Train popular deep learning models Fine-tune a neural network to improve the quality of results with transfer learning Build TensorFlow models that can scale to large datasets and systems Who this book is for This book is for Software Engineers, Data Scientists, or Machine Learning practitioners who want to use CNNs for solving real-world problems. Knowledge of basic machine learning concepts, linear algebra and Python will help.}
}

@phdthesis{10.5555/AAI29351245,
author = {Nagaria, Bhaveet},
title = {An Investigation of Human Error in Software Development},
year = {2021},
publisher = {Brunel University (United Kingdom)},
abstract = {Context: Software defects occurring in code bases lead to an increased cost for software production and maintenance. To err is human nature and the process of software development is human centric. My analysis of the literature shows that the use of human error theory is emerging as an important tool for software development. Aim: The aim of my thesis is to present a training tool aimed at reducing the number of human errors developers make while working within the development phase of the Software Development Life cycle (SDLC) by improving developer situation awareness. Methods: My first study uses semi structured interviews to gain insight into what Skill-based (SB) errors developers make and how they mitigate these errors. My second study employs an experimental setup where developers log all human errors they make during developmental tasks across two weeks. At the beginning of week two the developers are asked to complete an online training package which I have developed on situation awareness. Results: The first study shows that the complexity of the development environment is one of the most frequently reported reasons for errors. I found that software developers struggle with effective mitigation strategies for their errors, reporting strategies largely based on improving their own willpower to concentrate better on development tasks. The results from the second study show that training software developers in situation awareness does lead to a decrease in the number of human errors made by those software developers. Conclusion: My doctoral research shows that human errors are a problem for software developers and loss of situation awareness is key for many of these developers. My preliminary results show that training tools which address situation awareness can aid developers in reducing the number of human errors that they make. Further work is required to investigate other means of improving developer situation awareness and determine whether my findings are generalisable.},
note = {AAI29351245}
}

@book{10.5555/515908,
author = {Clark, Jeffrey E.},
title = {VBA for AutoCAD 2002: Writing AutoCAD Macros},
year = {2001},
isbn = {0130652016},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Book: Preface Data management has been a central focus of mine throughout most of my 30-plus-year career. As an architect engaged mostly in working with business managers, helping them define their project needs (Should I build, lease, expand, or what ... And how much do I need ), I needed a computer early on. I began using BASIC in 1970, on a timesharing network, to manage the data I collected and to produce reports. Thus began my involvement with the BASIC language. I have used many dialects, from the line-numbered, Beginners All-purpose Symbolic Instruction Code that came out of Dartmouth University around 1960, to the modern versions of VB and VBA. These are now among the favored languages for communicating with relational databases such as Oracle, SQL Server, and the like, and for creating content for the Internet. BASIC was so-named because, as an interpreted language, it was easy to learn by incrementally developing a program, entering a few lines of code at a time, and then testing the result. At the time, however, many professional programmers thought it to be something of a toy. Mr. Gates, on the other hand, was a strong supporter of the language, introducing three levels of BASIC with the first IBM PC in 1981. Cassette BASIC was hardwired into the machines' ROM, and you could only save files to a cassette tape. (I never actually did that!) The disk and advanced levels were built into Microsoft's first version of its operating system: MS-DOS 1.0. A BASIC compiler (BASCOM) was introduced shortly thereafter, which produced executable programs in .EXE format. Late in the 1980s, QuickBasic wasintroduced with the first Interactive Development Environment (IDE) for DOS. This was followed in 1991 by Visual Basic 1.0, in both DOS and Windows versions. Visual Basic (VB) continues its growth in popularity. Microsoft Word and Excel were the first components of Microsoft Office, and each had its own internal functionality for creating macros, essentially scripts that allowed certain procedures to be automated within the application. Visual Basic for Applications (VBA) was first introduced in Excel in 1993. With the addition of Access, PowerPoint, and other products to the mix, VBA now provides an object-oriented programming environment common to all the Office components. This not only allows the creation of macros within each of the applications, but macros that allow Access, for example, to actually start up and communicate with Excel. The AutoCAD World AutoCAD is and always has been a database program. Underlying its graphic interface are lists within lists that define the entities that make up your drawing. During its evolution into a fully Windows-compatible product, many new data-oriented features were added. Initially conceived as a drafting package and written by engineers, AutoCAD was designed with an accessible data structure that allows both the manipulation of its graphic entities as well as the attachment and extraction of textual and numeric data. Attribute extraction using AutoLISP was at first the only means of manipulating the data within the drawing, and this method was widely used in the DOS days of the mid-1980s, when I began using the application. The AutoCAD SQL Extension (ASE), introduced with Release 12 in the early 90s, provided a rudimentary interface to databases such as DBASE and PARADOX. AutoCAD was recast in object-oriented C++ with Release 13 and the introduction of the AutoCAD Runtime Extension (ARX). Using ARX requires a programmer capable of creating DLLs using C++, which the average AutoCAD user is not. About the same time, however, AutoCAD users began experimenting with programs written in Visual Basic 3.0 using Dynamic Data Exchange (DDE). Curiously, support for a more robust connection between VB and AutoCAD had been provided in Release 12, but was dropped in Release 13. Autodesk added support for Microsoft's ActiveX Automation interface standard to Release 14, first allowing VBA access to AutoCAD objects. Autodesk initially released AutoCAD 2000 in the spring of 1999, fully incorporating VBA. Further enhancements focusing on Internet functionality were a feature of the 2000i release in the summer of 2000. The development environment has a look and feel identical to that of any Microsoft Office component. With this convergence, the ability to write AutoCAD macros and communicate with other VBA-enabled applications is now fully accessible to the user. Now, with the release of AutoCAD 2002, Autodesk has adopted a policy of regular releases, incorporating "technology extensions" in the form of incremental upgrades. The extensions added over each period of time will be incorporated into the latest modular release. Who This Book Is For AutoCAD 2002 is the flagship product of Autodesk, which, according to the company's web page "is the world's leading supplier of PC design software and digital content creation." Since its initial release in 1982, well over two million copies of AutoCAD have been shipped. There are nearly 1000 Autodesk Training Centers worldwide, and over one million students are trained on Autodesk products each year. Eighty five percent of the companies in the Fortune 500 are Autodesk customers, and the firm's products are available in 19 languages. There are, according to Autodesk, over 200 user groups worldwide, along with almost 3000 registered developers. There are also countless unregistered developers who have no formal relationship with Autodesk. They produce add-ons and customizations both for sale and for their own and their companies' use. The CAD Manager who needs to convert the layers of several hundred drawings received from another consultant is a potential AutoCAD developer. The Facility Manager who needs to link drawing attributes representing occupied and available areas to an Excel spreadsheet can now use VBA to accomplish this. Automation using VBA allows the user to work on the AutoCAD side, or the Excel side, or in some other application, whichever he or she is more comfortable with. I have routinely used VBA to write SQL scripts to populate database tables, as well accessing and modifying drawings in AutoCAD. I used to do many of these things in AutoLISP, which was the only act in town for 15 years. This book is for the AutoCAD and Office user who has problems to solve. Users who are already programmers can use it to familiarize themselves with the AutoCAD object model. Users who are not programmers will be able to get started by studying the book's examples. My paramount goal, more than just presenting the components of VBA, is to tie together the disparate elements of AutoCAD with which you must be reasonably comfortable in order to use VBA effectively. The AutoCAD documentation treats, in unrelated discussions, many of those features that need to work in concert to get a job done. Understanding the DXF file representation, for example, together with some of the basic syntax of AutoLISP as it relates to the underlying database structure, provides the keys to unlocking the AutoCAD drawing and manipulating its data. It is the purpose of this book to provide those necessary links. What's in the Book VBA for AutoCAD 2002: Writing AutoCAD Macros is divided into three major sections: The AutoCAD VBA Environment (Chapters 1-4) Using the AutoCAD Object Model (Chapters 5-17) Communicating with Other Applications and the Internet (Chapters 18-20) There are four appendices containing additional reference material as well as some supplemental utilities and examples. Part Two, Using the AutoCAD Object Model, is by far the longest section, in which the application's numerous collections and objects will be discussed in detail. The following outline gives you an overview of what you can expect to see in each chapter. Part One: The AutoCAD VBA Environment Chapter 1, Taking Control of AutoCAD, introduces you to some of the concepts of Automation and Microsoft's Component Object Model (COM). In it we will talk about what we mean by object-oriented programming; its tripartite foundation of encapsulation, inheritance, and polymorphism; and define such terms as class, interface, and binding. We will look at a programming example that reads data from an Excel worksheet and creates an AutoCAD drawing with it, without either application being visible, in 30 lines of code. Finally, we will introduce a system that we will use to chart all of AutoCAD's methods, properties, and events, categorizing them in a concise reference format that will be used throughout the book. Chapter 2, The VBA Environment, shows you how to use the Interactive Development Environment (IDE); how to create, edit, and save VBA projects; and differentiates between global and embedded projects. Chapter 3, DXF: Key to the Drawing Structure, goes into some necessary detail about DXF that will add to your understanding of the drawing database. Subclass markers, which are the reflection of AutoCAD's internal object structure, are introduced. We will develop two small VBA projects. One lets you look at selected DXF entity data within an AutoCAD drawing (DWG) file, and the other searches for specified entities in a DXF file. At the end of the chapters we will look at two tiny procedures that do the same thing, one in VBA and one in AutoLISP. You can decide for yourself which is the more accessible of the two languages. Chapter 4, Elements of the Object Model, introduces the AutoCAD object model. We will talk a little more about Automation interfaces, but will concentrate on taking a high-level view of the collections and objects that VBA offers within AutoCAD. Part Two: Using the AutoCAD Object Model Chapter 5, Documents and the User Interface, begins our top-down examination of the object model in detail. In the first part of the chapter we will talk about file management: creating, opening, saving, closing, importing, and exporting drawings. Then we will turn our attention to the user interface, discussing how to control your display and how to handle views and viewports. Chapter 6, Collections and Objects, continues our traversal of the object model with a discussion of the Application and Document objects. We will discuss such document-related functions as layer management and then look at how to manage Collections and access the data within the Objects they contain. Chapter 7, Utility Objects, concludes our introduction of the AutoCAD object model. In this chapter we will concentrate on functions related to creating and editing drawing data, such as Selection Sets: How to select entities in the drawing in order to do something to them. The utility object includes methods for acquiring and converting the formats of data and accessing the Internet. Chapter 8, Blocks and External References, prefaces Chapters 9-11, which deal with the AutoCAD graphic objects, or entities, both two- and three-dimensional. Blocks are complete AutoCAD drawings that have been inserted into other drawings, as symbols in many instances. We discuss Attributes in this chapter, which provide one of the means by which alphanumeric data can be stored and accessed in AutoCAD drawings. External References, which are similar to blocks, exist when other drawings are not actually inserted, but references to them are created so that changes appear automatically when the referenced drawings are updated. In Chapter 8 we will create procedures to rename and redefine XRefs, as they are called, while preserving their insertion instances. Chapter 9, Entities, covers all 23 basic AutoCAD Entities exclusive of the modeled solids and dimensions. We will dwell at some length upon some of the more interesting ones such as the Multiline, seeing how it is constructed and how to create multiline styles using DXF representation. Both kinds of meshes are treated in detail. We will create a Polyface Mesh from a data table stored in a text file and a Polygon Mesh using an Excel VBA macro and a point matrix stored in a worksheet. Chapter 10, Solids, covers the 11 three-dimensional entities in the domain of AutoCAD's solid modeler. In addition to constructing them, we will concentrate on the methods for editing them, creating compound objects using Boolean functions, making sections, and slicing. Finally, we will discuss their mass properties, such as moment of inertia, and what these properties mean. Chapter 11, Dimensions, covers the seven basic dimension types together with the Leader and Tolerance objects. We begin this chapter with a discussion of dimension styles and how to manage them. Creating and using special symbols with the tolerance object for annotation purposes is also covered. The remainder of the chapter is devoted to the dimensioning properties, which are categorized according to AutoCAD's dimension style manager. Each property is defined, along with its corresponding system variable. Chapter 12, Editing, extends our vocabulary of methods for changing and manipulating AutoCAD objects, such as Copy, Move, and the like. We will spend some time on a subject that is often glossed over, the use of geometric transformation matrices. We will develop a procedure that uses the TransformBy method to dynamically zoom and scale a three-dimensional object. Chapter 13, Events, deals with AutoCAD's "inflection points," which occur whenever there is a change of state in the application itself, the drawing you are working on, or an entity. You can write VBA subroutines called event handlers that automatically execute whenever the event to which they are connected occurs. Chapter 14, Forms and Controls, covers the principal user interface with VBA programs. Forms are containers for dialog box controls such as command buttons, list boxes, and the like, through which the user directs the program. After introducing the standard toolbox and some tips for using it, we will spend the balance of the chapter developing a utility application to discover how to integrate the form interface with many of the entity creation and editing functions we have been discussing. The utility application, called Relative, copies or moves selected entities relative to an existing location, using a dialog box. Relative also draws lines and polylines relative to a specified start point. Chapter 15, PaperSpace and Plotting, will discuss Viewports along with the Plot Configuration and Layout objects, two alternative means of formatting drawings. PaperSpace is AutoCAD's environment intended for setting up drawings to print. We will look at the methods for detecting plotter and media characteristics in order to produce the desired output. We will develop a plotting application called BatchPlot, which will format (in paper space) and plot multiple drawings with your desired layer settings. You specify the drawings to be printed using either Visual Basic's common dialog box or a list contained in a text file. Chapter 16, Preferences, deals with parts of the AutoCAD object model that control characteristics of the application itself, its environment, and the means of manipulating it, rather than the drawing itself. These are of two types, preferences stored in the registry, corresponding to the tabs on the AutoCAD's dialog box for user options, plus user options stored in the drawing that can be accessed from other applications without the use of AutoCAD itself. Chapter 17, Menus, gives you the means of altering AutoCAD's main menu, toolbars, and other menus using VBA. Part Three: Communicating with Other Applications and the Internet Chapter 18, Extensibility, will demonstrate how you can use VBA to communicate with other applications. It covers extended entity data in detail, introducing some concepts and test data that will be used more extensively in Chapter 19's project. This chapter also covers working in a zero document state, the VBA interface itself, and a brief section on ARX application handling. Chapter 19, The Facility Project, ties together much of the material presented throughout the book into an application that links an AutoCAD drawing to a Microsoft Access database. The facility project (FP) uses Microsoft's Data Access Objects (DAO) object model to integrate AutoCAD with Access, in order to track area allocation in an office layout. Chapter 20, The DWF Object Model, will go beyond AutoCAD, using and give you an understanding of how to use this additional functionality in querying and displaying drawings in DWF format over the Internet. AutoCAD's Whip! viewer, which is freely downloadable for displaying drawings in a browser, also has a VBA programming interface. We will develop an Excel-based procedure that locates a specified floor drawing and room based on a row selection in a worksheet. Appendices Appendix A, System Variables: A dictionary of the AutoCAD system variables, other than those pertaining to dimensions covered in Chapter 11. Appendix B, Enums: A listing of AutoCAD's enumerated variables, defined integer constants used in developing VBA macros. Appendix C, Object Inheritance: A chart of AutoCAD's COM interface hierarchy. Appendix D, DXF Reference: A selection of the basic graphical entities as they are represented in DXF format. An Excel VBA application is included that will read your DXF files and list the various sections in a conveniently readable format.}
}

@article{10.1155/2021/3256924,
author = {Zhang, Yibo and Tang, Jianjun and Huang, Hui and Tsai, Sang-Bing},
title = {Motion Capture and Intelligent Correction Method of Badminton Movement Based on Machine Vision},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/3256924},
doi = {10.1155/2021/3256924},
abstract = {In recent years, badminton has become more and more popular in national fitness programs. Amateur badminton clubs have been established all over the country, and amateur badminton events at all levels have increased significantly. Due to the lack of correct medical supervision and health guidance, many people have varying degrees of injury during sports. Therefore, it is very important to study the method of badminton movement capture and intelligent correction based on machine vision to provide safe and effective exercise plan for amateur badminton enthusiasts. This article aims to study the methods of motion capture and intelligent correction of badminton. Aiming at the shortcoming of the mean shift algorithm that it is easy to lose the target when the target is occluded or the background is disturbed, this paper combines the mean shift algorithm with the Kalman filter algorithm and proposes an improvement to the combined algorithm. The improved algorithm is added to the calculation of the average speed of the target, which can be used as the target speed when the target is occluded to predict the area where the target may appear at the next moment, and it can also be used as a judgment condition for whether the target is interfered by the background. The improved algorithm combines the macroscopic motion information of the target, can overcome the problem of target loss when the target is occluded and background interference, and improves the robustness of target tracking. Using LabVIEW development environment to write the system software of the Japanese standard tracking robot, the experiment verified the rationality and correctness of the improved target tracking algorithm and motion control method, which can meet the real-time performance of moving target tracking. Experimental results show that 83% of amateur badminton players have problems with asymmetric functions and weak links. Based on machine vision technology, it can provide reliable bottom line reference for making training plans, effectively improve the quality of action, improve the efficiency of action, and promote the development of sports competitive level.},
journal = {Mob. Inf. Syst.},
month = {jan},
numpages = {10}
}

@phdthesis{10.5555/193660,
author = {Steinemann, Anne Carol},
title = {A critiquing expert system to assist in operations of the San Francisco water supply system},
year = {1993},
publisher = {Stanford University},
address = {Stanford, CA, USA},
abstract = {A critiquing expert system was developed to assist in operating San Francisco's water supply network. The critiquing system goes beyond a traditional expert system by refining (rather than supplying) the user's proposed operating plan through a critique. A traditional expert system requests problem-specific data, then provides the operator with a plan. In the critiquing approach, the operator submits not only relevant information to the system, but also a proposed plan. The system evaluates the plan and provides feedback, which includes suggestions for improvement, warnings, and alternatives. The SFWD was motivated to have the critiquing system developed because of the perceived benefits in formalizing operating expertise in a critiquing system. Operating decisions are based on heuristic knowledge, not mathematical models. When personnel leave, the SFWD loses key information about how to operate the water supply network. A critiquing system can improve operators' decisions by providing expert feedback on their proposed plans, and can aid in training novice operators.Building the critiquing expert system provided several technical challenges. A new paradigm was designed to implement critiquing in an expert system development tool. Also, developing a critiquing system is more complex than developing a traditional expert system. The critiquing paradigm and system development techniques designed for this research can be used to build critiquing systems in a variety of domains.The research included experiments to test the postulated advantages of the critiquing approach over the traditional approach to expert systems. The results were unambiguous; the critiquing system was preferred to the traditional expert system for each of several measures of system performance and acceptability.The research makes three main contributions. First, the research establishes the feasibility of implementing a critiquing system for decision support in a civil engineering problem domain. Second, the research demonstrates, both theoretically and empirically, the substantial benefits of the critiquing approach to expert systems. Third, the research reveals ways the organization influences the system's development, and how system development profoundly influences the organization. Not only the system itself, but also the development process that creates the system, fosters organizational acceptance and use of the system.},
note = {UMI Order No. GAX94-04052}
}

@phdthesis{10.5555/128516,
author = {Harding, William Thomas},
title = {Techniques for automatic knowledge base acquisition: applications for expert systems},
year = {1991},
publisher = {Virginia Commonwealth University},
address = {USA},
abstract = {Expert system knowledge bases have traditionally been manually loaded by a knowledge engineer. The "knowledge" was first extracted from a expert through a series of questions and answers conducted by the knowledge engineer. Then through establishing rules and/or specific examples from the real world, this information was carefully coded and loaded into a knowledge base. This is a process that sometimes took years to complete. More recently, advances in expert system technology have addressed reducing the involvement of the knowledge engineer and providing tools for users/experts to build their own knowledge bases. Most of these expert system building tools rely heavily on the human expert not only to provide the knowledge, but also to provide the logic structure of the knowledge. The next step in the enhancement of expert system development tools is not only to reduce the time required in collecting knowledge, but additionally to structure that knowledge into a usable expert system shell format. Rather than develop "intelligent interface" techniques through direct dialogue with a "human" expert, this study addresses the development of automatic knowledge base acquisition techniques that use "text" as a source of knowledge.Hard copy "text" knowledge is readily available in many areas suitable for expert system development. Acquisition of knowledge from text has not been successful thus far, primarily because most text is not presented in a format (rules, frames, or logic) that can be directly used to load a knowledge base. Most text books, manuals, training guides, etc. are written in a conceptual presentation format. This format may be amenable to human comprehension, but it generally does not spell out the "what-if-else" type details required to construct an expert system knowledge base. This study proposes techniques to over-come these inherent problems.This study develops a model to illustrate the techniques of automatic knowledge base acquisition from text. Several examples are used to show the theory of the techniques. A real world application is selected, and two validation exercises are conducted to support the credibility of the model. This study identifies and illustrates new automatic acquisition techniques that may greatly enhance the speed and structure with which knowledge bases can be constructed.},
note = {UMI Order No. GAX91-14962}
}

@phdthesis{10.5555/AAI29137972,
author = {Pe\~{n}aherrera, Esteban Eduardo Cando},
advisor = {Carlos, Grilo, and Jose, Ribeiro, and Maria, Lapina,},
title = {A Conversational Agent to Assist Users in Public Institutions of Ecuador},
year = {2020},
isbn = {9798835533800},
publisher = {Instituto Politecnico de Leiria (Portugal)},
abstract = {This project described in this dissertation is being carried out for the Ministry of Social and Economic Inclusion (MIES) of Ecuador based on its current problems. The de ciencies that government entities have to help their citizens with information and paperwork has exceeded their operational capacity. The MIES currently has a Marketing and Information department that is saturated with questions through channels such as call centers, information centers in their of- ces and social networks outside their o ce hours.In recent years there has been an increase in requests for information through digital channels such as its Facebook page, since internet access to places far from the big cities has increased. This has been seen as an opportunity for improvement by creating an chatbot agent that helps citizens 24 hours a day, 7 days a week. Most of the queries can be made through a query to their internal data and this allows the operational burden of the operators to be released to carry out management processes and not just information.Many public and private companies have resorted to using chatbots to help their users with simple information tasks. They have also relied on these technological tools to create registration and management functions. Today we can use chatbots development tools and platforms such as Dialog ow that allow to create chatbots in a manageable and scalable way according to needs. Facebook has multiple connection tools that allow it to integrate safely and e ciently with its platforms such as Facebook Messenger, allowing customers to create applications that will reach the majority of citizens who have internet access.Developing a chatbot agent can be a simple and straightforward task, but in which the remaining time must be invested in training and helping the chatbot agent to understand words or idioms of language used in di erent regions of the same country. In the development of a chatbot, the training cycle is constant and allows the chatbot to increase its ability to understand the users who use them.In the tests carried out on this chatbot, it was concluded that although the use of these tools allows to release the operational load, it does not avoid the need for users to interact with natural persons, but the requirement is drastically reduced. In the tests it was also obtained that many problems can be obtained by nding a wide range of synonyms and local expressions, but correct training and constant review of the answers provided helps the chatbot to self-train and evolve its answers over time interactions.},
note = {AAI29137972}
}

@book{10.5555/2505459,
author = {DeMarco, Tom and Lister, Tim},
title = {Peopleware: Productive Projects and Teams (3rd Edition)},
year = {2013},
isbn = {0321934113},
publisher = {Addison-Wesley Professional},
edition = {3rd},
abstract = {Few books in computing have had as profound an influence on software management as Peopleware . The unique insight of this longtime best seller is that the major issues of software development are human, not technical. Theyre not easy issues; but solve them, and youll maximize your chances of success. Peopleware has long been one of my two favorite books on software engineering. Its underlying strength is its base of immense real experience, much of it quantified. Many, many varied projects have been reflected on and distilled; but what we are given is not just lifeless distillate, but vivid examples from which we share the authors inductions. Their premise is right: most software project problems are sociological, not technological. The insights on team jelling and work environment have changed my thinking and teaching. The third edition adds strength to strength. Frederick P. Brooks, Jr., Kenan Professor of Computer Science, University of North Carolina at Chapel Hill, Author of The Mythical Man-Month and The Design of Design Peopleware is the one book that everyone who runs a software team needs to read and reread once a year. In the quarter century since the first edition appeared, it has become more important, not less, to think about the social and human issues in software development. This is the only way were going to make more humane, productive workplaces. Buy it, read it, and keep a stock on hand in the office supply closet. Joel Spolsky, Co-founder, Stack Overflow When a book about a field as volatile as software design and use extends to a third edition, you can be sure that the authors write of deep principle, of the fundamental causes for what we readers experience, and not of the surface that everyone recognizes. And to bring people, actual human beings, into the mix! How excellent. How rare. The authors have made this third edition, with its additions, entirely terrific. Lee Devin and Rob Austin, Co-authors of The Soul of Design and Artful Making For this third edition, the authors have added six new chapters and updated the text throughout, bringing it in line with todays development environments and challenges. For example, the book now discusses pathologies of leadership that hadnt previously been judged to be pathological; an evolving culture of meetings; hybrid teams made up of people from seemingly incompatible generations; and a growing awareness that some of our most common tools are more like anchors than propellers. Anyone who needs to manage a software project or software organization will find invaluable advice throughout the book.}
}

@phdthesis{10.5555/AAI29324138,
author = {Mahaju, Sweta and Jeff, Gray, and Chris, Crawford, and Randy, Smith, and Gary, Bradshaw,},
advisor = {Jeffrey, Carver,},
title = {Application of Human Error Theories in Managing Human Errors in Software Engineering},
year = {2022},
isbn = {9798368462530},
publisher = {The University of Alabama},
abstract = {Context: Software development, especially in its initial requirements phase, is a human-centric activity and hence vulnerable to human error. Human errors are flaws in the human thought process. To ensure software quality, it is essential for practitioners to understand how to manage these human errors. Organizations often introduce changes into the requirements engineering process to either prevent human errors from occurring or to mitigate the harm caused when those errors do occur. While there are studies on human error management in other disciplines, research studies on the prevention and mitigation of human errors in software engineering and requirements engineering specifically are scarce. The current studies in software engineering do not provide strong results about the types of changes most effective in requirements engineering. Objective: The goal of this dissertation research is to structure and organize the findings on human error prevention and mitigation approaches and provide an initial evaluation of their effectiveness. To that end, I developed a taxonomy of human error prevention and mitigation strategies based on data gathered from requirements engineering professionals. Furthermore, I validated its feasibility to be broadly representative and useful in real software development processes. Method: I performed a qualitative analysis of data from two practitioner surveys on requirements engineering practices to identify and classify strategies for preventing and mitigating human errors. Then, I attempted to fit human error prevention and mitigation strategies identified in software engineering and cognitive psychology domains into the taxonomy to enhance and broaden it. Finally, I evaluated the feasibility and usefulness of the taxonomy by training senior-level undergraduate students to use the error management strategies organized in the taxonomy to handle their software development problems. Results: I organized the human error management strategies into a formal taxonomy based on whether the changes primarily affect People, Processes, or the Environment. I further organized the strategies into low-level classes inside each of these high-level categories. I found that error management strategies focused on changes in Process are more frequently used and, hence, more effective than those focused on changes in People. Conclusions: The Human Error Management taxonomy (HEMT) provides a systematic classification and organization of strategies for the prevention and mitigation of human errors in software engineering. This systematic organization provides a foundation upon which future research can build. This dissertation research provides an initial structure to the scattered error management approaches and an initial evaluation of the feasibility and usefulness of that structure. Further empirical studies are needed in more real software development environments and settings to validate and generalize the findings reported in this dissertation.},
note = {AAI29324138}
}

@book{10.5555/2597859,
author = {Friedenthal, Sanford and Moore, Alan and Steiner, Rick},
title = {A Practical Guide to SysML: The Systems Modeling Language},
year = {2011},
isbn = {9780123852076},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2},
abstract = {A general purpose graphical modeling language used to specify, analyze, and design systems that may include hardware, software, and personnel, SysML is now being adopted by companies across a broad range of industries, including aerospace and defense, automotive, and IT system developers. This book is the bestselling, authoritative guide to SysML for systems and software engineers, providing a comprehensive and practical resource for modeling systems with SysML. Fully updated to cover newly released version 1.3, it includes a full description of the modeling language along with a quick reference guide, and shows how an organization or project can transition to model-based systems engineering using SysML, with considerations for processes, methods, tools, and training. Numerous examples to help readers understand how SysML can be used in practice, while reference material facilitates studying for the OMG Systems Modeling Professional (OCSMP) Certification Program, designed to test candidates knowledge of SysML and their ability to use models to represent real-world systems. Authoritative and comprehensive guide to understanding and implementing SysML A quick reference guide, including language descriptions and practical examples Application of model-based methodologies to solve complex system problems Guidance on transitioning to model-based systems engineering using SysML Preparation guide for OMG Certified Systems Modeling Professional (OCSMP) Table of Contents Part I Introduction Systems Engineering Overview Model-Based Systems Engineering3 SysML Language Overview SysML Language Overview Part II Language Description SysML Language Architecture Organizing the Model with Packages Modeling Structure with Blocks Modeling Constraints with Parametrics Modeling Flow-Based Behavior with Activities Modeling Message-Based Behavior with Interactions Modeling Event-Based Behavior with State Machines Modeling Functionality with Use Cases Modeling Text-Based Requirements and their Relationship to Design Modeling Cross-Cutting Relationships with Allocations Customizing SysML for Specific Domains Part III Modeling Examples Water Distiller Example Using Functional Analysis Residential Security System Example Using the Object-Oriented Systems Engineering Method Part IV Transitioning to Model-Based Systems Engineering Integrating SysML into a Systems Development Environment Deploying SysML into an Organization APPENDIXES A-1 SysML Reference Guide A-2 Cross Reference Guide to the OMG Systems Modeling Professional Certification Program (OCSMP) - NEW}
}

@book{10.5555/1210154,
author = {Lott, Joey and Schall, Darron and Peters, Keith},
title = {ActionScript 3.0 Cookbook},
year = {2006},
isbn = {0596526954},
publisher = {O'Reilly Media, Inc.},
abstract = {Well before Ajax and Microsoft's Windows Presentation Foundation hit the scene, Macromedia offered the first method for building web pages with the responsiveness and functionality of desktop programs with its Flash-based "Rich Internet Applications". Now, new owner Adobe is taking Flash and its powerful capabilities beyond the Web and making it a full-fledged development environment. Rather than focus on theory, the ActionScript 3.0 Cookbook concentrates on the practical application of ActionScript, with more than 300 solutions you can use to solve a wide range of common coding dilemmas. You'll find recipes that show you how to:Detect the user's Flash Player version or their operating systemBuild custom classesFormat dates and currency typesWork with stringsBuild user interface componentsWork with audio and videoMake remote procedure calls using Flash Remoting and web servicesLoad, send, and search XML dataAnd much, much more ...Each code recipe presents the Problem, Solution, and Discussion of how you can use it in other ways or personalize it for your own needs, and why it works. You can quickly locate the recipe that most closely matches your situation and get the solution without reading the whole book to understand the underlying code. Solutions progress from short recipes for small problems to more complex scripts for thornier riddles, and the discussions offer a deeper analysis for resolving similar issues in the future, along with possible design choices and ramifications. You'll even learn how to link modular ActionScript pieces together to create rock-solid solutions for Flex 2 and Flash applications.When you're not sure how ActionScript 3.0 works or how to approach a specific programming dilemma, you can simply pick up the book, flip to the relevant recipe(s), and quickly find the solution you're looking for.Adobe Developer Library is a co-publishing partnership between O'Reilly Media and Adobe Systems, Inc. and is designed to produce the number one information resources for developers who use Adobe technologies. Created in 2006, the Adobe Developer Library is the official source for comprehensive learning solutions to help developers create expressive and interactive web applications that can reach virtually anyone on any platform. With top-notch books and innovative online resources covering the latest in rich Internet application development, the Adobe Developer Library offers expert training and in-depth resources, straight from the source.}
}

@inproceedings{10.1145/100348.100412,
author = {Lewis, John A.},
title = {An experiment to determine software reusability factors (abstract)},
year = {1990},
isbn = {0897913485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/100348.100412},
doi = {10.1145/100348.100412},
abstract = {Software reusability has been proclaimed as the common sense solution to many software development problems. The concept of reusability promotes productivity because it avoids “reinventing the wheel.” Using existing components which are similar to the current needs can be much faster than creating components from scratch. Reusability can also be viewed as promoting reliability since reused components have the benefit of both experimental and field testing.However, reusability has not fulfilled its potential for revolutionizing the software development industry. Identifying the factors which cause current reuse efforts to fail is essential to its later success. Likewise, identifying the factors that seem to promote successful reusability is equally important. Furthermore, practical ways to eliminate the detrimental factors must be developed.An experiment designed to ferret out the causes of software reuse success and failure must consider several important issues: (1) The experiment must consist of actual development and reuse. Questionnaires and subjective measurements about whether to reuse, etc. are necessary but not sufficient. (2) The experiment must be greatly controlled to avoid extraneous factors from skewing the results. Factors which might influence the outcome must be deliberately tested for, or controlled such that they do not bias the experimental data. (3) The components to be reused must be determined. Reusing requirements and designs has been suggested, but with little success. On the other hand, reusing test cases has been greatly successful. In between is code. Current experiments should still concentrate on the ability to reuse source code. You must walk before you run. (4) Finally, the factors being tested must be established and they must consider two main tangents. First, specific factors concerning the code characteristics, the organization of components, and the development environment must be considered. Other concerns deal with the human factors. Predisposition, ego, training and skill must be taken into account for an accurate study of reusability.A current reusability experiment concentrates on the use of an object-oriented organization scheme, reusable code characteristics, and several human factors. The experimental subjects actually design and implement code under varying conditions. Subjects are divided into groups that must reuse whenever possible, may reuse if desired, and cannot reuse at all. Comparing the results of the various groups will lead to a better understanding of the problems faced in software reusability.},
booktitle = {Proceedings of the 1990 ACM Annual Conference on Cooperation},
pages = {405},
location = {Washington, D.C., USA},
series = {CSC '90}
}

@book{10.5555/535835,
author = {Miller, Nancy E.},
title = {File Structures Using Pascal},
year = {1987},
isbn = {0805370838},
publisher = {Benjamin-Cummings Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: Preface: Motivation After teaching file processing courses for years using COBOL as the vehicle language, I concluded that the students do learn to use COBOL for a variety of file organizations (sequential, indexed sequential, and relative) but do not gain an understanding of the data structures involved in implementing the more complex file structures such as direct files and indexed sequential files. A programming language with less support for file organizations than COBOL allows the students to gain greater in-depth knowledge about the implementation of routines that access data structures on external files. Pascal, with its support for relative files, fills this need. Goal This textbook meets the requirements for The Association for Computing Machinery (ACM) course CS5 as defined in the ACM curriculum guidelines. The goal of the book is to study the external data structures necessary for implementing different file organizations. Most texts currently available present file processing by using languages such as COBOL or PL/1, which have built-in support for direct access and indexed sequential access. Pascal does not have this built-in support. Instead, this language can be used in a more practical, prdagogical way by allowing students to gain more in-depth file implementation experience as they analyze data structures for efficiency and write their own access routines. The algorithms in this book are presented in a Pascal-like pseudocode, which provides students with a familiar environment in which to study the key concepts and structures necessary to implement a variety of file organizations.Datastructures such as trees, linked lists, stacks and queues are studied and analyzed for efficient use in the implementation of various file organizations. By using a superior pedagogical language such as Pascal and analyzing key data structures, students will gain a better understanding of design analysis and the implementation of file organization. Level/Audience/Prerequisites The prerequisites for the course addressed by this book, CS5, are two semesters of Pascal; in other words students should already have taken ACM CS1 and CS2 courses. Third-semester Computer Science majors constitute the primary audience for this book. Organization and Coverage Chapter 1 presents a conceptual overview of the file-processing environment, including discussions of common file organizations, file types and characteristics, and different ways of manipulating files as factors that affect file design. Several of the examples of file applications in this chapter are referenced in later chapters. Chapter 2 reviews the syntax for declaring and using records and for declaring and accessing files in Pascal. This chapter may be omitted for those students with a good understanding of Pascal records and files. Chapter 3 deals with the topics of blocking and buffering of records in a file. The central theme of the chapter is the fewer I/O operations required for a program that accesses a blocked file and the reduction of the time that the CPU waits for an I/O operation to be completed when using a buffered file. Interfacing algorithms for record blocking and deblocking are also presented. Quantitative measures of the effects of blocking and buffering effects are given solely in terms of the number of I/O accesses. Chapter 4 describes external storage devices as a background for understanding the impact of storage devices on file design and manipulation. Quantitative measures of the effects of blocking and buffering (similar to those in Chapter 3) are repeated in terms of physical access time of the devices using various blocking factors and different numbers of buffers. Chapter 5 deals with the design and maintenance of sequential files on both sequential and random-access storage devices. Algorithms for maintenance of sequential files stored on sequential devices are contrasted with algorithms for maintenance of sequential files stored on random-access devices. Sample data for a car-rental agency are used for implementating a sequential file. Quantitative measures of access times are given. Chapter 6 describes external sort/merge techniques, which are necessary for sorting very large sequential files. Sorting is a common file-processing task, especially for manipulating sequential files. Sorting methods discussed at length are the two-way merge, the balanced k-way merge, and the polyphase merge. These methods are compared in terms of the number of merge cycles and external storage devices needed for several example data sets. Chapter 7 begins with a discussion of the basic structures of direct files. A variety of techniques are presented for obtaining random access to data files, including the use of hashing. Examples are used to illustrate several methods for handling hashing collisions. Also included are some algorithms for creating and maintaining random-access files in versions of Pascal with the random-access files extension. In order to compare random and sequential access, the car-rental agency data used in Chapter 5 are stored in a random-access file and quantitative measures of access times are computed. Chapter 8 describes several types of tree structures that are useed to accessing random-access files sequentially. The most important tree structure is the B-Tree, and ways of representing and manipulating the B-tree are discussed along with accompanying algorithms. The chapter also discusses the application of trees that allow sequential and random access to the car-rental agency data file created in Chapter 7. Chapter 9 describes common implementations of indexed sequential organization, including implementations that use a tree structure, such as a B+-Tree, for the indexes. The chapter studies Scope Indexed Sequential files used on CDC computers, cylinder-and-surface-indexed sequential files (ISAM) used on IBM computers, and VSAM file organization used on IBM computers. Also included are algorithms for implementing indexed sequential files using a variety of data structures. Applications include the car- rental agency data, and access times for sequential, random, and indexed sequential files are compared. Chapter 10 investigates other types of file organization that use linked lists or tree structures to provide multiple-key access to random-access data files. Included in this chapter is a discussion of inverted files and multilist files along with creation and manipulation algorithms. The car-rental agency data are implemented as an inverted file and multilist files to provide access by several keys. Quantitative measures of access times are given by comparing these file organizations with others discussed previously. Outstanding Features Pedagogy Case Studies: Chapter 5 introduces a case study based on an actual car-rental agency, and this case study is used throughout the book as an on-going example illustrating practical file concepts and issues. Additional practical, real-world case studies are presented in Chapter 2. They include discussions of an inventory of products, student class schedules, and the assignment of course grades for a class. Examples/Illustrations: Throughout the book algorithms are presented in Pascal-like pseudocode. Students learn best by working with files of varying organizations rather than just reading about them. A variety of exercises and programming projects have been provided that illustrate the creation and manipulation of files for each type of organization. Students can implement the algorithms from the book in a hands-on, file organization programming environment, thus gaining experience and greater knowledge of all key concepts. The program solutions for these exercises are available from the author. Solutions to odd- numbered exercises are provided at the back of the book while the solutions to even-numbered exercises are provided in the Instructor's Guide. Glossary/Key Terms: Key terms are highlighted and defined as they occur in the test, and are also included in the glossary in the end of the book. Class Tested: This book was thoroughly class tested for six semesters in a sophomore-level file structures course. The readability of the book was greatly enhanced because of student and reviewer feedback over the course of several drafts. The Use of Pascal Chapter 2 reviews the Pascal syntax for declaring, using, and accessing Pascal records and files. This chapter can be omitted for those students with a good understanding of Pascal. The Pascal syntax for records, linked lists and trees is included in corresponding sections. The first two sections of the book (I and II) reference the ISO standard Pascal, while sections III and IV require random-access files, which are not included in the ISO standard. Most versions of Pascal have been extended to allow random access (OMSI Pascal, TURBO Pascal, UCSD P-System Pascal, and VAX-11 Pascal, to name a few). Appendix A includes a sequential simulation of random access using arrays in internal memory as an easy alternative to those versions of Pascal without random access. The syntax for random access for various versions of Pascal is included in Appendix B. Instructor's Guide The accompanying Instructor's Guide includes: guidelines for presenting the material in each chapter additional examples for classroom use, including points to be emphasized solutions to all even-numbered exercises transparency masters of various illustrations and tables from the book quizzes for each chapter Software: A disk of solutions to all programming problems is available from the author for a nomimal fee to all instructors. Acknowledgements I am grateful to the numerous individuals that have helped me in preparing this book. I am indebted to the faculty of the Computer Science Department of Mississippi State University for providing equipment and an environment conducive to writing a book. My thanks also go to the reviewers: James D. Schoeffler, Cleveland State University; Rayno D. Niemi, Rochester Institute of Technology; James Blahnik, St. Norbert's College; Medhi Owrand, University of Oklahoma; Robert Uzgalis, University of California at Los Angeles; Walter Scacchi, University of Southern California. Special thanks to the students in my classes who corrected typing errors in earlier versions of the book. I express my appreciation to my Editor Alan Apt and all those individuals at Benjamin/Cummings who have organized the reviewing and production of this book. Finally, I thank my husband for his continued support, encouragement, and understanding. Nancy E. Miller}
}

@inproceedings{10.5555/520047.854883,
author = {Ushakov, I. and Coleman, D. and Costa, L. da},
title = {Object Technology},
year = {1999},
isbn = {0769500684},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This panel discusses the impact OT has made and is making on traditional software development methodologies. The scope of the panel is defined by, but not limited to, the following areas: - Object-Oriented (OO) Modeling and Design; - OO Architectures and Frameworks; - Distributed objects and web technology; - Roadmap to OT adoption, - Transition to OT; - OT and Standardization.The goal of this panel session is to study the aforementioned topics, define research areas and state-of-the-art in more detail, and to identify open problems.1. OT and software development. OO Analysis and UML are currently combined with various requirements analysis techniques, e.g., goal-oriented analysis, user centered work modeling, user interaction scenarios and business modeling. While a domain model and use-case model describe a system as a black box, a logical architecture (analysis model) describes what the system's structure [2, 3]. Layered object architecture facilitates better design of software components.2. OO Architectures and Frameworks. Some major impacts of OT on software development practice are definition and usage of OO frameworks and design patterns to facilitate reuse of code and design. Software architecture is a conceptual model defining system components, their interfaces and how they interact between each other. Frameworks provide services and mechanisms for a set of components and can define structural relationships between components. Components vary in granularity and have value within specified environments. OMG Business Object Component Architecture serves as an architectural template for defining domain- and technology-specific architectures and frameworks.3. OT and Web Technology. Internet middleware architecture adopted a concept of "object Web", i.e., universal connectivity, heterogeneous distributed environments, and cross platform interoperation of both infrastructure and applications. Three major drivers: CORBA, DCOM and Enterprise JavaBeans.4. OT adoption. A successful OT adoption is normally based on a plan that includes: resources, scope, management, process, technology and training.5. OT and Standardization. UML is a standard modeling language for OO Analysis and Design. It has been widely recognized and is currently used in many areas. An OO software development process (SDP) is harder to standardize. A Unified Rational Approach which can serve as a framework for definition of OO SDP specifics to a particular project or software development environment. CORBA standards specify many services that can be used in a consistent manner across platforms. More work has to be done to standardize components.},
booktitle = {Proceedings of the 4th IEEE International Symposium and Forum on Software Engineering Standards},
pages = {245},
series = {ISESS '99}
}

@book{10.5555/558603,
author = {Deitel, Harvey M. and Deitel, Paul and Nieto, Tem},
title = {The  Complete XML Programming Training Course},
year = {2001},
isbn = {0130895571},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Publisher: Includes the #1 XML Interactive Training Courseware: XML Programming Multimedia Cyber Classroom! Quickly master XML markup and programming with 10+ hours of detailed audio explanation of 10,000+ lines of sample XML document and program code Learn to program XML applications using Java, VBScript, ASP, and Perl Powerful programs show you how to manipulate XML documents using XSL, XSLT, and XPath Coverage includes schemas, DTDs, DOM, SAX, SOAP protocol, VoiceXML, XML Query, SMIL and more Includes the worlds #1 XML textbook: XML How to Program in print and in fully-searchable electronic format on CD-ROM. Master XML development - hands-on! Youre holding the fastest, most effective way to master real-world XML development! This start-to-finish multimedia training course and book package covers key XML technologies and skills. You wont just learn basic XML markup: youll discover how to build effective DTDs and schemas, write powerful Java applications using DOM and SAX, and work with the powerful technologies that build on XML - including XPath, XSL, XSLT, VoiceXML, and much more. Its everything you need, in one box! XML programming multimedia cyber classroom 10+ hours of detailed audio explanations step you through 10,000+ lines of fully tested program code from 250+ complete XMLdocuments and programs Hundreds of interactive self-review questions and programming exercises to test your knowledge CD-ROM includes fully searchable electronic copy of the book XML How to Program Hundreds of tips to avoid and troubleshoot problems - and maximize performance, interoperability, and reusability Includes Copy of XML How to Program The worlds #1 XML programming developers guide Start-to-finish, 900-page guide to XML Friendly, practical, and full of examples Hands-on tutorials with tips and tricks for troubleshooting and optimization Coverage includes: XML, DTDs, schemas, DOM, SAX, XSLXSLT, XLink, XML Query, SMIL, VoiceXML, and more! Real-world insight into leading XML applications and scripting tools: SOAP, BizTalk, CDF, RDF, Perl, VBScript, Java, ASP, and more Bonus 2nd CD-ROM! Packed with live XML source code, links to hundreds of the Webs best XML resources, and all these great XML development tools: Amaya, Cocoon, Jakarta Tomcat, Xalan, FOP (XSLT), Xerces, Crimson, and more! Raves for Deitel training courses! Im an adult student currently in an OOC++ class (earning my bachelors in Computer Info. Systems). Our instructor is great in C and Java but has never taught OOC++. But, today, Ive been blessed. I found your Complete C++ Training Course. I also plan on learning Java as well, and now, thanks to your Java Course, I can fulfill my dream. - Broxi Thomas, University student I wanted to offer kudos for your product. If every training CD utilized the same format, then learning new products would be easy. The format, resources and layout of the classroom is refreshing and useful. It is clean, quick, and effective. I began with Chapter One and took notes and tried every exercise; reviewed and listened to every example. I understand the material so much better now that I am amazed. PLEASE produce cyber classrooms for EVERY software package and technology that is in existence today. You would be doing a beyond the call of duty service to the computer industry. Of course, you would be a little busy for the next millennium.... :) - Jonathan Gravois SPECIALLY DESIGNED FOR All developers seeking to master XMLWeb professionals building on their existing HTML skillsContent developersEnterprise application integrators System Requirements Windows 9598NT 4.x2000Up To 120MB disk space64MB RAM8x CD-ROM drive &amp; sound card support}
}

@article{10.1155/2021/7106104,
author = {Wu, Shumei and Lv, Zhihan},
title = {Intelligent Communication Management Terminal in the Construction of Human Resource Management Mode},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/7106104},
doi = {10.1155/2021/7106104},
abstract = {With the rapid development of the economy, the integration of corporate strategic management and human resource management has become an issue of concern. This research mainly discusses the role of intelligent communication management terminal in the construction of human resource management mode. In this research, the system development process of this research mainly uses the class library in the software architecture layer to support the software development process. The main development language of Android, JAVA, is to install the Android Develop Tools plug-in on eclipse and install the Android SDK in the computer operating system to build the Android development environment. The development and application of the system not only make the enterprise managers more convenient and efficient in the process of managing the enterprise but also smooth the operation of the enterprise while reducing the human resource investment and also gives the employees more right to know and the right to participate in the enterprise construction. By creating more value while reducing human resource input, enterprises will enable it to obtain more benefits, and thus enter a cycle of good development and contribute to society. The system has the functions of personnel management, recruitment management, attendance management, training management, work management, and salary management. The recruitment management function of the system is mainly composed of recruitment plan management, recruitment information management, and talent pool. In the system’s recruitment plan management function, important information such as the recruitment part, the number of recruits, personnel requirements, and the specific arrival time of the personnel must be clarified. The personnel in charge of the enterprise personnel department shall conduct corresponding regulations according to the specific needs of the enterprise and shall be experienced by the personnel department. The review is carried out, and all parts of the enterprise are coordinated and completed at the same time. In the platform performance test, when the number of concurrent users reaches 50000, the request time is about 6 seconds, which meets the requirement that the response time of 10000 people per second is less than 10 seconds. This research puts forward suggestions and countermeasures for the optimization of human resource management, which can not only improve the efficiency of Y company’s human resource management but also provide useful reference and reference for other enterprises facing the same problem.},
journal = {Wirel. Commun. Mob. Comput.},
month = {jan},
numpages = {14}
}

@inproceedings{10.1145/1805986.1806014,
author = {Vanderheiden, Gregg},
title = {Building national public infrastructures on our way to a global inclusive infrastructure},
year = {2010},
isbn = {9781450300452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1805986.1806014},
doi = {10.1145/1805986.1806014},
abstract = {Broadband technologies are rapidly becoming integral to education, commerce, employment, community participation, health and safety Yet there remain multiple barriers to effective and affordable access by people with disabilities, elder, or those with low literacy creating an increasing digital divide. There are assistive technologies that can provide access for some. However it is not available for all disabilities, not affordable by many, and lags mainstream developments and deployments. Even when the latest AT is close to the latest IT, few people have the latest version. The cost of keeping up with mainstream technologies reduces resources available for innovation in assistive technologies and new directions in broadband technologies will require an already strapped AT industry to retool and re-architect their products. We are moving to an ICT environment with a profusion of hardware models (desktop, laptop, netbook, smartphone, tablet, set top box, game systems, players), multiple operating systems (Windows, Mac, Linux, Chrome OS, iPhone, Android, Windows Mobile, Symbian, Maemo (Nokia), Bada (Samsung), WebOS, etc.), hundreds of software applications that embed another universe of widgets, plug-ins, and players, and a networked information environment that adheres to no standard and mutates far beyond the initial conception of the Web. Our current access technologies and infrastructure cannot handle this; the assistive technologies that now exist do not address all disabilities well, particularly cognitive, language, and learning disabilities, deaf-blindness and the mixed problems faced by elders; current assistive technologies often add, rather than reduce, complexity; finally, but importantly, people are not aware of what is possible, see it as complicated, and do not have any easy way to determine that there is something that can help themA coalition of academic, industry and non-governmental organizations and individuals are coming together to promote the creation of a National Public Inclusive Infrastructure (NPII) to address these problems. The purpose is to ensure that everyone who faces accessibility barriers due to disability, literacy or aging, regardless of economic status, can access and use the Internet and all its information, communities, and services for education, employment, daily living, civic participation, health and safety.An NPII would provide key software enhancements to the physical infrastructure to allow lower cost accessibility that could be invoked on any computer, anywhere. Its key components would be a cloud based delivery system that would allow anywhere, any computer access, a personal preference system to allow systems to automatically configure themselves to users, a system of wizards to make creation of a preference profile simple even when a professional is not available, a metadata server to allow users to find accessible media or captions or descriptions for inaccessible media, a trusted source for malware free solutions, a rich development environment with common building blocks, and an awareness program to make more people aware of what is possible for them. All of the NPII components are being designed to support both commercial assistive technologies and free, built-in access features (universal design). The NPII will include a delivery system, personalization profiles and a rich development system and common modules. In addition to lowering development costs and increasing the number of solutions for different disabilities, the NPII can also enable new types of assistive technologies and services, including assistance-on-demand services that allow consumers to invoke computer or human assistance whenever and wherever they need it. The goal is a richer set of access options that it is less expensive to create and distribute and that can address the needs of a wider range of disabilities than is possible today. And a model infrastructure that can be replicated internationally and bring this wide variety of access options and the lower cost delivery system for both commercial and free access features to countries world-wide.},
booktitle = {Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A)},
articleno = {19},
numpages = {1},
location = {Raleigh, North Carolina},
series = {W4A '10}
}

@article{10.5555/2038836.2038854,
author = {Bloch, Stephen},
title = {Program By Design: graphics-first programming without drowning in syntax},
year = {2011},
issue_date = {December 2011},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {27},
number = {2},
issn = {1937-4771},
abstract = {The "Program By Design" (ne\'{e} "TeachScheme!") curriculum has been used to great success in middle schools, high schools, community colleges, liberal-arts colleges, and research universities, and won Matthias Felleisen the two highest awards in computer science education, the ACM Karlstrom award and the SIGCSE Distinguished Educator award. The curriculum starts with a beginner-friendly IDE (DrRacket) and a simple language (a tiny subset of Scheme), emphasizing an explicit, concrete problem-solving strategy and motivating central CS and math concepts (variables, functions, composition, data types, classes, polymorphism, etc.) with graphics and animation. Using this curriculum, I routinely teach non-CS-majors to write separable-model, event-driven GUI programs involving higher-order functions and recursive traversal of linked data structures, all within their first semester. For CS majors, we then show (in a second semester) how the same concepts and strategies apply in a more difficult language like Java.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {125–126},
numpages = {2}
}

@article{10.1145/3555810,
author = {Kalantar, Amin and Zimmerman, Zachary and Brisk, Philip},
title = {FPGA-based Acceleration of Time Series Similarity Prediction: From Cloud to Edge},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3555810},
doi = {10.1145/3555810},
abstract = {With the proliferation of low-cost sensors and the Internet of Things, the rate of producing data far exceeds the compute and storage capabilities of today’s infrastructure. Much of this data takes the form of time series, and in response, there has been increasing interest in the creation of time series archives in the past decade, along with the development and deployment of novel analysis methods to process the data. The general strategy has been to apply a plurality of similarity search mechanisms to various subsets and subsequences of time series data to identify repeated patterns and anomalies; however, the computational demands of these approaches renders them incompatible with today’s power-constrained embedded CPUs.To address this challenge, we present FA-LAMP, an FPGA-accelerated implementation of the Learned Approximate Matrix Profile (LAMP) algorithm, which predicts the correlation between streaming data sampled in real-time and a representative time series dataset used for training. FA-LAMP lends itself as a real-time solution for time series analysis problems such as classification. We present the implementation of FA-LAMP on both edge- and cloud-based prototypes. On the edge devices, FA-LAMP integrates accelerated computation as close as possible to IoT sensors, thereby eliminating the need to transmit and store data in the cloud for posterior analysis. On the cloud-based accelerators, FA-LAMP can execute multiple LAMP models on the same board, allowing simultaneous processing of incoming data from multiple data sources across a network.LAMP employs a Convolutional Neural Network (CNN) for prediction. This work investigates the challenges and limitations of deploying CNNs on FPGAs using the Xilinx Deep Learning Processor Unit (DPU) and the Vitis AI development environment. We expose several technical limitations of the DPU, while providing a mechanism to overcome them by attaching custom IP block accelerators to the architecture. We evaluate FA-LAMP using a low-cost Xilinx Ultra96-V2 FPGA as well as a cloud-based Xilinx Alveo U280 accelerator card and measure their performance against a prototypical LAMP deployment running on a Raspberry Pi 3, an Edge TPU, a GPU, a desktop CPU, and a server-class CPU. In the edge scenario, the Ultra96-V2 FPGA improved performance and energy consumption compared to the Raspberry Pi; in the cloud scenario, the server CPU and GPU outperformed the Alveo U280 accelerator card, while the desktop CPU achieved comparable performance; however, the Alveo card offered an order of magnitude lower energy consumption compared to the other four platforms. Our implementation is publicly available at https://github.com/aminiok1/lamp-alveo.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {dec},
articleno = {12},
numpages = {27},
keywords = {Field-programmable gate array (FPGA), time series, Matrix Profile}
}

@inproceedings{10.1145/3568812.3603473,
author = {Chen, Melissa and O'Rourke, Eleanor},
title = {Designing a Real-Time Intervention to Address Negative Self-Assessments While Programming},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603473},
doi = {10.1145/3568812.3603473},
abstract = {Enrollments in university-level introductory computing courses are skyrocketing [3], but many students struggle in these courses [2]. Recent research suggests that student perceptions of the programming process may contribute to this problem. Students often have inaccurate expectations of programming that may lead them to negatively assess their abilities in response to natural programming moments [6]. For example, many students believe they are doing poorly when they use resources to look up syntax, even though this is considered good practice [7]. This is important because negative self-assessments correlate with lower self-efficacy [6], or one’s belief that they can achieve a goal [1], and students with lower self-efficacy tend to exhibit lower persistence in undergraduate computing programs [9]. In this poster, we present an initial design and evaluation of an intervention that aims to reduce overly negative self-assessments and improve self-efficacy by providing real-time feedback as students program. We created an extension to the jGRASP development environment [4] that delivers feedback messages in response to eight self-assessment moments that can be automatically detected by an expert system developed in prior work [5] (see Figure 1). Informed by recommendations from the feedback literature [8, 10], we developed six messages for each moment that aim to help students develop more accurate expectations by normalizing the moment or highlighting how it could support future growth (see Figure 2). By delivering this feedback automatically, in real-time, and in the context of the task, this intervention aims to address negative self-assessments as they occur. This approach has been successful in other domains [8, 10] and allows us to provide individual feedback at scale, which is particularly challenging as course enrollments grow [3, 11].  We conducted a formative user study with 10 CS1 and 11 CS2 students to understand how they perceived the intervention and which feedback messages they preferred, with the goal of informing future design iterations. First, participants completed a modified version of the survey from [6] to measure their self-efficacy and self-assessments; this served as a pretest. Then, they worked on a programming problem with the intervention for twenty minutes. Finally, participants completed the same survey as a posttest and we interviewed students about their reactions to the intervention as they watched a video of their session. The pretest results showed that many participants do not negatively self-assess in response to these eight programming moments, which is surprising since previous research with other populations has found that negative self-assessments are common [6]. Our preliminary analysis indicates that some participants found the messages reassuring and timely while others found them unhelpful. Participants expressed preferences for some message designs over others, and overall the feedback resonated most with participants who had more negative self-assessments or struggled more on the programming problem. Based on this feedback, we are refining the intervention and collecting data with other populations ahead of a summative evaluation to measure the intervention’s impact on self-assessments and self-efficacy.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {9–10},
numpages = {2},
keywords = {CS1, self-assessments, self-efficacy},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1109/FIE.2018.8658513,
author = {Plaza, Pedro and Sancristobal, Elio and Carro, German and Castro, Manuel and Blazquez, Manuel and Garc\'{\i}a-Loro, F\'{e}lix},
title = {Multiplatform Educational Robotics Course to Introduce Children in Robotics},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE.2018.8658513},
doi = {10.1109/FIE.2018.8658513},
abstract = {Robotics and computational thought are ideal tools for developing science, technology, engineering and mathematics (STEM) pedagogy. Throughout this paper a modular and adaptive course is presented, the main objective of which is to make known simple and economic tools of educational robotics. This course is aimed at those who want to discover the possibilities of educational robotics in the context of the introduction to robotics. Today, robotics training tools are raised with the aim of promoting innovation and motivation of students during the learning process. Robots are becoming more and more common in our daily lives; therefore, it is important to integrate robots into all levels of our society. The course is designed to work with the Scratch, Crumble and Arduino tools as STEM enhancers. Using Scratch, interactive stories, games and animations can be programmed. Scratch helps young people to acquire and improve skills such as think creatively, think systematically, and work collaboratively. Scratch is a project of MIT Media Lab's Lifelong Kindergarten Group. It is offered free of charge. On the other hand, Crumble is an easy-to-use programmable controller. Its programming interface uses a block programming language based on Scratch that makes it easy for children from 10 years old to use it. In addition, the hardware elements associated with Crumble are very intuitive and easy to connect. Last, but not least, Arduino is an open source electronic platform based on hardware and software that is easy to use. It is a platform that incorporates a simple microcontroller and an interface development environment to create the applications to be downloaded on the board. The course offers a three-tiered journey through three levels with each of the three tools. It consists of a total of 9 modules. This course has a very practical approach. A project-based pedagogical methodology is used. Experiments are promoted, where trial and error are part of learning and self-discovery. The student learns to have more autonomy and responsibility. Knowledge is acquired in different disciplines. It develops: motor skills (scale mobility in the hands), group skills, allowing people to socialize, creative abilities, and learning in a fun way. The operational details, materials used and examples of activities for some modules are also presented with the expectation that all teachers will be able to adapt these activities in their class. In addition, results are included from several groups of students who have already completed some modules. Despite not having a large number of students, the experience provided results that may be useful for other teachers to promote a course with similar or equal content for more results. The results of this work show that it is important to combine theory and practice to include fun tasks intertwined with the challenges of applying theory to problem solving.},
booktitle = {2018 IEEE Frontiers in Education Conference (FIE)},
pages = {1–9},
numpages = {9},
location = {San Jose, CA, USA}
}

@inproceedings{10.1145/800214.806560,
author = {Cheriton, David R. and Malcolm, Michael A. and Melen, Lawrence S. and Sager, Gary R.},
title = {Thoth, a portable real-time operating system (Extended Abstract)},
year = {1977},
isbn = {9781450378673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800214.806560},
doi = {10.1145/800214.806560},
abstract = {Thoth is a portable real-time operating system which has been developed at the University of Waterloo. Various configurations of Thoth have been running since May 1976; it is currently running on two minicomputers with quite different architectures (Texas Instruments 990 and Data General NOVA).This research is motivated by the difficulties encountered when moving application programs from one system to another; these difficulties arise when interfacing with the hardware and support software of the target machine. The problems encountered interfacing with the new software support are usually more difficult than those of interfacing with new hardware because of the wide variety of abstract machines presented by the compilers, assemblers, loaders, file systems and operating systems of the various target machines. We have taken the approach of developing portable system software and porting it to “bare” hardware. Because the same system software is used on different hardware, the same abstract machine is available to application programs. Thus most application programs which use Thoth can be portable, if not machine independent.Most previous work on software portability has focused on problems of porting programs over different operating systems as well as hardware. To our knowledge, this is the first time an entire system has been ported. Our experience indicates that this approach is practical both in the cost of porting the system and its time and space performance.The design of Thoth strives for more than portability. A second design goal is to provide a system in which programs can be structured using many small concurrent processes. Thus we have aimed for efficient interprocess communication to make this structuring technique attractive. We have also provided safe dynamic process creation and destruction.A third design goal is that the system meet the demands of real-time applications. To meet this goal, the system guarantees that the worst-case time for response to certain external events (interrupt requests) is bounded by a small constant.A fourth design goal is that the system be adaptable to a wide range of real-time applications. A range of system configurations is possible: A stand-alone application program can use a stripped version of the Thoth kernel which supports dynamic memory allocation and interprocess communication. Such a configuration requires less than 2000 16-bit words of memory. Larger configurations can support process destruction, a device-independent input-output system, a tree-structured file system, and multiple teams of processes. (A team is a set of processes which share the same logical address space and therefore can share data.)Thoth is implemented in a high-level language called Eh (a descendant of BCPL) and a small amount of assembly language. The major job in porting the system seems to be in redesigning the code generation parts of the compiler.Since it appears impractical to design system software to be portable over all computers (even over all existing machines), we have aimed at making Thoth portable over a subset of machines. Machines in the set can be characterized by a set of properties such as: a word must be at least 16 bits in length, a pointer to a word must fit into a word, etc. Roughly, this set of machines includes most modern minicomputers. It is important that many machines which do not yet exist will be included in it.A number of application programs have been written using Thoth. In addition to software development tools, communications and real-time control programs have been written. All of these programs require few if any changes when ported to new hardware. Some of these programs have been developed by inexperienced programmers who were not planning on porting their program. Hence, it seems to take less skill to write portable software in this system than using conventional techniques. However, existing software written for other systems is incompatible with Thoth and usually difficult to port to the Thoth system.Although, at the time of this writing, we have limited experience with porting the system to new hardware, we feel that Thoth has been highly successful in terms of our original objectives. Among other things, it has partially demonstrated the feasibility of building a portable operating system for a specified class of machines.},
booktitle = {Proceedings of the Sixth ACM Symposium on Operating Systems Principles},
pages = {171},
location = {West Lafayette, Indiana, USA},
series = {SOSP '77}
}

@book{10.5555/552037,
author = {Kain, Eugene and Wingo, Scot},
title = {The  MFC Answer Book: Solutions for Effective Visual C++ Applications},
year = {1998},
isbn = {0201185377},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: PREFACE: Why Another Book on Microsoft Foundation Classes Programming To answer this question, let us look at a typical MFC programming scenario. First, you attend an MFC training session or read some introductory books on MFC programming. You quickly become able to write and customize small tutorial applications. AppWizard and ClassWizard allow you reach an unprecedented level of productivity. Your applications support the multiple document interface (MDI) and have a professional-looking user interface with a floating toolbar, a status bar, printing and print preview, and so on. You then go back to work and start using MFC to produce great-looking applications. Code flows freely from your keyboard, the wizards work hard at your side, and life looks great under the MFC sun. One day, you start wondering about how to implement new features that were not explicitly covered in the training session. For example: Make your application remember the last active document and automatically reopen it. Support multiple kinds of views on the same document and allow the user to explicitly open any kind of view. Add ToolTips to the controls in a form view. Dynamically switch the view displayed in a window to replace it with another kind of view. Implement an expanding dialog box. Embed a property sheet (tabbed dialog box) inside another window, such as a form view, a dialog box, or a mini frame window. Display a progress indicator in a status bar pane. Have a menu pop up when the user clicks a button on a toolbar or in a dialog box. Support headers and footers in your print and print preview. Displaya custom Printing . . . dialog box with a progress indicator. You feel that implementing these features cannot be that difficult: after all, you have already seen them in other Windows applications. But where do you start looking for an answer The solution may be as easy as knowing the specific MFC virtual functions that you must override to produce the desired effect or knowing the Windows messages you should trap and handle appropriately. For some features, however, more involved techniques may be neededeven to the point of tracing into MFC's source code to understand just where and how you can act to modify your application's default behavior. One infuriating fact of life is that the answer to your particular question may be lying around somewhere: buried in some MFC programming book or magazine article, on the Microsoft Developer's Network CD-ROM, in the Microsoft Knowledge Base, in the various threads and mailing lists maintained on the Internet, or even in the online books or samples contained on the Visual C++ CD-ROM. The problem is this: How are you going to locate the most relevant and reliable source of information among all these resources How are you going to find the solution you need right now Introducing The MFC Answer Book This book is intended to provide ready-to-use techniques that answer the most common real-world questions that typically confront MFC developers. The structure of this book is specifically designed to help you quickly locate the answers youire looking for and integrate the relevant solutions into your own programs. The FAQ format of this book makes it ideally suited to the needs of the developer looking for a quick answer to a pressing question. At the same time, you will find that many techniques will give you a better understanding of the inner workings of MFC applications and more generally help you improve your MFC programming skills. In particular, the Explanations and Additional Comments sections often delve into the MFC source code or undocumented functions to explain how the techniques discussed work and how they differ from or integrate with MFC's default behavior. Key Features of This Book Although most books about Visual C++ and MFC programming answer valid questions about MFC programming and provide useful tips if you read them from cover to cover, most of them are not structured in a way that allows you to quickly find an answer to a given problem. Moreover, even if you find the answer, it is likely to be buried inside a larger discussion and not readily available as a step-by-step technique that you can simply incorporate into your current project to add a required feature. In contrast, The MFC Answer Book is specifically designed to help MFC developers solve their programming problems in the most efficient way: This book is organized so that the table of contents will help you to quickly zoom in on the FAQs that answer your questions. I have made every effort to build a convenient and comprehensive index that will direct you to all the pages relating to any keyword or function referenced in this book. Each FAQ is written in a concise way that first gives you the step-by-step answer you need. Explanations and additional comments are deferred to later sections so that they do not get in the way of the solution but are readily available for those who want to go further than the cookbook recipe and wish to understand what goes on under the hood. Each explanation comes with tested and reusable sample code that you can plug into your MFC application in a few minutes to integrate the required functionality immediately. To summarize: The goal of this book is to offer you the shortest way from a problem to the corresponding step-by-step solution that you can integrate immediately into your current project. Who Should Read This Book This book is written for all MFC developers who wish to solve their MFC-related problems and at the same time learn advanced MFC techniques that will allow them to add a range of sophisticated features to their applications. This book assumes a basic proficiency both in the C++ language and in MFC programming as well as a knowledge of how to use the Visual C++ integrated development environment and tools such as AppWizard and ClassWizard. The Visual C++ wizards are discussed only when used in nonstandard ways to achieve a specific result. To benefit fully from this book, you should already understand the basic MFC concepts presented in the Scribble tutorial described in the Visual C++ documentation: the document/view architecture, message maps, the UPDATE_COMMAND_UI mechanism, dialog data exchange (DDX), and so on. Typically, you will either have followed the Scribble tutorial, attended a training session in MFC programming, or read one of the many introductory books on this topic. Of course, having a more extensive background in MFC programming will not hurt! Quite to the contrary. Based on feedback from reviewers and colleagues, I know that this book will also appeal to experienced MFC developers, who will find many useful techniques to add to their bag of MFC programming tricks. Finally, reading this book will allow all MFC developers to improve their understanding of fundamental MFC concepts and sharpen their MFC programming skills. How To Use This Book This book focuses on the 32-bit MFC version 4.x for Windows 95 and Windows NT. However, most techniques and concepts discussed here also apply to older versions of MFC. They should also remain valid for future MFC versions, because they rely on core MFC classes and behaviors that are not likely to evolve in a way that breaks existing code. I tried to write this book so that it will become a flexible tool that you can use as you want to. This means that you can either read this book from cover to coverI would certainly appreciate it if you door use it as a reference to look up only the specific topics that interest you. Most FAQs are cross-referenced to help you locate all the relevant information you might need even if you jump into the middle of the book. However, before you start hunting for answers to your MFC questions, I suggest that you take a few minutes to read Chapter 0 (Terminology and Conventions) and Chapter 1 (Document/View Architecture Backgrounder) to make sure that we start on the same ground with respect to fundamental document/view architecture concepts. What Is on the CD-ROM The companion CD-ROM contains source code and executables for all of the book's sample programs. The folder hierarchy is organized first by chapter number and then by project name. Thus, the AutoSaveDoc project for Chapter 2 is located in the d:Chap02AutoSaveDoc folder, where "d:" is your CD-ROM drive's letter. All the executables are located under their respective chapter folders. For example, all the executable sample programs for Chapter 2 are located in the d:Chap02 folder. The EkUtil.h and EkUtil.cpp files located at the root of the hierarchy contain the various helper Ek . . . . . . functions and classes that are presented throughout the book. You can choose to copy the whole folder hierarchy from the CD-ROM to your hard disk, copy only the examples that are of interest to you, or access the files directly from the CD-ROM. If you copy files from the CD-ROM to your hard disk, remember to remove the read-only attribute from the files on your hard disk. All sample programs have been compiled and tested under both Visual C++ 5.0 and Visual C++ 6.0. They will also work properly with Visual C++ 4.x, but you will have to manually create the appropriate .mdp project file. Note, however, that the .dsp project files on the CD-ROM have the Visual C++ 5.0 format: if you open them with Visual C++ 6.0, simply answer Yes to the dialog box asking whether you want to convert these files to the new format. Your Feedback Is Welcome I have done my best to accurately present topics that I feel should be of interest to most MFC developers. However, if you think that a topic should be covered differently or should use another technique, don't hesitate to send me e-mail at ekain@awl.com. Also, e-mail me if you want to submit a topic idea or a technique of your own that solves a problem you have encountered, if you find an error or have any problem with this book, or if you have suggestions or want to discuss anything with me. I can promise that I will read all e-mail messages, take them into account, and try to respond to each of them as soon as possible. Note, however, that I may not have the time to answer specific MFC programming questions. You can also visit my Web site at ...}
}

@inproceedings{10.1145/3626252.3630816,
author = {Vahid, Frank},
title = {CS1 Instructors: Flexibility in Content Approaches is Justified, and Can Enable More Cross-University Cooperation},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630816},
doi = {10.1145/3626252.3630816},
abstract = {Many CS1 teachers focus on specific content approaches in CS1. Some want objects early, some functions early, some decisions/loops first. Some put emphasis on language details, some on language-neutral problem solving. Some demand real-world IDEs, code versioning tools, industry-quality comments, specifications, documentation, or test coverage. While the focus shows teachers care and may indeed provide benefits, those specific focuses can also prevent increased cooperation among universities in defining a more "common" CS1 curricula. With a more common curriculum, better content and tool support is enabled due to economies of scale. Such cooperation could yield a more powerful approach to teaching CS1, elevating the role of CS1 instructors. CS1 instructors, by being more flexible in their content approaches, may help show the college education community the great benefits of increased cooperation among universities, especially in the design and delivery of introductory gateway courses taken by large numbers of students. We describe results of discussions with over 100 instructors at over 50 universities during the past decade, highlighting frequently-stated content approaches that have little or no evidence supporting the approach and that may hamper cooperation, and we end by encouraging flexibility in content approaches to enable the community and publishers to provide better CS1 support.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1368–1373},
numpages = {6},
keywords = {common courses, cooperation, cs1, programming, teaching},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@phdthesis{10.5555/2522400,
author = {Luo, Juan},
advisor = {Brodsky, Alexander},
title = {Regression learning in decision guidance systems: models, languages, and algorithms},
year = {2012},
isbn = {9781267316615},
publisher = {George Mason University},
address = {USA},
abstract = {The goal of state-of-the-art research in decision guidance applications is to build complex systems with predicting capability. Systems can make decisions intelligently rewarded by more desirable outcomes. Predictions are actually made based on a dynamically collected amount of information. Some models with unknown parameters in the application can be learned from the collected information and domain knowledge as well [1]. A development tool called Decision Guidance Management System (DGMS) is proposed to develop decision guidance applications. Four phases are involved in the decision guidance applications and will be repeated as needed. They are data collection, model learning, prediction, and optimization phases for decision-making [1]. This dissertation focuses on a framework, models, languages, and algorithms to integrate the machine learning functionality (regression learning) into DGMS applications as their first class citizen. A framework CoReJava (Constraint Optimization Regression in Java), which extends the Java programming language with regression learning or the ability of parameter estimation for a function, is proposed and developed. CoReJava is unique in that functional forms for regression analysis are expressed as first class citizens, that is, as Java programs, in which some parameters are not given in advance, but will be learned from learning data sets provided as input. To implement regression learning, the CoReJava compiler (a) analyzes the structure of the parameterized Java program that represent a functional form; (b) automatically generates a constraint optimization problem, in which constraint variables are the unknown parameters, and the objective function to be minimized is the sum of squares of errors with regarding to the learning set; and (c) solves the optimization problem using an external nonlinear optimization solver. The parameterized Java programs are executed as a regular Java program, with the initially unknown parameters substituted by the found optimal values. CoReJava syntax and semantics are formally defined and exemplified using a simple supply chain example. The if-then-else decision structures of the Java language are naturally adopted to represent piecewise functional forms of regression. Thus, minimization of the sum of squared errors involves an optimization problem with a search space that is exponential to the size of the learning set. A combinatorial restructuring algorithm is proposed to guarantee learning optimality and reduce the search space to be polynomial in the size of the learning set, but exponential to the number of piecewise bounds. A Heaviside restructuring algorithm, which expresses the piecewise linear regression function using a unified functional format [2], instead of multiple pieces, is proposed to decrease the searching complexity further to be polynomial in both the size of the learning set and the number of piecewise bounds, while the learning outcome will be an approximation of the optimality. An Expectation Maximization-based (EM-based) Multi-step Piecewise Surface Regression Algorithm (EMMPSR) is proposed to solve piecewise surface regression problem. The multiple steps involved are local regression on each data point of the training data set and a small set of its closest neighbors, clustering on the feature vector space formed from the local regression, regression learning for each individual surface, and classification to determine the boundaries for each individual surface [3]. An EM-based iteration process is introduced in the regression learning phase to improve the learning outcome [4]. The reassignment of a cluster identifier for every data point in the training set is determined by predictive performance of each submodel [5]. Clustering quality validity indices are applied to the scenario in which the number of piecewise surfaces is not given in advance. The Relational Database Management System (RDBMS) is extended with the piecewise regression learning capability as well. The functional forms are represented as database tables. The EMMPSR algorithm is implemented as stored procedures. A case study is undertaken to describe the decision optimization process based on the learning outcome of the EMMPSR algorithm. Evaluation of the resulting research is established by experiments and empirical analysis in comparison with those of related regression learning packages.},
note = {AAI3506265}
}

@phdthesis{10.5555/925703,
author = {Avila, Roberto Antonio and Moore, James A.},
title = {Methodology and design of a decision support system to predict tree growth response from forest fertilization},
year = {1997},
isbn = {059164634X},
publisher = {University of Idaho},
address = {USA},
abstract = {Decision support system (DSS) technologies are becoming very important supporting tools for helping people in the decision-making process. DSS have been used in forestry and are evolving very rapidly as foresters are demanding more system functionality to improve forestry management operations along with development of dynamic and user-friendly software to cope with the increased demand for information. The Intermountain Forest Tree Nutrition Cooperative (IFTNC) at the University of Idaho initiated a DSS project including, as its primary functional part, a real-time expert system prototype for Central Washington that focuses on tree nutrition management. The project's main objective was to design and develop a system methodology that involves a microcomputer program to predict Douglas-fir growth response based on fertilization treatments of 200 or 400 lb of nitrogen per acre during a six-year period. The system methodology began with the definition of the problem and ended with a preliminary design and operation of an integrated microcomputer application prototype.Nine experts on forest fertilization management issues were interviewed with the purpose to acquire heuristic knowledge they use to conduct fertilization operations and also to obtain their input on how a fertilization-supporting tool could strengthen the decision-making process. In order to reinforce the qualitative information collected from the interviews, quantitative data from different sources were also gathered i.e., data mining. For instance, the IFTNC database was a source for individual tree measurements and site characteristics. Geographic Information Systems (GIS) database related to soil parent materials and potassium level were generalized to include these parameters as input attributes in the data set. Global Positional System (GPS) was used to locate stands and input site-specific conditions.Interpretation and analysis included the interpretation of qualitative and quantitative information to look for system component functioning and then an analysis of how different components would operate under an integrated environment. To facilitate system understanding, a theoretical fertilization control system input was designed. This framework made it easy to design and operate the logic for various system module components. Also, preliminary definitions of the modeling and system architecture were studied.Modeling consisted of searching for an appropriate mathematical technique for system prediction. The system mathematical module uses a neural network approach where input/output data pairs on individual tree measurements and physical site characteristics are trained to predict Douglas-fir growth response based on 200 or 400 lb of nitrogen per acre during a six-year span. This component proved to be a quick and robust prediction technique for the prototype under development.Finally system development primarily dealt with the design and operation of the prototype itself. This system uses distinct software packages within an integrated personal computer environment using Microsoft Visual Basic as the integration development language. Besides the mathematical module, the prototype includes a visualization module implemented with MapObjects that produces maps of geographic stand location and also serves for data interpretation and analysis.The research project involved designing process that involves a feasible and rapid way of using different tools to deal with forest fertilization management operations. It uses different software components within an integrated computer development environment, resulting in a state-of-the-art fertilization prediction tool. The system supports management decisions, and helps people involved in forest fertilization design and administer fertilization prescriptions in the Intermountain Northwest.},
note = {AAI9813712}
}

@inproceedings{10.1145/3517428.3550408,
author = {Mack, Kelly},
title = {Accessible Communication and Materials in Higher Education},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3550408},
doi = {10.1145/3517428.3550408},
abstract = {Students with disabilities face numerous access barriers in higher education institutions. For example, many students struggle to receive the accommodations that they legally have a right to, and many course materials and tools are inaccessible (e.g., textbooks, required software, slide decks). Consequently, students with disabilities drop out of college at a higher rate than nondisabled students. In this dissertation, I aim to improve two core areas of inaccessibility for students with disabilities. First, I will learn about the common issues that arise when three stakeholders (disabled students, professors, and people working in disability service offices) work to fulfill technology-focused accommodations (e.g., slides, IDEs, lecture videos) for a student. Through this two part survey and interview/co-design study, I will develop design recommendations around how technology can better support this process. Second, I will apply techniques like optimization and natural language processing to build tools to identify and automatically repair common accessibility issues in a ubiquitous tool for teaching across departments: slide show presentations. By conducting this work, I will contribute software tools and design recommendations that will support disabled students in obtaining an accessible education.},
booktitle = {Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {91},
numpages = {6},
keywords = {accessibility, accommodations, disability, higher education, slideshows},
location = {<conf-loc>, <city>Athens</city>, <country>Greece</country>, </conf-loc>},
series = {ASSETS '22}
}

@proceedings{10.1145/1366230,
title = {CF '08: Proceedings of the 5th conference on Computing frontiers},
year = {2008},
isbn = {9781605580777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to introduce the technical program of the 2008 ACM Conference on Computing Frontiers, the fifth in the series. The conference was established as a forum to present and discuss explorations of territory at the edge of computing, perhaps risky, but intellectually challenging and with the potential to substantially improve the state of the art. In this context, papers have been solicited and submitted on theory, methods, technologies, and implementations concerned with innovations in computing paradigms, computational models, architectural paradigms, computer architectures, development environments, compilers, and operating environments.This year's program includes 30 technical papers selected out of 110 submissions (a 27% acceptance rate). There were more submissions of high quality and relevance than could be accommodated in a three-day technical program. To capture a wider spectrum of contributions, a number of authors were invited to present their work in a poster session. This session will enrich the program and will provide further occasions of lively debate on exciting and innovative ideas. Overall, the conference will cover a broad range of topics. On the architectural side, there are papers on chip multiprocessors, graphic processing units, and reconfigurable architectures. A number of contributions explore how algorithms in a variety of application domains can take advantage of these novel platforms. A few presentation will consider the challenge of compiling for innovative architectures. Other papers focus on interesting programming paradigms and their hardware support.The technical program is further enriched by three invited keynote talks. Keshav Pingali will explore the vast, important, but largely uncharted territory of irregular parallel programs, where considerable progress is needed before the performance potential of current and future hardware can be fully harnessed. Valentina Salapura will assess the state of the art of high-end supercomputing and its scalability to ever higher performance, based on the experience of the novel Blue Gene/P system. Raffaele Tripiccione will report on the innovative JANUS system, illustrating how, for certain classes of scientific problems, reconfigurable architectures can deliver very substantial performance improvements over traditional microprocessors.A scientific event can only be as good as its scientific content. Thus, for the excellent technical program of the conference, we want to express our appreciation to the large number of authors who submitted their work to Computing Frontiers 2008 as well as to the speakers who have accepted to deliver keynote talks. Selecting a subset of the submissions under the scientific as well as the logistic constraints of the conference is the hard, delicate job of the Program Committee. We are truly thankful for the terrific work done by the Computing Frontiers 2008 program committee members as well as by the referees enlisted by them in the process. This effort has resulted in nearly 400 reviews, which have been crucial to the selection process and, equally importantly, have provided a very substantial amount of feedback to the authors, which we hope they have found useful and constructive.It goes without saying that the General Chair has a significant impact on the overall success of a conference. However, we do want to take this opportunity to especially thank our General Chair, Alex Ramirez, for the extremely generous help he has given us on all matters, large and small, from advice on how to strengthen the tradition of this young conference to the effective supervision of the submission site, which ensured a smooth submission and review process.We are proud to have a Stamatis Vassiliadis Best Paper Award, established with the support of the Technical University of Delft. The award will recognize the best contribution to the conference and at the same time honor the memory of Stamatis, a founder of Computing Frontiers and a dear colleague, tragically taken from us by a sudden illness last year.Two workshops, respectively on "memory access on future processors" and on "radiation effects and fault tolerance in nanometer technologies" will be co-located with Computing Frontiers 2008. A special one-day session will present projects funded under the first Computing System call in the 7th Framework Program of the European Union. These three events will provide further opportunities for exchanges and debates on relevant topics and enrich the technical experience in Ischia.},
location = {Ischia, Italy}
}

@inproceedings{10.1145/3341525.3387369,
author = {Passerini, Nicol\'{a}s and Lombardi, Carlos},
title = {Postponing the Concept of Class When Introducing OOP},
year = {2020},
isbn = {9781450368742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341525.3387369},
doi = {10.1145/3341525.3387369},
abstract = {The literature on programming education describes different problems found in courses that introduce the basic concepts of Object-Oriented Programming (OOP). Some of these problems arise from the large amounts of abstract concepts that are needed even for the simplest programs. Other difficulties are related with the concepts of class and instantiation, and the duality between classes and objects. Educators and researchers have proposed several alternatives to define a gradual path for the introduction of OOP.A group of educators from several universities in the Buenos Aires area crafted a learning path for a first course about OOP in which the concepts of class and instantiation are introduced several weeks after the beginning of the course. Gradualism is achieved in this proposal by starting with a minimal metamodel based on self-defined objects, which is progressively enlarged. Following this learning path, by the time students are introduced to classes and instantiation, they already have a good acquaintance with object definition and interaction, and are also able to quickly understand the convenience of the new concepts.The same group conceived and developed a didactically-oriented programming language along with an IDE; and produced several exercises that can be solved using the initial metamodels.In this article, we discuss which concepts and language elements can be introduced before classes and instantiation, the need for a programming language that supports the proposed learning path, and the results of its application in several universities.},
booktitle = {Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {152–158},
numpages = {7},
keywords = {CS0, CS1, CS2, educational tools, instructional approaches},
location = {Trondheim, Norway},
series = {ITiCSE '20}
}

@book{10.5555/1869711,
author = {Aiello, Robert and Sachs, Leslie},
title = {Configuration Management Best Practices: Practical Methods that Work in the Real World},
year = {2010},
isbn = {0321685865},
publisher = {Addison-Wesley Professional},
edition = {1st},
abstract = {Successfully Implement High-Value Configuration Management Processes in Any Development Environment As IT systems have grown increasingly complex and mission-critical, effective configuration management (CM) has become critical to an organizations success. Using CM best practices, IT professionals can systematically manage change, avoiding unexpected problems introduced by changes to hardware, software, or networks. Now, todays best CM practices have been gathered in one indispensable resource showing you how to implement them throughout any agile or traditional development organization. Configuration Management Best Practices is practical, easy to understand and apply, and fully reflects the day-to-day realities faced by practitioners. Bob Aiello and Leslie Sachs thoroughly address all six pillars of CM: source code management, build engineering, environment configuration, change control, release engineering, and deployment. They demonstrate how to implement CM in ways that support software and systems development, meet compliance rules such as SOX and SAS-70, anticipate emerging standards such as IEEE/ISO 12207, and integrate with modern frameworks such as ITIL, COBIT, and CMMI. Coverage includes Using CM to meet business objectives, contractual requirements, and compliance rules Enhancing quality and productivity through lean processes and just-in-time process improvement Getting off to a good start in organizations without effective CM Implementing a Core CM Best Practices Framework that supports the entire development lifecycle Mastering the people side of CM: rightsizing processes, overcoming resistance, and understanding workplace psychology Architecting applications to take full advantage of CM best practices Establishing effective IT controls and compliance Managing tradeoffs and costs and avoiding expensive pitfalls Configuration Management Best Practices is the essential resource for everyone concerned with CM: from CTOs and CIOs to development, QA, and project managers and software engineers to analysts, testers, and compliance professionals. Praise for Configuration Management Best Practices Understanding change is critical to any attempt to manage change. Bob Aiello and Leslie Sachss Configuration Management Best Practices presents fundamental definitions and explanations to help practitioners understand change and its potential impact. Mary Lou A. Hines Fritts, CIO and Vice Provost Academic Programs, University of Missouri-Kansas City Few books on software configuration management emphasize the role of people and organizational context in defining and executing an effective SCM process. Bob Aiello and Leslie Sachss book will give you the information you need not only to manage change effectively but also to manage the transition to a better SCM process. Steve Berczuk, Agile Software Developer, and author of Software Configuration Management Patterns: Effective Teamwork, Practical Integration Bob Aiello and Leslie Sachs succeed handsomely in producing an important book, at a practical and balanced level of detail, for this topic that often goes without saying (and hence gets many projects into deep trouble). Their passion for the topic shows as they cover a wonderful range of topicseven culture, personality, and dealing with resistance to changein an accessible form that can be applied to any project. The software industry has needed a book like this for a long time! Jim Brosseau, Clarrus Consulting Group, and author of Software Teamwork: Taking Ownership for Success A must read for anyone developing or managing software or hardware projects. Bob Aiello and Leslie Sachs are able to bridge the language gap between the myriad of communities involved with successful Configuration Management implementations. They describe practical, real world practices that can be implemented by developers, managers, standard makers, and even Classical CM Folk. Bob Ventimiglia, Bobev Consulting A fresh and smart review of todays key concepts of SCM, build management, and related key practices on day-to-day software engineering. From the voice of an expert, Bob Aiello and Leslie Sachs offer an invaluable resource to success in SCM. Pablo Santos Luaces, CEO of Codice Software Bob Aiello and Leslie Sachs have a gift for stimulating the types of conversation and thought that necessarily precede needed organizational change. What they have to say is always interesting and often important. Marianne Bays, Business Consultant, Manager and Educator}
}

@inproceedings{10.1109/SMC.2018.00599,
author = {Teshima, Yunosuke and Watanobe, Yutaka},
title = {Bug Detection Based on LSTM Networks and Solution Codes},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC.2018.00599},
doi = {10.1109/SMC.2018.00599},
abstract = {Debugging a program is always an obstacle to programmers and learners. In particular, novice programmers waste a lot of time finding bugs, so a feedback system to support debugging is required. Although existing editors and IDEs support finding syntax errors, their functions for detecting logical errors are limited. In the present paper, we present bug detection methods for the feedback system of an online judge system which contains many programming problems and accumulates numerous lines of solution source code. The proposed method uses the solutions and a language model based on long short-term memory (LSTM) networks for bug detection. In addition, since LSTM networks have some hyperparameters, we investigate the best model for bug detection in terms of perplexity and training time. The results of experiments show that models trained by solutions can detect bugs in a compiled code based on the static structure of a program.},
booktitle = {2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {3541–3546},
numpages = {6},
location = {Miyazaki, Japan}
}

@inproceedings{10.1007/978-3-319-03889-6_23,
author = {Gargiulo, Francesco and Persechino, G. and Lega, M. and Errico, A.},
title = {IDES Project: A New Effective Tool for Safety and Security in the Environment},
year = {2013},
isbn = {978-3-319-03888-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03889-6_23},
doi = {10.1007/978-3-319-03889-6_23},
abstract = {In the region of Campania in south-west Italy there is growing evidence, including a World Health Organization (WHO) study of the region, that the accumulation of waste, illegal and legal, urban and industrial, has contaminated soil, water, and the air with a range of toxic pollutants including dioxins. An effective environmental monitoring system represents an important tool for an early detection of the environmental violations. The IDES Project is a Geo-environmental Intelligence System developed by the CIRA with the contribution of universities and other government bodies and it aims at implementing an advanced software and hardware platform for image, data and document analysis in order to support law enforcement investigations. The IDES main modules are: Imagery Analysis Module to monitor land-use and anthropogenic changes; Environmental GIS Module to fuse geographical and administrative information; Epidemiological domain Module; Semantic Search Module to discover information in public sources like: Blog, Social Network, Forum, Newspapers; This paper focuses on Semantic Search Module and aims to provide the greatest support to the extraction of possible environmental crimes collecting and analyzing documents from online public sources. Unlikely people denounce criminal activity to the authorities. On the other hand many people through blogs, forums and social networks every day expose the status of land degradation. In addition, journalists often, have given the interest of the public, documenting the critical environmental issues. All this unstructured information are often lost due to the difficulty to collect and analyse. The IDES Semantic Search Module is an innovative solution for aggregating of the common uneasiness and thoughts of the people able to transform and objectify the public opinion in human sensors for safety environmental monitoring. In this paper we introduce methods and technologies used in some case studies and, finally, we report some representatives results, highlighting innovative aspects of this applied research.},
booktitle = {Algorithms and Architectures for Parallel Processing: 13th International Conference, ICA3PP 2013, Vietri Sul Mare, Italy, December 18-20, 2013, Proceedings, Part II},
pages = {201–208},
numpages = {8},
keywords = {Illegal dumping, Landfills monitoring, interoperability, Text semantic search, Information retrieval, Geographical Information Systems},
location = {Vietri sul Mare, Italy}
}

@book{10.5555/515475,
author = {Tamres, Louise},
title = {Introducing Software Testing: A Practical Guide to Getting Started},
year = {2002},
isbn = {0201719746},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: The chaotic testing environment A project is in panic mode and the deadline is rapidly approaching. Management starts to think about the need to test this product, having already missed some prime opportunities for improving software quality. One unfortunate programmer is assigned the task of software testing, which is often viewed as being transferred to purgatory. Needless to say, this poor hapless soul is given no guidance, and nobody in the organization is capable of providing any help. Despite the poor condition of requirements and other product documentation, the product is being built and it will be shipped. The task given to the tester is to minimize the surprises that could manifest themselves after the product is installed at customer sites. Under extreme pressure, this untrained tester is very inefficient and is at a loss how to begin. A clueless manager may even purchase testing tools, despite there being no useful tests to automate. This is the scenario that gives software testing a bad name. Software testing is a specialized discipline requiring unique skills. Software testing is not intuitive; one must learn how to do it. Na\"{\i}ve managers erroneously think that any programmer can test software if you can program, then you can test. This is the motivation behind this book: to provide a step-by-step approach for getting started with the testing effort. Many fine books on software testing are available today. Those that do address test case design describe proven methods such as boundary value analysis, equivalence class partitioning, decision tables, syntax testing, cause-effect diagrams, data-flow methods, and other suchconcepts. Some novice testers wonder how to weed though poor specifications, before even being able to apply these methods. Many texts state that good requirements are necessary for the test effort assuming that requirements exist yet I have not seen any that explain the transition from requirements to test cases. In the chaotic software development environment, adequate requirements are rarely provided, and if they are, their completeness and correctness are questionable. In a situation when no one has analyzed the requirements adequately, the burden falls on the tester to pursue requirements issues prior to defining any tests. It is often impossible to perform thorough testing, given the tight schedules and limited resources. It is possible, however, to make intelligent choices and maximize the effectiveness of the testing effort. The goal is to learn how best to approach the testing tasks and eventually produce a workable test process for future projects. The author's philosophy To introduce the ideas on how to begin testing, I will work through several detailed examples, each containing defective "requirements". By definition, good requirements are testable and unambiguous. The fact that the sample requirements are deficient does not prevent useful test activities from occurring. I use the word "requirements" loosely and equate it with some sort of product description. While the sample test scenarios would not be permissible in a mature software organization, the work described will help the lone tester jumpstart the testing process under duress. The goal is to show that some product information, however deficient, can be used to start the testing effort. I do not advocate working from poor requirements. Properly analyzing requirements corrects many deficiencies. Reviews and inspections have been proven to provide the most cost effective method for finding problems early in the development cycle. Many times, I have had to bite my tongue to avoid blurting out to project managers, "The requirements are absolute garbage and there's no way that we can begin a productive testing effort until you clean up your act." Actually, this phrase would contain unprintable language and be uttered under one's breath. We have undoubtedly all shared this fantasy, and the ugly truth is that despite this valid complaint, the product delivery deadline is fast approaching. Although I do not advocate cutting corners, there are some shortcuts that will help document the testing activities. A crude list of tests is better than no list. The minimum you will have is a documented trail, though rudimentary, that records your testing effort should you need to prove or demonstrate what you did. Subsequent testing efforts will improve on this initial work, producing test documents and developing a test process that is more in line with accepted practices. Incremental changes lead to successful process improvements. Just knowing how to get started with testing is a feat in itself. The tester must understand how to transform product information into test cases; this is the book's chief goal. Many existing books do an outstanding job of explaining software testing concepts and methods. Rather than reiterate what others have written, I make many references to their work. This book is a primer on getting started. It supplements currently available literature on software testing by providing an introduction to known software testing techniques. Intended audience This book is aimed at several types of readers: persons new to software testing who have no guidance or training; managers or mentors, who may themselves be experienced testers, seeking ideas on how to provide guidance to novice testers; experienced programmers who have been assigned testing tasks; knowledgeable testers looking for new ideas. While readers are not assumed to be knowledgeable about software testing concepts, they should be computer literate and able to use a word processor and spreadsheet. Job descriptions The general job description terms used throughout the book are as follows: Tester: The person who defines and executes tests. Developer: The person who produces the application, including the design, source code, and final product integration. Project manager: The person with authority regarding schedules and staffing. Project authority: The domain expert with authority to define and clarify the requirements. I refer to these descriptive titles without implying an organization structure or employee reporting chain. Project staffing decisions and job responsibilities vary across organizations. Depending on how the project is staffed, the tester could be either in the same or in a separate group from the developer. Other projects could require that the same person performs both development and t changing mindset in mid-project. Ideally, a trained software test engineer performs the testing activities. However, some projects simply assign the testing role to whichever person is available. The project authority can be the marketing manager, company executive, or customer support liaison, provided that this person has full authority to define the project contents. This role is necessary to prevent further chaos. Someone must be in charge of deciding which features to incorporate into a product; lack of such control is a well-known cause of problems when trying to get bad requirement definitions sorted out. Your organization may use different job titles than those listed above. The key point is to assign people to perform the necessary tasks each of which requires specialized skills. Mature and immature software development environments The examples cited in this book, with their incomplete product information, are what one could expect to find in an immature software organization. I will refrain from critiquing the work environment and from preaching about software process improvement. The reality is that many companies operate under less than ideal conditions. Despite the lack of suitable software processes, products are still being developed and shipped to customers. Testing, however minimal, can still be done. With poor requirements, the tester spends more time identifying product definition deficiencies rather than proceeding with testing-related tasks. A mature software organization displays the characteristics listed below. An immature organization often does not understand how the following points can improve product quality: provide useful requirements and product descriptions; conduct reviews and inspections; have signoffs or checkpoints before proceeding to the next step; mentor and train personnel; schedule adequate time and resources for testing; overlap testing and development activities; provide defined software development and software testing processes; enforce configuration management. Testing is a responsibility shared with the rest of the development team. The old view of testing as an afterthought design, code, and then you test has never produced good testing results. The adversarial and destructive "developer vs tester" mentality has often resulted from the developers' ignorance about software testing activities more proof that testing is a unique discipline. It is often the case that a tester often knows more about programming than a developer knows about software testing. A collaborative approach between testers and developers fosters goodwill and good communication. By working closely with the testers, many developers learn more about software testing, even if all the developers see is how their knowledge about the product filters into the test documentation. Effective software testing requires co-operation among all the members of a project. Book overview Chapter 1 deals with the unfortunate "you're new to testing, have no idea where to start, and the product ships Friday" nightmare. Hoping that your next project gives you more time to carry out testing activities, Chapter 2 illustrates the use of outlines, which is al analyzing requirements if no one else has done this task. Chapter 3 transforms the outline contents into test cases. Tables and spreadsheets are an integral tool used by software test engineers, and Chapter 4 shows several table formats and shortcuts for documenting test cases. Chapter 5 shows additional usages of tables. Applications built using object-oriented methods can use many of the same test design techniques outlined in the previous chapters. However, Chapter 6 describes some issues particular to testing object-oriented systems. Chapter 7 lists the challenges faced when testing web applications, although many of the strategies presented apply equally to client-server environments. No uniform software testing method exists. Each example uses a different approach for producing tests. You may wonder why one method was used in one example instead of another. The answer is simple: I selected a method based on my experience. You may very well try a different approach that will be just as successful in your testing effort. Although the examples cover different types of applications, the core software testing themes apply equally to all examples, and some concepts are reiterated among all chapters. I recommend that you read through each scenario and not dismiss the subject simply because the example does not reflect your type of application. By following the ideas and methods presented, you will have defined and documented many test cases. Chapter 8 will help you identify the most pertinent tests and thus reduce the necessary number of test cases to execute. Producing a set of test cases to execute is only part of the overall software testing picture. Chapter 9 lists other testing and quality related tasks that are necessary for producing quality software. If this is the organization's first venture into methodical software testing, you will have established a good baseline. Although the work produced will be a vast improvement over prior chaotic efforts, it will fall short of satisfying, and conforming to, industry standards. Chapter 10 briefly describes some of the more common software engineering standards and how each affects the test case examples. Consider this a launching point for improving the testing effort.}
}

@article{10.5555/1791129.1791170,
author = {Bloch, Stephen},
title = {Teachscheme, ReachJava: introducing object-oriented programming without drowning in syntax: poster session},
year = {2010},
issue_date = {June 2010},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {25},
number = {6},
issn = {1937-4771},
abstract = {This poster presentation summarizes the major pedagogical innovations of the TeachScheme!, ReachJava approach, reports on the results of our past faculty workshops (particularly the adoption of our approach by respected colleges, universities, and high schools), and invites visitors to a free (NSF-funded) faculty workshop in Summer 2010. Several textbooks and other instructional materials using this approach, e.g. (Bloch, 2010), (Felleisen F. F., 2008), (Felleisen F. F., 2010), (Felleisen F. F., 2001), (Sperber, 2009) will be available for examination.A first course in computer programming should not be about the current "hot" language in industry -- which may be obsolete by the time today's freshmen graduate -- but rather about lasting, transferable concepts and practices of good programming. Yet beginning programming students spend much of their time wrestling with the language, and often mistake that as the subject of the course. The programming language distracts from the course material; on the other hand, students need a real language to write real programs that really run on real computers.We resolve this dilemma by starting in a language with simple, consistent syntax and semantics, currently a subset of Scheme (omitting I/O, assignment, sequence, higher-order functions, and local definitions). Our pedagogically-oriented IDE enforces this subset, and gives error messages appropriate to the current subset, but allows students as they outgrow each subset to advance to a larger one with a few mouse clicks. Students become comfortable with fundamental programming concepts --- variables, function composition, function definition, parameter passing, data types, design for reuse and modifiability, conditionals, fields, polymorphism, self-reference and recursion, functional abstraction, event-driven programming, model/view separation, etc. --- in this sheltered environment before encountering the same concepts in the more bewildering world of Java, C++, etc.Simultaneously, students are trained in a step-by-step design recipe for software development: a series of concrete questions, with concrete products at each stage:1) Identify the purpose, inputs, and outputs of the program (function, method, whatever) to be written;2) Identify (and, if necessary, define) data types relevant to the problem at hand;3) Write examples or test cases of how the program will be invoked, in legal syntax and accompanied by expected results, using the data types from step 2 as a guide;4) Write a program skeleton, the syntax to define a function with the name and parameters chosen above;5) Write an inventory of available and likely-to-be-needed expressions, based on parameter names and their data types;6) Choose and combine items from the inventory to form a complete program body (the hardest part, but in practice step 5 often does most of the work);7) Test the program by running it on the examples from step 3.We emphasize data types throughout, not only as a fundamental concept, but as an invaluable tool in coding: to every data type correspond both a natural coding pattern, which provides a rough draft of the code and helps students avoid "blank page syndrome", and a natural testing pattern, which provides guidance in building test suites. In particular, recursion is introduced as simply the application of already-learned coding patterns to a self-referential data type. The concrete methodology also provides a handy grading rubric that shows students that every step matters, not only the coding.For non-majors, we aim to convey important programming concepts and methodology in one language. For CS majors, the course switches from Scheme to Java late in the first semester or between first and second semesters. The Java stage is not independent, but builds on and reinforces the programming concepts and methodology already learned in Scheme, with explicit discussion of similarities, differences, and the continued applicability of the concepts and methodology. Students learn to apply the same test-driven, step-by-step design recipe in Java that they've been using in Scheme. The result is a student who, after a year of coursework, can approach programming problems in a principled manner (not "hack it until it works"), with understanding and perspective.Although we currently use Scheme as a first language and Java as a second, the approach is applicable to other languages. Whatever the language, however, we believe the first exposure to programming should be functional rather than imperative/sequential/procedural: not only do functional programs have simpler semantics, relying on the familiar model of algebraic expression evaluation rather than a load/store machine model, but it's enormously easier to write test cases for functional programs than for stateful ones. Stateful testing, along with stateful programming, can be introduced late in the first semester after students have thoroughly internalized functional techniques.Our approach has been adopted at a number of colleges and universities, including Rice, Northeastern, the University of Chicago, Northwestern, the University of Utah, Cal. Poly San Luis Obispo, Vassar College, and the University of Delaware, as well as dozens or hundreds of high schools. We'll be offering free (NSF-funded) one-week workshops in Summer 2010 at four locations around the U.S.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {218–220},
numpages = {3}
}

@article{10.5555/3512733.3512734,
author = {Barlow, Max and Cazalas, Ibraheem and Robinson, Chase and Cazalas, Jonathan},
title = {MOCSIDE: an open-source and scalable online IDE and auto-grader for introductory programming courses},
year = {2021},
issue_date = {October 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {37},
number = {5},
issn = {1937-4771},
abstract = {Programming is learned through practice, with said practice in introductory programming courses often translating to a prohibitively large number of assignments, increasing the grading workload for faculty and/or teaching assistants. In short, this is unsustainable. Several publishers and a few notable companies have provided meritable solutions, although most are plagued with problems including minimal problem sets, limited customization options, high cost, and even a disconnect with the pedagogical needs within academia. This paper presents a survey of the more popular solutions currently available, followed by a presentation of our newly-developed web application, MOCSIDE: open-source and scalable online IDE and auto-grader for computer science education.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {11–20},
numpages = {10}
}

@article{10.1145/3036686.3036691,
author = {Arya, Kavi and Coelho, Blossom and Pandya, Shraddha},
title = {A model based design approach to system building using the e-Yantra educational robot},
year = {2017},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
url = {https://doi.org/10.1145/3036686.3036691},
doi = {10.1145/3036686.3036691},
abstract = {The e-Yantra robot is the basis for a highly scalable embedded systems teaching program setting up 500 embedded systems labs in Indian engineering colleges. A key strategy to encourage rapid prototyping of applications has been to encourage reuse of code using a commodity robot with a standard API along with excellent documentation and training material. An important challenge has been to teach the reasoning process from a design through to an implementation deployed on an actual machine. Model based design is key to articulating such reasoning. A further challenge is to do this in an affordable manner where most available model- based IDEs are expensive proprietary systems using languages such as Esterel and SCADE. We illustrate with a "Valet Parking" application how our robotic eco-system facilitates the learning of important model-based design principles taking a high-level specification of a problem down to working code and even deriving test cases in the process. A novel feature of our approach is that we carry out design-time scheduling of various (concurrent) activities by analyzing dependencies between modules and obtain purely sequential C-code implemented on a microcontroller without the need for an RTOS. This case study is an exemplar of a model-based design approach for a large class of such robotic projects.},
journal = {SIGBED Rev.},
month = {jan},
pages = {37–43},
numpages = {7},
keywords = {UML, design time scheduling, educational robot, model based design, model based testing, project based learning, state machines}
}

@inproceedings{10.1145/2829957.2829963,
author = {Arya, Kavi and Coelho, Blossom and Pandya, Shraddha},
title = {A Model Based Design Approach To System Building Using The e-Yantra Educational Robot},
year = {2014},
isbn = {9781450330909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2829957.2829963},
doi = {10.1145/2829957.2829963},
abstract = {The e-Yantra robot is the basis for a highly scalable embedded systems teaching program setting up 500 embedded systems labs in Indian engineering colleges. A key strategy to encourage rapid prototyping of applications has been to encourage reuse of code using a commodity robot with a standard API along with excellent documentation and training material. An important challenge has been to teach the reasoning process from a design through to an implementation deployed on an actual machine. Model based design is key to articulating such reasoning. A further challenge is to do this in an affordable manner where most available model-based IDEs are expensive proprietary systems using languages such as Esterel and SCADE. We illustrate with a "Valet Parking" application how our robotic eco-system facilitates the learning of important model-based design principles taking a high-level specification of a problem down to working code and even deriving test cases in the process. A novel feature of our approach is that we carry out design-time scheduling of various (concurrent) activities by analyzing dependencies between modules and obtain purely sequential C-code implemented on a microcontroller without the need for an RTOS. This case study is an exemplar of a model-based design approach for a large class of such robotic projects.},
booktitle = {Proceedings of the WESE'14: Workshop on Embedded and Cyber-Physical Systems Education},
articleno = {5},
numpages = {8},
keywords = {Design Time Scheduling, Educational Robot, Model Based Design, Model Based Testing, Project Based Learning, State Machines, UML},
location = {New Delhi, India},
series = {WESE'14}
}

@inproceedings{10.1145/2828959.2828972,
author = {Santos, Andr\'{e} L.},
title = {Collaborative course project for practicing component-based software engineering},
year = {2015},
isbn = {9781450340205},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2828959.2828972},
doi = {10.1145/2828959.2828972},
abstract = {Empirical studies revealed that computer science and engineering students have difficulty in mastering concepts such as interfaces and information hiding. These concepts are central to component-based software engineering (CBSE), a challenging subject to address in a university course, given that some degree of software development complexity is necessary for effectively practicing it. This paper describes an experiment carried in an advanced programming course offered at our institution, consisting of having a collaborative course project targeting the practice of CBSE. In this collaborative project, different student teams developed parts of an IDE whose designs were tested by other teams in terms of component interoperability and extensibility. Although students were able to practice the CBSE-related concepts in approximate real settings, the necessary technical supervision from instructors might consist of a pitfall.},
booktitle = {Proceedings of the 15th Koli Calling Conference on Computing Education Research},
pages = {142–146},
numpages = {5},
keywords = {IDEs, collaborative learning, component-based software engineering, pedagogy},
location = {Koli, Finland},
series = {Koli Calling '15}
}

@inproceedings{10.1145/3478432.3499125,
author = {Cazalas, Jonathan and Barlow, Max and Cazalas, Ibraheem and Robinson, Chase},
title = {MOCSIDE: An Open-source and Scalable Online IDE and Auto-Grader for Computer Science Education},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478432.3499125},
doi = {10.1145/3478432.3499125},
abstract = {Programming is learned through practice, with said practice in introductory programming courses often translating to a prohibitively large number of assignments, increasing the grading workload for faculty and/or teaching assistants. In short, this is unsustainable. Several publishers and a few notable companies have provided meritable auto-grading solutions, although most are plagued with problems including minimal problem sets, limited customization options, high cost, and at times even a disconnect with the pedagogical needs of academia. This poster presents our newly-developed web application, MOCSIDE, an open-source and scalable online IDE and auto-grader for computer science education. Results indicate a positive user experience from students and instructors alike, with cost savings, ease of use, and code collaboration highlighted as key features.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1114},
numpages = {1},
keywords = {auto-grading, computer science education, cs1, cs2, online ide},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{10.1145/3626252.3630913,
author = {Jefferson, Thomas and Gregg, Chris and Piech, Chris},
title = {PyodideU: Unlocking Python Entirely in a Browser for CS1},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630913},
doi = {10.1145/3626252.3630913},
abstract = {In this paper, we present an education-focused Python IDE and runtime library which can run entirely in desktop, laptop, tablet, and mobile device web browsers. Our solution provides features useful for an engaging CS1 course, and eliminates the need for a server-based runtime. We describe a new, open source, methodology for running interactive Python entirely in the browser by solving the "WebAssembly blocking problem," a core technical challenge to a web-based Python solution.Because our method enables Python entirely in the browser, it unlocks many new features. For example, students can share their code with others, without incurring extra costs to the instructors or institutions. Other features include line by line code highlighting as a program executes, highly intuitive interactive graphics, mouse and touch integration, and use of a wide selection of Python modules such as Numpy and Pandas. Currently, our IDE has been used in 5 classes, covering more than 10,000 students and teachers, with over 350,000 projects created. We found that students and instructors appreciated the variety of tools and abilities the IDE made possible. We benchmark the performance of running code with our method against other online Python solutions and we discuss the benefits and additional possibilities that our method allows, such as mobile device and/or offline code execution. We provide full free public access to our IDE and open source the core libraries which enable the conversion of student written Python to WebAssembly.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {583–589},
numpages = {7},
keywords = {cs1, ide, integrated development environment, mobile, python, web browser, webassembly},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@book{10.5555/553639,
author = {Maloney, Jim},
title = {Distributed COM: Application Development Using Visual Basic 6.0},
year = {1999},
isbn = {0130213438},
publisher = {Prentice-Hall, Inc.},
address = {USA},
abstract = {From the Book: PREFACE: The day C. J. Buck Trayser of Digital Equipment Corporation suggested that I become certified to teach Microsofts Visual Basic course was a major turning point in my career. The last time I had used the BASIC programming language was when I was working on an extra-credit project while pursuing a degree in Computer Science in 1976. The program was to run on a DEC PDP-8 and had to be saved on paper tape. I was trying to write a program that played backgammon, but ran out of memory after creating routines to make an opening move from an openings book and drawing the board on a character cell terminal. Now, nearly fifteen years later, I had considered myself a seasoned C and C++ programmer, and was on a contract assigned to teach VMS programmers how to call System Services using the C programming language. If Buck had not been such a good friend and respected colleague, I may have laughed at his suggestion. But he convinced me that becoming certified to teach Visual Basic 3.0 would be a good business decision - the demand for such training was skyrocketing. I have since taken quite a liking to Visual Basic, and have used the language in developing corporate applications for many of my customers, often combining it with Visual C++. Its appeal stems from its many interesting attributes. For one, Visual Basic is amazingly easy to use, and is probably responsible for making many new programmers out of folks who were once just users. It is quite non-intimidating to approach, yet can end up challenging the most experienced programmer due to its rich feature content. It reminds me of what is printed on the box of MasterMind, a game in which one attempts to guessacolor code created by an opponent - A minute to learn, a lifetime to master. I thought of Buck again when Andrew Scoppa suggested that I write a book on developing distributed applications with Visual Basic. My affiliation with Andrews company, UCI Software Training Centers, goes back over 16 years, and my respect for him is immense. As a quality provider of developer training, I take any suggestions Andrew makes quite seriously. Of course, at the time Buck suggested I look into Visual Basic, its features were not quite rich enough to develop distributed applications. But that has changed. The Evolution of Visual Basic As a language for developing Windows dialog-based user interfaces, Visual Basic is in its original element. As a teaching language, Visual Basic is ideal for illustrating not only Windows user interface programming concepts, but topics related to modular software development, code reuse, data scope, and even object-oriented design and programming techniques. For testing COM objects developed in C++ and ATL, Visual Basic offers the quickest solution. The popularity of Visual Basic is no doubt responsible for its evolution from a graphical user interface development tool to an advanced, high-performance application development environment. Its ease of use, data access capabilities, multi-threading conformance, COM compliance, and native code generation capabilities now make it an excellent choice for the rapid development of the server-hosted business components of a distributed application. Having taught Visual Basic to developers for a number of years, I am beginning to see the language finally receive the respect it deserves. With over 20 years of software development experience and a degree in Computer Science, I feel qualified to author a book that treats Visual Basic with the same kind of respect. About This Book This is a book for serious developers of distributed applications. While it is, strictly speaking, an intermediate-level book, I expect the reader to be an experienced programmer. The main theme of this book is to get you started with the development of distributed applications using Visual Basic in the quickest way possible. I have provided complete examples of both 2-tier and 3-tier applications - these applications have thousands of lines of Visual Basic code. Complementing these complete applications are dozens of small demos that illustrate one or two important concepts. While instructions for running these demos are included in the chapters that follow, you should check the README.TXT file on the enclosed CD-ROM for additional and last-minute information regarding the use of the demos. Probably the most unusual thing about this book is that I start with a chapter on deploying multi-tiered distributed applications. This approach has worked well in the classroom when I teach my courseware, and I expect the same results in this book. Since many of the demos used in later chapters depend on the sample applications being installed, it is essential that you read the first chapter and go through the installation procedures. In Chapter 2, I discuss how to use objects from a Visual Basic application, using the sample video store objects provided with this book. Even if you are familiar with using COM objects in a Visual Basic application, you should read this chapter. The chapter may seem like a show-and-tell for an existing application, but it provides good examples of object design and documentation. In Chapter 3, I cover how to develop Win32 user interface applications that use COM objects. In addition to providing more examples of object use, the chapter is an excuse for me to introduce some advanced user interface techniques. In this chapter, you will learn how to use the Windows registry, display a splash screen, use OLE drag-and-drop, and use advanced user interface controls such as the tree view, list view, tab strip, and toolbar controls. My advanced Visual Basic students often ask to see how to use these controls and techniques in their programs. I consider Chapter 4 to be the paying your dues chapter. In earlier days, programmers had to use APIs such as the TCPIP socket API to develop distributed applications. The Windows platform SDKs and developer tools such as Visual Basic have made the creation of distributed applications much easier. If you are pressed for time, you can skip Chapter 4. As an essential and fundamental introduction to developing the middle tiers of a distributed application, Chapter 5 is required reading. This chapter introduces Activex Data Objects (ADO), showing both their programmatic use and their use through the new ADO Data Control. Additionally, the chapter explains Data Environments and how to create programs that generate reports from a database. Chapters 6, 7, and 8 are all about objects. First, I cover the use of class modules in Visual Basic, then move to their use in creating Activex Components. Finally, the internals of COM, the Component Object Model, are revealed in more detail than the typical Visual Basic developer would probably want - but more is better than less! If you are familiar with using class modules in Visual Basic, you can skip Chapter 6, but since some of the examples in Chapter 7 extend concepts presented in Chapter 6, it may be better to skim it rather than skip it. In Chapter 9, I provide all you will need to know, and then some, about creating Activex Controls in Visual Basic. Although their development and use is, strictly speaking, not required in a distributed application, it is one of the most popular and requested topics of my students. If you wish, you can skip this chapter. Interesting uses of COM are presented in Chapter 10, which covers automation fundamentals. In this chapter, youll learn how to develop Visual Basic applications that launch and control other applications. Youll also learn how to add an automation interface to your own applications so that other programs can control yours. Its a short chapter, and one of my favorite topics, so I suggest you read this one. A fundamental and hands-on introduction to DCOM, the Distributed Component Object Model, is presented in Chapter 11. Reading this chapter and performing the walkthroughs will give you a solid introduction to DCOM, as well as an appreciation for the features provided by Microsoft Transaction Server (MTS). The serious distributed application developer will want to read and understand this chapter. Based on my experience, I am quite certain that many of you purchased this book for the topic covered in Chapters 12 and 13 - using MTS. For over a year now, I have often entertained myself by seeing the reaction of students when I ask, How many of you are interested in learning about MTS Even people who have never raised their hands in public since the third grade eagerly raise one, if not both arms, often adding a vigorous circular motion. It goes without saying that youll want to read both of these chapters. One of the strongest objections to using a distributed approach to software development is the inherent difficulty in maintenance and troubleshooting - How would you debug that thing In Chapter 14, I attempt to show you that it is not that hard to maintain a distributed application. Additionally, I provide information about the often-dreaded binary compatibility features of Visual Basic. This chapter is essential reading for the serious developer. Entire books have been written on the topic covered in Chapter 15 - creating Internet interfaces using Active Server Pages (ASP). I have purposely kept this chapter light, because I never really bought into the concept of mixing a program (e.g., script) with graphic content (e.g., HTML). Certainly there are thousands of developers who love this sort of thing, and are capable of creating interesting Web pages while also providing the programming behind them. The chapter goes a bit beyond getting you started, and I would consider it to be required reading, if only because chances are the existing Web applications you now have were implemented as ASP pages. A far better approach to developing Web applications was introduced in Visual Basic 6.0, and is the subject of Chapter 16 - creating Internet IIS applications. This new approach allows an applications programmer to program applications and a graphics designer to design graphics. I highly recommend reading this chapter thoroughly. In Chapter 17, I introduce Activex Documents, which many Visual Basic developers are likely to find as appealing solutions for intranet development. While their reach is limited, due to browser compatibility, if you dont need broad reach capabilities for your intranet application, Activex Documents are at least worth a look. Wrapping up the book is Chapter 18, in which you will learn how to use the Internet Transfer and Web Browser controls. In this chapter, you will see how to embed browser capabilities directly in your Visual Basic application, and automate file transfers via FTP. Acknowledgements I have quite a list of people to thank for assistance in writing this book. First, I want to thank Buck Trayser of Digital Equipment Corporation for persuading me that learning Visual Basic would be a good thing. I must also thank Cheryl Jacobs and Tony Todd of M&amp;MMars, great customers of mine, for asking that I do a course on Visual Basic 3.0. That course turned out to be the start of a total of eight courses I developed covering Visual Basic from its fundamentals to its advanced features. Certainly this book would not have been possible without the help of Andrew Scoppa and Donna Thayer of UCI Software Training Centers in Stoneham, Massachusetts. As a senior consulting partner with UCI, I have enjoyed working with Andrew and Donna for nearly 16 years. In my opinion, they offer the best developer training anywhere. Thanks to Dave Libertone, also of UCI, for his help on this book - as a published author himself, he was a great pathfinder for me. Dave provided me with a terrific opportunity to get my feet wet as an author when I wrote a short chapter for his excellent book Windows NT Cluster Server Guidebook, also published by Prentice Hall. In addition to being a great friend, Daves technical advice is always as good as gold. Claudio Ghisolfi and Kay Connolly, also of UCI, deserve a great deal of credit for this book as well. I want to especially thank Claudio for his patience and determination in examining the demos and walkthroughs presented in this book. Thanks to the excellent instructors who have used my courseware and provided feedback on errors, omissions, and suggestions, especially Karen Gallagher and Andy Macentee. Thanks also to Art Kane and Olivia Kane of AmeriteachUCI, Chip Hillman, Ed Stepian, and Debby Stepian of the Orange County Sheriffs Office; Mary Anne Vaughn of Microsoft Corporation; Melody Glover at the Kennedy Space Center; Michael Gorman and Dominic Vergata of Avon Corporation; Rick Wallace and Vicki Kyle-Flowers of Digital Equipment Corporation; Mike Meehan of Prentice Hall; Bob Barnes of Allen-Bradley; and Jim Slate, Bruce Kepley, and Ken Kelly. Also, I want to give a very big thank you to Robin at Sir Speedy. Thanks to everyone at the Clearwater Research Group for providing the fresh academic and research environment that one needs to experiment with new technologies such as those discussed in this book. The many students I have had the pleasure of teaching have helped a great deal with their comments and suggestions. In particular, I want to thank Connie Patton and everyone at Liberty Mutual; Rich Lagasse at Mathworks; Michael Joy at Sensitech; and Daniel Bagley. Kasey, Tracy, Mary, and James, my children, deserve credit for their support. I wish to mention James in particular, a freshman at the University of Florida, for his fantastic job of data entry for the Mom-n-Pop Video Store database, and for providing me with the excellent opening paragraph of Chapter 16. Lastly, and most of all, I cannot begin to thank Patty, my wife, for her support, encouragement, faith, and companionship during the development of this book and the courses that preceded it, and for all the errands, big and small, she ran for me, and for the excellent job she has done keeping the household running while I wrote this book. Without Patty, this book would literally have not been possible, especially since she helped me through a scare in which I thought the entire book was lost on my laptop - Patty found the hidden reset button on my ThinkPad! Jim Maloney, MCSD, MCT Honorary Research Fellow, Clearwater Research Group St Petersburg, FL, November 1998}
}

@inproceedings{10.1145/3548771.3570193,
author = {Fraser, Gordon},
title = {Gamifying software testing (keynote)},
year = {2022},
isbn = {9781450394543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548771.3570193},
doi = {10.1145/3548771.3570193},
abstract = {Writing good software tests is difficult and not every developer's favourite occupation. If an activity is so difficult, boring, or otherwise unattractive that people do not want to engage with it, then gamification offers a solution: By turning the activity into a fun and competitive task, participants engage, compete, and excel. In this talk, I will explore how this idea can be integrated into software testing tools (e.g. IDEs), processes (e.g. continuous integration), and education. Our experiences with gamified testing illustrate the potential of using gamification to address some of the many problems that we are facing today in software testing. There are, however, many challenges ahead, and I will outline some of the challenges and research opportunities related to gamifying software testing.},
booktitle = {Proceedings of the 1st International Workshop on Gamification of Software Development, Verification, and Validation},
pages = {1},
numpages = {1},
location = {Singapore, Singapore},
series = {Gamify 2022}
}

@inproceedings{10.1145/3287324.3293867,
author = {Peveler, Matthew and Gurjar, Tushar and Maicus, Evan and Aikens, Andrew and Christoforides, Alexander and Cutler, Barbara},
title = {Lichen: Customizable, Open Source Plagiarism Detection in Submitty},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293867},
doi = {10.1145/3287324.3293867},
abstract = {Prior education research, including Computer Science, has established that students will attempt to cheat and violate academic integrity, with one of the more common forms being code plagiarism. The majority of existing tools for software plagiarism are closed source, requiring instructors to use them in a prescribed configuration and sending student code to a third-party server for analysis. At the core of this analysis is the need to perform a language-specific tokenization of the input program and then to use "digital fingerprinting" on the code to identify significant markers. This has required developers to write their own parser for each supported language, which is time-consuming to create and keep up-to-date, and thus a barrier to creation of these tools. Instead we bootstrap new languages into our plagiarism system by leveraging the "Language Server Protocol", an initiative to create open-source parsers and tokenizers for many languages (principally to be used within a range of popular IDEs). In this poster, we present our work on Lichen, the open source plagiarism detection tool that is integrated into the Submitty course management platform we use at Rensselaer Polytechnic Institute. This tool is a pipeline of modules for the specific tasks of tokenizing, fingerprinting, and then comparing the fingerprints for any number of files. Through this, a similarity score is generated for pairs of files, and these are used to help instructors determine to what extent code plagiarism has occurred.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1270},
numpages = {1},
keywords = {code plagiarism, lichen, plagiarism},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/3159450.3162207,
author = {Garcia, Rita and Falkner, Katria and Vivian, Rebecca},
title = {Parsons Problems usage within a MOOC Pedagogy: (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162207},
doi = {10.1145/3159450.3162207},
abstract = {Parsons Problems is an effective Introductory Programming teaching tool that allows students to arrange code to form a functional program, reducing their cognitive load so they can focus on programming concepts. This demonstration shows how Parsons Problems can be used as a pedagogical strategy in a Massive Open Online Course (MOOC) for introductory programming. A MOOC is a learning platform, where compilers and IDEs have already been embedded for CS students to immerse themselves in a single learning environment. The Parsons Problems package, js-parsons, available at https://github.com/js-parsons/js-parsons has been successfully integrated into the University of Adelaide's edX's MOOC platform as a component that can potentially be made available for other edX platforms. The demonstration presents the educators' interface to add new questions and receive students' results, along with students' perspective, including subgoals. The demonstration prototypes new feedback models when students encounter Parsons Problems errors, opening discussion up to the audience for opinions and input. This demonstration is intended for anyone wanting to know how to use Parsons Problems within MOOCs as a pedagogical approach; those seeking to incorporate MOOCs into their coursework with focused exercises; and those wanting to contribute to future Parsons Problems enhancements. Laptops are recommended, since participants will have the hands-on opportunity to evaluate the package during the demonstration.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1111},
numpages = {1},
keywords = {introductory programming, mooc, parsons problems},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/3328778.3372701,
author = {Cooper, Saraah and Clinkscale, Ben and Williams, Briana and Lewis, Myles},
title = {Exploring the Impact of Exposing CS Majors to Programming Concepts using IDE Programming vs. non-IDE Programming in the Classroom},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3372701},
doi = {10.1145/3328778.3372701},
abstract = {Due to the increased demand for computer scientists in the, the importance to improve the retention rate of CS majors who could potentially fill such positions has been ongoing. Literature has produced many efforts for increasing the engagement of CS majors in the field while also exploring ways to improve their ability to develop the ideal skill sets for success. In such efforts, our research explores the impact of visual and/or command-line based programming editors and their ability to shape the students' mental model as they learn to program. This abstract discusses a "think-aloud" protocol assessment that was conducted on two entry level programming courses at a university in the United States during the 2018-2019 school year. The objective of this assessment was to determine whether Repl.IT, a web-based IDE, and Cygwin/Nano Editor, a command line-based tool, impacted student performance while being used for programming. Our preliminary results showed that 41% of the students using Repl.IT completed the assigned task for the assessment, while 53% of the students using the Cygwin/Nano editor did the same. The results concerning assignment incompletion revealed that students who did not complete their assignment and used Repl.IT did so because of assignment complexity and programming language difficulty, whereas assignment complexity and tool usage difficulty lead to assignment incompletion for students using Cygwin/Nano. This assessment also revealed that Repl.IT students exhibited a higher comfort, confidence, and fondness for using their editor than the Cygwin/Nano students.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1422},
numpages = {1},
keywords = {cs-majors, programming-editors, verbal-protocol-assessment},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@book{10.5555/515385,
author = {Sall, Kenneth B.},
title = {Xml Family of Specifications},
year = {2002},
isbn = {0201703599},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: XML: Its a cheese spread. No, its a floor wax. No, its two—two—two products in one! Or maybe its everything but the kitchen sink Say, did you hear the one about the XML Kitchen Sink Language (see http:blogspace.comxkitchensink) XML: What Its All About It has been said that XML, the Extensible Markup Language, will become the ASCII of the twenty-first century because it is rapidly becoming ubiquitous. XML is expected to have an impact on both the Web and application development comparable to that of Java and JavaScript because it has opened up a wide variety of new capabilities and has been embraced by so many sectors of human endeavor. XML is a metalanguage—a syntax for describing other languages. These languages span diverse vertical industries including accounting, advertising, aerospace, agriculture, astronomy, automotive products, biology, chemistry, database management, e-commerceEDI, education, financial institutions, health care, human resources, mathematics, publishing, real estate, software programs, supply chain management, and many more (for the many more, see http: ). In one sense, XML is really a very trivial thing—just a markup syntax for describing structured text using angle brackets. But in another sense, XML is a basic building block—an enabling technology that makes it possible to develop more complex, more interesting, and more powerful tools. In the Web arena, XML is facilitating exciting improvements such as user-controllable views and filtering of information, creation oftruly device-independent content that can be re-purposed for vastly different devices, highly focused searching based on element hierarchies, and more sophisticated and flexible linking mechanisms. In the business and application arena, XML makes it easier to deliver filtered content from databases, to more readily share data between applications and between companies, and to exchange EDI messages that describe complex transactions. In the scientific arena, XML is a natural fit for describing complex datasets, models, control of instruments, images, chemical compounds, and much more. Just as Java made data processing platform-independent, XML has done the same for data, making the exchange of information much easier than ever before. But, no, XML is not the kitchen sink; it is not the solution to all of the worlds problems in one tidy package; nor is it the solution to all your computer needs either, at least not alone. Rather, XML is a tool, or more accurately, a set of tools from the same toolbox. That toolbox is the XML family of specifications. This book will help you see what XML can and cannot do by describing how to use each tool. Although XML shares a number of concepts with its ancestor, SGML (Standard Generalized Markup Language), XML is said to yield 80 percent of the benefits of SGML, but with only 20 percent of the complexity. It is precisely this 8020 rule that has excited countless companies and developers, encouraging them to support the efforts of the World Wide Web Consortium (W3C) in the development of XML. A few of the more than 500 companies and organizations that actively support XML development as members of the W3C include IBM, Sun, Microsoft, Oracle, Commerce One, and NASA. Audience: Who Should Read This Book The book is intended for Web developers, which includes programmers, content writers, and designers. Depending on your background and interests, some chapters may be more relevant to you than others. Its intended for those who may be familiar with particular aspects of XML but who have not been formally exposed to all of the major W3C specifications, as well as those who have never dealt with XML before. Later in this preface, I provide a roadmap to help orient you. Ive assumed that most readers are familiar with HTML elements and syntax, although the XML and DTD syntax discussions in Chapters 3 and 4 pretty much cover the concepts of elements, attributes, types, entities, and content that carry over from HTML to XML. In other words, you can get by without knowing HTML, except the XHTML chapter, which will make much more sense to you if you do. For those who would like to brush up on HTML, see For Further Exploration: HTML and Java at the end of this preface. Some examples require programming knowledge, but for most examples, anyone with general Web development skills will find them beneficial. Generally, scope and breadth of treatment is favored over depth. On the other hand, some readers will find that the depth is more than they expected, but they should still be able to tread the water. My intent in writing this book was to cover a number of XML-related technologies in varying degrees of detail. Id like to make it clear that although there are three chapters containing Java examples, this is not a book about Java and XML. You dont need a Java background for the vast majority of whats in this book. Although I do assume the Windows operating system, this is not a statement of preference. My formative years were spent on UNIX (I still use UNIX utilities to maintain a ski club site) at the office and a Mac at home. Rather, since Windows tends to be somewhat ubiquitous, it seems appropriate to show Windows command lines and mention some Windows-only tools. UNIX and Mac users are encouraged to share their experiences with fellow readers via the books Web site. Personally, I have found cygwin—a UNIX environment for Windows developed by Red Hat—to be very handy (see http:cygwin.com). Whats Special About This Book There are several features that contribute to making this book an invaluable resource for anyone beginning to plunge into the somewhat turbulent seas of XML. XML Family of Specifications Big Picture —Since early 1998, Ive periodically updated a diagram I call The Big Picture of the XML Family of Specifications. This unique diagram (front inside cover) depicts virtually all of the key W3C efforts related to XML, with colors to indicate each specifications status (maturity); it includes related non-W3C efforts as well. Physical positioning denotes a relationship among neighboring specifications, as explained in Chapter 2. Best of all, the Big Picture diagram appears as an imagemap on the CD-ROM and on this books Web site, possibly as a more up-to-date version. The Big Picture imagemap on the Web site expands acronyms as your mouse hovers over a term. Clicking on the acronym or name connects you instantly to the actual specification or, in some cases, a collection of documents relating to that specification. History Timeline —A detailed History of the Web and XML in timeline form—the product of a considerable amount of research—is broken down into three time periods in Chapter 1, which should be interesting to many readers. Historical perspectives are also presented for particular specifications in their own chapters. A rather unique pullout at the back of the book shows, in bar chart format, the gestation periods of all of the XML specifications in this book, giving you a visual picture of what developments occurred in sequence andor in tandem. Coverage —Ive selected what are generally considered to be the most significant XML-related specifications from the W3C: XMLDTDs, XML Namespaces, XML Schema, the DOM, CSS, XSLT, XPath, XSLFO, XLink, XPointer, XHTML, and RDF. Several of the less frequently discussed specifications, such as XML Infoset, Canonical XML, XML Base, and XML Inclusions, are also covered. In addition, Ive included four topics that are not under the purview of the W3C: RDDL, SAX, JDOM, and JAXP. The focus is on breadth rather than depth of coverage because if you have a general understanding of a lot of XML topics, you can better appreciate which are most relevant to your needs and you can drill down to the details by following the links I provide. The hope is that as you become more familiar with each of the topics I present, youll know which areas youll want to explore by buying more specialized Addison-Wesley or Prentice Hall books (e.g., about XSLT, XML with Java, or XHTML). Ive tried hard to make the information current and have spent a good bit of time in the final months polishing and updating details here and there. All topics are as up-to-date as possible, except where noted otherwise. For Further Exploration —Each chapter ends with a section called For Further Exploration, which presents quite a few links that serve not only as my bibliography, but also points to resources that contain more details than what can be provided here without killing way more than my fair share of trees. Links are provided to the specifications themselves, to articles that explain the specs in more everyday language than the precision required for formal specifications, and to articles describing subtleties or nuances of the specs. Links to tutorials, books, software, special references, and so on are also supplied. My intention is that readers will use the links, so they all appear in HTML form on the books CD. Professors may wish to consider some of these links for students research assignments. Tables —Im a big fan of the use of tables. When I read a technical book, I seldom read it word for word, cover to cover. Often I want to locate some particular detail pretty quickly, so I look it up in the table of contents or index—I dont want to have to skim through paragraph after paragraph to find the little tidbit I need. Therefore, I feel that tables will help you do the same thing, maximizing the use of your time. The List of Tables is something with which you might want to familiarize yourself—let a table be your friend. CD-ROM —The CD that accompanies the book contains all the sample code presented in the text, as well as most of the software I used while writing this book, including the following: - Code Examples—every example that appears as a code listing plus a number of variations - XML Environment—batch files to simplify using XML with Java on Windows operation systems - For Further Exploration—all links from the end of each chapter - Big Picture of XML Family of Specifications Imagemap—links to more than 60 specifications, including many not covered in this book (see Chapter 2) - W3C XML Specifications in PDF Form—every W3C specification discussed in this book is available (unedited) for offline reading(hours and hours of fun for the whole family) - Glossary of terms - Chapter 12, Practical Formatting Using XSLFO by G. Ken Holman, in HTML format with two useful appendices which arent included in the printed book - Freeware and evaluation copies of commercial software (XMLDTDXML Schema editors, validators, parsers, XSLT processors, and more) Web Site —The books main Web site is hosted by Web Developers Virtual Library, an Internet.com site. I maintain the extensive XML section of WDVL.com. The books URL there is http:WDVL.Internet.comAuthoringLanguagesXMLXML-Family . There youll find all the links from the For Further Exploration sections organized by chapter, as well as the online version of the Big Picture imagemap, and of course the inevitable corrections to the text. While this material appears on the CD-ROM, the Web site versions may be more up-to-date. The Web site will be updated periodically; you can register to receive e-mail when the site is updated, if you wish. Organization and Roadmap: How You Should Read This BookThis book is divided into five conceptual parts. With the exception of a few chapters in Part I, it is not absolutely necessary to read this book chapter by chapter (and Ill tell you right up front: the butler did it). Chapter 1, History of the Web and XML, provides an interesting historical perspective of the development of XML, but some readers may prefer to skip it entirely, or at least defer reading it until theyve completed other chapters or find themselves on a long, boring plane flight with neither good movies nor readable magazines. Readers without a Java background may wish to gloss over the three chapters that contain Java examples, instead focusing on the concepts that are discussed in these chapters. The following describes the books organization and suggested reading emphasis. Introduction: History of the Web and XML —As mentioned, Chapter 1 provides an historical perspective. Its divided into three eras: Ancient History (1945 to 1984), Medieval History (1986 to 1994), and Modern History: From HTML to XML (1994 to 2001). Part I: Fundamental XML Concepts and Syntax —This part introduces XML Syntax, DTD Syntax, the XML Infoset abstraction, Canonical XML, Namespaces, RDDL (Resource Directory Description Language), and XML Schema, corresponding to Chapters 2 through 6, intended to be read in sequence. All readers should read these chapters, although if you wont be developing your own vocabularies, you might be able to skim the DTD and XML Schema chapters (4 and 6, respectively). Although XML Schema is expected to replace the use of DTDs in many applications, your own project needs may dictate sticking with DTDs, in which case you could skip the XML Schema chapter, although I still recommend that you read the sections in Chapters 4 and 6 that highlight DTD limitations and XML Schema advantages. If you are tempted to skip the chapter on Infoset, Canonical XML, Namespaces and RDDL (Chapter 5), be sure to at least read the Namespaces section because this concept is central to many XML specifications. All chapters following 5 assume you are familiar with XML Namespaces. Although RDDL is a recent grassroots effort as I write this, its bound to have gathered a lot of momentum by the time you read this. Part II: Parsing and Programming APIs —This part presents SAX (Simple API for XML), DOM (Document Object Model), JAXP (Java API for XML Processing) and JDOM—Chapters 7 through 9. All of these are application programming interfaces (APIs) to parsing and manipulating XML documents. This is the part of the book with the most Java examples. While all readers are encouraged to read the initial sections of the SAX and DOM chapters, non-Java developers can completely skip Chapter 9, which covers JAXP and JDOM, as well as the code examples in the SAX and DOM chapters. However, be sure to read the explanation of parsing at the beginning of Chapter 7 and study the comparison, SAX vs. DOM vs. JDOM vs. JAXP—Who Wins at the end of Chapter 9. Part III: Displaying and Transforming XML —This part covers CSS (Cascading Style Sheets), XSLT (Extensible Stylesheet Language Transformations), XPath (XML Path Language), XSLFO (Extensible Stylesheet Language Formatting Objects), presented in Chapters 10 to 12. Of these, the lengthy Chapter 11 on XSLT and XPath is essential reading for anyone who wishes to display or transform XML into other formats (including HTML, XHTML, text, or other kinds of XML, particularly in e-commerce applications). Chapter 10 on CSS is more important if your XML display needs are more modest and your transformation needs are nil. The chapter can be skimmed for XML hooks if you are already familiar with CSS. Chapter 12 concerns XSL Formatting Objects, sort of the next generation CSS for desktop publishing quality layout, PDF, and targeting your output for different devices. The XSLFO chapter was contributed by noted XSL expert and instructor, G. Ken Holman, chair of the OASIS XSLTXPath Conformance Technical Committee (see his home page at http: ). Part IV: Related Core XML Specifications —This part focuses on XLink (XML Link Language) and XPointer (XML Pointer Language)—Chapters 13 and 14. Most developers will benefit from reading about XLink and XPointer because they greatly extend the notion of linking and fragment access beyond what is possible in HTML 4.01, including one-to-many links, multidirectional links, links stored external to the documents, and linking to specific elements without hooks being provided by the original author. Part V: Specialized XML Vocabularies —This part presents two unrelated XML-based languages: XHTML (Extensible HyperText Markup Language) in Chapter 15 and RDF (Resource Description Framework) in Chapter 16. Please consider Chapter 15 on XHTML as essential reading for all developers. As youll see, XHTML is its own nuclear family of specifications that is currently replacing HTML, especially in the increasingly popular world of handheld devices, voice browsers, and other alternative Web interfaces. RDF should be of particular interest to developers and scientists with an interest in metadata (data about data), site descriptions, catalogs, intelligent software agents, and so on. RDF attempts to add semantics to the Web; related concepts are the recent XML Topic Maps (XTM) effort and the older Dublin Core work. The RDF chapter was contributed by Ora Lassila, co-author of the Resource Description Framework Model and Syntax Specification for the W3C and contributor to the RDF Core Working Group and Web-Ontology (WebOnt) Working Group (see his home page at http: ). This book does not cover XQuery, an XML Query language, nor Scalable Vector Graphics (SVG), except in passing. XQuery was still very much in flux at the time of this writing. As for SVG, with a more than 500-page specification, I felt I could not do the topic justice in the time I had left after writing the rest of this book. Well, theres always the Second Edition, I guess. What You Need to Get the Most Out of This Book All code examples have been developed on a Dell Dimension XPS R450 PC (a paltry 450 MHz) running Windows 98. DOS .bat files are provided to help you configure your environment so that you can run the examples on your own. UNIX developers should be able to study the .bat files and set environment variables accordingly, such as CLASSPATH for Java and variables that point to the location of XML parsers and XSLT processors. Im afraid I cant say much to Mac developers at this point (sadly, my own ancient PowerMac 710080 hasnt been used for the better part of three years), but if you contact me via the Web site and want me to share your experiences with others, I will gladly do so. Ill give you credit and a free copy of this book—it makes a great gift and keeps its flavor longer than fruitcake. XML and DTD examples are plain text, so they are viewable in their raw form on all platforms using any text editor. To process XML in a browser, however, youll need the most current generation of browsers, such as Netscape 6.x, Internet Explorer 5.5 or 6.x, Amaya 5.x, or Opera 5.x or higher. If youre not the type of reader who has to try out every example in his or her own browser, then perhaps the many screenshots in this book will be sufficient. Evaluation copies of commercial XML, DTD and XML Schema editors appear on the CD that accompanies this book; XML parsers and XSLT processors also appear there. The CD also contains a page of links to the current versions of all provided software, as well as links to software that couldnt be included on the CD for a variety of reasons. The Java code examples should compile and run fine with either JDK 1.2.x or 1.3.x, also known by other confusing names and numbers such as Java 2 SDK, J2EE, and J2SE—or their equivalent as provided with your favorite Java IDE (Integrated Development Environment). This book does not attempt to teach Java; on the other hand, you really dont need to know Java to follow most of the discussions. Interested readers who desire a better Java background should refer to the key Java resources listed in For Further Exploration: HTML and Java that follows. I truly hope you enjoy this book and find the XML family of specifications as fascinating as I do. Conventions Used in This Book The typographic conventions used in this book are as follows: Glosssary terms look like this where they are defined: node-set Code excerpts, code listings, command lines, filenames, element names, and attribute names look like this: &lt;xsl:template match=CD&gt; or collection8.xml. Quotations (material excerpted from another source) is indented both left and right and is set in a smaller type size. Notes, important information or things to watch out for, are set off by an arrow in the margin and rules above and below their text. For Further Exploration: HTML and JavaDave Raggetts Getting Started with HTML http: Web Design Groups HTML 4.0 Reference http: Googles HTML Tutorials category http:directory.google.comTopComputersData_FormatsMarkup_LanguagesHTMLTutorials Java Technology Products and APIs http:java.sun.comproducts The Java Tutorial http:java.sun.comdocsbookstutorial Google Web Directory: Java includes a Books category http:directory.google.comTopComputersProgrammingLanguagesJava Google Web Directory: Java IDEs http:directory.google.comTopComputersProgrammingLanguagesJavaDevelopment_ToolsIntegrated_Development_Environments Cafe au Lait Java FAQs, News, and Resources http:}
}

@article{10.14778/3554821.3554836,
author = {Mishchenko, Andrey and Danco, Dominique and Jindal, Abhilash and Blue, Adrian},
title = {Blueprint: a constraint-solving approach for document extraction},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554836},
doi = {10.14778/3554821.3554836},
abstract = {Blueprint is a declarative domain-specific language for document extraction. Users describe document layout using spatial, textual, semantic, and numerical fuzzy constraints, and the language runtime extracts the field-value mappings that best satisfy the constraints in a given document.We used Blueprint to develop several document extraction solutions in a commercial setting. This approach to the extraction problem proved powerful. Concise Blueprint programs were able to generate good accuracy on a broad set of use cases. However, a major goal of our work was to build a system that non-experts, and in particular non-engineers, could use effectively, and we found that writing declarative fuzzy constraint-based extraction programs was not intuitive for many users: a large up-front learning investment was required to be effective, and debugging was often challenging.To address these issues, we developed a no-code IDE for Blueprint, called Studio, as well as program synthesis functionality for automatically generating Blueprint programs from training data, which could be created by labeling document samples in our IDE. Overall, the IDE significantly improved the Blueprint development experience and the results users were able to achieve.In this paper, we discuss the design, implementation, and deployment of Blueprint and Studio. We compare our system with a state-of-the-art deep-learning based extraction tool and show that our system can achieve comparable accuracy results, with comparable development time, for appropriately-chosen use cases, while providing better interpretability and debuggability.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3459–3471},
numpages = {13}
}

@article{10.1145/7902.214916,
author = {Frenkel, Karen A.},
title = {Special issue on parallelism},
year = {1986},
issue_date = {Dec. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/7902.214916},
doi = {10.1145/7902.214916},
abstract = {The articles presented in our Special Issue on parallel processing on the supercomputing scale reflect, to some extent, splits in the community developing these machines. There are several schools of thought on how best to implement parallel processing at both the hard- and software levels. Controversy exists over the wisdom of aiming for general- or special-purpose parallel machines, and what architectures, networks, and granularities would serve these best. The efficiency of processing elements that are loosely or tightly coupled is still in question. And, in the software amphitheatre, there is debate over whether new languages should be written from scratch, whether old languages should be modified into parallel processing dialogues, or whether investments in old, sequential programs should be leveraged by recompiling them into parallel programs.These issues were readily apparent at this year's International Conference on Parallel Processing (ICPP), as they have been during the 15 years since the annual conference first met. Few expect resolutions to these questions within the next 15 years, which is not to say that these subfields are not progressing rapidly; quite the contrary. As we now see, an outpouring of recent commercial unveilings of both super and supermini parallel processors represents the expected potpourri of design philosophies.Related to the general- versus special-purpose issue is the direct, applications-oriented question: What do parallel processor architects expect their machines to be used for? And, taking this a step further, what would they like to see next-generation machines, with one hundred times more speed and memory, do? I asked ICPP attendees and other computer scientists these questions. Many answered straightforwardly that they see simulation as the main application now and in the future. Others deflected the question. Said one such architect quite flatly, "I just build the things. You'd be better off asking users." Yes and no; I wanted to know what was on architects' minds. Why were their imaginations so dry? But then others plunged in quite daringly, answering at a completely different level as reported below. Perhaps the range of all responses reflects differences in priorities that are to be expected in a young field in flux. Some of their thoughts convey that they are somewhat overwhelmed by new choices and freedoms. It seems that the advent of parallelism may be more than just the beginning of a new era within computer science. "Historically, computer scientists have been unable to predict many of the uses of faster machines," says DARPA/Information Science Technology Office Program Manager Stephen H. Kaisler, "because each such machine opens up new regimes of computing to explore." Indeed, parallel computing itself—not just its promise of improved speed—is all at once exposing new, unforeseen possibilities: a wide vista of architectures, languages, and operating systems. "To date, we've been playing around with a very limited range of these, but now many novel, promising combinations are within our grasp," says George Adams, a research scientist at the Research Institute for Advanced Computer Science who is studying parallel processors for NASA Ames Research Center. Until recently, the technology was not advanced enough to create a machine with large numbers of processing elements, for example. 
Today, says Adams, cheap chips and improved communications permit running new permutations of languages and operating systems on such machines. These combinations were never before testable.According to Adams, the balance between CPU capability and memory should be of prime concern for next-generation parallel processors and supercomputers, which he predicts are on a collision course and will become one and the same. Scientists at Ames are most often limited not by CPU speed, he says, but by memory size. Because data space for problems involving simulation of physical systems cannot be held entirely in memory, portions must reside on disk. This causes wall-clock problem solution time to "suffer drastically," says Adams. Since disk access is typically 100,000 times slower than memory access, users prefer not to wait for a value to be retrieved. Instead, they often recalculate values even if hundreds of mathematical operations are involved. So, if a parallel supercomputer with two orders of magnitude more CPU speed and memory suddenly appeared, "these scientists would likely cheer," says Adams. "Then they would ask for (still) more memory."For Paul Castleman, president and CEO of BBN Advanced Computers, there is really no reason to limit CPU and memory increases for the next generation of general-purpose parallel processors to two orders of magnitude: "That's thinking relatively conservatively" for the next decade, he says. "We have in the works a prototype processor that is 1000 times faster than today's, and we are beginning work on configurations that are tens of thousands times faster." But a product's usability is the final determinant, says Castleman, "not the macho of how many more MIPS you can get . . . not whether you've souped up your sports car to run that much faster, but whether it feels comfortable to use." The solution is in the software development environment, which is why DEC and IBM have done so well, he says. Consequently, BBN Advanced Computers is now putting most of its effort into software tools for both scientific and nonscientific users. Graphics, for example, can further a user's understanding of many simultaneous processes—each using information from a common database—with graphs of processing elements' [PEs') results. An economist may watch one PE number crunching dollars flowing into consumption and another PE measuring capital accumulation: or a physical plant operator may observe calculations of pressure that is causing a tank to overflow while another PE handles variables affecting a chemical reaction. Besides being an aid to users, if graphics tools are also provided, each user's applications programmer would employ these utilities to generate the necessary aggregate graphics.But DARPA's Kaisler says that, in exploiting the first wave of commercially available parallel processors, little effort has been expended toward using these machines for research and development in computer science. "What is needed is a new effort, a new push to open up new avenues of algorithm development," he says, "beginning from first principles about what constitutes an algorithm and how to map it to the computational model provided by a specific parallel processor."The impact of commercial unveilings draws different if not diametrically opposed conclusions, however. A cautionary note about "hardware revelations" comes from David Gelernter, associate professor of computer science at Yale University. Hardware designers thus stricken will build machines from "the bottom up" that no one will know how to program, he says. Already, a dozen models have a dozen different low-level parallel programming systems. Moreover, Gelernter bemoans the fact that "machine-independent methods for parallel programming have been slow to emerge, and that consequently programmers have been forced to accommodate themselves to the machines rather than vice versa." He proposes that researchers imagine new kinds of programs before they imagine new kinds of machines. Once again the computer science equivalent of the age-old chicken-before-the-egg question arises. How far can hard- and software developments proceed independently? When should they be combined? Parallelism seems to bring these matters to the surface with particular urgency.The first article in our Special Issue is "Data Parallel Algorithms," by W. Daniel Hillis and Guy L. Steele, Jr. These authors, from Thinking Machines Corporation, discuss algorithms and a new programming style for fine-grained single instruction multiple data (SIMD) parallel processors like the Connection Machine®. They cite examples such as parsing and finding the ends of linked lists—problems that they had assumed were inherently sequential—as milestones in their transition from "serial to parallel thinkers."The next article, "Advanced Compiler Optimizations for Supercomputers," is by David A. Padua and Michael J. Wolfe, of the University of Illinois and Kuck and Associates, respectively. They represent those who believe sequential algorithms should be recompiled to accommodate vector, concurrent, and multiprocessor architectures. In a discussion of data dependence testing in loops, they show how parallelism in sequential codes for operations, statements, and iterations can be automatically detected for vector supercomputers. Further, they discuss improving data dependence graphs and optimizing code for parallel computers.In a more theoretical vein, Robert Thomas and Randall Rettberg of BBN Advanced Computers discuss contention, or "hot spots," the phenomenon that some have predicted might cripple certain parallel processors. In their article, "Contention Is No Obstacle to Shared-Memory Multiprocessing," the authors describe engineering approaches to controlling the backup of data in networks and switches. Besides reporting specific methods used to control contention, they offer benchmarks on their own machine, the Butterfly™.Two applications-oriented articles complete the set. "Toward Memory-Based Reasoning," by Craig Stanfill and David Waltz, suggests that memory of specific events, rather than of rules like those used in expert systems, is the right foundation for intelligent machines. In "Parallel Free-Text Search on the Connection Machine System," Craig Stanfill and Brewster Kahle harness unique properties of massive parallelism to implement a successful document-retrieval search paradigm. They use simple queries and relevance feedback techniques to produce intriguing results on a Reuters news database of 16,000 stories.},
journal = {Commun. ACM},
month = {dec},
pages = {1168–1169},
numpages = {2}
}

@article{10.1016/j.jvlc.2014.10.017,
author = {Chambers, Christopher and Scaffidi, Christopher},
title = {Utility and accuracy of smell-driven performance analysis for end-user programmers},
year = {2015},
issue_date = {February 2015},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {26},
number = {C},
issn = {1045-926X},
url = {https://doi.org/10.1016/j.jvlc.2014.10.017},
doi = {10.1016/j.jvlc.2014.10.017},
abstract = {This paper proposes a technique, called smell-driven performance analysis (SDPA), which automatically provides situated explanations within a visual dataflow language IDE to help end-user programmers to overcome performance problems without leaving the visual dataflow paradigm. An experiment showed SDPA increased end-user programmers' success rates at finding performance problems and decreased the time required for finding solutions. Another study, based on using SDPA to analyze a corpus of example end-user programs, revealed that it is usually accurate at identifying performance problems. Based on these results, we conclude that SDPA provides a reliable basis for helping end-user programmers to troubleshoot performance problems, as well as a potential foundation for future work aimed at training users and at aiding code reuse. Smell-driven performance analysis (SDPA) finds dataflow performance problems.SDPA provides situated explanations within the visual dataflow language.We present an extended form of the technique that incorporates runtime profiling.In a user study, participants could more easily diagnose performance problems.A second study confirmed that profiling improves accuracy.},
journal = {J. Vis. Lang. Comput.},
month = {feb},
pages = {1–14},
numpages = {14},
keywords = {End-user programming, Performance, Visual language}
}

@inproceedings{10.1145/3510454.3516832,
author = {Young, Mitchell and Nan, Zifan and Shen, Xipeng},
title = {IDE augmented with human-learning inspired natural language programming},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3516832},
doi = {10.1145/3510454.3516832},
abstract = {Natural Language (NL) programming, the concept of synthesizing code from natural language inputs, has garnered growing interest among the software community in recent years. Unfortunately, current solutions in the space all suffer from the same problem, they require many labeled training examples due to their data-driven nature. To address this issue, this paper proposes an NLU-driven approach that forgoes the need for large numbers of labeled training examples. Inspired by how humans learn programming, this solution centers around Natural Language Understanding and draws on a novel graph-based mapping algorithm. The resulting NL programming framework, HISyn, uses no training examples, but gives synthesis accuracies comparable to data-driven methods trained on hundreds of samples. HISyn meanwhile demonstrates advantages in terms of interpretability, error diagnosis support, and cross-domain extensibility. To encourage adoption of HISyn among developers, the tool is made available as an extension for the Visual Studio Code IDE, thereby allowing users to easily submit inputs to HISyn and insert the generated code expressions into their active programs. A demo of the HISyn Extension can be found at https://youtu.be/KKOqJS24FNo.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {110–114},
numpages = {5},
keywords = {code editor, natural language programming, program synthesis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1109/IECON.2019.8927573,
author = {Oltean, Gabriel and Oltean, Victor and Balea, Horea Alin},
title = {Method for Rapid Development of Arduino-based Applications Enclosing ANN},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IECON.2019.8927573},
doi = {10.1109/IECON.2019.8927573},
abstract = {The implementation of stand-alone applications enclosing artificial neural networks (ANNs) running on low-cost platforms tends to be prohibitive, primarily due to the demand for (very) large computational resources. This paper proposes a very effective method to solve this kind of problem. We rely on the existing approach of first configuring and training the ANN on a platform offering the necessary computing power, memory, and appropriate software tools, and then implementing the ANN model on the host platform. In our method, the code for the ANN model is automatically generated and integrated into the final application. The MATLAB environment, running on a PC, plays the roles of both a training platform and a code generator. The Arduino IDE is used for automatic code integration, while Arduino development boards provide the host for the stand-alone application. Some experimental tests confirm the proper operation of our approach. Quite a large ANN can be simulated on low-cost Arduino boards: Uno can accommodate an architecture with 315 parameters (83% utilization of 2KB SRAM) while Mega can accommodate an architecture with 1805 parameters (94% utilization of 8KB SRAM).},
booktitle = {IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society},
pages = {138–143},
numpages = {6},
location = {Lisbon, Portugal}
}

@article{10.1145/77556.77557,
author = {Frenkel, Karen A.},
title = {The European community and information technology},
year = {1990},
issue_date = {April 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/77556.77557},
doi = {10.1145/77556.77557},
abstract = {The world has watched Eastern Europe erupt into such political turmoil that historians are expected to call this period the Revolutions of 1989. Economic evolution was also underway as the Continent progressed toward a single European market. The goal—a market without national borders or barriers to the movement of goods, services, capital and people—was first outlined over 30 years ago by the 12 countries which became members of the Common Market. In the mid 1980s, the effort was renewed when these same countries approved an ambitious plan outlining hundreds of legislative directives and policies that would harmonize and re-regulate those of the member states. The measures are drafted by the European Commission, voted on   by the Council of Ministers, amended if necessary, and then assigned budgets by the Parliament. They include competition law, labor law, product regulation and standardization, taxation and subsidies, and quota and tariff guidelines. In 1987, the Single European Act created a timetable for the passage of legislation with a formal deadline for the removal of barriers by December 31, 1992, hence the term Europe '92 (EC '92). But many have described EC '92 as a process that will continue throughout the 1990s. The ouster of communist leaderships throughout Eastern Europe, however, has raised unexpected questions about the participation of the Eastern countries, and this could alter or delay the process.Nevertheless, the changes have begun and are taking place during the Information Revolution. It is therefore natural to ask what impact EC '92 will have on the computer industry. Inevitably, several of the directives and policies relate primarily, and many secondarily, to information technology. Table 2 lists the policies in effect and those being proposed. In the following pages, Communications presents several points of view regarding the impact of EC '92 on the information technology market in Europe.As of July 1988, the European information systems market was estimated at $90 billion by Datamation magazine and is expected by many to be the fastest growing market this decade. But during the last ten years, European-based computer companies have had difficulty keeping pace with American and Japanese firms. In 1988, European companies managed only a 20 percent market share on their own turf, according to market researcher International Data Corporation. Not much had changed since 1982 when their market share was 21 percent. As reported in the Wall Street Journal last January, European computer companies have been hindered by lack of economies of scale, narrow focus on national markets, and difficulty in keeping pace with Japanese and IJ.S. product innovations. But the occasion for the Journal article was the news that Germany's Siemens AG was merging with the ailing Nixdorf Computer AG. The result would possibly be the largest computer company based in Europe, and the sixth or seventh largest in the world. And in October of 1989, France's Groupe Bull announced the purchase of Zenith Electronics Corporation's personal computer unit. Bull claimed that it would become the sixth largest information service company in the world. Such restructurings have been predicted with the approach of EC '92, as corporate strategies would begin to take into account directives and trade rules regarding the computer and telecommunications industries. Smaller European and American computer companies are anticipating battle with giants like IBM and DEC, which have long-established European divisions or subsidiaries. IBM has been the leader in mainframes, minicomputers, and personal computers, but it is expected that all computer companies, European-based or not, will face greater competition in Europe.The Netherlands' NV Philips, the largest European semiconductor and consumer electronics company, says it has been preparing for EC '92 since the 1970s. And North American Philips Chairman Gerrit Jeelof has claimed company credit for initiating the 1987 European Act. In a speech delivered at a Business Week and Foreign Policy Association Seminar last May, Jeelof said that while American companies had forsaken consumer electronics, Philips and France's Thompson have held their own against the Japanese. But he indicated that American dominance of the European semiconductor market was a major impetus for EC '92. Jeelof said:
. . . because of the lack of European strength in the field of computers, the integrated circuits business in Europe is dominated by Americans. Europe consumes about 34 percent of all ICs in the world and only 18 percent are made in Europe by European companies. The rest are made by American companies or are imported. It is not a surprise then that in 1984 we at Philips took the initiative to stimulate a more unified European market. At the time, we called it Europe 1990. Brussels thought that 1990 was a bit too early and made it 1992. But it has been the electronics industry in Europe together with other major companies, that have been pushing for Europe 1992. Why did we want it? We wanted a more homogeneous total market in Europe and, based on that, we wanted to become more competitive. The process is on its way and obviously we see some reactions. If you take action, you get reaction.One reaction has been concern on the part of non-European companies and their governments that the EC is creating a protectionist environment, a “Fortress Europe.” As walls between nations are coming down, some fear that other more impenetrable ones are going up on the Continent's edges. Jeelof argues against this perception in another speech, “Europe 1992—Fraternity or Fortress,” reprinted in this issue in its entirety.Communications also presents an analysis of several trade rules relating to semi-conductors in “The Semiconductor Market in the European Community: Implications of Recent Rules and Regulations,” by Roger Chiarodo and Judee Mussehl, both analysts in the Department of Commerce Office of Microelectronics and Instruments. The authors outline the consequences of Europe's Rules of Origin, anti-dumping measures that are supposed to prevent companies from using assembly operations in an importing country to circumvent duty on imported products. In the United States, if the difference between the value of parts or components from the dumping country and the value of the final product is small, then duty will be placed on those parts or components used in U.S. assembly operations. By contrast, the EC rule says that if the value of parts or components exceeds 60 percent of the value of all parts and materials, then duty will be placed on those parts and materials upon assembly in Europe. Since 1968, origin was also determined according to “the last substantial process or operation” resulting in the manufacture of a new product. In the case of printed circuit boards, some countries interpreted this as assembly and testing, while others thought it meant diffusion. In 1982, the EC began harmonizing these interpretations, and as of 1989, the last substantial operation was considered diffusion: the selective introduction of chemical dopants on a semiconductor substrate. As a result, American and Japanese semi-conductor manufacturers have spent millions building foundries on European soil. To reveal the Japanese interpretation of such changes, Japanese Commerce Minister Eiichi Ono, with the Japanese Embassy in Washington, DC, expresses his country's impressions of EC '92 in this issue. In his speech, “Japan's View of EC '92,” delivered at an Armed Forces Communications and Electronics Association (AFCEA) conference on Europe '92, Ono states that while the EC's intentions might not be protectionist, they could 
become so upon implementation. His discussion focuses on semi-conductors and technology transfer issues.Although not a formal directive, in July 1988, the European Council decided to promote an internal information services market (the last “L” document in Table 2). To present the reasoning and objectives behind this initiative, we reprint the Communication from the Commission to the Council of Ministers, “The Establishment at Community Level of a Policy and a Plan of Priority Actions for the Development of an Information Services Market,” and the resulting July 1988 “Council Decision” itself. Funds allocated for 1989 and 1990 are approximately $36 million, $23 million of which was slated for a pilot/demonstration program called IMPACT, for Information Market Policy Actions. This may seem a pittance in comparison to the programs of other governments, but this Decision and other EC legislation are the first steps toward an EC industrial policy. Recognizing that Europe's non-profit organizations and the public sector play a very important role in providing database services, in contrast to the U.S. where the private sector is now seeding the production of such database services, IMPACT has prepared guidelines to help the public sector cooperate with the private sector in marketing information. These guidelines would also allow private information providers to use public data and add value to it to create commercial products. IMPACT is providing incentives to accelerate innovative services for users by paying 25 percent of a project's cost. After the first call for proposals, 16 of 167 projects proposed by teams composed of 300 organizations were funded. American-based companies can apply for funds if they are registered in Europe. Unlike the U.S., the EC allows registration regardless of who owns a company's capital. Projects funded are to develop databases that would be accessible to all members of the Community either on CD-ROM or eventually on a digital network, an ISDN for all Europe, as planned by the fifth recommendation listed in Table 2. One project in the works is a library of pharmaceutical patents on
 CD-ROM that will enable users to locate digitized documents. Users will also have direct access to on-line hosts for all kinds of patents. A tourism information database and a multi-media image bank of atlases are other pilot projects chosen, and another project will provide information on standards. Eventually, audiotext might be used to retrieve data by telephone instead of a computer terminal. When the initial projects have been completed, the Commission will inform the market place about the results of the implementation. Plans for a five-year follow-up program, IMPACT-2 are also under discussion.These projects depend to some extent on the implementation and passage of directives or the success of larger and better funded projects. On-line access to databases depends on the recommendation for an ISDN as well as on the standardization directive for information technology and telecommunications. The certification, quality assurance, and conformity assessment issues involved in that directive are too numerous and important to just touch on here and will be covered in a later issue of Communications. To make these databases accessible not only technically, but also linguistically, the EC has funded two automatic language translation projects called Systran and Eurotra. Systran is also the name of the American company in La Jolla, CA, known for its pioneering work in translation. In conjunction with the EC, Systran Translation Systems, Inc., has completed a translation system for 24 language pairs (English—French, French—English, for example, are two language pairs) for the translation of IMPACT- funded databases. The system resides on an EC mainframe; there will be on-line access by subscription; and it will also be available on IBM PS/2s modified to run VMS DOS. It is already on France's widespread Minitel videotext network.As this practical, market-oriented approach to technology implementation is beginning, Europe's cooperative research effort, ESPRIT, is also starting to transfer its results. Last year, the second phase, ESPRIT II, set up a special office for technology transfer. Its mission is to ensure the exploitation, for the benefit of European industry, of the fruits of the $1.5 billion ESPRIT I program that began in 1984, as well as the current $3.2 billion program (funding through 1992). The EC contributes half of the total cost, which is matched by consortia comprised of university and industry researchers from more than one country. About 40 percent of ESPRIT II's funds will be devoted to computer related-technologies.Every November, ESPRIT holds a week-long conference. Last year for the first time it devoted a day to technology transfer. Several successful technology transfers have occurred either from one member of the program to another or out of the program to a member of industry that had not participated in the research. An electronic scanner that detects and eradicates faults on chips, for example, was developed by a consortium and the patents licensed by a small company. This automatic design validation scanner was co-developed by CSELT, Italy, British Telecom, CNET, another telecom company in France, IMAG, France, and Trinity College, Dublin. The company that will bring it to market is ICT, Gmbh, a relatively small German company. It seems that in Europe, as in the United States, small companies and spin-offs like those found in the Silicon Valley here, are better at running quickly with innovative ideas, says an EC administrator.Another technology transfer success is the Supernode computer. This hardware and software parallel processing project resulted in an unexpected product from transputer research. The Royal Signal Radar Establishment, Inmos, Telmat, and Thorn EMI, all of the UK, APTOR of France, and South Hampton University and the University of Grenoble, all participated in the research and now Inmos has put the product on the market.Three companies and two universities participated in developing the Dragon Project (for Distribution and Reusability of ADA Real-time Applications through Graceful On-line Operations). This was an effort to provide effective support for software reuse in real-time for distributed and dynamically reconfigurable systems. The researchers say they have resolved the problems of distribution in real-time performance and are developing a library and classification scheme now. One of the companies, TXT, in Milan, will bring it to market.Several other software projects are also ready for market. One is Meteor, which is aimed at integrating a formal approach to industrial software development, particularly in telecommunications. The participants have defined several languages, called ASF, COLD, ERAE, PLUSS, and PSF for requirements engineering and algebraic methods. Another project is QUICK, the design and experimentation of a knowledge-based system development tool kit for real-time process control applications. The tool kit consists of a general system architecture, a set of building modules, support tools for construction, and knowledge-based system analysis of design methodology. The tool kit will also contain a rule-based component based on fuzzy logic. During the next two years, more attention and funds will be indirectly devoted to technology transfer, and the intention to transfer is also likely to be one of the guides in evaluating project proposals.Some industry experts maintain that high technology and the flow of information made the upheaval in Eastern Europe inevitable. Leonard R. Sussman, author of Power, the Press, and the Technology of Freedom: The Coming Age of ISDN (Freedom House, 1990), predicted that technology and globally linked networks would result in the breakdown of censorious and suppressive political systems. He says the massive underground information flow due to books, copiers, software, hardware, and fax machines, in Poland for example, indicates that technology can mobilize society. Knowing that computers are essential to an industrial society, he says, Gorbachev faced a dilemma as decentralized computers loosened the government's control over the people running them. Glasnost evolved out of that dilemma, says Sussman.Last fall, a general draft trade and economic cooperation accord was signed by the European Commission and the Soviet Union. And both American and Western European business interests are calling for the Coordinating Committee on Multilateral Export Controls (COCOM) to relax high technology export rules to the Eastern Bloc and the Soviet Union. The passage of that proposal could allow huge computer and telecommunications markets to open up. And perhaps the Revolutions of 1989 will reveal themselves to have been revolutions in communication and the flow of information due in part to high technology and the hunger for it.},
journal = {Commun. ACM},
month = {apr},
pages = {404–410},
numpages = {7},
keywords = {regulation, standards, statistics, transborder data flow}
}

@inproceedings{10.1007/978-3-030-90235-3_32,
author = {Mohamed Salleh, Faridah Hani and Dewi, Deshinta Arrova and Liyana, Nurul Azlin and Md Nasir, Naziffa Raha},
title = {A Model for Teaching and Learning Programming Subjects in Public Secondary Schools of Malaysia},
year = {2021},
isbn = {978-3-030-90234-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90235-3_32},
doi = {10.1007/978-3-030-90235-3_32},
abstract = {In recent years, more and more countries have included programming as one of the subjects in the national education curriculum. However, comparatively less attention has been paid to reviewing the methodologies and tools, according to our observations. This paper aims to review methods and tools that have been applied in higher education levels and identify the most effective one to be applied in teaching and learning programming in high schools. The possible methods to be applied in high schools are highly dependent on the education landscape of the country itself. Therefore, the methods proposed in this paper are identified by considering education issues in Malaysia such as language of communication, digital divide and schools’ teaching and learning time. We conducted an interview with the teachers and students to identify the real problems of teaching and learning programming in Malaysia public secondary schools. From the interview and extensive review of literature, possible model elements have been identified. We found that teaching and learning programming at high school level should incorporate the following main features; incorporating computational thinking, IDE-centric learning, relation to life-example, reiterative method and spaced exercise, effective questioning, support multi-language and self-study. However, all of these recommendations should be studied for their effectiveness by conducting a detail testing. Thus, we conducted an expert evaluation by using a learning management system (LMS) that we created specifically to represent our suggested model components. The findings gathered from the expert evaluation confirms on the needs to give high priority to the following model components; reiterative and chunking, effective questioning, designing instructional materials, followed by adaptive learning, language and self-study. The components identified during the research process that are worthwhile to continue to prove their level of efficiency are AI, support think-pair, competition-based, gamification, mobile friendly and low usage of system resources (small memory footprint or RAM usage and low CPU usage). It is hoped that our model can be adopted by public secondary schools in Malaysia to produce the best tools or methods for teaching programming. Finally, we discuss the implications of our findings and suggest future research directions that could develop a more holistic understanding of this pedagogical technique.},
booktitle = {Advances in Visual Informatics: 7th International Visual Informatics Conference, IVIC 2021, Kajang, Malaysia, November 23–25, 2021, Proceedings},
pages = {362–373},
numpages = {12},
keywords = {Programming, Secondary schools},
location = {Kajang, Malaysia}
}

@article{10.1016/j.advengsoft.2022.103292,
author = {Brindha Merin, J and Aisha Banu, W},
title = {An efficient web service annotation for domain classification and information retrieval systems using HADLNN classifier},
year = {2022},
issue_date = {Dec 2022},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {174},
number = {C},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2022.103292},
doi = {10.1016/j.advengsoft.2022.103292},
journal = {Adv. Eng. Softw.},
month = {dec},
numpages = {12},
keywords = {Web Service Description Language (WSDL), Hadoop Distributed File System (HDFS), Hybrid Artificial Deep Learning Neural Network (HADLNN), Cat swarm Optimization (CSO) and Modified K-means (MK-means)}
}

@phdthesis{10.5555/2019765,
author = {Mulhern, Anne},
advisor = {Fischer, Charles and Liblit, Ben},
title = {Polytypic proving},
year = {2010},
isbn = {9781124220956},
publisher = {University of Wisconsin at Madison},
address = {USA},
abstract = {Formal methods are a class of techniques for automatically verifying software correctness. They are a common topic in computer science research. However, they are less well-known in software development and in undergraduate computer science education. As society's reliance on software increases so does the potential severity of the consequences of software failure. Formal methods are the surest way of guaranteeing software correctness. Moreover, the study of formal methods leads to better informal understanding of software correctness and thus to better software. Unfortunately, the study and use of formal methods can be difficult and this impedes their adoption in software engineering and the computer science curriculum. Coq is an automated proof-assistant for developing certified programs and proofs. It is powerful and expressive and has been used in a large variety of significant developments. Experienced developers are accustomed to picking up new languages rapidly. Unfortunately, it is difficult for a new user to learn the use of any proof-assistant with anything like the same rapidity. Furthermore, developers have become accustomed to sophisticated IDEs with tools for refactoring and for auto-generating code. Very little such support is available for proof-assistants. Coq suffers from both these drawbacks. Our work addresses these problems in several ways. We provide two alternative approaches that facilitate general induction and recursion. We demonstrate methods for enhancing structural recursion principles to increase their generality and transparency. We demonstrate graphical techniques for improved proof visualization and an impact analysis tool for predicting the consequences of a proof refactoring.},
note = {AAI3424020}
}

@inproceedings{10.1145/3287324.3293851,
author = {Day, Melissa and Gonzalez-Sanchez, Javier},
title = {A Neural Network Model for a Tutoring Companion Supporting Students in a Programming with Java Course},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293851},
doi = {10.1145/3287324.3293851},
abstract = {With large class sizes and instructors who may not be equipped to assist struggling students, many students abandon the field, deeming it to be too difficult and not for them. Consistent, constructive, supportive feedback through a Tutoring Companion can scaffold the learning process for students. This poster describes a reasoning model, using neural networks techniques, for a tutoring companion embedded into the Eclipse IDE. The companion provides support for students in a first-year university Java programming course. The companion collects data from students' events and programming assignments, analyzes it for relevant trends, and estimates each student's situation. The input data for the neural network comes from areas with which beginning computer science students often struggle, such as the presence of important keywords and the amount of time spent in a state with errors. Then, it determines the feedback to be provided for students to overcome a detected challenging situation, providing both hints on how to fix the problem with the code, as well as encouragement to help keep students motivated and learning. The effectiveness of the approach is examined among first-year computer science students through the completion of recursion and control flow programming assignments. The students complete surveys regarding their learning experience to assist in evaluating the companion's pedagogical effectiveness, which is discussed with an emphasis on the value of feedback provided.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1268},
numpages = {1},
keywords = {computer science education, eclipse ide, neural networks, programming tutoring, teaching programming, tutoring companion},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@book{10.5555/516065,
author = {Hewitt, Eben},
title = {Core Coldfusion 5.0 with Cdrom},
year = {2001},
isbn = {0130660612},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Book: Preface Core ColdFusion 5 is the book I wish I'd had when I began writing applications. One way of teaching programming is for the author to entice you, gentle reader, into the Technicolor landscape of the programming language at handthe author is your friendly guide, pointing out the dazzling array of colors and scents, while you, the Maui-shirted, floppy-hatted tourist, peer dimly into your map as you stand gawking dead center at the intersection of 42nd and Broadway. You get your required caramel milkshake at Ellen's Stardust Diner and move on to stand in line for Cats tickets. Or something like that. This is a different kind of book. I learned ColdFusion on the flydevouring websites and books devoted to the topic, nosing about in newsgroups and loitering on lists while I tried to get my first applications up and running. I learned by reading, by asking endless questions, by trying things over and over until something clicked. I learned from the generosity of hundreds of programmers it has been my pleasure to work with. I don't view it as an academic subject. This learning process was all very glamorous, but I think I wasted a lot of time that I didn't need to waste. The approach is this: programmers are, above all, problem solvers. That's what they get paid to do. Therefore, the language is presented as a set of tools that are available to you to solve business problems. The code you will work with will consist of numerous non-trivial, practical examples that you can build on and incorporate into your own websites. My belief is that you can learn best how it all fits together by seeing it in action, working together. Forinstance, it's fun to learn about cookies, and it's great to know that by using the CFLOCATION tag in your templates you can quickly jump to another URL. This is the sort of thing one learns as a programming tourist. Only when discussing a real-world application where all the parts have to fit together do you discover that, because of HTTP headers, you can't effectively set a cookie in the same template that you use CFLOCATION to get out of. But that's exactly the sort of thing you need to know when you sit down to write ColdFusion. You are a person in the world where people live. This indicates a certain relationship with space and time. This book was designed with these key factors in mind. There is only so much space in a book, so what you've got in your hands is the set of tools with which to continue on your own: There's the complete tag and function reference. There are lists of dozens of websites where you can get ColdFusion code, find more information on related topics, discover hundreds of places where you can host your ColdFusion website, and more. You get a list of the common ColdFusion errors you'll run into when debugging your applications, what they mean, and how to fix them. You put together a ColdFusion application for Web-enabled cell phones and a working e-commerce site, complete with rotating banner ads. The idea is to save your walls from the repeated impact of your head. This book will give you everything you need to get up and running quickly, effectively, and with an understanding of the real-world implications of developing with ColdFusion. The remainder of this preface briefly illuminates key things you will need to start using this book, and ColdFusion, quickly. Who You Are This book was written for anybody who wants practical knowledge of how to make Web applications on an easy, scalable, powerful platform. You know HTML already. You do not need to know another application programming language. You do not need knowledge of Web servers or data servers. Maybe you know ASP or JSP and want to learn another language. This book is perfect for you. Maybe you just took an IT job at Symantec or Doctor Solomon's or Bank of America or the University of Utah or the Recording Industry of America. Or maybe you were just elected Senator of your fine state, and, while browsing at http://www.Senate.gov noticed the little ".cfm" extension on your website and wondered what it was. All of the above organizations entrust their Web transactions to ColdFusion. And with good reason. This book was also written for Web developers and for people who have done a static website or two and want to make their next one dynamic. The ability to create an online store, to interact with data warehouses via enterprise-level features such as stored procedures and data probing, lets you get really serious. The ability to personalize your website, catering its content to each individual visitor, will allow you to create truly compelling relationships with your users. The ability to search and share documents in just about any format will allow your organization to stay ahead of the information cycle. Project managers and sales engineers will benefit from this book's discussions of planning an enterprise-level website and making all of the components come together. Despite a seemingly universal predilection toward Mountain Dew, Web programmers are a diverse species. Whether you are a pleasantly dressed co-ed working in a college computer lab with well-modulated air, or if you're grinding out 18-hour days in a high-tech job shop and have recently started looking like the long-lost fourth member of ZZ Top, or whether you're all by yourself in your basement playing gladiator with IIS and scramming the cat from chewing your cables, this is the right book for you. What You Need You will need several things to work successfully through the exercises in this book. You will need a computer with a text editor. You will need access to ColdFusion Server 5 software, a Web server, and an Internet connection. A 30-day evaluation version of ColdFusion 5 comes on the CD-ROM with the book, Notepad or Pico comes with your OS, and Apache is free, so you're in pretty good shape. What You Get This is a practical book. Its purpose is to answer the questions you need answered by showing you how everything works together (HTTP 1.1 headers, the Web server, your applications, conditional logic, and so forth). Some of this may be old hat to you. I have therefore tried to flag you about beginner material that you may want to skip. Here is what you get: A clear, detailed explanation of the ColdFusion language and how you can leverage it to build fantastic Web applications. Information, tips, and tricks about becoming a ColdFusion Certified Developer. Thousands of lines of complete, working code. On the CD-ROM, you get 30-day evaluation versions of Macromedia ColdFusion Enterprise Server 5.0 for Windows, HP-UX, Sun Solaris, and Linux; ColdFusion Studio 4.5.1 optional development environment for creating ColdFusion templates); HomeSite 4.5.1, Macromedia's award-winning HTML editor; The Harpoon Flash Toolkit; ColdFusion Express, the nonexpiring, limited-functionality product for serving ColdFusion pages; JRun 3.0.1, the award-winning server for Java Server Pages, Servlets, and Enterprise Java Beans; Macromedia Spectra 1.5 for Windows and Sun Solaris, the packaged application solution for content management, e-commerce, and personalizationwritten in ColdFusion! A website companion to the book filled with even more ColdFusion resources. The website is located at http://www.CoreColdFusion.com. Check out http://www.conditionallogic.com as well. How This Book Is Organized This book begins where HTML left off. We start with an overview of how the Internet works and an overview of what ColdFusion does. We then move into a discussion of SQL (the Structured Query Language), the language used to create and manipulate databases and their data. If you have a solid understanding of relational databases and SQL, you can probably skim Chapters 9 and 10. Each chapter will have roughly the same structure. You will be introduced to the key concepts in a general way, and quickly move into details as they pertain to ColdFusion or application development. You'll come out of most chapters with a working example that you can take with you. Because each chapter builds on the last, it is a good idea to read the book from beginning to end. If you're used to programming in another Web application language such as PHP, you might want to at least skim the introductory chapters on the World Wide We HTTP, as well as the SQL chapters, just so I don't make unfortunate assumptions. As the book continues, and you've got the key concepts and a number of tags and functions under your belt, we'll start putting together bigger applications with a number of pieces that have to fit together. That is the only way you can learn how to work with ColdFusion in large-scale websites. Many of the examples in this book were created with the ColdFusion Application Server running on a Windows 2000 with IIS 5. The database used in most cases is Microsoft SQL Server 2000. Oracle 8i is used in some examples. Access has generally been eschewed because, while it is inexpensive and therefore easier to get hold of, it is desktop software and unsuited to a production environment. You can make most of the examples in this book work with Access or Paradox with little or no modification. You can even use Excel spreadsheets and plain text files with some modification. That's a fabulous aspect of ColdFusion, and just one demonstration of its flexibility. However, I try to discourage the use of desktop database software with ColdFusion for applications that are likely to have more than a couple of concurrent connections or any frequency of use. About the Website If you point your browser to http://www.CoreColdFusion.com, you will discover the companion website to this book. It is meant to provide you with a resource for taking your applications to the next level and continuing your work. Some features of the website include: Databases, files, and code from the book Code for creating the website Enhancements and expansions to discussions i book Links to ColdFusion hosting providers Resource links for ColdFusion User Groups and the most useful ColdFusion sites on the Web Information about becoming a Macromedia ColdFusion Certified Developer A newsletter with product and informational updates on ColdFusion and Spectra A forum for reader feedbacktell me what you think! A schedule of ColdFusion and Spectra training seminars happening every month, all over the United States Fun things like the ColdFusion challengea compendium of tips and tricks Security bulletins If I've done my job, this book will teach you what you need to know about ColdFusion in order to go do it in the real world. Hopefully it will also prove useful long after you know what you're doingit has been organized to serve also as a reference. It's got the complete language in it, updated for ColdFusion 5. And it's got working examples that represent the most commonly needed tasks in Web programming today. You can give me feedback at writer@CoreColdFusion.com. I welcome your comments and suggestions for future editions. Thank you for picking this up. I really hope you like it.}
}

@book{10.5555/3202541,
author = {Vasic, Milos},
title = {Mastering Android Development with Kotlin: Deep dive into the world of Android to create robust applications with Kotlin},
year = {2017},
isbn = {1788473698},
publisher = {Packt Publishing},
abstract = {Key Features Leverage specific features of Kotlin to ease Android application developmentAn illustrative guide that will help you write code based Kotlin language to build robust Android applicationsFilled with various practical examples build amazing Android project using Kotlin so you can easily apply your knowledge to real world scenarios Book Description Kotlin is a programming language intended to be a better Java, and it's designed to be usable and readable across large teams with different levels of knowledge. As a language, it helps developers build amazing Android applications in an easy and effective way. This book begins by giving you a strong grasp of Kotlins features in the context of Android development and its APIs. Moving on, youll take steps toward building stunning applications for Android. The book will show you how to set up the environment, and the difficulty level will grow steadily with the applications covered in the upcoming chapters. Later on, the book will introduce you to the Android Studio IDE, which plays an integral role in Android development. Well use Kotlins basic programming concepts such as functions, lambdas, properties, object-oriented code, safety aspects, type parameterization, testing, and concurrency, which will guide you through writing Kotlin code into production. Well also show you how to integrate Kotlin into any existing Android project. What you will learn Understand the basics of Android development with KotlinGet to know the key concepts in Android development See how to create modern mobile applications for the Android platform Adjust your applications look and feel Know how to persist and share application databaseWork with Services and other concurrency mechanisms Write effective tests Migrate an existing Java-based project to Kotlin About the Author Milo Vasi is a software engineer, author, and open source enthusiast. He holds a bachelor's degree in the programming of computer graphics and a master's degree in the field of Android programming; both degrees were gained at Singidunum University. He published his first book, Fundamental Kotlin, in October 2016, thus achieving his dream of becoming an author. He's currently employed at the Robert Bosch company, where he's working on SDKs for the auto-industry. When he is not working on new books, Milo works on his open source projects.}
}

@article{10.1109/TCBB.2022.3168676,
author = {Rashid, Shamima and Sundaram, Suresh and Kwoh, Chee Keong},
title = {Empirical Study of Protein Feature Representation on Deep Belief Networks Trained With Small Data for Secondary Structure Prediction},
year = {2022},
issue_date = {March-April 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3168676},
doi = {10.1109/TCBB.2022.3168676},
abstract = {Protein secondary structure (SS) prediction is a classic problem of computational biology and is widely used in structural characterization and to infer homology. While most SS predictors have been trained on thousands of sequences, a previous approach had developed a compact model of training proteins that used a &lt;bold&gt;C&lt;/bold&gt;-&lt;bold&gt;A&lt;/bold&gt;lpha, C-&lt;bold&gt;B&lt;/bold&gt;eta &lt;bold&gt;S&lt;/bold&gt;ide Chain (&lt;bold&gt;CABS&lt;/bold&gt;)-algorithm derived energy based feature representation. Here, the previous approach is extended to Deep Belief Networks (DBN). Deep learning methods are notorious for requiring large datasets and there is a wide consensus that training deep models from scratch on small datasets, works poorly. By contrast, we demonstrate a simple DBN architecture containing a single hidden layer, trained only on the CB513 dataset. Testing on an independent set of G Switch proteins improved the Q&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$_{3}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="rashid-ieq1-3168676.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; score of the previous compact model by almost 3%. The findings are further confirmed by comparison to several deep learning models which are trained on thousands of proteins. Finally, the DBN performance is also compared with &lt;bold&gt;P&lt;/bold&gt;osition &lt;bold&gt;S&lt;/bold&gt;pecific &lt;bold&gt;S&lt;/bold&gt;coring &lt;bold&gt;M&lt;/bold&gt;atrix (&lt;italic&gt;PSSM&lt;/italic&gt;)-profile based feature representation. The importance of (i) structural information in protein feature representation and (ii) complementary small dataset learning approaches for detection of structural fold switching are demonstrated.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {apr},
pages = {955–966},
numpages = {12}
}

@book{10.5555/3181138,
author = {Abedin, Jaynal},
title = {Modern R Programming Cookbook: Recipes to simplify your statistical applications},
year = {2017},
isbn = {1787129055},
publisher = {Packt Publishing},
abstract = {Key FeaturesDevelop strategies to speed up your R codeTackle programming problems and explore both functional and object-oriented programming techniquesLearn how to address the core problems of programming in R with the most popular R packages for common tasksBook DescriptionR is a powerful tool for statistics, graphics, and statistical programming. It is used by tens of thousands of people daily to perform serious statistical analyses. It is a free, open source system whose implementation is the collective accomplishment of many intelligent, hard-working people. There are more than 2,000 available add-ons, and R is a serious rival to all commercial statistical packages. The objective of this book is to show how to work with different programming aspects of R. The emerging R developers and data science could have very good programming knowledge but might have limited understanding about R syntax and semantics. Our book will be a platform develop practical solution out of real world problem in scalable fashion and with very good understanding. You will work with various versions of R libraries that are essential for scalable data science solutions. You will learn to work with Input / Output issues when working with relatively larger dataset. At the end of this book readers will also learn how to work with databases from within R and also what and how meta programming helps in developing applications. What you will learn Install R and its various IDE for a given platform along with installing libraries from different repositories and version control Learn about basic data structures in R and how to work with them Write customized R functions and handle recursions, exceptions in R environments Create the data processing task as a step by step computer program and execute using dplyr Extract and process unstructured text data Interact with database management system to develop statistical applications Formulate and implement parallel processing in RAbout the Author Jaynal Abedin is currently doing research as a PhD student at Unit for Biomedical Data Analytics (BDA) of INSIGHT at the National University of Ireland Galway. His research work is focused on the sports science and sports medicine area in a targeted project with ORRECO --an Irish startup company that provides evidence-based advice to individual athletes through biomarker and GPS data. Before joining INSIGHT as a PhD student he was leading a team of statisticians at an international public health research organization (icddr,b). His primary role there was to develop internal statistical capabilities for researchers who come from various disciplines. He was involved in designing and delivering statistical training to the researchers. He has a bachelors and masters degree in statistics, and he has written two books in R programming: Data Manipulation with R and R Graphs Cookbook (Second Edition) with Packt. His current research interests are predictive modeling to predict probable injury of an athlete and scoring extremeness of multivariate data to get an early signal of an anomaly. Moreover, he has an excellent reputation as a freelance R programmer and statistician in an online platform such as upwork.}
}

@book{10.5555/517956,
author = {Flannery, Ron M.},
title = {The  Informix Handbook with Cdrom},
year = {2000},
isbn = {0130122475},
publisher = {Pearson Higher Education},
edition = {1st},
abstract = {From the Book: PREFACE: Introduction: How to Use This Book What Makes This Book Unique This book is intended to be a comprehensive reference to Informix products. It provides a substantial amount of detail as well as reference information. You can find much of this information in the many Informix manuals, but this book organizes it into one place and helps you find additional information. In addition, the book works in conjunction with its CD and Web site (www.informixhandbook.com) to provide a complete and long-term, ongoing reference. We want this to be one reference that can be on the shelf of any Informix developer or administrator. The information is organized into various functional groups, simplifying the process of finding what you need. My vision in designing this book is this: Create one reference that will help every level of Informix database administrator, application developer, system administrator, and end user. Organize the information in an easy-to-find fashion for all major Informix product lines. In conjunction with the book, its CD, and Web site, provide appropriate examples and detail, directing the readers to additional places to look for information. Supply a reasonable amount of "behind-the-scenes" information but focus on functionality. Use authors that specialize in particular Informix products and are well known in the Informix world. Make this the book to choose if you would like one reference on Informix. That's a pretty tall order, isn't it This book does not claim to be "everything to everybody" but it can help you on many different levels. If it can't provide a specific answer, it will tell youwhereto find the answer. One of the major values with this book is that its Web site (www.informixhandbook.com) provides updated reference material for each chapter as well as numerous links that help you find more information. Many comprehensive Informix references in the world just don't exist--at least not one-volume references--and I'm one Informix devotee who wants to fill that gap. The book also spans different versions of Informix. We provide plenty of information to get you up and running with older versions of the product, including INFORMIX-SE and OnLine Server (version 5.x). We offer more of a focus on the family of Informix Dynamic Server (IDS) products, including IDS versions 7.x, IDS.2000, Informix Internet Foundation.2000, and the data warehouse products. You may already know the names of many of the authors of this book. They were chosen from the local and international user groups as well as Internet newsgroups, ensuring that you will learn from some of the most knowledgeable Informix experts in the world. When choosing authors, I considered critical the fact that they had a special focus in the areas about which they wrote. In addition, all authors worked within the design and vision of this book, helping provide a consistent look and feel. To ensure overall consistency, I read and edited all of the chapters after they were submitted. We make this book more enjoyable by sprinkling it with amusing stories from Informix employees and others in the industry. These stories help add spice and make your learning experience much more enjoyable. All the way from this introduction to the CD-ROM to the Web site, we are crafting this book to meet the needs of Informix users worldwide. Please learn how to use it and have fun with it! The Web Site: An Ongoing Reference This book was designed by technologists who believe in using the best tools for the job (I guess that's why we choose Informix products!). One of the major values that I think you can find with this book is that it has a very useful Web site. The site is designed to enhance and supplement the book on an ongoing basis. Using it in conjunction with this book, you will always be "current" on what is going on with Informix and the particular products. The site is found at www.informixhandbook.com and includes the following: Many informational Web site links about Informix and its products Enhanced and current information for each chapterfor example, if a product has changed or enhanced, we discuss it Further Web site links for each chapter Errata (error corrections, if any!) for each chapter Ability to use our monthly e-newsletter, which provides news and information about Informix and the book The latest Informix announcements and product information When the next edition of the book is released, most of the updates for each chapter will be included in the book. I truly intend the site to make this book a long-term, complete reference for your Informix needs. Please be sure to view the site and learn how to use it. If you want monthly news and updates about Informix, be sure to subscribe to our e-newsletter. As you read through each chapter, be sure to check the Web site for any new or updated information. I think that the site will prove to be a great value you are receiving by purchasing this book. Our Target Readers This book is designed to be an excellent tool for all levels of Informix users, including: Application developers Database administrators Informix server administrators End users The information is designed and organized in such a way that it should be easily accessible by all of these groups. As you'll soon see, the book is divided into six major functional sections, placing related information together. In addition, each chapter has numerous cross-references that help you know where to find more information. The book can help all types of users because it provides many levels of instructional information, detailed reference and review materials, and links for where to get more information. The remainder of this introduction explains how to use the book to its fullest capacity. What You Can Gain from This Book This book helps you understand current and future products and technologies related to Informix. We describe "how Informix works" and how to use Informix servers and their application development tools. We help you with database administration, server setup, and operating system interaction. We also discuss new technologies like Web/database applications and Informix Internet Foundation.2000 and how they fit into the future of Informix. As previously described, one of the main values of this book is its Web site. The site turns the book into an ongoing reference--it provides updated information on each chapter, errata (correction of errors in the book), and numerous Web site links that are grouped by functional category. The book provides references and working examples of the main functional areas of Informix. Our intent is to let you use the book in many different ways, each helping you find information for the Informix product lines. The extensive cross-referencing helps train you where to look for more complete answers. By consulting with many others in the Informix world, we believe that the organization of the book provides a long-desired "one-stop shopping" reference for Informix. This is accomplished by: Organizing the sections into major functional areas Creating a Web site that provides ongoing information about Informix and its products Selecting writers that are well known in the Informix world Providing examples that apply to major areas of Informix usage, spanning different products and servers Supplying excellent cross-references to finding more detailed information by using other reading materials and the World Wide Web Using the CD-ROM to efficiently allow searching and referencing information The design of this book follows these principles and should allow you to know exactly how to find the information you need, be it from this book or another reference. How to Use This Book, the CD, and the Web Site This book provides a unique way to get the information you need about Informix. From its beginnings, the book itself was structured to help you understand the core information about various aspects of Informix. But as the book progressed, so did the Web and other technologies. At that point, I decided to create an even better way to get the information to you. Here's how it works: This book usually serves as your starting point. You can try to find the exact information that you need in the appropriate chapter. If you can't, each chapter has references that direct you to more information elsewhere in the book, Informix manuals, the Web, or other books. If convenient, you can check the other books or manuals, or continue with the rest of this list. Check the book's Web site for the specific chapter. You might find updated information about the material or Web site links to direct you to more information. In addition, you can use the site's search feature to try to find an answer. After that, you can use the site's "Information about Informix" links. Finally, the site offers an e-newsletter that will keep you updated on news about Informix and the book. Check the book's CD-ROM for additional answers. You can use the well-organized information in the CD to find documentation or tools that you'll need. Also, be sure to watch the Web site for CD-ROM upgrades and search capabilities for the CD. All in all, the book, its CD, and Web site should help you find the answers you need. The following sections provide detail about each of these items. How This Book Is Organized Let's take a look at the organization of this book. The book is broken into six major functional sections. You will be able to easily determine where to look for answers. Within each of the six sections, chapters provide more detail. Here is a summary of the sections: I. Core Concepts of Informix This section describes the Informix Corporation, its products, and the future plans of Informix. We begin by giving a history of the Informix Corporation and how it evolved into what it is today. We describe the different types of Informix databases and tools and how they fit into your needs. Chapter 3 helps you get started; it teaches you how to use Informix's sample stores database and its utilities. Chapter 4 provides an extremely detailed view of what's behind the architecture of Informix, now and into the future. Chapter 5 describes the Informix version of SQL and how to use it. Finally, the last two chapters in this section show you how to access data in Informix databases and describe Informix's data warehousing direction. II. Informix SQL In this section, we provide detail about how to use Informix's version of SQL. We describe and show you how to create databases, tables, and indexes. We give detailed examples and tips about how to use the Informix SELECT, UPDATE, INSERT, and DELETE statements. Stored procedures and triggers are discussed, as well as other miscellaneous Informix commands. Finally, Chapter 16 explains some of the additions to Informix in version 7.30. All in all, this section gets you "online" with Informix and allows you to access data. III. Server Administration This section explains how to administer Informix database servers. The section is split into two sub-sections: Setting Up Informix and Ongoing Administration. We describe everything from simple INFORMIX-SE installations to detailed Informix Dynamic Server administration. We provide detail about how to set up and debug various servers on UNIX, NT, and Windows 2000, and describe how to work with the operating system. We help you set up your configuration parameters (the onconfig file, for example) and troubleshoot problems. We provide a comprehensive functional reference of the command-line utilities, such as onstat (complete syntax for this command is given in the appendix). Finally, we help you learn how to keep your servers up and running and how to back them up. As always, we tell you where to find more detailed information. IV. Performance Tuning The Performance Tuning section describes performance tuning both for application developers and Informix administrators. We explain how to use the tools that Informix provides, how to interpret the output, and how to make changes. For application developers, we offer a summary of fundamental design and tuning issues, and information about how to work with Informix tuning methodologies such as SET EXPLAIN and UPDATE STATISTICS. We give a detailed description of how to use the newer tuning tools, including SMI and the sysmaster database. For administrators, detailed sections explain how to work with a number of tools and methodologies and how to continue to monitor performance. V. Application Development This section explores the application development tools and methodologies available for Informix products. We explain how to make your databases work on a network by using Informix connectivity tools. We describe overall strategies for application development and how to plan for the future. Detailed sections are available about specific Informix tools like 4GL, Dynamic 4GL, Ace Reports, Perform Screens, and reporting tools. VI. Web Applications and Object Relational Databases This section focuses mostly on Web applications and how object relational database management systems (ORDBMSs) might fit into your future. Simply put, no one can deny that objects and the Web will enable database systems well into the 21st century. This section goes into detail about what these methodologies mean, how they are used, and how they might be used with Informix products. The first two chapters provide an overview of current Web technologies and how to use them in a database-enabled Web application. After that, we describe how to use Informix Internet Foundation.2000, its DataBlades, and other tools. Again, we help point you to the right resources so that you can remain on top of object technologies and how they fit into your future. VII. Appendices The appendices provides copies of many useful reference materials, Web site addresses, tools, and other information. The appendices alone will serve as an excellent desktop reference for just about any Informix user! We include Informix utilities (which are also on the CD-ROM) and describe how they work. We show you how to maximize the Informix resources like Informix Developer's Network, user groups, newsgroups, and the Web. The appendices also provides a comprehensive glossary of Informix-related terms and a more complete reference on INFORMIX-SQL and command-line utilities. Don't miss Appendix F, "Quick Up-and-Running Guide," which provides a quick reference of how to install and configure Informix products from the box to being online. After that, a list of sample queries and command line utilities help you see how to implement various Informix commands. Finally, the extremely detailed Glossary explains a large number of Informix-related terms and concepts. As you can see, we have very carefully split the sections of this book so that information is easy to find. We fully intend to provide an excellent reference for your Informix needs and make it very easy to use, now and long into the future. Now, let's find out about Informix and why we're doing this in the first place. Structure of Each Chapter The chapters are formatted to make finding information easy. Here is a summary of the items in each chapter: Summary of the information in the chapter. At the beginning of each chapter is a brief overview followed by a list of the major topics that are included in the chapter. List of topics, split functionally into subcategories. Within each chapter are several small sections, each explaining a certain category of information. Within each of these sections are sub-sections, providing details. By looking at the table of contents, you should be able to quickly find what you need. Notes, tips, warnings, Web links, and other chapters, code listings, and diagrams. The chapters are rich with specific items of interest (notes, tips, warnings) and references to other chapters, books, and Web sites. In addition, many graphical figures and code samples are provided to help enhance your understanding. References to other chapters. At the end of each chapter is a section called "For More Information." This points you to other chapters in the book that relate to the current chapter. Informix and Other References. Following the "For More Information" section, this tells you which Informix manuals and other books can further help you. I'm confident that you'll find the structure of these chapters very easy to use. You may want to take a quick look through some of the chapters or the table of contents to better understand this structure. Contents of the Web Site The Web site is one of the core parts of this book's solution. Because it is so important, I described it in the previous section, "The Web Site: An Ongoing Reference." To summarize, the site will be constantly updated, providing additional information about each chapter, as well as current and useful Web site links. The idea is to use the book in conjunction with the site to provide a complete solution. The site can be found at www.informixhandbook.com. Contents of the CD The CD includes a Web browser-driven interface that allows you to easily locate information by using your Web browser. Simply insert the CD and open the index.html file in the root directory. The CD includes a plethora of utilities and SQL files, allowing you to copy commands and utilities that you can use. In addition, the CD includes the following: Informix Guide to SQL Syntax The Administrator's Guide for Informix Dynamic Server (official Informix version for 7.3) A copy of Informix Dynamic Server for Linux and NT All of the book's SQL statements, scripts, and programs (i.e., all that were included as Listings) A computer-based training (SmartForce) course entitled "Managing the Instance" Several whitepapers Easy-to-use links to the book's Web siteif you are online when using the CD, you can use various links that send you directly to the proper portion of the Web site Hundreds of utilities for administering or programming for Informix products. Products are written in shell scripts, SQL, 4GL, and C. A trial copy of Server Studio from AGS (www.agsltd.com), a comprehensive GUI-based tool for Informix databases In addition, on the Web site we will offer enhanced search functionality and a possible upgrade for the CD. As you can see, the CD, Web site, and the book work together to give you the information you need about Informix. Considering the Future One of the goals of this book is to help you consider the future of technologyand Informixin your design and development of applications. We, the authors of this book, made a special effort to consider how database technology and application development will be evolving. And you should, too. What works now might not integrate or make use of upcoming technologies. This book and its Web site provide you with details about how technology is changing and how to take keep up with its evolution. Again, we provide many cross-references so that you're sure that you have the latest information. We have dedicated an entire section to Web technologies and object-relational databases. Section VI, "Web Applications and Object Relational Databases," describes object technology and how to use Web applications and Informix Internet Foundation.2000 as well as DataBlades. The Web is going to be a big part of the future of database and application development. Chapter 41, "Web Application Overview," explains the concepts behind Web computing, and Chapter 42, "Building Web/Database Applications," explains how to use Java and other tools to create an Informix-enabled Web application. Again, using the book along with its Web site should help you keep current about the best ways to create Informix applications. For More Information... This introduction provided an explanation of the goals and layout of this book. Please skim through the book so that you can really understand how and why we organized it the way we did. We're hoping that you find this book extremely easy to use, providing detail, reference, and even some amusement. Don't forget to use the Web site or the CD-ROM and its HTML-driven menu. Here are some other good places to start: To find many different tools and references, see this book's CD. For complete and up-to-date information on the material in the chapters and current Informix information, see the book's Web site at www.informixhandbook.com. For more history of Informix, see Chapter 1, "Now and the Future," and Chapter 2, "History of Informix: Live from Silicon Valley." For an explanation of Informix database servers and products, see Chapter 1, "Now and the Future." To find out how to start using Informix now, see Chapter 3, "Creating and Using the stores Database." For an explanation of Informix architecture, see Chapter 4, "Understanding Informix Architecture." For a complete reference and examples of Informix SQL, see Section II, "Using Informix SQL." For complete details about administration, see Section III, "Server Administration." To find out how to tune your Informix servers and programs, see Section IV, "Performance Tuning." To obtain a better understanding of how to develop applications in Informix, see Section V, "Application Development." For many references about the Web and object relational technology, see Section VI, "Web Applications and Object Relational Databases." For large amounts of reference material, see the book's appendices. Don't miss Appendix F, "Quick Up-and-Running Guide," or the intensive Glossary.}
}

@book{10.5555/553611,
author = {Klander, Lars and Mercer, Dave},
title = {Access 2000 Developer's Black Book with Cdrom},
year = {1999},
isbn = {1576103498},
publisher = {Coriolis Group Books},
address = {USA},
abstract = {From the Book: Introduction Introduction This book is about power, the growing power of databases, computers, and networks to slash costs and dramatically increase effectiveness of communications and management. Databases touch everyone's lives in some way or another, and a clear understanding of what works and what doesn't puts that power within reach. This book is aimed at everyone who must participate in a database project to ensure success: database designers, end users, database administrators, senior managers, front-line managers, as well as those who must wear all these hats at once. • For experienced database designers and administrators, this book contains complete coverage of Microsoft Access 2000 in easy-to-understand (and use) examples, with plenty of reusable code and screen shots. • For managers and end users, this book contains plain-English explanations of how databases are constructed, what the limitations are, and a broad, exciting view of the potential. • For those who must act as designer, manager, and user, this book takes you from the most basic fundamentals to the most advanced programming steps, without requiring a degree in computer science. In every sense of the word, this book is a practical, day-to-day guide for people involved in building database solutions. Not only does it guide you through the phases of successful database projects (large and small) and the pitfalls that have ruined some, it teaches you the language and terminology used on all sides as you go: project management, process reengineering, relational models, programming fundamentals, and so on. The emphasis throughout the book is on enhancing communications, because clear and timely communication is the primary attribute of a successful database solution. Communication takes work. Everyone must be working from the same playbook for a database application to be effective and achieve widespread use. Traditionally, databases have been designed by computer scientists far removed from the day-to-day activities of work. A team of systems analysts would show up one day, gather what information they could about a process, then spend a year or two in the ivory tower building the application. The application would be instituted, workers trained to fill out the forms, and the reports would print, all according to the now-dated but assuredly very accurate specifications. For some applications, this system worked quite well, but for others it failed miserably. Where failure occurred, the primary cause was rapid change: changes in processes, requirements, business conditions, even changes in computer literacy. Today, because change is constant and the pace of change continues to accelerate, only excellent communication among everyone involved can overcome the swirling confusion born of change. This book gives everyone the playbook that they need to achieve these implementation goals. It takes the best of all traditional methodologies for rebuilding an organization's processes and for developing and constructing database solutions, explains them concisely, and blends them together into a powerful toolkit for building effective applications in a rapidly changing environment. The power of Microsoft Access 2000 combined with the proven methods outlined in this book increase the probability that your database application, no matter the size, will "work" from all perspectives. For the managing members of the team, the book helps you create a plan for effective and consistent implementation of your applications, whether destined for internal use throughout the enterprise or for public consumption. For those responsible for the creation of the implementation-the developers, power users, and users who will interact with the application on a regular basis-this book teaches you everything you need to know about making the application not only perform its tasks, but perform them well. No matter what environment you are developing for-from standalone databases at workstations to databases that will serve intranet and Internet users-this book teaches you how to address development issues in that environment and make sure your product not only works, but shines. Real-world examples, step-by-step explanations, and thousands of lines of program code all work together to ensure that you have all the tools you need to be successful. Contents Of This Book This book is divided into 8 parts, intended to guide you through the steps of database development with Access from beginning to end. Part I, "Fundamentals Of Information," contains three chapters that consider the nature of information and how information relates to the design of databases. The three chapters in the section, Chapter 1, "Foundations For Database Construction," Chapter 2, "The Nature Of Information," and Chapter 3, "Data Organization," guide you through the principles of information theory and the ways in which data is organized. Each chapter provides you with important information that you must understand to master effective techniques of database design. Part II, "Database Fundamentals," takes the information theory that you learned about in the first three chapters and brings it to the level of database design theory and principles. Chapter 4, "Relational Databases," introduces you to the principles of database design when working with relational databases like Access 2000. Chapter 5, "Database Structures," looks at the overall theory of database design and reviews the principles of relational database design that you learned in Chapter 4. Chapter 6, "Advanced Database Systems," considers the nature of advanced database architectures and the networks required to support them. By the time you finish Part II, you will have a solid knowledge base for database design-not only with Access, but with any relational database product. Part III, "Modern Database Implementation," moves on to some of the specific types of database uses in business today. Chapter 7, "Data Warehousing," covers the construction of data warehouses in depth. Chapter 8, "Applications and Operating Systems," covers practical application and operating systems (OS) issues, namely, how to decide whether to buy or make apps and OSs, and how to find and use them if you do decide to buy. Chapter 9, "Marketing," discusses the important considerations for you to keep in mind when preparing to distribute your Access products. From identifying your target market to measuring and adjusting your market strategy, effective marketing techniques can make a product successful or, if implemented poorly, can ensure it never sells a copy. Part IV, "Microsoft Access 2000 Overview," contains four chapters that address the specific improvements and changes to Access 2000, and the specific purposes for which Microsoft designed the Access 2000 product. Chapter 10, "Access 2000 Technologies," gives you a broad overview of some of the many component technologies that Access uses to simplify user access. Chapter 11, "New Features And Trends In Access 2000," considers some of the directions in which Microsoft has moved the Access product, including a discussion of the new Jet 4 engine and new integration with Microsoft's BackOffice products, specifically SQL Server. Chapter 12, "Access Purchasing And Installation," discusses such important implementation issues as who needs Access installations and what level they need, what the different types of Microsoft Office suites are, and specific installation concerns to keep in mind when purchasing the new Access 2000 product. Chapter 13, "Access 2000 Distribution And Training," addresses specific issues related to the deployment of the Access program in your enterprise. It also discusses built-in training support in the Access product and issues to consider when determining how and who to train. Part V, "Microsoft Access 2000 Usage," contains three chapters, each of which considers a general category of the target market for the Access product and how to design databases for that market. Chapter 14, "Access For Personal And Small Office/Home Office Use," addresses common uses of Access at home and in the small office setting. It discusses both common situations in which you might use Access databases and ways in which to create those databases. Chapter 15, "Using Access In A Corporate Environment," addresses common techniques for Access deployment within companies. It also contains your first introduction to the new Access Data Projects (ADPs) and their use as a SQL Server database front-end. Chapter 16, "Using Access For Scientific And Medical Purposes," considers common methods and implementations for Access in the scientific and medical communities. It also provides an introduction to the use of Access's graphing capabilities and presents useful information in both types of deployment environments. Part VI, "Database Application Design Reference," moves on to the creation of databases in Access 2000. The five chapters in this section provide you with a method that you can use to define and create databases to meet any need. Chapter 17, "Problem Definition And Design Planning," discusses the specific steps you should take in planning the design of a database to solve a particular problem and walks you through an extended example of these crucial steps in the design process. Chapter 18, "Planning And Design," moves on to the specific discussion of designing a database in accordance with the design planning that you performed in Chapter 17. Chapter 19, "Database Construction," shows you how to take an actual design diagram and convert it into table and database definitions in Access. Chapter 20, "Implementation-Beta Testing And Bug Checking," moves on to the testing and implementation phases of application design, including discussions of the testing process you should use and more. Chapter 21, "Completing The Implementation," discusses post-release improvements you can make to the application, including optimization, compacting, and repair of the database, as well as using the Access-provided tools to analyze and improve performance of your application. Part VII, "Microsoft Access 2000 GUI And VBA Programming Reference," covers the low-level, "nuts and bolts" of Access 2000 programming. The chapters take you from the initial creation of a database and its component tables through advanced programming with ActiveX Data Objects (ADO), Data Access Objects (DAO), and database management and security. Chapter 22, "Installation, Setup, And Configuration," discusses the installation specifics of the Access product, including the options you have during setup. It also introduces you to some of the specifics of Access database design-both for the standalone and the client-server environment. Chapter 23, "Developing Tables And Relationships," introduces you to the specifics of table creation and relationship definition, the core of database design. Chapter 24, "Creating Queries," takes you into the heart of relational database work, by teaching you how to create the different types of queries that lie at the heart of SQL's power. Chapter 25, "Creating Forms And Reports," teaches you the knowledge you need to create user interfaces and design effective reports that output data in the most usable form. Chapter 26, "Creating Macros And Modules," explains Access's macro language and introduces you to modules, which will contain Visual Basic for Applications code-code which will, in turn, unlock significant additional power for your database applications. Chapter 27, "Using Modules And Visual Basic For Applications," builds on the knowledge you gained in Chapter 26 to teach you what you need to know about writing VBA programs to "power-up" your Access applications. Chapter 28, "Working With DAO And ADO," introduces you to the database objects that VBA lets you use to manipulate Access, SQL Server, Oracle, and other ODBC- and OLE DB-compliant databases. Chapter 29, "Using Class Modules With Access," describes some basics of object-oriented programming and how you can use VBA class modules to implement custom objects within your Access 2000 applications. Chapter 30, "Advanced Database Design Techniques," shows you how to take advantage of VBA and Access's built-in features to make your applications more professional. It also focuses in-depth on the administration of security within your Access database. Part VIII, "Microsoft Access 2000 And Client-Server Development," contains five chapters that teach you about client-server programming with Access 2000 and different server-based database products, as well as how to create World Wide Web front-ends for Access or server databases. Chapter 31, "Client-Server Programming With Access 2000," introduces you to SQL Server and working with back-end products from Access. It also covers, in detail, important conceptual information about client-server design that is applicable to any back-end. Chapter 31 also introduces you to the Microsoft SQL Developer Engine (MSDE), a local implementation of SQL Server that you can use to design SQL Server databases on your development machine. Chapter 32, "Using Oracle And Access For Client-Server," teaches you the fundamental concepts of Oracle database design and the differences in development between Access front-ends for SQL Server and Oracle. Chapter 33, "Advanced Client-Server Techniques," takes the knowledge from Chapters 31 and 32 and sends it to the next level with important information about topics such as triggers and stored procedures, transaction processing, Access Data Projects (ADPs), and more. Chapter 34, "Web Front-End Development," moves to the largest client-server environment in the world-the Internet. It covers historically proven and commonly used techniques for exposing databases through HTML pages. Chapter 35, "Using Data Access Pages For Web Front Ends," moves on to Microsoft's proprietary Access front-end technology, Data Access Pages (DAPs), which let you develop highly customized, highly responsive front ends for your Access databases, all from within the Access Interactive Development Environment (IDE). In addition to a complete index, the book also contains an appendix of additional resources. From the first 9 chapters that step you through the fundamentals of database design and relate them to good programming practice and business process reengineering, to the next 26 chapters that cover every detail about how Access 2000 works (including how it interacts with the Web and mainframe databases), the purpose of this book is to build a common ground on which all people, from the novice user to the most sophisticated IT developer, can work together. Remember, people in organizations today recognize how important it is to make the powerful software tools on their desktops useful, and they need a tool like this book to make it happen.}
}

@book{10.5555/1540613,
author = {Foxwell, Harry and Tran, Christine},
title = {Pro OpenSolaris},
year = {2009},
isbn = {1430218916},
publisher = {Apress},
address = {USA},
edition = {1st},
abstract = {OpenSolaris is a rapidly evolving operating system with roots in Solaris 10, suitable for deployment on laptops, desktop workstations, storage appliances, and data center servers from the smallest singlepurpose systems to the largest enterpriseclass systems. The growing OpenSolaris community now has hundreds of thousands of participants and users in government agencies, commercial businesses, and universities, with more than 100 user groups around the world contributing to the use and advancement of OpenSolaris. New releases of OpenSolaris become available every six months, with contributions from both Sun engineers and OpenSolaris community members; this book covers the OpenSolaris 2008.11 release. Pro OpenSolaris was written to demonstrate that you can host your open source applications and solutions on OpenSolaris, taking advantage of its advanced features such as containers and other forms of virtualization, the ZFS file system, and DTrace. It's assumed that you are already fairly knowledgeable about developing on Linux systems, so the authors give an overview of the similarities and differences between Linux and OpenSolaris, and then present details on how to use the Service Management Facility (SMF), ZFS, zones, and even a bit of DTrace. They also provide pointers to the many project communities associated with new OpenSolaris features. Special focus is given to web development using familiar applications such as Apache, Tomcat, and MySQL, along with the NetBeans IDE, and showing you how to exploit some of OpenSolaris's unique technologies. What youll learn Discover the secrets of the ZFS, the most powerful file system ever conceived Explore OpenSolaris AMP (Apache, MySQL, PHP) and GlassFish in the context of Web 2.0 and Linux/Solaris, respectively Familiarize yourself with the new security administration features of OpenSolaris, including changes in DTrace Who is this book for? Linux system administrators and programmers who would like to know what they have missed since Solaris became an open source operating system. About the Apress Pro Series The Apress Pro series books are practical, professional tutorials to keep you on and moving up the professional ladder. You have gotten the job, now you need to hone your skills in these tough competitive times. The Apress Pro series expands your skills and expertise in exactly the areas you need. Master the content of a Pro book, and you will always be able to get the job done in a professional development project. Written by experts in their field, Pro series books from Apress give you the hardwon solutions to problems you will face in your professional programming career.}
}

@book{10.5555/579266,
author = {Unhelkar, Bhuvan},
title = {Process Quality Assurance for Uml-Based Projects},
year = {2002},
isbn = {0201758210},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: Purpose of this Book The convenor of the OOSIG (object-oriented Special Interest Group) of the Australian Computer Society is occasionally referred to as Chairperson. For past two years, this honorary and honourable title has been conferred upon me and, as the title suggests, it provides me with — amongst other things — the unenviable job of moving and organising chairs before the monthly meeting starts and ensuring they are stacked back against the wall after the meeting in the societys office is over. Getting the flipcharts and whiteboard ready, booking the room, sending the invitations, organising coffee and keeping the data projector light bulb from blowing up are some things keep the adrenaline level of the chairperson always on high. However, I had no such challenges to face when Canada-based Bran Selic, kindly addressed my SIG. Many members turned up to listen to one of the original contributors to the Unified Modelling Languages meta-model, particularly to the behind-the-scene stories. One of the interesting features of Brans talk was the candid highlighting of the strengths and weaknesses of the UML. Couple of reasons for UMLs popularity, as emerged during the talk were: UML is a standard and therefore accepted within the larger IT community, and UML came on the IT scene at the right time. The UML fills the void that existed in software development — a modelling mechanism that enables capture and expression of requirements, documentation of design, facilitates architectural discussion and supports software implementation. The modelling capabilities of the UML, supported by CASE tools, are widely usedin numerous practical software projects. Further, professional training courses on business analysis, architecture, design and testing are routinely based on the UML standard. The UML is also popular in the academic world as many university courses use the standard notations and diagrams of the UML to teach the students principles of software engineering. Finally, through relatively less-technical and more business-focused works, object technology and the UML have shown to be capable of being used in non-software related work, such as modelling business processes (BPR), business workflows, and even software workflows. Despite its popularity, however, the UML literature still needs discussion on and application of quality with UML. While we have some excellent literature on the processes of software development it seems to fall short of separate and detailed discussions on quality. On the other hand, works like Binders focus on the technical aspects of testing, using the UML notations, do not provide the process-aspect of improving the quality of software development. Indeed, none of this literature deserves any criticism for the lack of quality discussion — because these literary works do not purport to be discussing quality. The focus of these respectable and popular works is either development or testing. This book is written with an aim of directly addressing the paucity of literature in the area of process quality assurance for UML-based projects. Good Quality is all about satisfying the needs of the user. However, good is a highly subjective term. The reference-point against which quality is judged depends on time, place, and situation — all of which can change! Hence, the essential ingredients in producing good quality are: A product that satisfies the changing needs of the user A process that enables creation, verification and validation of such a product A common mechanism to establish communication Continuous improvement of the process of producing product When applied to software development, these quality requirements translate into producing a software product that would evolve, scale and change, according to the needs of its users — primarily the business. Not only do we need a process for developing such a software product, we also need significant checking and crosschecking of the models and processes that have built the software product. There is a need to ensure the syntactical correctness, semantic consistency and aesthetic symmetry in the models that will be used to produce good quality software. There is also a need to create, follow and check all necessary process steps in order to achieve maturity of processes that will result in good quality software products. Furthermore, these process steps must be executed iteratively, incrementally and sufficiently. Process steps should also be malleable enough to suit various development environments, and various types and sizes of projects. The specific and significant areas of quality related work required in a process incorporating the UML are addressed in this book. The quality techniques discussed in this book include how to organize the overall quality function, the process steps to be followed in creation of UML diagrams, the steps in verification and validation of these diagrams, when to conduct such verificat how the interpret the results of quality activities, who should create and validate the UML diagrams, and how to create a quality control (testing) strategy. Because of the process focus in this book, the techniques of creation of UML diagrams is assumed to be known to the readers. Summary of the Book This book is divided into 6 chapters as summarized below. Chapter 1: The Quality Game In this background chapter on quality assurance we discuss the elusive nature of quality in the context of software. Modelling, particularly with the UML, is shown as means to improve communication and quality and is conducted in the three distinct yet related modelling spaces of Problem, Solution and Background. Process is discussed in the context of its three dimensions of technology (what), methodology (how) and sociology (who). This is followed by discussion on the various checks (syntax, semantics and aesthetics) needed to validate and verify UML-based models and the checks of necessity, sufficiency and malleability needed for a good quality process. Organization of the quality function, and its application to various types of projects (development, integration, package implementation, outsourcing, data warehousing, and educational) as well as various sizes (small, medium, large) of projects are also discussed here. Chapter 2: Quality Environment: Managing the Quality function Process aspect of quality encompasses the management functions of creating and managing a quality environment. This is because software quality is not just verifying and validating what has been produced but also a sustained effort at following the discipline of producing models and software. This discipline encompasses the process or the steps involved in producing good models and good software. This part of this book comprehensively considers organization and execution of the quality function with detailed emphasis on the process of developing UML based software. In other words we discuss how the quality function is organized and carried out in UML-based projects. The people issues (who) is also given due relevance in this part of the book. Chapter 3: Quality Process Architecture This chapter discusses what constitutes such a process, and how it will be helpful in enhancing quality in a UML-based project. This chapter does not propose a new process, but discusses a most generic Process including the Technological, Methodological and Sociological dimensions — what constitutes a process, and what are its major dimensions of a process is described here. The technological dimension of a process deal with the what, the methodological dimension with the how and the sociological dimension with the who, of an overall process. These dimensions are described with common workday examples. Furthermore, the generic process also describes the most commonly used activities and tasks that should be there in any process. These activities and tasks, and the related roles and deliverables, are described with the aim of improving the discipline in a process, resulting in enhanced quality of UML-based deliverables and eventually the software product. Chapter 4: Enacting the Quality Software Process In this chapter we discuss the enactment of an example process including practical issues of configuring an iterative, incremental and parallel project plan, based on the process-components discussed in the previous chapter, are discussed here. We also discuss practical issues of tracking the progress of a project as well as modifying the project plan based on that tracking. An iterative and incremental project plan will facilitate better absorption of changes than a sequential project plan. Creation and management of such a changing plan, derived from the malleability aspect of the process, are also discussed here. This chapter discusses what happens when the rubber hits the road in terms of application of a process. Chapter 5: Estimates and Metrics for UML-based Projects This chapter discusses the important issues of measurements and estimates in UML-based software projects. Starting with an argument for the need to make good estimates, and how good metrics help in making good estimates, this chapter delves into the importance of these measures and estimates in improving the quality of models and processes in the project. Technical measures related to sizes and complexities of the UML artefacts and diagrams is also discussed. Estimates for the example implementation project using the UML are shown with a view to demonstrate the application and significance of metrics in a practical project. Chapter 6: Testing the product This chapter will discuss in detail the quality control and testing aspect of a quality lifecycle. While we discussed process quality in previous chapters, quality control, or testing, is a major process-component dedicated to verifying and validating the results of our efforts thus far in creating models and following a process. Good quality control is inherently negative as it is aimed at breaking everything in a system — its logic, its execution, its performance. Thus, although Quality control is an integral part of quality assurance, but is not synonymous with it. This separation is given its due importance in this separate part of this book. CD &amp; Potential Web Support The CD contains details of the chapters, diagrams, and a set of templates that can be customised for use in projects. Suggested metrics for improving quality (e.g. size of use cases, effort in creating classes) are also incorporated in the CD. Evaluation copies of relevant process tools that deal with quality process have also been provided, with permissions. Literary Audience There are a large number of books written on UML and similarly on processes. Their scope encompasses both academic research and practical applications. This book attempts to synergies the application of quality processes in UML-based projects. With the process focus, the reader is expected to be familiar with UML and its modelling techniques as the book does not purport to discuss the modelling techniques of the UML. However, a person responsible for quality assurance will find this work self-sufficient and may even be encouraged after reading this material to extend their understanding further in to UML. Semantics This author firmly believes in gender-neuter language. Person is therefore used wherever possible. However, in order to maintain simplicity of reading he has been used as freely, and has been balanced by equal, if not more, use of she. Terms like programmer and quality manager, unless otherwise mentioned, represent roles performed by actors. These terms dont tie down real people like you and me who, in a short span of time, can jump from the role of a programmer to a quality manager to a director and back. It is also recognised that people may be playing more than one role at a time. For example, a business analyst may also be a part-time academic or a researcher. We throughout the text primarily refers to the reader and the author — you and me. Occasionally, we refers to the general IT community of which the author is a member. We also refers to the teams in which the author has worked. Therefore, although this is a single author book, you may encounter we as a reference by the author to himself, as well as to the IT community. Real conversations, as you and I are having through this work, cannot be statically typed. Mapping to a Workshop The practical aspects of UML and Quality, displayed in this book, have been popular in seminars and conferences. Amongst many presentation, particular noteworthy are its acceptance as a tutorial at the UML2001 conference in Toronto, Canada and the two-day seminar series in Mumbai, Bangalore and Delhi, in India. Here is a generic outline of the two-day workshop based on this book. For the education and academic community, each chapter in this book can correspond to a 3-hour lecture topic, with earlier part of the semester used in simply creating the UML-based models based on the case study. Acknowledgements Encouragement and support can take various forms —a word of encouragement here, hint of a smile there! And then there are those detailed discussions and arguments with honest reviewers of the manuscript on what should be included and how it should be presented. This is interspersed with the arduous task of typing sections of the manuscript, drawing the figures and the rather trying job of proofreading someone elses writing. All this has come to me through many wonderful people whom I acknowledge here gratefully: Anurag Agarwal Rajeev Arora Craig Bates Paul Becker Christopher Biltoft Bhargav Bhatt Graham Churchley Kamlesh Chaudhary Sandra Cormack Joanne Curry Sanjeev Dandekar Edward DSouza Con DiMeglio Julian Edwards Nandu Gangal Athula Ginige David Glanville Mark Glikson Nitin Gupta Brian Henderson-Sellers Murray Heke Ivar Jacobson Sudhir Joshi Ashish Kumar Vijay Khandelwal Akshay Kriplani Yi-chen Lan Chinar &amp; Girish Mamdapur Javed Matin Sid Mishra Rahul Mohod Navyug Mohnot Narayana Munagala Karin Ovari Les Parcell Chris Payne Andrew Powell Abhay Pradhan Amit Pradhan Anand Pradhan Prabhat Pradhan Rajesh Pradhan Tim Redrup Tracey Reeve Prashant Risbud James Ross Magdy Serour Bran Selic Ashish Shah Paresh Shah Prince &amp; Nithya Soundararajan Pinku Talati Amit Tiwary Murat Tanik Asha Unhelkar Sunil Vadnerkar Suresh Velgapudi John Warner Houman Younessi Paul Becker, my editor at Addison-Wesley, has provided invaluable support in this work and deserves special recognition. Bearing with my delayed submissions, passing encouraging comments when the progress was slow and accommodating my changes up to the last minute are some of the traits of this considerate editor that are gratefully acknowledged. Finally, my family makes all this possible by just being around me even, and especially, when I am mentally lost. I am grateful to my wife Asha, my daughter Sonki Priyadarshini whose view on quality took a jagged turn as she stepped into her teens, my son Keshav Raja who can appreciate quality in cars, bikes and planes — which is the ability of these tools of the kindergarten trade to withstand punishment meted out by rather tough 6 year olds. Finally, this work acknowledges all trademarks of respective organisations, whose names andor tools have been used in this book. Specifically, I acknowledge the trademarks of Rational (for ROSETM), TogetherSoft (for TogetherControlCenterTM), Object-oriented Pty Ltd (for ProcessMentorTM) and eTrackTM. Critiques It reflects a healthy state of affairs within the IT world, and especially the UML and process community, if work of this nature receives its due share of criticism. All criticisms have an underlying rationale and that they should all be accepted in a positive and constructive vein. All comments on this work, both positive and negative will be accepted positively. Thus, to all my prospective critics, whose criticisms will not only enrich my own knowledge and understanding of the quality topics discussed in this book, but which will also add to the general wealth of knowledge available to the IT community, I wish to say a big thank you in advance. Bhuvan UnhelkarSydney, July 2001}
}

@inproceedings{10.1145/1362702.1362708,
author = {Kidd, Eric},
title = {Terrorism response training in scheme},
year = {2007},
isbn = {9781450378444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1362702.1362708},
doi = {10.1145/1362702.1362708},
abstract = {The Interactive Media Lab (IML) builds shrink-wrapped educational software for medical professionals and first responders. We have teams focusing on media production, script-level authoring, and low-level engine development. Our most recent project is Virtual Terrorism Response Academy. VTRA uses 3D simulations to teach students about radiological, chemical and biological weapons. Our software is now undergoing trials at government training centers and metropolitan police departments. VTRA consists of approximately 60,000 lines of Scheme, and a similar amount of C++. All of our product-specific code is in Scheme, and we make extensive use of macros and domain-specific languages.From 1987 to 2002, we used a C++ multimedia engine scripted in 5L, the "Lisp-Like Learning Lab Language". This was Lisp-like in name only; it used a prefix syntax, but didn't even support looping, recursion, or data structures. We needed something better for our next project! We ultimately chose to use Scheme, because (1) it was a well-known, general-purpose programming language, and (2) we could customize it extensively using macros. Migrating to Scheme proved tricky, because we needed to keep releasing products while we were building the new Scheme environment. We began by carefully refactoring our legacy codebase, allowing us to maintain our old and new interpreters in parallel. We then rewrote the front-end in a single, eight-day hacking session. But even once the Scheme environment was ready, few of our employees wanted to use it. In an effort to make Scheme programming more accessible, we invested significant effort in building an IDE. Today, our environment is much more popular---a third of our employees use it on a regular basis, including several professional artists.After migrating to Scheme, we added support for 3D simulations. And Scheme proved its worth almost immediately: we faced several hard technical problems, which we solved by building domain-specific languages using Scheme macros. First, we needed to simulate radiation meters. For this, we used a reactive programming language to implement a Model-View-Controller system. Second, we needed to guide students through the simulation and make teaching points. For this, we relied on a "goal system", which tracks what students need to accomplish and provides hints along the way. In both these cases, Scheme proved to be a significant competitive advantage. Not all problems have clean imperative solutions. A language which supports functional programming, macros, and combinator libraries allows us to do things our competitors can't.This summer, we'll be releasing our engine as open source, and starting work on a GUI editor. We welcome users and developers!},
booktitle = {Proceedings of the 4th ACM SIGPLAN Workshop on Commercial Users of Functional Programming},
articleno = {6},
numpages = {3},
location = {Freiburg, Germany},
series = {CUFP '07}
}

@book{10.5555/558527,
author = {Mullin, Eileen and Rubin, Jared T.},
title = {Free-Commerce: The Ultimate Guide to E-Business on a Budget},
year = {2001},
isbn = {0130337676},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Book: Foreword Why Free Whatever you may think of Bill Gates, chairman of Microsoft and arguably one of the world's wealthiest individuals, you must give him credit for seeing-and capitalizing on-the rise of the personal computer. Therefore, it was more than mere press release fodder when the bespectacled billionaire proclaimed the Internet the most significant advancement since the personal computer. He was wrong, though, or at least uncharacteristically understated. The Net has been far more significant than the PC. Although the PC has been instrumental in driving adoption of the Net, particularly in the United States, the Internet and World Wide Web were born independently of it. While most people access the Internet from Windows today, the PC's Net suitability and Microsoft's Internet strategy were late to blossom. The Net holds promise for having its greatest impact yet outside the U.S. through non-PC devices, such as advanced mobile phones, which are more common in Japan and Europe. The real reason why the Net is far more important than the PC ever was, though, is that the PC failed to fundamentally change the economy. The PC may have improved productivity in virtually all industries. It may have created sea changes in many industries, such as publishing and film production. It may have even virtually created intrinsic industries, such as software development. However, the PC failed to tear down walls among industries. It did not raise the questions of whether music artists should distribute their music without their labels' involvement. It did not cause CNN and the New York Times to compete in the same medium. It did not give rise to retailers like Amazon.com that have Wal-Mart in their sights. It did not cause NBC to question whether it was a media or commerce company. Calling the Internet a force in the evolution of the PC is like saying that television's major impact was changing radio. LOOKING, HOOKING, AND BOOKING There is another way the Internet has changed our thinking about the economy. The Net's unique combination of direct marketing potential and wildfire distribution combined with new financial valuation models and a glut of startups, all desperate to rise above the fray, has led to a new way of thinking about customers. This is a world that has seen the tremendous success that companies like America Online have had once they have acquired customers. All AOL has really done, though, is replicate what financial and telecommunications companies have known for years. Once you have a billing relationship with customers, it is relatively simple to extract additional revenue from them through premium services, cross-sold services, merchandise, bundles, and other marketing techniques. The difference in the Net space, though, in spite of the market correction of early 2000, is that investors have been willing to forego the billing relationship for extended periods on the logic that it is only after getting someone's attention that you can sell them goods and services. Microsoft bought Hotmail, which provided free Web-based e-mail, for over $400 million because it had acquired millions of users who might be more easily converted into customers. Regardless of whether their revenue model includes advertising (as many sites profiled in this book do) every company with a Web presence must understand the principles of media. That was only in the first wave, though. The Internet has certainly blurred boundaries across media and channels, but motivating consumers is still a difficult task, particularly since the low bandwidth available to the Web makes it difficult to elicit the kinds of emotional responses possible in television. The rise of online retail, the transformation of online media, and the investment in crossover online services have pushed the Net from the domain of the consumer out to meeting the needs of businesses, particularly small businesses. AN EVOLUTION OF SERVICES The first wave of excitement around the Net came around selling access to consumers. Early Internet service providers such as Netcom seemed poised to dominate the landscape. Then came flat-rate pricing, which wiped away the juicy margins in the access business. Furthermore, as we'll discover, distracted Internet service providers were so focused on survival that they slowly saw most of their value-added services, such as e-mail and home pages, usurped by more nimble Web sites. Selling goods to consumers marked the second phase. Amazon.com started with a franchise in book sales and has slowly branched out to music, video, toys, and even consumer electronics. Amazon's blitzkrieg quickly forced consolidation in sectors ranging from CD sales to beauty supplies, where the retailer has a major investment in Drugstore.com. Undeterred in its strategy of horizontal domination, Amazon has embraced consumer-to-consumer auctions and even launched zShops, which leads smaller vessels of commerce into its raging river. What is often overlooked about Amazon, though, is that its storefront masks a services play. Books were merely an early vehicle through which to develop tools that provide one of the best online shopping experiences today, including excellent customer service, recommendations driven by collaborative personalization, and a huge selection. And what is selection if not a way to avoid running around, i.e., a convenience service Despite Amazon's success, though, the bloom is off online retail's rose. The threat of shopping bots that compare prices across Web sites; the eventuality of established branded retailers moving online; the reality of lowballers such as Buy.com, who are willing to trade profits for advertising revenue; and the twist of reverse auctioneers such as Mercata, which allow consumers to aggregate demand, have made selling goods online something less than the frictionless mecca it once seemed. Similarly, in the media side of the Web, major portals such as Yahoo!, which has established itself as one of the few successful online media companies (and only through strong offline ties, at that), have made a fundamental shift in the past few years. Most of the attention on its transition over the past few years has focused on the migration from a search site pointing consumers to resources to a directory focusing on internal resources. The more significant change, however, has been Yahoo!'s development of a series of communications and transactional services, all under a unified login-Yahoo!'s answer to Amazon's one-click ordering. Yahoo! itself has started offering its own wallet, from which its members can shop at sites from such retail stalwarts as Macy's and Nordstrom's. AN OVERLAP OF MARKETS The relatively distinct market for consumer goods has little overlap in terms of corporate competition. Outside of uniforms, most corporations have little use for clothing. Outside of functions, they have little need for food. And outside of desks, they have little use for furniture. A floor of 200 people, each of whom may have his or her own VCR at home, may share a single VCR in a corporation. Likewise, the average consumer has little use for copy machines, and the largest consumer PC companies in the world derive only a small fraction of their total revenue from the consumer market. Moreover, these markets account only for finished goods, leaving out major sectors of the economy focused on components and parts. On the other hand, while some services, such as health care, education, and entertainment, remain largely consumer-focused, what consumers spend on some of their most expensive services is dwarfed by what corporations spend on those services in real estate, telecommunications, insurance, financial services, and travel. We are at a crossroads in the Net's evolution. It's not that the Internet economy has given up on the consumer. It's that business models-ever craving the steady renewable revenue that services can bring-are evolving to a point where it's easier to make the case to the business customer than to the consumer. The disadvantage is that while changes in the consumer Internet economy led to a crumbling of walls among industry and through the supply chain, the business Internet economy will have relatively low impact in comparison. ENTER THE ENTREPRENEUR As an entrepreneur, you represent a very attractive target to the provider of free services. You're willing to take action and responsibility, and may even influence the spending of others, such as employees and customers. Furthermore, while the key to motivating customers is often to evoke an emotional reaction, capitalism's unadorned invisible fingers-seeking opportunity, saving costs, and improving efficiency-guide entrepreneurs. Sites such as eToys had to invest millions of dollars to buy their servers, develop their Web sites, get online, host their sites, promote their brands, and communicate with their customers. They also had to spend millions to shore up their own organizational needs. Today, technology has advanced and organizations have launched to greatly ease the pain of starting a robust commercial site. Web content management systems, team development environments, and commerce servers have allowed new online stores to get up and running faster. Application service providers and commerce service providers let you rent programs and outsource development so you don't have to hire as much staff to build your own engines of e-commerce. Nevertheless, we won't kid you. To launch a world-class commerce site still costs millions of dollars because the bar has been raised. Now, for example, staying on customers' radar involves personalized opt-in direct mail campaigns, auctions, and sophisticated data management. However, while the Internet may not quite have allowed all of the smallest guys to unseat the biggest guys, it has allowed the smallest guys to take on the somewhat bigger guys. SOHO (small officehome office) is really where free-commerce has bloomed, allowing entrepreneurs to concentrate on their skills and look to other resources to mind the shop. SELLING YOUR SOUL: BEST OF BREED VS. INTEGRATION Even though some services profiled in Free-Commerce have a strong offline component, Web-based services have, in general, started to push the limits of Web development. Desktop.com, for example, has made extensive use of JavaScript to create an online experience that behaves more like a traditional Windows desktop than a Web page. Other advanced developments lurk behind the interface. As has often happened when weighing costs and benefits, decisions need to be made between choosing the best of breed and an integrated experience. For example, in the PC market, the best-of-breed components won. Apple's Macintosh may not have had the best video cards, the biggest hard disk, or the widest selection of software, but its integrated experience allowed a user experience that many argue the PC still has not matched. Nevertheless, the PC far outsold the Mac. Advocates of Microsoft Office argue that each of its applications were best of breed, but in the productivity applications space, the packaging together of word processor, spreadsheet, presentation program, and database unseated the former leaders of those categories-WordPerfect, Lotus 1-2-3, Harvard Presents, and dBase. With that in mind, let's return to Yahoo!, which along with other portals, has built some of the Web's most comprehensive service offerings. Among its prodigious collection are Yahoo! Messenger, Yahoo! Finance, Yahoo! Mail, and Yahoo! Auctions. Registering for one of these services registers you for all of them, but they have other ties. For example, Yahoo! Calendar can be accessed easily from within Yahoo! Mail, which can be monitored from My Yahoo!, the site's personalized front door. Undoubtedly, moving from Yahoo! service to service is more convenient than having to log in to several different services on different sites. But while Yahoo!'s services are generally good, they are not necessarily the best. Other free mail programs, for example, offer the option to have text read over the phone or free mail forwarding. Yahoo! Messenger has far fewer members than the equivalent service by America Online, while Yahoo! Auctions has a far smaller audience than eBay. Datek and Realtimequotes.com offer free real-time market information; Yahoo! Finance's quotes are delayed by 20 minutes. The competitive nature of the Web means that no one should own an advantage for too long. The integrated sites are certainly worth registering for and provide an easy way to experiment with services you might not ordinarily try. They also tend to be more established compared to many startups. In general, it's not very difficult to switch among services online, but sites use features such as personalization to raise those barriers. THE PRICE OF FREEDOM Almost all the free services have a price, and that's personal information. Often times, it can be as innocuous as your name and e-mail address, but it often includes more sensitive items like your address, phone number, and income. Surprisingly, few ask much about the specifics of your business, apart from company size and industry. Critics contend that privacy online is like a nuclear plant. Everyone enjoys the cheap power until someone's space gets contaminated. Increasingly, Web sites offer privacy policies that describe what they will and won't do with data you provide, but there have been instances of companies breaching their policies intentionally or with the unsolicited help of hackers. Worse than having your free service provider abuse your trust is having one of its partners solicit you after buying or trading for their member list. Efforts to enforce strict privacy standards have ranged from industry organizations such as TrustE to largely stillborn technical standards with obscure acronyms like P3P. Technology companies such as Zero Knowledge Systems and eNonymous have also been sprung up to anonymize the online experience. In fact, they don't even know who their own users are. If your privacy is tantamount to you, your safest bet is to avoid these services. A less extreme measure is to limit your relationships to those you trust or to companies that have more to lose than you by violating their own policies. In general, that means larger companies like America Online and Microsoft, but smaller companies also have their reputation on the line. The main difference between the Net and other channels, though, is the speed with which information travels. Don't do or divulge anything online that you wouldn't want the rest of the world to know about, although determined snoops can probably get the information through other means. If other companies, like banks and telcos, were held to the same strict standards privacy advocates are proposing for the Net, we could kiss junk mail and telemarketing goodbye. Privacy concerns also tend to be overstated by the media and are a price of progress. The telephone represented a major threat to privacy. And yet, as we complain about telemarketers who are far more annoying than direct e-mail marketers, we rush to adopt cellular telephones to ensure we have even less privacy. ABOUT FREE-COMMERCE There may be other prices to pay for free services. Some companies explicitly request the right to send you e-mail as part of the registration. Many of the free PCs discussed in Chapter 1 are tied to contracts. These have parallels in the physical world, where cellular carriers like AT&amp;T Wireless have offered free cellphones in the past in exchange for annual or even multi-year contracts. Nevertheless, the shift of the Net to services and the attractiveness of the small business as a target customer have allowed you to get things done on the Net. You can get online, find the right software, build your Web site, attract and retain customers, and stay organized throughout it all-all without spending a penny. There are over 150 free services profiled in Free-Commerce, each with information on what they offer, what they'll ask for, and even some inexpensive premium services they offer for you big spenders. The book seeks to provide access to the best and most useful services to you in running your business; we didn't include many free consumer services because they just couldn't be tied back to productivity gains. Whether you already run a small business with a few employees or have thought about launching a home office, embracing free-commerce will let you focus on what you need to, including the bottom line. Ross Scott Rubin Vice President &amp; Chief Research Fellow, Jupiter Research}
}

@inproceedings{10.1145/76619.76644,
author = {Mayer, P. J.},
title = {Entering the Ada systems design and coding market},
year = {1989},
isbn = {0897912853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/76619.76644},
doi = {10.1145/76619.76644},
abstract = {Advice is cheap, and we all know that you get what you pay for. There are many books, courses, and seminars on how to start a business. They have been written or are presented by professionals usually with far greater experience than I. While my general management background has been invaluable, the thing that best qualifies me to address this subject is the fact that we at Strictly Business Computer Systems have recently established an Ada programming shop.I'll share with you our experiences, from inception to the present. I must preface my remarks with the comment that they represent only our single effort in this area. I was fortunate in that my primary associates had successfully established and were operating a profitable business in the computer field, and it was their proven philosophy of adding value that became the keynote of our Ada effort.Additionally, we had the good fortune to make some valuable acquaintances early on in the process — relationships which enabled us to avoid some potentially costly pitfalls. Perhaps we can do the same for some of you.Now, to the subject at hand.What would seem to be the obvious first step in establishing any business is worth stating and that is the conscious act of making a commitment to the project. In our experience, the commitment was initially made about three years ago — two full years before the project was actually initiated. The delay occurred because the computer system integration business in which Strictly Business was totally immersed was growing at a pace that precluded devoting the time required to explore the Ada market.Then, a file less than a year ago, I joined Strictly Business with the sole responsibility of researching the Ada shop possibilities, and then managing the shop if the research was positive — which it obviously was. The fact that Strictly Business was willing to add me to the staff, as pure overhead from the business standpoint, clearly demonstrates that a true commitment existed. That commitment is really three-fold because undertaking such a project requires a dedication of, and money. Beyond that, you must assume the posture that characterizes the entrepreneur, and that is a total immersion in the business. You must identify with it and make it the focus of all that you do.If you and your organization are unwilling to pledge a full-fledged effort, your chances of success substantially diminish.Secondly, since the first phase of this project should be a marketing study, you must select an underlying theme that will provide a framework and give specific direction to your research. From the outset, we were convinced that within the Ada market a definite need existed for additional systems design and coding capacity. The corollary is that this appeared to provide a significant business opportunity. Our research was active — not passive or neutral. We saw an opportunity, and our purpose was to objectively and concretely confirm our perceptions.At each step in the process we were looking at what value was being added by the person, business or agency that we were exploring. Strictly Business was founded and has flourished on two basic concepts — namely, adding value through our involvement in each transaction and providing quality products and service to our clients. We scrupulously avoid being hardware and software “brokers” collecting fees for merely placing products with customers. We consider ourselves as partners with our clients and work to enhance their businesses with the products and services we provide.Having made the commitment and articulated your role and objectives, you must now begin the real work. This part of my message may be preaching to the choir. The fact that you are involved in the Ada community indicates that you have or are beginning to acquire a knowledge of the Ada marketplace. That's essential.Gather as much information as possible about every aspect of Ada. If you know the language, great. If not, that should not deter you from learning as much as you can exclusive of Ada per se. No one in our organization knew Ada before we began hiring our staff, yet several of us became knowledgeable and conversant enough to find our way around Ada circles — and in the Ada community, that's essential.Regardless of how much you learn in your explorations, the input of people active in Ada is indispensable. One of the most gratifying things our research revealed was the generosity and willingness of Ada experts to share their knowledge. We knocked on a lot of doors and did not find one that was not opened wide for us.Let me share with you some of the avenues we explored in trying to determine whet her or not a real Ada opportunity existed. We first had the advantage of coming from West Virginia whose senior U.S. Senator is Robert C. Byrd who has seen the potential of Ada and has for some years been one of its strongest advocates. With the assistance of two of his staff members, we were directed to the Software Valley Corporation which has been very much involved in bringing the advantages of Ada and Ada-related ventures to our Mountain State.Bob Verhotz, the Executive Director of Software Valley Corporation, in addition to other helpful suggestions, recommended that we contact Mr. Ralph Crafts. Bob had worked with Ralph on a number of occasions and spoke highly of his credentials and performance. We have not been disappointed.Ralph knows his way around the Ada community as well as anyone, and better than most. Almost a year ago, we employed Ralph as our consultant to define the state of the Ada market and give initial direction to our study. During intensive meetings with him, we received a great deal of background information and recommendations of additional areas into which we should extend our Ada network.These three initial contacts — Ralph, Software Valley, and Senator Byrd — confirmed that quality-conscious and professional systems developers could definitely find a place in the Ada market.At this point I think you can begin to see two things. The more obvious is the snowball effect of Ada contacts. Your first contact leads to two others which each lead to two or three more, and so on. The second thing is that we were strongly encouraged by each of these contacts, and our perceptions that excellent opportunities existed in Ada were reinforced. If anything, the potential began to look even greater than we had at first anticipated.Our tentacles, at that point, began to extend into additional areas of the Ada community. We have come to share Ralph's belief that the more people you know in this still relatively small group, the better off you are.We traveled to Washington to visit again with Senator Byrd's office. While there, with an introduction from the senator's staff, we also met with a number of people at the Ada Joint Programming Office, including the then-Air Force Deputy Director Major Al Kopp. More support and encouragement. On the same trip we cultivated an acquaintance at the Ada Information Clearinghouse. More support, encouragement, and a wealth of published information. We also briefly visited the STARS office and met with someone who was encouraging and informative about that extensive Ada project. Each of these organizations and individuals had a specific mission designed to enhance and increase the value of the Ada contribution.At that point we had begun to look at equipment and it was here that we found one of our more valuable allies and associates. From our initial contact with the personnel at RATIONAL we found them to be most helpful and open. Our sales representative made it possible for us to meet with two large firms handling major project work in Ada for the Defense Department.I don't need to tell you how valuable it can be to speak with someone who is engaged in the type of work you are contemplating and who has no ax to grind or hidden agendas as far as discussing things with you. Other vendors may have been equally helpful, but I doubt that any could have been more so. We met people doing actual project work in Ada for the government, extending our network and also making some contacts we would later pursue as we sought to put together our Ada staff.In March of this year, we attended the SlGAda conference in Phoenix where we researched a number of vendors, but more importantly, met others in the Ada community — on the commercial as well as the governmental side. We, admittedly, understood very little of the technical content of the meeting, but our purpose in attending was not technical in nature. We were networking, and our network was rapidly expanding.This might be a good point at which to remind you of the three-fold commitment required in this undertaking — time, energy, and money. By March our exploratory had gotten into its fifth month and had occupied practically all of my time and a substantial portion of the time of two of my colleagues at Strictly Business. Our travels had included a couple of trips to Washington and the trip to Phoenix as well as visits to Morgantown, WV (where the Software Valley Corporation is located) and Pittsburgh where we met with an active Ada development firm and some folks at the Software Engineering Institute of Carnegie-Mellon University. For a small firm such as ours, the budget for this venture was becoming substantial, but we were making valuable progress toward our objective.Speaking of budgets, probably the largest single start-up expenditure will be the development system you select. Spend sufficient time in making this decision. In equipment, you have a myriad of choices. With the recent validation of a large number of compilers, Ada development can be done, in one form or another, on anything from PC's to the much more sophisticated full-blown systems requiring major financial expenditures — and cost, at least in our case, was a significant consideration. But cost was only one factor.We also were concerned with other areas. Our initial plans called for a system to support ten (10) developers designing systems and/or writing code. Most hardware suppliers could accommodate that in one way or another. With our lack of experience in Ada, we were also looking for ease of familiarization and operation. And we were very much concerned with the level of support a supplier could provide. Who seemed most qualified and willing to “hold our hand,” as it were, until we gained some experience?The last major consideration was credibility. We knew that as a start-up operation gaining entree and establishing our credentials with potential contractors was critical. Our development system could say a lot about our commitment and dedication. Technical capabilities being a given, we were willing to pay some premium to project the most professional image. Bottom Line: find the system that will best enable us to efficiently and effectively develop software — to give our future clients value for their programming expenditures.We investigated three major suppliers — DEC and DG, both of whom seemed quite capable; and RATIONAL, whose development environment was written in and expressly for Ada.Weighing all the factors — system capabilities, support, ease of integration and use, reputation, cost, efficiency, etc. — we came down on the side of RATIONAL, and later decided that we would supplement them with SUN Microsystems work stations.We're happy with our decision and believe, as I said at the outset about the cost of advice, that you get what you pay for. At this time, we are confident that our system configuration will satisfy our objectives and meet our expectations. Something similar mayor may not be right for you. Your situation and needs, not our experience, should dictate your direction on equipment selection. We can only recommend that you thoroughly explore the alternatives.So where were we? We had done a lot of reading and travelling; met a lot of people with whom we'd like to be professionally associated: gotten a tremendous amount of encouragement that had been tempered with some pragmatic cautions; and made some preliminary system selections. Now we were getting down to the nitty-gritty — putting our plans and a proposal down on paper so that we could launch a sales effort to put together the financing needed to make it go.In formalizing your proposal or business plan, be prepared to spend a lot of hours at a desk with all of your background notes, a dictionary, a thesaurus, calculator, plenty of paper and pencils with generous erasers. With access to a word processor and a good spreadsheet program, you are facing a formidable task; without these two tools, it will seem, and may actually be, virtually “undoable.”Your proposal or business plan can take any of several forms, and no one is necessarily more or less appropriate or effective than any other. The plan should reflect your corporate style and philosophy. But regardless of the form, there are some elements which are indispensable.Your presentation must inspire confidence in a potential investor, assuming that you, like we, have to seek outside capital to launch your effort. The plan must clearly demonstrate that you have done your homework and thoroughly researched the subject and the market. It should deal with the principal players in your scenario, their credentials, and what they can contribute to the success of the venture — what value can each add? If yours is to be an extension of an existing business, the proposal must provide business and financial history in a realistic light, yet do so as favorably as possible. Finally, the plan must provide business forecasts in the form of projected financial statements and balance sheets. Have your accountant or someone with a strong financial background assist with the financials if that is not an area in which you have experience and confidence.Ultimately, the plan must convince its readers that you have (a) identified a need in the market and (b) that you are prepared and positioned to meet it. Experienced business pros will be reviewing the plan, so make the effort, and do it right. In preparing all of this information, keep in mind that an investor who decides to participate based on the plan will view it as your commitment. He very likely will measure your success, or lack of it, by using the plan as his yardstick. So, be conservative or at least realistic. Don't put anything into your plan that you might regret. if it were referenced some time later.One of our new acquaintances offered to review our proposal. He was doing Ada work so he could evaluate the presentation from that perspective. He was also very much involved with a managing board composed of experienced venture capitalists, so he could also take a look from that viewpoint. He gave us sound advice.My point is that you should have some disinterested parties whose opinions you value and respect, and who can freely and dispassionately critique your work, review it before you run with it. And, believe me, unless you are superhuman, you will go through several drafts and revisions before you submit the plan for outsider review. Our final plan was the sixth major revision, excluding the many internal changes and edits. Preparing an acceptable and effective plan is a humbling experience that will teach you the value of patience.One final note regarding your proposal — don't overlook its appearance. A copy of the plan and an introductory letter may be your only exposure as you try to get personal appointments to market your idea. Prepare them with care and attention to detail. Ensure that they reflect the high degree of professionalism that went into their re-search and preparation and which will characterize your business efforts. The content of the plan may not even be considered if the plan itself is not attractively presented.Now that you have what you believe is a good marketing piece, where do you go with it?Our objective was to secure local financing (within our community or at least within the state of West Virginia). We drew on personal contacts, a list of local venture capitalists that we obtained from the chamber of commerce, and suggestions offered by the CEO of one of the banks with whom we had an on-going personal and business relationship.We thoroughly explored various loan, grant, and incentive programs offered by municipal, county and state governments to attract business. If you have a university near you, they may have an office that assists with business start-ups. They may be very helpful if you choose to apply for loans or grants since this is an art form in itself. Don't overlook these potentially attractive sources of advice or capital; they could make the difference.Be prepared to make phone calls, personal visits and send written correspondence in cultivating potential investors. And be sure to have your ducks in line because most of these people did not accumulate their wealth or acquire their positions because they are fiscally naive or stupid. They are, by and large, very good business people who ask direct and probing questions and expect direct, succinct, supportable answers — and a wrong answer can quickly kill an opportunity.If local capital is not available, you will have to look farther afield. That's an area in which we can't offer much advice as we did not have to pursue it. We anticipated that if we had had to look elsewhere we would have to be even more on our toes, since we would give up the advantage of common ground. We would be negotiating on their turf rather than being from the same community as the people we were soliciting.One of the biggest difficulties we encountered was in selling something intangible. As sophisticated as many lenders and investors are, some are still uncomfortable with the computer field, and especially software, as an area of opportunity.Unless high tech businesses are already an established and accepted investment arena in your area, lenders may have difficulty grasping the concept of investing in intellectual property. Loans or investments for plant and equipment are a piece of cake — you can survey, touch, walk around or kick the tires of the collateral. In dealing with software, you lose that advantage, and many people are still wary of getting financially involved with something they can't see, touch, taste, or smell.Anticipate some initial skepticism and prepare to overcome it. BEGIN NOW. This is one area where you can't start sowing seeds and nurturing them too soon. Look for or create occasions to discuss with the financial powers in your community the role and advantages and success stories and opportunities in software development. When you come across a good article — one that's not too technical — that supports your point, send copies to appropriate people. Most will be read, and you'll be strengthening your case and laying a foundation you can build on later.Aside from the “intangibility factor,” we found that the key concern of potential investors is the make-up of your staff. If you have on board people with strong credentials and proven track records in Ada, your job will be much easier. We didn't. In fact, we had the chicken-and-egg situation of having financiers citing staff as a prerequisite on the one hand; and our inability to recruit and hire a staff until we had secured financing on the other. It was one of the most frustrating aspects of the whole process.We leaned heavily on the proven track records of those of us who were organizing the venture, even though they included no Ada experience. Special expertise has to be addressed, but good basic management skills and experience are highly regarded, well-respected, and carry a lot of weight. We also capitalized on the credentials of our consultant with whom we had reached an agreement for his continuing services after our start-up, making him a legitimate member of our team. It was true that we had considerable background in computer sales, and had on our staff experienced programmers doing custom work for clients, though not in Ada. Many people perceive experience in one area of the computer field as qualification to perform in what we knew to be largely unrelated areas. Since it worked to our advantage, we did not discourage that perception.While we were putting together our financing, we did some preliminary recruiting. We secured resumes and expressions of interest from programmers by contacting the colleges and universities that were graduating students from computer science programs in our area. From the outset, our objective had been to get our programmers locally, if possible. We believe that local residents, particularly in an area like ours, are more easily attracted to job opportunities near home and are more likely to remain with us because of their ties to the area.We recognized, however, that it was critical for us to attract at least one highly experienced Ada professional to direct the programming effort. We drew on the contacts we had made and also secured the services of two firms specializing in Ada placements. Use every tool you can muster, because this is a difficult area with the explosive growth that Ada is enjoying. Experienced people are hard to find, and you must be prepared for a difficult search and the possibility that you may not have adequately budgeted for this position. This person is key, however, and if you find the right one: the time, effort, and money expended in the search will have been well worth it.Look into training, particularly if you do as we did and recruit most of your staff with little or no practical Ada experience. Budget the time and money to allow for proper training of your people and recognize that they will be unproductive for some period of time after they come on board. We completed the hiring of our staff in early Fall. Theirs is a ten-week-long training program. We anticipate beginning work on our first contract no earlier than the first of the year. Our staff will have been on the payroll for more than three months before they take their first steps toward providing a return on the investment in them.So things have finally come together. With financing secured, you have ordered and scheduled installation of your system; hired and are training your staff; and are ready to undertake some work!Getting that first contract may be a challenge. Use every means at your disposal. If you can hire a professional who can bring contracts with him, so to speak, great! If the contacts you have made in your investigations can't help open doors for you, then you haven't been contacting the right people. If you have a Senator Byrd to lend support, bravo! Get all the help you can. Don't be bashful — most people are more than willing to lend as much help as they're able. Don't leave any stone unturned. And don't wait too late to begin looking.If, somehow, you can get a contract before you configure your shop it will certainly make it easier to attract financing. We were unable to do that. Few people will let a contract to a non-existent shop. We began to actively seek a contract as soon as we had our financing in place and our hiring underway.Use every advantage to secure that first contract, but recognize that future work will be contingent on your performance and the reputation for quality that you establish. Personal relationships will become much less a factor. Don't bite off more than you can chew on that first contract. Find something manageable that will give you some experience, allow you to establish some credibility, and is small enough to be completed in a reasonable length of time. And go all out to deliver the best product possible on or ahead of schedule. Then you're on our own, and relying on your performance record — and that's as it should be. The ball will be in your court and how you handle it will determine the flow of the game for the future.I've covered a lot of ground, and again I emphasize that this has been a review of our experience - a case study in which the last chapters are just now being written - and not a “how to” course, per se. In retrospect, I don't believe that there is much that we would do differently if we were to do it again. We approached the project as a marketing problem and treated it accordingly, drawing on the expertise of others in technical and financial areas. Some of the things we learned would enable us to compress the timeframe to establish a new venture if we were to do it again, but we are relatively well satisfied with how things went.Let me close by just saying that you can become discouraged if you allow it to happen. If you are like we were, the potential of the opportunity is so enormous and so obvious that you won't be able to easily accept the reluctance and skepticism of others. Why can't they see what's as plain as day to us? Why are things taking so long? Be patient and persist. If you're committed, do your homework, lay the groundwork, and do a good selling job, things will ultimately work out. Don't lose your sense of urgency; don't allow your interest to flag; and be patient…be patient…be patient.If we have been able to give you any ideas, then we've accomplished our objective. We wish you well. Thank you.},
booktitle = {Proceedings of the Conference on TRI-Ada '88},
pages = {567–580},
numpages = {14},
location = {Charleston, West Virginia, USA},
series = {TRI-Ada '88}
}

@phdthesis{10.5555/997603,
author = {Diab, Mona Talat and Resnik, Philip},
title = {Word sense disambiguation within a multilingual framework},
year = {2003},
publisher = {University of Maryland at College Park},
address = {USA},
abstract = {Word Sense Disambiguation (WSD) is the process of resolving the meaning of a word unambiguously in a given natural language context. Within the scope of this thesis, it is the process of marking text with explicit sense labels. What constitutes a sense is a subject of great debate. An appealing perspective, aims to define senses in terms of their multilingual correspondences, an idea explored by several researchers, Dyvik (1998), Ide (1999), Resnik &amp; Yarowsky (1999), and Chugur, Gonzalo &amp; Verdejo (2002) but to date it has not been given any practical demonstration. This thesis is an empirical validation of these ideas of characterizing word meaning using cross-linguistic correspondences. The idea is that word meaning or word sense is quantifiable as much as it is uniquely translated in some language or set of languages. Consequently, we address the problem of WSD from a multilingual perspective; we expand the notion of context to encompass multilingual evidence. We devise a new approach to resolve word sense ambiguity in natural language, using a source of information that was never exploited on a large scale for WSD before. The core of the work presented builds on exploiting word correspondences across languages for sense distinction. In essence, it is a practical and functional implementation of a basic idea common to research interest in defining word meanings in cross-linguistic terms. We devise an algorithm, SALAAM for Sense Assignment Leveraging Alignment And Multilinguality, that empirically investigates the feasibility and the validity of utilizing translations for WSD. SALAAM is an unsupervised approach for word sense tagging of large amounts of text given a parallel corpus—texts in translation—and a sense inventory for one of the languages in the corpus. Using SALAAM, we obtain large amounts of sense annotated data in both languages of the parallel corpus, simultaneously. The quality of the tagging is rigorously evaluated for both languages of the corpora. The automatic unsupervised tagged data produced by SALAAM is further utilized to bootstrap a supervised learning WSD system, in essence, combining supervised and unsupervised approaches in an intelligent way to alleviate the resources acquisition bottleneck for supervised methods. Essentially, SALAAM is extended as an unsupervised approach for WSD within a learning framework; in many of the cases of the words disambiguated, SALAAM coupled with the machine learning system rivals the performance of a canonical supervised WSD system that relies on human tagged data for training. Realizing the fundamental role of similarity for SALAAM, we investigate different dimensions of semantic similarity as it applies to verbs since they are relatively more complex than nouns, which are the focus of the previous evaluations. We design a human judgment experiment to obtain human ratings on verbs' semantic similarity. The obtained human ratings are cast as a reference point for comparing different automated similarity measures that crucially rely on various sources of information. Finally, a cognitively salient model integrating human judgments in SALAAM is proposed as a means of improving its performance on sense disambiguation for verbs in particular and other word types in general.},
note = {AAI3115805}
}

@book{10.5555/560389,
author = {Brookshier, Daniel and Govoni, Darren and Krishnan, Navaneeth and Soto, Juan Carlos},
title = {JXTA: Java P2P Programming},
year = {2002},
isbn = {0672323664},
publisher = {Sams},
address = {USA},
abstract = {From the Book: Introduction By Daniel Brookshier When thinking about how to introduce this book, I thought I might start by welcoming you to a new concept in software. I have worked with many types of software, and I have programmed exclusively in Java since it was introduced in 1995. I've seen my share of new concepts and ideas that would change the world. Java has had the biggest impact in my life, and I believe the evidence shows that it has changed the computer world. What about JXTA Why should you or I use a technology that is so new and a departure from Web Services and client server technology When I started looking around, I found that JXTA is not so much a new concept as it is a revolution. Not a revolution in the sense of new or groundbreakinga revolution like the French Revolution. As with most situations where things go wrong, you blame those in power. The French had some rather large grievances with their government. Under Louis XIV and Louis XV, there was extravagant spending, unpopular wars on foreign soil, state bankruptcy, and high taxes imposed mostly on the common man. The French revolutionaries decided that the monarchy and the elite class all had to go. And, as they say, heads would roll. Peer-to-peer is a response to a sort of server-based tyranny. Client-server, multi-tier, and Web server technologies are like kings. Servers concentrate power and resources, limit access, and restrict an individual's ability to access and control his or her own data. This is not exactly an affront to our civil rights, but it can mean that a corporation has my data on their servers. There is also a barrier to the entry. The rich and noble born and elite of France controlled resources, and only large organizations have the resources to buy and maintain large servers. With the rise of Linux, you can create a shoestring operation, but you still need to pay for bandwidth and other resources. Servers hold the applications and data we use, but we have no stake or control in them. As individuals and even large groups, we cannot muster the resources to create our own servers unless we were born rich like a noble or have the resources like a corporation. Oust the king and suddenly you are looking for someone else to govern. The French, while architecting their revolution, had some of the same thoughts as today's JXTA developers. On August 26, 1789, the National Assembly of France approved a document, entitled Declaration of the Rights of Man and of the Citizen. They based it somewhat on the declaration of independence written in America. The French document seems to be more about individuals operating in a society and, thus, more like a peer-to-peer system. Let's look at a few of the articles of the declaration to see where the French revolutionaries and JXTA agree. Men are born and remain free and equal in rights. Social distinctions may be founded only upon the general good (Article 1). Peers also achieve social status via the information or unique processing they contribute. In a server world, the server has almost all resources, while clients have little or none. The principle of all sovereignty resides essentially in the nation. No body nor individual may exercise any authority which does not proceed directly from the nation (Article 3). JXTA creates a community where no individual computer has the ability to affect the entire network unless other member peers allow it. In a sense, this is like a democracy but on a more personal level, because you vote by participation in a group or application. Rights of individual computers are also granted by the protocols that every computer on the peer-to-peer network must follow. In a server environment, the client must follow the rules of the server software and the owners of the server. When there are many servers, there are also many masters, causing the clients to follow too many different and conflicting rules. Liberty consists in the freedom to do everything which injures no one else; hence the exercise of the natural rights of each man has no limits except those which assure to the other members of the society the enjoyment of the same rights. These limits can only be determined by law (Article 4). Liberty in JXTA, like real liberty, is difficult to define. But the key difference from client/server technology is the ability to be an acting part of the application. The benefits are a bit ethereal, but imagine the ability to truly control your data. You can also process the data at any time. Is this freedom Hard to say, but it is a start. JXTA promotes freedom as well as the right to punish those that abuse it. Even a free society has laws. For a network to succeed, there needs to be some way to know when others are harmed and provide a consequence to those responsible. In a P2P network, the ability of one member to do harm is limited. The redundancy of the network reduces the impact on the society of peers but, like any society, there are criminals (or at least perceived to be). JXTA has the notion of a credential. If a peer fails to be a good citizen, its rights may be forfeited and the credential invalidated. Server environments are a bit different. Beyond denial of service attacks, being a good or a bad client is a gray area, mainly because the applications are very constrained for normal users. The server is often cast as the villain, as a hoarder of data and even breaking the trust of clients by allowing the sale of a client's data. All the citizens have a right to decide, either personally or by their representatives, as to the necessity of the public contribution; to grant this freely; to know to what uses it is put; and to fix the proportion, the mode of assessment and of collection and the duration of the taxes (Article 14). Taxation should be compared to a service fee or cost to create a service. A JXTA peer determines the level of participation in the network and, thus, the cost of its hardware and other resources. Like a consumption tax, there is a tendency to pay more, the more you use the network. Due to redundancy and shared processing, all users benefit, rather than suffering because of poor hardware. Users make their own decisions on how they configure and use their P2P software. Inappropriate and draconian controls instituted by a server's owners or chosen software are eliminated. In another way, article 14 also shows the difference between server and P2P technology. With servers, an infrastructure must be maintained. Server software, because of its costs, looks like a government that requires a tax to operate that is usually flat rather than based on participation. With a peer network, peers share resources and each peer pays its share by its existence and level of participation. A society in which the observance of the law is not assured, nor the separation of powers defined, has no constitution at all (Article 16). This is sort of an obvious statement for JXTA. If you don't use JXTA protocols (our Constitution and basic laws), you cannot be a member of the community. If you are using JXTA and do abuse its community, you are usually just hurting yourself. Since property is an inviolable and sacred right, no one shall be deprived thereof except where public necessity, legally determined, shall clearly demand it, and then only on condition that the owner shall have been previously and equitably indemnified (Article 17). P2P started to become popular with the introduction of Napster. Sadly, the implication was that P2P was associated with piracy. Although Napster was originally formed with the idea that only valid owners of music would access digital versions, there was probably more piracy than legitimate use. Consequently, Napster has suffered in court with a severe reduction in the number of users. P2P networks, such as Gnutella, are also devoid of rights management. These systems cannot be taken to court as Napster was because they are truly distributed. However, because of their uncontrolled nature, corporations and ISPs are restricting their traffic, and individual users are being charged with crimes or losing rights to services. It is highly probable that these systems will be disabled or at least inconvenienced. The ultimate goal for JXTA is to be a good citizen and respect copyright and property laws. The reason is simple, without respectability, JXTA is seen as another Napster or Gnutella and will be filtered by ISPs and corporations. Respect others' rights to their property and you will be treated as a fellow citizen and allowed to use the Internet and corporate infrastructures. Most of us live in a commercial society, and we deal with commercial entities. Where there is unfair trade or criminal activity, the system of government or those affected will tend to remove those who abuse the system. Although you may argue that entities like record companies are not acting fairly, the fact is that the laws are currently written to protect themnot those who dislike the law and protest it by circumvention. Napster and the newer incarnations have not changed any laws through their public protests and active breaking of laws. We still need to follow the rule of law to succeed. JXTA Scale Another revolutionary idea of JXTA is what it empowers you to build. Without a central server, with its costs and limits, much more is possible. This does not necessarily mean new types of applications, just a greater scale than was possible in a server environment. A good example of the scalability of JXTA applications is simple catalog for e-commerce. Normally, you would need a large number of clustered servers to handle a large number of transactions. With JXTA, the catalog and its software are distributed automatically among peer computers. Instead of a server that must show the same catalog to millions of users, you just need one PC to distribute the first copy and any updates. All that needs to be centralized is the final order acceptance and credit card transaction, and even that is distributable to some extent. There are many benefits of a P2P catalog from cost savings to the ability of a user to access the catalog offline. The application also runs faster because the user is not as limited to his or her connection speed or waiting in a queue of other users. Add to this 100 percent availability to most users, and you ensure that the verities of the Internet or of a server farm are no longer a part of the risk equation. Another scale feature is raw computing power. In a server environment, each client has access only to limited resources that must be shared by all users. With JXTA, each peer has all the power of the machine it is running on, plus the shared power of all the other peers with which it is collaborating. Is JXTA a New Concept Just by reading this far, you may have seen very familiar concepts. In the prior examples on scale, it is very easy to associate the same goals with distributed computing. The examples of P2P throughout the book are all possible using other methods. However, the point of JXTA is not necessarily to replace these methods. JXTA is a platform with specific protocols to talk to other JXTA platforms in a peer-to-peer network. It is not an application or a library created to build specific applications. The reason JXTA exists is to enable the refactoring of many different applications in a P2P environment. Like the catalog example, the idea is to move away from centralized infrastructures to gain the benefits of a distributed system. RMI, CORBA, and Web Services are distant cousins of JXTA. They are either oriented toward a client/server or limited point-to-point communications. JXTA may seem to provide similar services, but the framework beneath is very different. For example, you can implement remote method invocation. The key difference between JXTA and others is that the delivery of the command to execute can span barriers like firewalls. The remote command can be sent to groups of computers or just a single computer, depending on the type of task. JXTA Risks I think we can safely agree that JXTA is not like anything else. Is JXTA something to bet your time as well as your fortune on There are risks. Some are new and others are well known. Some are being fixed as you read this book, and others simply need to be implemented on the current JXTA platform. The largest risk now is that JXTA will be in flux over the next couple of years. The good news is that the community of developers will try to keep the network stable for the purpose of keeping their products working. When you reach a certain point, developers learn to hate change, even when the project is open source. It is not all a bed of roses in other areas. There are aspects to a P2P system that can be problematic. In our catalog example, it does take time to propagate the catalog to all users. The same time delay is true of updates and transactions. We are at the start of the JXTA revolution. It is time to think revolutionarily thoughts. The reign of client server is about to fall. Read on and join the revolution. Viva la revolution! Daniel Brookshier JXTA Community Member, Java Consultant January 2002, Dallas, Texas What This Book Covers This book will only cover the Java J2SE reference platform implementation of JXTA. We will not cover the C++, J2ME, Pearl, or other languages that are being used to create JXTA platforms. The J2SE version is the reference platform and best for experimentation or explanation of JXTA protocols. Java is also the most popular language for JXTA development at this time. This book is intended to introduce new developers to the JXTA API and selected applications and services. Our goal is for the reader to understand P2P concepts and be able to build useful applications using JXTA. We do not cover detailed aspects of how the JXTA platform is implemented unless it adds value to the explanation on how to use it. Who Should Use This Book This book is written for readers who need an introduction to P2P and for those who want to learn JXTA. You should already be comfortable with Java. You do not need to know anything about JXTA or peer-to-peer programming. By the end of the book, you should be able to create simple P2P applications using JXTA and the J2SE JXTA reference platform. How This Book is Organized This book is organized with two goals. The first goal is to explain P2P and JXTA in general terms. The second goal is to create applications that use JXTA. Finally, we cover specific applications with the aim of furthering an understanding of JXTA while showing how more complete applications are written. This arrangement was chosen so that the reader can get an overview of JXTA and then build an understanding of how to use its various parts. Web Resources and Example Code You can download the source code for examples presented in this book from http://www.samspublishing.com. When you reach that page, enter this book's ISBN number (0672323664) in the search box to access information about the book and a Source Code link. The NetBeans IDE was used for much of the code that is found in the book. NetBeans is available at http://www.netbeans.org. Because Forte from Sun Microsystems is derived from NetBeans, it should work as well. You can also use your favorite editor or IDE, but the ANT scripts were created within NetBeans and Forte. Also on the site are files that can be used with MagicDraw from NoMagic at http://www.MagicDraw.com. The tool is written in Java and runs on most Java-compatible platforms. The demo version will allow you to browse and print the JXTA diagrams used in the book, but it will not allow you to save changes. The MagicDraw files follow the XMI standard for UML representation in XML, so other UML tools that support the standard should work (drawings may look different). © Copyright Pearson Education. All rights reserved.}
}

@book{10.5555/580423,
author = {Karlins, David},
title = {FrontPage 2002 Virtual Classroom},
year = {2001},
isbn = {0072191724},
publisher = {McGraw-Hill Professional},
abstract = {From the Book: Introduction Who Will Enjoy This You will, if you want to create attractive, feature-packed, and easy-to-use Web sites with FrontPage. Because of its Microsoft Office-like interface, FrontPage is a very accessible Web design tool. But beneath the surface, youll find powerful features that allow you to edit pictures, generate JavaScripts, and collect input data-features not available in any other Web design package. My goal with this book is to make those features accessible to both brand new Web designers, as well as veteran FrontPage designers who would like to add advanced features to their sites. Ive been teaching folks like yourself to use FrontPage for five years now. Ive written Microsoft authorized books on how to pass the Microsoft FrontPage MCSD exam (and Ive passed the Microsoft FrontPage Certified Professional exam myself). But Ive also taught people FrontPage who have never created a Web site before. Perhaps most importantly, I use FrontPage almost every day to create Web sites. Ive learned through trial and error the best ways to create Web sites with FrontPage, and also the best ways to learn FrontPage. Beginning level, intermediate, and many advanced FrontPage designers will all find important resources in this book. Many chapters approach concepts like tables, frames, and input forms on different levels. At first, new FrontPage users might want to try the more basic step-by-step sections early in the chapter, while more advanced developers will push the envelope using all the features covered in a chapter. And, this book is much more than a book. The accompanying CD-ROM, which is discussed in detail in the remainder of this introduction, has more than an hour of videos with demonstrations, tips, and candid advice. FrontPage 2002 Virtual Classroom CD This CD contains an exciting new kind of video-based instruction to help you learn FrontPage faster. We believe this learning tool is a unique development in the area of computer-based training. The author actually talks to you, right from your computer screen, demonstrating topics he wrote about in the book. Moving screencams and slides accompany the presentation, reinforcing what youre learning. The technology and design of the presentation were developed by Brainsville.com. The content on the CD-ROM was developed by OsborneMcGraw-Hill, David Karlins, and Brainsville.com. Patents (pending), copyright, and trademark protections apply to this technology and the name Brainsville.com. To ensure that the lessons play as smoothly as possible, please read the following directions for usage of the CD-ROM. Getting Started The CD-ROM is optimized to run under Windows 9598MENT2000 using the QuickTime player version 5 (or greater), from Apple. This CD-ROM is not designed to run on a Mac. If you dont have the QuickTime 5 player installed, you must install it either by downloading it from the Internet at http:www. quicktime.com, or running the Setup program from the CD-ROM. If you install from the Web, its fine to use the free version of the QuickTime player. You dont need to purchase the full version. To install the QuickTime player from the CD-ROM on a Windows PC: 1. Insert the CD-ROM in the drive. 2. Use Explorer or My Computer to browse to the CD-ROM. 3. Open the QuickTime folder. 4. Double-click the setup program there. 5. Follow the setup instructions on screen. Running the CD in Windows 9598MENT2000 Minimum Requirements: QuickTime 5 player Pentium II P300 (or equivalent) 64MB of RAM 8X CD-ROM Windows 95, Windows 98, Windows 2000, Windows ME, or Windows NT 4.0 with at least Service Pack 4 16-bit sound card and speakers FrontPage 2002 Virtual Classroom CD-ROM can run directly from the CD (see the following for running it from the hard drive for better performance if necessary) and should start automatically when you insert the CD in the drive. If the program does not start automatically, your system might not be set up to automatically detect CDs. To change this, you can do the following: 1. Choose Settings I Control Panel, and click the System icon. 2. Click the Device Manager tab in the System Properties dialog box. 3. Double-click the Disk drives icon and locate your CD-ROM drive. 4. Double-click the CD-ROM drive icon, and then click the Settings tab in the CD-ROM Properties dialog box. Make sure the Auto Insert Notification box is checked. This specifies that Windows will be notified when you insert a compact disc into the drive. If you dont care about the auto-start setting for your CD-ROM, and dont mind the manual approach, you can start the lessons manually. Heres how: 1. Insert the CD-ROM. 2. Double-click the My Computer icon on your Windows desktop. 3. Open the CD-ROM folder. 4. Double-click the startnow.exe icon in the folder. 5. Follow the instructions on screen to start. When the program autostarts, youll see a small window in the middle of your screen with an image of the book; click that image to launch the QuickTime player and start the lessons. The QuickTime player window should open and the Virtual Classroom introduction should begin running. On some computers, after the lesson loads you must click the Play button to begin. The Play button is the big round button with an arrow on it at the bottom center of the QuickTime player window. It looks like the play button on a VCR. You can click the links in the lower-left region of the QuickTime window to jump to a given lesson. The author will explain how to use the interface. The QuickTime player will completely fill a screen that is running at 800 x 600 resolution. (This is the minimum resolution required to play the lessons.) For screens with higher resolution, you can adjust the position of the player on screen, as you like. If you are online, you can click the Brainsville.com logo under the index marks to jump directly to the Brainsville.com Web site for information about additional video lessons from Brainsville.com. (See the description in the back of this book about the Web Design CD Extra for more details.) Improving PlayBack Your Virtual Classroom CD-ROM employs some cutting-edge technologies, requiring that your computer be pretty fast to run the lessons smoothly. Many variables determine a computers video performance, so we cant give you specific requirements for running the lessons. CPU speed, internal bus speed, amount of RAM, CD-ROM drive transfer rate, video display performance, CD-ROM cache settings and other variables will determine how well the lessons play. Our advice is to simply try the CD. The disk has been tested on laptops and desktops of various speeds, and in general, youll need at least a Pentium II-class computer running in excess of 300Mhz for decent performance. (If youre doing serious Web-design work, its likely your machine is at least this fast.) Close Other Programs For best performance, make sure you are not running other programs in the background while viewing the CD-based lessons. Rendering the video onscreen takes a lot of computing power, and background programs such as automatic e-mail checking, Web-site updating, or Active Desktop applets (such as scrolling stock tickers) can tax the CPU to the point of slowing the videos. Adjust the Screen Color Depth to Speed Up Performance Its possible that the authors lips will be out of synch with his voice, just like Web-based videos often look. There are a couple solutions: Lowering the color depth to 16-bit color makes a world of difference with many computers, laptops included. Rarely do people need 24-bit or 32-bit color for their work anyway, and it makes scrolling your screen (in any program) that much slower when running in those higher color depths. Try this: 1. Right-click the desktop and choose Properties. 2. Click the Settings tab. 3. In the Colors section, open the drop-down list box and choose a lower setting. If you are currently running at 24-bit (True Color) color, for example, try 16-bit (High Color). Dont use 256 colors, because video will appear very funky if you do. 4. OK the box. With most computers these days, you dont have to restart the computer after making this change. The video should run more smoothly now, because your computers CPU doesnt have to work as hard to paint the video pictures on your screen. If adjusting the color depth didnt help the synch problem, see the following section about copying the CDs files to your hard disk. When lessons are playing youre likely to not interact with the keyboard or mouse. Because of this, your computer screen might blank, and in some cases (such as with laptops) the computer might even go into a standby mode. Youll want to prevent these annoyances by turning off your screen saver and checking the power options settings to ensure they dont kick in while youre viewing the lessons. You make settings for both of these parameters from the Control Panel. 1. Open Control Panel, choose Display, and click the Screen Saver tab. Choose None for the screen saver. 2. Open Control Panel, choose Power Management, and set System Standby, Turn off Monitor, and Turn off Hard Disks to Never. Then click Save As and save this power setting as Brainsville Courses. You can return your power settings to their previous state, if you like, after you are finished viewing the lessons. just use the Power Schemes drop-down list and choose one of the factorysupplied settings, such as HomeOffice Desk. Copy the CD Files to the Hard Disk to Speed Up Performance The CD-ROM drive will whir quite a bit when running the lessons from the CD. If your computer or CD-ROM drive is a bit slow, its possible the authors lips will be out of synch with his voice, just like Web-based videos often look. The video might freeze or slow down occasionally, though the audio will typically keep going along just fine. If you dont like the CD constantly whirring, or you are annoyed by outof-synch video, you might be able to solve either or both problems by copying the CD-ROMs contents to your hard disk and running the lessons from there. To move CD content to your hard disk: 1. Using My Computer or Explorer, check to see that you have at least 650M free space on your hard disk. 2. Create a new folder on your hard disk (the name doesnt matter) and copy all the contents of the CD-ROM to the new folder. (You must preserve the subfolder names and folder organization as it is on the CD-ROM). 3. Start the program by opening the new folder and double-clicking the file startnow.exe. This will automatically start the lessons and run them from the hard disk. 4. (Optional) For convenience, you can create a shortcut to the startnow.exe file and place it on your desktop. You will then be able to start the program by clicking the shortcut. Update Your QuickTime Player The QuickTime software is updated frequently and posted on the Apple QuickTime Web site ( ). You can update your software by clicking Update Existing Software, from the Help menu in the QuickTime player. We strongly suggest you do this from time to time. Make Sure Your CD-Rom Drive is Set for Optimum Performance CD-ROM drives on IBM PCs can be set to transfer data using the DMA (Direct Memory Access) mode, assuming the drive supports this faster mode. If you are experiencing slow performance and out-of-synch problems, check this setting. These steps are for Windows 98 and Windows ME: 1. Choose Control Panel I System. 2. Click the Device Manager tab. 3. Click the plus (+) sign to the left of the CD-ROM drive. 4. Right-click the CD-ROM drive. 5. Choose Properties. 6. Click the Settings tab. 7. Look to see if the DMA check box is turned on (has a check mark in it). If selected, this increases the CD-ROM drive access speed. Some drives do not support this option. If the DMA check box remains selected after you restart Windows, this option is supported by the device. In Windows 2000, the approach is a little different. You access the drives settings via Device Manager as above, but click IDEATAPI Controllers. Right-click the IDE channel that your CD-ROM drive is on, choose Properties, and make the settings as appropriate. (Choose the device number, 0 or 1, and check the settings.) Typically its set to DMA If Available, which is fine. Its not recommended that you change these settings unless you know what you are doing! TroubleShooting This section offers solutions to common problems. Check for much more information about the QuickTime player, which is the software the Virtual Classroom CD uses to play. The CD Will Not Run If you have followed the instructions above and the program will not work, you might have a defective drive or CD. Be sure the CD is inserted properly in the drive. Test the drive with other CDs to see if they run. The ScreenCam Movie In! A Lesson Tangs If the author continues to talk, but the accompanying screencam seems to be stuck, just click the lesson index in the lower-left region of the QuickTime window to begin your specific lesson again. If this doesnt help, close the QuickTime window; then start the Virtual Classroom CD again. Volume Is Too Low or Totally Silent 1. Check your system volume first. Click the speaker icon next to the clock, in the lower-right corner of the screen. A little slider pops up. Adjust the slider, and make sure the Mute check box is not checked. 2. Next, if you have external speakers on your computer, make sure your speakers are turned on, plugged in, wired up properly, and the volume control on the speakers themselves is turned up. 3. Note that the QuickTime player also has a volume control setting. The setting is a slider control in the lower-left of the QuickTime player window. 4. The next place to look if youre still having trouble is in the Windows volume controls. Double-click the speaker next to the clock and it will bring up the Windows Volume Control sliders. Make sure the slider for Wave is not muted, and make sure its positioned near the top. For Technical Support Phone Hudson Software at (800) 217-0059 Visit Visit}
}

