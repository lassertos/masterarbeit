[
    {
        "title": "JupyTEP IDE as an online tool for earth observation data processing",
        "abstract": "The paper describes a new tool called JupyTEP integrated development environment (IDE), which is an online integrated development environment for earth observation data processing available in the cloud. This work is a result of the project entitled JupyTEP IDE-Jupyter-based IDE as an interactive and collaborative environment for the development of notebook style EO algorithms on network of exploitation platforms infrastructure carried out in cooperation with European Space Agency. The main goal of this project was to provide a universal earth observation data processing tool to the community. JupyTEP IDE is an extension of Jupyter software ecosystem with customization of existing components for the needs of earth observation scientists and other professional and non-professional users. The approach is based on configuration, customization, adaptation, and extension of Jupyter, Jupyter Hub, and Docker components on earth observation data cloud infrastructure in the most flexible way; integration with accessible libraries and earth observation data tools (sentinel application platform (SNAP), geospatial data abstraction library (GDAL), etc.); adaptation of existing web processing service (WPS)-oriented earth observation services. The user-oriented product is based on a web-related user interface in the form of extended and modified Jupyter user interface (frontend) with customized layout, earth observation data processing extension, and a set of predefined notebooks, widgets, and tools. The final IDE is addressed to the remote sensing experts and other users who intend to develop Jupyter notebooks with the reuse of embedded tools, common WPS interfaces, and existing notebooks. The paper describes the background of the system, its architecture, and possible use cases.",
        "keywords": "Docker; eEarth observation data processing; IDE; IPython; Jupyter notebook; web application; web processing service",
        "released": 2019,
        "link": "https://doi.org/10.3390/rs11171973"
    },
    {
        "title": "A distributed modular platform for the development of cloud based applications",
        "abstract": "In this paper we describe the CIRANO platform, a modular Integrated Development Environment (IDE) for cloud based applications. The proposed platform is built to support Model Driven Development (MDD) and team collaboration, facilitating the rapid development of advanced applications in the cloud. The paper presents at a first stage the state of the art in the field of cloud IDEs and describes the design, implementation and technical details of the CIRANO platform. The main features of the proposed platform are presented in two case studies concerning the development of an application from scratch and porting of an existing application. The paper discusses the findings in comparison with existing tools and proposes extensions of the platform as future work. (C) 2017 Elsevier B.V. All rights reserved.",
        "keywords": "Cloud based development; Model Driven Development; Cloud Integrated Development Environment",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.future.2017.02.035"
    },
    {
        "title": "Codetrail: Connecting source code and web resources",
        "abstract": "When faced with the need for documentation, examples, bug fixes, error descriptions, code snippets, workarounds, templates, patterns, or advice, software developers frequently turn to their web browser. Web resources both organized and authoritative as well as informal and community-driven are heavily used by developers. The time and attention devoted to finding (or re-finding) and navigating these sites is significant. We present Codetrail, a system that demonstrates how the developer’s use of web resources can be improved by connecting the Eclipse integrated development environment (IDE) and the Firefox web browser. Codetrail uses a communication channel and shared data model between these applications to implement a variety of integrative tools. By combining information previously available only to the IDE or the web browser alone (such as editing history, code contents, and recent browsing), Codetrail can automate previously manual tasks and enable new interactions that exploit the marriage of data and functionality from Firefox and Eclipse. just as the IDE will change the contents of peripheral views to focus on the particular code or task with which the developer is engaged, so, too, the web browser can be focused on the developer’s current context and task. (C) 2009 Elsevier Ltd. All rights reserved.",
        "keywords": "Software development; Development environment; Web browser; Human-computer interaction",
        "released": 2009,
        "link": "https://doi.org/10.1016/j.jvlc.2009.04.003"
    },
    {
        "title": "Watcher: Cloud-based coding activity tracker for fair evaluation of programming assignments",
        "abstract": "Online learning has made it possible to attend programming classes regardless of the constraint that all students should be gathered in a classroom. However, it has also made it easier for students to cheat on assignments. Therefore, we need a system to deal with cheating on assignments. This study presents a Watcher system, an automated cloud-based software platform for impartial and convenient online programming hands-on education. The primary features of Watcher are as follows. First, Watcher offers a web-based integrated development environment (Web-IDE) that allows students to start programming immediately without the need for additional installation and configuration. Second, Watcher collects and monitors the coding activity of students automatically in real-time. As Watcher provides the history of the coding activity to instructors in log files, the instructors can investigate suspicious coding activities such as plagiarism, even for a short source code. Third, Watcher provides facilities to remotely manage and evaluate students’ hands-on programming assignments. We evaluated Watcher in a Unix system programming class for 96 students. The results showed that Watcher improves the quality of the coding experience for students through Web-IDE, and it offers instructors valuable data that can be used to analyze the various coding activities of individual students.",
        "keywords": "online learning; coding activity; cloud platform; Web-IDE",
        "released": 2022,
        "link": "https://doi.org/10.3390/s22197284"
    },
    {
        "title": "Web-STAR: A visual web-based IDE for a story comprehension system",
        "abstract": "We present Web-STAR, an online platform for story understanding built on top of the STAR reasoning engine for STory comprehension through ARgumentation. The platform includes a web-based integrated development environment, integration with the STAR system, and a web service infrastructure to support integration with other systems that rely on story understanding functionality to complete their tasks. The platform also delivers a number of “social” features, including a community repository for public story sharing with a built-in commenting system, and tools for collaborative story editing that can be used for team development projects and for educational purposes.",
        "keywords": "web-based IDE; story understanding; argumentation; reasoning; visual programming; collaboration",
        "released": 2019,
        "link": "https://doi.org/10.1017/S1471068418000443"
    },
    {
        "title": "Engineering cloud: Flexible and integrated development environment",
        "abstract": "Nowadays product development must be done speedily and in a way that can respond to changing business environments. Fujitsu has created a development environment that uses technical computing and is known as Flexible Technical Computing Platform (FTCP), on a cloud. Fujitsu is providing it as Engineering Cloud to its customers and Product Development Department. Based on an example of applying Engineering Cloud to mobile phones, this paper describes the characteristics of FTCP, an overview of Fujitsu’s Engineering Cloud service and its merits, and technology for promptly displaying the results of large-scale simulations, which is important technology for achieving Engineering Cloud. Based on Fujitsu’s know-how that it has accumulated during its in-house operation of Engineering Cloud, it has become possible to link the created FTCP and customers’ existing development environments. This in turn raises hopes for a shortened product development time and higher product quality.",
        "keywords": "",
        "released": 2011,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000306819100007"
    },
    {
        "title": "Model-driven agent-based web services IDE",
        "abstract": "Web services have not exploited sufficient semantics and approaches to dynamic service-oriented operations. Software agents have been developed for these operations. Current efforts on integrating Web services and agents are predominantly concerned with enabling the agents in existing systems to request, provide or broker Web services. This paper presents the notion of agent-based Web services (AWS) and describes an integration framework that utilizes software agents’ model to construct semantic and dynamic Web services. Essential concerns about the integration have been addressed. Basic AWS meta model and model-driven integrated development environment (IDE) have been developed. The IDE has been evaluated by a case study of an e-Marketplace service, reverse auction.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000225684500043"
    },
    {
        "title": "An overview of platforms for cloud based development",
        "abstract": "This paper provides an overview of the state of the art technologies for software development in cloud environments. The surveyed systems cover the whole spectrum of cloud-based development including integrated programming environments, code repositories, software modeling, composition and documentation tools, and application management and orchestration. In this work we evaluate the existing cloud development ecosystem based on a wide number of characteristics like applicability (e.g. programming and database technologies supported), productivity enhancement (e.g. editor capabilities, debugging tools), support for collaboration (e.g. repository functionality, version control) and post-development application hosting and we compare the surveyed systems. The conducted survey proves that software engineering in the cloud era has made its initial steps showing potential to provide concrete implementation and execution environments for cloud-based applications. However, a number of important challenges need to be addressed for this approach to be viable. These challenges are discussed in the article, while a conclusion is drawn that although several steps have been made, a compact and reliable solution does not yet exist.",
        "keywords": "Cloud computing; Integrated Development Environment (IDE); Code repositories; Software modeling; Orchestration tools",
        "released": 2016,
        "link": "https://doi.org/10.1186/s40064-016-1688-5"
    },
    {
        "title": "A page-centric approach to web application development",
        "abstract": "This paper proposes a page-centric approach for developing Web applications in which an application is modeled as a state machine of a UML that takes into account the state of a Web page. Application variables are divided into page-related variables and others. Business logics are also divided into presentation-related logics and others. In this paper, based oil our model, a Web application development environment is also described. Using this environment, we can intuitively define the behavior of a Web application. Examples of applications developed by our method are given. (C) 2009 Wiley Periodicals, Inc. Electron Comm Jpn, 92(7): 47-56, 2009; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/ecj.10039",
        "keywords": "Web application; application framework; integrated development environment; UML",
        "released": 2009,
        "link": "https://doi.org/10.1002/ecj.10039"
    },
    {
        "title": "Construction of integrated design development environment and its deployment on cloud",
        "abstract": "It is crucial for manufacturers in fierce competition with global vendors to continue providing products incorporating the functions, performance, quality and environmental resistance desired by customers faster and at lower costs while meeting the diversifying market needs. Accordingly, there is an urgent need to construct a global design and development environment that accelerates development while utilizing more than ever the “knowledge” existing in the fields of development and manufacturing. Fujitsu has launched and utilized a design and development environment for wide-ranging products including supercomputers, servers, network devices and mobile phones as an in-house private cloud for integrated design. We also offer this environment to external customers through forms of clouds in line with the respective customer needs as a next-generation manufacturing infrastructure service. This paper presents the integrated development environment that Fujitsu established by developing and introducing advanced cloud technology to a design and development environment that contains Fujitsu’s many years of manufacturing know-how. The overall picture, characteristics and future direction of this integrated development environment are described.",
        "keywords": "",
        "released": 2012,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000310171300003"
    },
    {
        "title": "An integrated software development environment for web applications",
        "abstract": "In recent years, the World Wide Web has become an ideal platform for developing Internet applications. World Wide Web service and application engineering is a complex task. Many web applications at present are large-scale and involve hundreds or thousands of web pages and sophisticated interactions with users and databases. Thus, improving the quality of web applications and reducing development costs are important challenges for the Internet industry. One way to resolve the difficulty is to provide web application developers with an integrated development environment. In this paper, I propose an efficient methodology and development environment for web application programs. This environment includes a design model to represent data and navigational structure, a modeling language for the notation technique of the design model, and a process model to define development stages.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000236904500011"
    },
    {
        "title": "Pyiron: An integrated development environment for computational materials science",
        "abstract": "To support and accelerate the development of simulation protocols in atomistic modelling, we introduce an integrated development environment (IDE) for computational materials science called pyiron (http://pyiron.org). The pyiron IDE combines a web based source code editor, a job management system for build automation, and a hierarchical data management solution. The core components of the pyiron IDE are pyiron objects based on an abstract class, which links application structures such as atomistic structures, projects, jobs, simulation protocols and computing resources with persistent storage and an interactive user environment. The simulation protocols within the pyiron IDE are constructed using the Python programming language. To highlight key concepts of this tool as well as to demonstrate its ability to simplify the implementation and testing of simulation protocols we discuss two applications. In these examples we show how pyiron supports the whole life cycle of a typical simulation, seamlessly combines ab initio with empirical potential calculations, and how complex feedback loops can be implemented. While originally developed with focus on ab initio thermodynamics simulations, the concepts and implementation of pyiron are general thus allowing to employ it for a wide range of simulation topics.",
        "keywords": "Modelling workflow; Integrated development environment; Complex simulation protocols",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.commatsci.2018.07.043"
    },
    {
        "title": "Evaluations of JaguarCode: A web-based object-oriented programming environment with static and dynamic visualization",
        "abstract": "To increase program comprehension and overcome learning obstacles of Object-Oriented Programming (OOP), various visualization techniques have been adopted in educational OOP environments. Some provide software development with visual notations without source code, while others support programming with visual aids. Our research supports Java programming along with UML diagrams (class, object, and sequence) and dynamic execution traces of program synchronized in a web-based programming environment - JaguarCode. It aims to help students better understand the static structure and dynamic behavior of Java programs, as well as object-oriented design concepts. This paper reports on the evaluation results of JaguarCode to investigate its effectiveness and user satisfaction through quantitative and qualitative experiments. The experimental results revealed that having both static and dynamic visualizations did positively impact the correctness of program understanding and tracing problems, while the visual representations did affect students’ understanding on the program execution of the problems to higher accuracy. It was also observed that students were satisfied with the aspects of those visualizations provided in JaguarCode.",
        "keywords": "Web-based; Object-oriented; Programming environment; Static and dynamic visualization; UML diagram",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.jss.2018.07.037"
    },
    {
        "title": "Agent-based web services framework and development environment",
        "abstract": "The current Web services technologies have not exploited sufficient semantics and approaches to dynamic service-oriented operations in open environments. This paper argues that such operations can be realized through agent-oriented interaction approaches. The key challenge is to develop an integration framework for the two paradigms, agent- and service-oriented, in a way that capitalizes on their individual strengths. This paper proposes the notion of agent-based Web services (AWS). We address several critical issues, including the appropriate architectural framework and the structure of its main elements (agent-based Web services), their meta-model, supporting technologies, integration method, and implementation approach. An integrated development environment for this framework called SOAStudio has been developed and tested by implementing a case study of a reverse auction e-marketplace.",
        "keywords": "Web services; software agents; agent-based Web services; semantic description; knowledge representation; e-marketplaces",
        "released": 2004,
        "link": "https://doi.org/10.1111/j.0824-7935.2004.00260.x"
    },
    {
        "title": "Demonstration of panda: A weakly supervised entity matching system",
        "abstract": "Entity matching (EM) refers to the problem of identifying tuple pairs in one or more relations that refer to the same real world entities. Supervised machine learning (ML) approaches, and deep learning based approaches in particular, typically achieve state-of-the-art matching results. However, these approaches require many labeled examples, in the form of matching and non-matching pairs, which are expensive and time-consuming to label. In this paper, we introduce Panda, a weakly supervised system specifically designed for EM. Panda uses the same labeling function abstraction as Snorkel, where labeling functions (LF) are user-provided programs that can generate large amounts of (somewhat noisy) labels quickly and cheaply, which can then be combined via a labeling model to generate accurate final predictions. To support users developing LFs for EM, Panda provides an integrated development environment (IDE) that lives in a modern browser architecture. Panda’s IDE facilitates the development, debugging, and life-cycle management of LFs in the context of EM tasks, similar to how IDEs such as Visual Studio or Eclipse excel in general-purpose programming. Panda’s IDE includes many novel features purpose-built for EM, such as smart data sampling, a builtin library of EM utility functions, automatically generated LFs, visual debugging of LFs, and finally, an EM-specific labeling model. We show in this demo that Panda IDE can greatly accelerate the development of high-quality EM solutions using weak supervision.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.14778/3476311.3476332"
    },
    {
        "title": "An analysis of HTML and CSS syntax errors in a web development course",
        "abstract": "Many people are first exposed to code through web development, yet little is known about the barriers beginners face in these formative experiences. In this article, we describe a study of undergraduate students enrolled in an introductory web development course taken by both computing majors and general education students. Using data collected during the initial weeks of the course, we investigate the nature of the syntax errors they make when learning HTML and CSS, and how they resolve them. This is accomplished through the deployment of openHTML, a lightweight web-based code editor that logs user activity. Our analysis reveals that nearly all students made syntax errors that remained unresolved in their assessments, and that these errors continued weeks into the course. Approximately 20% of these errors related to the relatively complex system of rules that dictates when it is valid for HTML elements to be nested in one another. On the other hand, 35% of errors related to the relatively simple tag syntax determining how HTML elements are nested. We also find that validation played a key role in resolving errors: While the majority of unresolved errors were present in untested code, nearly all of the errors that were detected through validation were eventually corrected. We conclude with a discussion of our findings and their implications for computing education.",
        "keywords": "Human Factors; Web development; code editors; computational literacy",
        "released": 2015,
        "link": "https://doi.org/10.1145/2700514"
    },
    {
        "title": "Large-scale ontology development and agricultural application based on knowledge domain framework",
        "abstract": "The key activity to build semantic web is to build ontologies. But today, the theory and methodology of ontology construction is still far from ready. This paper proposed a theoretical framework for massive knowledge management - the knowledge domain framework (KDF), and introduces an integrated development environment (IDE) named large-scale ontology development environment (LODE), which implements the proposed theoretical framework. We also compared LODE with other popular ontology development environments in this paper. The practice of using LODE on management and development of agriculture ontologies shows that knowledge domain framework can handle the development activities of large scale ontologies. Application studies based on the principle of knowledge domain framework and LODE was described briefly.",
        "keywords": "massive knowledge management; knowledge domain framework (KDF); large-scale ontology development environment (LODE); agricultural application",
        "released": 2012,
        "link": "https://doi.org/10.1016/S2095-3119(12)60071-9"
    },
    {
        "title": "Cloud evolutionary computation system for advanced engineering analytics",
        "abstract": "The range of applications of artificial intelligence (AI) that is based on nature-inspired metaheuristics is rapidly increasing across various scientific fields as it is used to solve complex engineering problems. This work develops a cloud evolutionary machine learning system, called the nature-inspired metaheuristic optimization and prediction system (NiMOPS) that is composed of metaheuristic AI and web modules. The objective of the proposed system is to provide a user-friendly web analytics for making efficient, effective, and accurate predictions as solutions to engineering problems. For the purposes of web development, this work connects two programming languages, which are MATLAB and Java. A MATLAB Compiler is used to package the system into Java Archive (JAR) files, which provide the core modules for the development of the NiMOPS website using an integrated development environment (IDE). IDE compiles the JAR files, and web utilities (JavaScript, CSS, Servlet, and other utility files) to form the response-request connection between the user and the server. Therefore, the web-based system does not require the installation of an application by the users because they can access the cloud computing system ubiquitously with a browser or mobile device. Furthermore, it has many functions, including export-import file, train model, optimize prediction, save model and visualize results. Several case studies of this system, involving classification and regression problems, were examined. The analytic results of using the system to solve classification problems revealed that the system had a fault diagnosis accuracy of 96.5% and an accidental small fire accuracy of 52.4%. In solving regression problems, the root mean square errors were 28.58-68.82% better than those of previous methods. In particular, the proposed system performed multiple performance measures that were utilized in a regression analysis and were found to be more reliable evaluation metrics than used in elsewhere. The numerical experiments verified that cloud computing provides an innovative way to enable decision-makers to solve engineering problems.",
        "keywords": "Optimized machine learning system; Cloud soft computing; Human&#8211; machine interface; Nature-inspired metaheuristic algorithm; Evolutionary computing; Classification and regression for&#160; engineering analytics",
        "released": 2022,
        "link": "https://doi.org/10.1007/s00366-020-01249-8"
    },
    {
        "title": "ETWatch cloud: APIs for regional actual evapotranspiration data generation",
        "abstract": "Increased model complexity and data quantities have raised the computing power requirement for efficient evapotranspiration (ET) estimation. A cloud-based service is presented to encapsulate and publish the ETWatch modeling algorithms as web application programming interfaces (APIs) in a consistent style to provide extensible model calculation service and data storage service in a cloud platform for water managers and stakeholders. The prototype system, named ETWatch Cloud allows users to rapidly and easily set up an ET generation project for any region of interest by invoking APIs directly to produce ET data using a web browser or local integrated development environment. The case study demonstrates that ETWatch Cloud can provide a highly scalable and interoperable ET generation tool for stakeholders from ET community, helping to facilitate the application of remote sensing-based ET algorithms for water management in hydrology sector.",
        "keywords": "Actual evapotranspiration; Remote sensing; Web API; Cloud computing",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.envsoft.2021.105174"
    },
    {
        "title": "Using java and JavaScript in the virtual programming laboratory: A web-based parallel programming environment",
        "abstract": "The Virtual Programming Laboratory (VPL) is a Web-based virtual programming environment built based on a client-server architecture. The system can be accessed on any platform (Unix, PC or Mac) using a standard Java-enabled browser, Software delivery over the Web imposes a novel set of constraints on design, We outline the tradeoffs in this design space, motivate the choices necessary to deliver an application, and detail the lessons learned in the process, We discuss the role of Java and other Web technologies in the realization of the design, VPL facilitates the development and execution of parallel programs. The initial prototype supports high-level parallel programming based on Fortran 90 and High Performance Fortran (HPF), as well as explicit low-level programming with the MPI message-passing interface, Supplementary Java-based platform-independent tools for data and performance visualization are an integral part of the VPL, Pablo SDDF trace files generated by the Pablo performance instrumentation system are used for post-mortem performance visualization. (C) 1997 by John Wiley & Sons, Ltd.",
        "keywords": "",
        "released": 1997,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:A1997XG61200007"
    },
    {
        "title": "A practical experience in workspace separation for developing multiple storefronts on customized commerce engines",
        "abstract": "In this paper, we describe our experience in separating workspaces, using the IBM VisualAge for Java development tool, for multiple web storefronts development for several major IBM public and private commerce sites. There was a need to create multiple workspaces since different storefronts may be developed at the same time. In addition, we wanted to keep the completed software packages for compatibility checking. Instead of tuning the current program setting to a desired mixed-version environment for each project, we generate a dedicated workspace by applying dependency analysis. New icons are created to conveniently invoke the development tool under different software packages and workspaces. We also present a systematic approach to facilitate the generation of a plan with instruction steps to produce new workspaces for their dedicated and required resources. (C) 2003 Elsevier Inc. All rights reserved.",
        "keywords": "software development; commerce engines; web storefronts; eCommerce; integrated development environment; workspace separation; dependency analysis",
        "released": 2004,
        "link": "https://doi.org/10.1016/S0164-1212(03)00243-7"
    },
    {
        "title": "Test-driven development for generated portable javascript apps",
        "abstract": "With the advent of HTML 5, we can now develop rich web apps in Javascript that rival classical standalone apps. Nevertheless, developing in Javascript is still challenging and error prone because of the language’s ambiguous semantics and quirks. In this paper, we advocate that the popular solution of using another language with better semantics and constructs is not enough. Developers should be provided with an IDE that eases the integration of Javascript libraries and enables testing an application across the many available Javascript interpreters. We introduce PharoJS,(1) an infrastructure that allows Test Driven Development (TDD) in Pharo Smalltalk of applications that ultimately run on a Javascript interpreter. PharoJS makes it possible to run interactive tests within the Pharo IDE, so as to fully exploit the debugging and development environment, while UI and libraries reside on the Javascript side. (C) 2018 Elsevier B.V. All rights reserved.",
        "keywords": "Test-driven development; Javascript transpilation; IDE; Crossplatform; Web development",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.scico.2018.02.003"
    },
    {
        "title": "Track signal intrusion detection method based on deep learning in cloud-edge collaborative computing environment",
        "abstract": "Aiming at the low accuracy of the track signal intrusion detection (IDe) algorithm in the traditional cloud-side collaborative computing environment, this paper proposes a deep learning (D-L)-based track signal IDe method in the cloud edge collaborative computing environment. First, the main framework of the IDe method is constructed by comprehensively considering the backbone network, network transmission and ground equipment, and edge computing (EC) is introduced to cloud services. Then, the The CNN (Convolutional Neural Networks)-attention-based BiLSTM (Bi-directional Long Short-Term Memory) neural network is used in the cloud center layer of the system to train the historical data, a D-L method is proposed. Finally, a pooling layer and a dropout layer are introduced into the model to effectively prevent the overfitting of the model and achieve accurate detection of track signal intrusion. The purpose of introducing the pooling layer is to accelerate the model convergence, remove the redundancy and reduce the feature dimension, and the purpose of introducing the dropout layer is to prevent the overfitting of the model. Through simulation experiments, the proposed IDe method and the other three methods are compared and analyzed under the same conditions. The results show that the F1 value of the method proposed in this paper is optimal under four different types of sample data. The F1 value is the lowest of 0.948 and the highest of 0.963. The performance of the algorithm is better than the other three comparison algorithms. The method proposed in this paper is important for solving the IDe signal in the cloud-edge cooperative environment, and also provides a theoretical basis for tracking the signal IDe direction.",
        "keywords": "IDe; cloud-edge collaboration; D-L; track signal; BiLSTM",
        "released": 2023,
        "link": "https://doi.org/10.1142/S0218126623502675"
    },
    {
        "title": "Patterns of developers behaviour: A 1000-hour industrial study",
        "abstract": "Monitoring developers’ activity in the Integrated Development Environment (IDE) and, in general, in their working environment, can be useful to provide context to recommender systems, and, in perspective, to develop smarter IDEs. This paper reports results of a long (about 1000 h) observational study conducted in an industrial environment, in which we captured developers’ interaction with the IDE, with various applications available in their workstation, and related them with activities performed on source code files. Specifically, the study involved six developers working on three software systems and investigated (i) how much time developers spent on various activities and how they shift from one activity to another (ii) how developers navigate through the software architecture during their task, and (iii) how the complexity and readability of source code may trigger further actions, such as requests for help or browsing/changing other files. Results of our study suggest that: (i) not surprisingly, developers spend most or their time (similar to 61%) in development activities while the usage of online help is limited (2%) but intensive in specific development sessions; (ii) developers often execute the system under development after working on code, likely to verify the effect of applied changes on the system’s behaviour; (iii) while working on files having a high complexity, developers tend to more frequently execute the system as well as to use more online help websites. (C) 2017 Elsevier Inc. All rights reserved.",
        "keywords": "Monitoring developers’ activities; Case study",
        "released": 2017,
        "link": "https://doi.org/10.1016/j.jss.2017.06.072"
    },
    {
        "title": "Analyzing learners’ engagement and behavior in MOOCs on programming with the codeboard IDE",
        "abstract": "Massive Open Online Courses (MOOCs) can be enhanced with the so-called learning-by-doing, designing the courses in a way that the learners are involved in a more active way in the learning process. Within the options for increasing learners’ interaction in MOOCs, it is possible to integrate (third-party) external tools as part of the instructional design of the courses. In MOOCs on computer sciences, there are, for example, web-based Integrated Development Environments (IDEs) which can be integrated and that allow learners to do programming tasks directly in their browsers without installing desktop software. This work focuses on analyzing the effect on learners’ engagement and behavior of integrating a third-party web-based IDE, Codeboard, in three MOOCs on Java programming with the purpose of promoting learning-by-doing (learning by coding in this case). In order to measure learners’ level of engagement and behavior, data was collected from Codeboard on the number of compilations, executions and code generated, and compared between learners who registered in Codeboard to save and keep a record of their projects (registered learners) and learners who did not register in Codeboard and did not have access to these extra features (anonymous learners). The results show that learners who registered in Codeboard were more engaged than learners who did not register (in terms of number of compilations and executions), spent more time coding and did more changes in the base code provided by the teachers. The main implication of this study suggests the need for a trade-off between designing MOOCs that allow a very easy and anonymous access to external tools aimed for a more active learning, and forcing learners to give a step forward in terms of commitment in exchange for benefitting from additional features of the external tool used.",
        "keywords": "MOOCs; Programming; Learners’ engagement; Codeboard; Learning analytics; edX",
        "released": 2020,
        "link": "https://doi.org/10.1007/s11423-020-09773-6"
    },
    {
        "title": "COMFIT: A development environment for the internet of things",
        "abstract": "This paper presents COMFIT (Cloud and Model based IDE for the Internet of Things), a development environment for the Internet of Things that was built grounded on the paradigms of model driven development and cloud computing. COMFIT is composed of two different modules: (1) the App Development Module, a model-driven architecture (MDA) infrastructure, and (2) the App Management and Execution Module, a module that contains cloud-based web interface connected to a server hosted in the cloud with compilers and simulators for developing Internet of Things (IoT) applications. The App Development Module allows the developers to design loT applications using high abstraction artifacts (models), which are tailored to either the application perspective or the network perspective, thus creating a separation between these two concerns. As models can be automatically transformed into code through the App Development Module, COMFIT creates an environment where there is no need of additional configurations to properly compile or simulate the generated code, integrating the development lifecycle of IoT applications into a single environment partially hosted in the client side and partially in the cloud. In its current version, COMFIT supports two operating systems, namely Contiki and TinyOS, which are widely used in loT devices. COMFIT supports automatic code generation, execution of simulations, and code compilation of applications for these platforms with low development effort. Finally, COMFIT is able to interact with loT-lab, an open testbed for loT applications, which allows the developers to test their applications with different configurations without the need of using local loT devices. Several evaluations were performed to assess COMFIT’s key features in terms of development effort, quality of generated code, and scalability. (C) 2016 Elsevier B.V. All rights reserved.",
        "keywords": "IDE; Wireless sensor networks; Internet of Things; Cloud computing; Model-driven development; Testbeds",
        "released": 2017,
        "link": "https://doi.org/10.1016/j.future.2016.06.031"
    },
    {
        "title": "Automated assessment in a programming tools course",
        "abstract": "Automated assessment systems can be useful for both students and instructors. Ranking and immediate feedback can have a strongly positive effect on student learning. This paper presents an experience using automatic assessment in a programming tools course. The proposal aims at extending the traditional use of an online judging system with a series of assignments related to programming tools. Some empirical results on how students use an automated assessment system in a CS2 course are presented. Research suggested that automated assessment systems promoted the students’ interest and produced statistically significant differences in the scores between experimental and control groups.",
        "keywords": "Active learning; competitive learning; e-learning; programming tools; Web tool",
        "released": 2011,
        "link": "https://doi.org/10.1109/TE.2010.2098442"
    },
    {
        "title": "Integrated service component development tool on web services",
        "abstract": "The web services based on the HTTP protocol and XML are the efficient way to integrate distributed applications on the Internet. The web services contain SOAP, WSDL and UDDI standard, and various technologies related to them are researched now. In this paper, we design and implement an efficient integrated development tool for web services components. Generally a structure of web service components is constructed by combining other components closely. For this, it may be required the efficient mechanism during design of service components. Therefore we propose an integrated model of the functions required and some composition techniques of service components. Then we develop the integrated service component development tool. This automatic tool provides UML design on the GUI and includes some procedures that are creation and registration of WSDL document for accessing the web service component efficiently and that create wrapper class composing with the other services. As a result, this tool will provide the opportunity of faster and stable component development.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000226132600166"
    },
    {
        "title": "Cloud computing for software development environment -in-house deployment at numazu software development cloud center-",
        "abstract": "In its development of middleware, Fujitsu has faced three problems: increasing server operation costs, server over/under-utilization, and increasing labor hours for development environment construction. To solve these problems, Fujitsu has been working since 2008 on moving its software development environment to a cloud platform at its Numazu Software Development Cloud Center. In addition to solving the above problems, this transition to a cloud environment is expected to reduce costs by 700 million yen annually from 2011. Converting to cloud computing within Fujitsu itself is also seen as a way to refine the cloud products and services that Fujitsu offers to its customers. This paper describes the background to converting to cloud-based software development, Fujitsu’s approach to this conversion, the effects of cloud deployment, and the expansion of services as a result of using a cloud platform. It also introduces the Numazu Cloud Center tour, which conveys cloud computing know-how through face-to-face conversations with customers.",
        "keywords": "",
        "released": 2011,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000306819000012"
    },
    {
        "title": "Internet of things mobile-air pollution monitoring system (IoT-mobair)",
        "abstract": "Internet of Things (IoT) is a worldwide system of “smart devices” that can sense and connect with their surroundings and interact with users and other systems. Global air pollution is one of the major concerns of our era. Existing monitoring systems have inferior precision, low sensitivity, and require laboratory analysis. Therefore, improved monitoring systems are needed. To overcome the problems of existing systems, we propose a three-phase air pollution monitoring system. An IoT kit was prepared using gas sensors, Arduino integrated development environment (IDE), and a Wi-Fi module. This kit can be physically placed in various cities to monitoring air pollution. The gas sensors gather data from air and forward the data to the Arduino IDE. The Arduino IDE transmits the data to the cloud via the Wi-Fi module. We also developed an Android application termed IoT-Mobair, so that users can access relevant air quality data from the cloud. If a user is traveling to a destination, the pollution level of the entire route is predicted, and a warning is displayed if the pollution level is too high. The proposed system is analogous to Google traffic or the navigation application of Google Maps. Furthermore, air quality data can be used to predict future air quality index (AQI) levels.",
        "keywords": "Air pollution monitoring system; air quality index (AQI); air-pollution safe route; Android; cloud; distributed systems; global positioning system (GPS); sensors",
        "released": 2019,
        "link": "https://doi.org/10.1109/JIOT.2019.2903821"
    },
    {
        "title": "A combined runtime environment and web-based development environment for web application engineering",
        "abstract": "In the last 5 years, server side application server technology became very popular. The two most popular, NET and J2EE have a large mindshare in the commercial world. Application servers have raised the productivity of server side web developers. In this paper we give a short overview of the history of application servers, and try to answer the question: what options or features would raise the productivity even more? While building commercial websites at <GX>, we found out that that a combined web-based development environment and persistent runtime environment raised our productivity, even compared to NET or J2EE environments. In this paper we describe such a system built at <GX>, Much we called a WRDE (Web-based Runtime and Development Environment), and show the strength and weaknesses of this approach.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000222262200023"
    },
    {
        "title": "Coordinating distributed components on the web: An integrated development environment",
        "abstract": "Component-based software has become an important alternative for building applications, especially distributed ones, so it is essential to define new software development processes based on components. Within this trend, we propose a complete framework-based method which guides application developers in exactly determining application requirements. It also guides how to build them using the compositional framework MultiTEL from the collaborative and multimedia applications domain. Although many multimedia frameworks are available, none of them offer a design methodology for understanding and adapting the framework classes or components to each derived application. By applying an architecture description language (ADL) we are able to document the framework and help designers in: constructing; reusing, and connecting components; extending the framework architecture; and adding components to meet user requirements. Tools for the automatic generation of code from the ADL specifications are also described. Copyright (C) 2001 John Wiley & Sons, Ltd.",
        "keywords": "compositional framework; distributed component platform; framework-based development environment; collaborative and multimedia applications",
        "released": 2001,
        "link": "https://doi.org/10.1002/spe.362"
    },
    {
        "title": "The r language: An engine for bioinformatics and data science",
        "abstract": "The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.",
        "keywords": "R; statistics; bioinformatics; programming; CRAN; data science",
        "released": 2022,
        "link": "https://doi.org/10.3390/life12050648"
    },
    {
        "title": "Umple: Model-driven development for open source and education",
        "abstract": "Umple is an open-source software modeling tool and compiler. It incorporates textual language constructs for UML modeling, including associations and state machines. It includes traits, aspects, and mixins for separation of concerns. It supports embedding methods written in many object-oriented languages, enabling it to generate complete multilingual systems. It provides comprehensive analysis of models and generates many kinds of diagrams, some of which can be edited to update the Umple code. Umple runs on the command line, in a web browser or in integrated development environments. It is designed to help developers reduce code volume, while they develop in an agile, model-driven manner. Umple is also targeted at educational users where students are motivated by its ability to generate real systems from their software models. (C) 2021 The Author(s). Published by Elsevier B.V.",
        "keywords": "Model-driven development; Code generation; Compiler",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.scico.2021.102665"
    },
    {
        "title": "<I>RaDMaX</i> <i>online</i>: A web-based program for the determination of strain and damage profiles in irradiated crystals using x-ray diffraction",
        "abstract": "RaDMaX online is a major update to the previously published RaDMaX (radiation damage in materials analysed with X-ray diffraction) software [Souilah, Boulle & Debelle (2016). J. Appl. Cryst. 49, 311-316]. This program features a user-friendly interface that allows retrieval of strain and disorder depth profiles in irradiated crystals from the simulation of X-ray diffraction data recorded in symmetrical theta/2 theta mode. As compared with its predecessor, RaDMaX online has been entirely rewritten in order to be able to run within a simple web browser, therefore avoiding the necessity to install any programming environment on the users’ computers. The RaDMaX online web application is written in Python and developed within a Jupyter notebook implementing graphical widgets and interactive plots. RaDMaX online is free and open source and can be accessed on the internet at https://aboulle.github.io/RaDMaX-online/.",
        "keywords": "X-ray diffraction; Python; computing; web development; radiation damage",
        "released": 2020,
        "link": "https://doi.org/10.1107/S1600576720002514"
    },
    {
        "title": "A WWW software development environment to support cooperative and spread working groups",
        "abstract": "This article presents a software development environment based on hypertext techniques to support object-oriented software construction performed by cooperative working groups spread all over the world. The environment uses the World-Wide Web to support distributed software development. (C) 1998 Elsevier Science B.V.",
        "keywords": "World-Wide Web; software process management; software development environments",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0010-4655(97)00175-6"
    },
    {
        "title": "Simulator for a multi-programming environment for computer science learning and teaching",
        "abstract": "The objective of this paper is to explore improvements in the learning process for Computer Science using a new tool (an IDE simulator) and to demonstrate the pedagogic and cognitive aspects of the tool. This work presents the design and implementation of a web-based self-assessment environment with multi-language programming questions. The application has been implemented in a complete e-learning system, known as Aula Web, and is being used as a facility to encourage students on computer science courses to practice programming techniques with different programming languages, for example, Java and C/C++. Furthermore, this paper describes the pedagogical methodology and some results drawn from the experience.",
        "keywords": "IDE simulator; web-based systems; programming languages; learning-teaching strategies; assessment",
        "released": 2009,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000269869600004"
    },
    {
        "title": "Improvement of software quality and productivity using development tools",
        "abstract": "Information systems, which successively meet the requirements of on-site management and staff, have become enormous and complicated after many years of modification. To make IT investments efficient, it is essential to reuse current software assets effectively, quickly develop systems, and reduce testing costs. To help customers achieve these goals, Fujitsu provides a systematic development environment called the System Development Architecture & Support facilities (SDAS). This environment covers the entire lifecycle of application development. This paper introduces some systematic development tools for SDAS-based Web application development. Specifically, it introduces Interstage Apworks, which is an integrated development environment based on open standards; the SIMPLIA series of testing support tools; the PROSPECS reverse engineering tool; and the NetCOBOL multiplatform COBOL compiler.",
        "keywords": "",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000240248200007"
    },
    {
        "title": "Cooperative ontology development environment CODE and a demo semantic web on economics",
        "abstract": "Ontology is the backbone of Semantic Web. Ontology has drawn more and more concerns with the research works developing in the Semantic Web. The construction of large-scale ontologies will involve collaborative efforts of multiple developers. In this paper, we give a demonstration of our Cooperative Ontology Development Environment CODE and discuss a demo Semantic Web on Economics.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000228780500104"
    },
    {
        "title": "Little languages with lex, yacc, and MFC",
        "abstract": "Whether designed to perform complex mathematical calculations, control specialized equipment, or specify text layout in web-based documents, little languages offer users a great deal of power while sparing nonprogrammers from the complexity of general-purpose languages. In this article, I will describe how to use lex, yacc, and MFC to create integrated Win32 development environments for little languages. More specifically, I’ll develop a multidocument-interface application called “Slide” (short for “Small Language Integrated Development Environment”) and integrate it with lex and yacc. The source code and related files for Slide, bison, and flex are available electronically (see “Resource Center,” page 5).",
        "keywords": "",
        "released": 1999,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000077187700019"
    },
    {
        "title": "Securing medical data by role-based user policy with partially homomorphic encryption in AWS cloud",
        "abstract": "Cloud technology provides services for storing and accessing a large amount of data with ease of access and less cost. Sensitive data such as patients’ electronic health information should be encrypted before outsourcing into the cloud. Many traditional encryption methods are used for protecting data in the cloud, but unable to perform computation on encrypted data. Homomorphic encryption operates directly on the ciphertext. In this study, a Secure Partially Homomorphic Encryption (SPHE) algorithm is proposed to secure the outsourced data and perform multiplication and division operations on the ciphertext. The access control policy in the cloud environment is more flexible. An attacker can easily collect sensitive data by abusing the access policy of another user. Therefore, the database privacy is compromised. Creating a role hierarchy and managing the session is difficult in the cloud environment. The above issues motivate us to develop a model which is the integration of the proposed scheme SPHE with role-based user policy. The model is implemented in Eclipse IDE (Integrated Development Environment) and AWS (Amazon Web Service) Toolkit for Eclipse and deployed in Amazon Elastic Beanstalk (EB) environment. This model is particularly used for securing the patient e-health details and performing computation on outsourced data. The patient details are encrypted by the algorithm SPHE and uploaded in AWS S3 (Simple Storage Service) bucket. The users are created by AWS Identity and Access Management (IAM) service and the access level policy is defined based on user roles in EB environment. The proposed model performance is studied by comparing with other partially homomorphic methods Elgamal, Pailler, and Benaloh. This model achieves data integrity and data confidentiality using the role-based user policy with SPHE.",
        "keywords": "Partially homomorphic encryption; Cloud security; Access policy; AWS S3; IAM; Elastic Beanstalk",
        "released": 2023,
        "link": "https://doi.org/10.1007/s00500-022-06950-y"
    },
    {
        "title": "Web environment for programming and control of a mobile robot in a remote laboratory",
        "abstract": "Remote robotics laboratories have been successfully used for engineering education. However, few of them use mobile robots to to teach computer science. This article describes a mobile robot Control and Programming Environment (CPE) and its pedagogical applications. The system comprises a remote laboratory for robotics, an online programming tool, and a virtual learning environment. It allows experiments with a mobile robot to be carried out, tested and validated remotely, using a simple graphic interface via web browser, without additional software installation on the user’s computer. Students can control and manipulate the robot with simple commands without worrying about hardware details or use C/C++ programming language to develop complex programs. The robot has been built with inexpensive components and has sensors that enable its use in several experiments. CPE can be used in contextualized teaching in various subjects of computer science, both in class and at distance. A practical application of the whole system using CPE, introducing programming concepts, is also described in this paper. The initial assessment of the environment as an educational tool shows high levels of interest among our students, an improvement in their academic scores, and potential for application in the context of computing education, when properly introduced as a learning tool.",
        "keywords": "Computing education; remote laboratories; mobile robots; online programming; bistance learning",
        "released": 2017,
        "link": "https://doi.org/10.1109/TLT.2016.2627565"
    },
    {
        "title": "Feature location using probabilistic ranking of methods based on execution scenarios and information retrieval",
        "abstract": "This paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty. The solution to the problem is formulated as a combination of the opinions of different experts. The experts in this work are two existing techniques for feature location: a scenario-based probabilistic ranking of events and an information-retrieval-based technique that uses Latent Semantic Indexing. The combination of these two experts is empirically evaluated through several case studies, which use the source code of the Mozilla Web browser and the Eclipse integrated development environment. The results show that the combination of experts significantly improves the effectiveness of feature location as compared to each of the experts used independently.",
        "keywords": "program understanding; feature identification; concept location; dynamic and static analyses; information retrieval; Latent Semantic Indexing; scenario-based probabilistic ranking; open source software",
        "released": 2007,
        "link": "https://doi.org/10.1109/TSE.2007.1016"
    },
    {
        "title": "Integrating XP project management in development environments",
        "abstract": "Extreme Programming (XP) is an Agile Methodology (AM) which does not require any specific supporting tool for being successfully applied. Despite this starting observation, there are many reasons leading a XP team to adopt Web based tools to support XP practices. For example, such tools could be useful for process and product data collection and analysis or for supporting distributed development. In this article, we describe XPSuite, a tool composed of two parts: XPSwiki, a tool for managing XP projects and XP4IDE, a plug-in for integrating XPSwiki with an Integrated Development Environment (IDE). Moreover, we will show how the full Object Oriented implementation provides a powerful support for extracting all data represented in the model that the system implements. (C) 2006 Elsevier B.V. All rights reserved.",
        "keywords": "Extreme Programming; XP tools; Agile Methodologies; process metrics; distributed development",
        "released": 2006,
        "link": "https://doi.org/10.1016/j.sysarc.2006.06.006"
    },
    {
        "title": "Modeap: Moving desktop application to mobile cloud service",
        "abstract": "Thanks to the availability of various mobile applications, lots of users shift from desktop environments, e.g., PCs and laptops, to mobile devices, e.g., smartphones and tablets. However, there are still some desktop applications without counterparts on mobile devices, such as some integrated development environments (e.g., eclipse) and automatic industry control systems. In this paper, we propose Modeap, a platform-independent mobile cloud service that can push all desktop applications developed for various operating systems from cloud servers to mobile devices. Modeap follows a design principle of complete detachment and regeneration of desktop user interface, i.e., the essential graphical primitives of the original desktop applications will be intercepted and then translated into standard web-based graphical primitives such that the interactions between users and remote cloud applications become possible via mobile web browsers. In this way, all desktop applications built upon the same set of graphical primitives can be used on mobile devices in great flexibility without installing any new software. We have developed a proof-of-concept prototype that provides Windows applications from cloud server to mobile web browsers. The results of extensive experiments show that the proposed framework can achieve our design goals with low latency and bandwidth consumption.",
        "keywords": "Mobile cloud; Smartphone; Remote desktop protocol; Virtual network computing",
        "released": 2014,
        "link": "https://doi.org/10.1007/s11036-014-0518-7"
    },
    {
        "title": "Crowdsourced evaluation of robot programming environments: Methodology and application",
        "abstract": "Industrial robot programming tools increasingly rely on graphical interfaces, which aim at rendering the programming task more accessible to a wide variety of users. The usability of such tools is currently being evaluated in controlled environments, such as laboratories or companies, in which a group of participants is asked to carry out several tasks using the tool and then fill out a standardized questionnaire. In this context, this paper proposes and evaluates an alternative evaluation methodology, which leverages online crowdsourcing platforms to produce the same results as face-to-face evaluations. We applied the proposed framework in the evaluation of a web-based industrial robot programming tool called Assembly. Our results suggest that crowdsourcing facilitates a cost-effective, result-oriented, and reusable methodology for performing user studies anonymously and online.",
        "keywords": "robot programming; user interface evaluation; crowdsourcing",
        "released": 2021,
        "link": "https://doi.org/10.3390/app112210903"
    },
    {
        "title": "Designing the user interface and functions of a search engine development tool",
        "abstract": "Search engine development tools have been made to allow users to build their own search engines. However, most of these tools have been designed for advanced computer users. Users without a full understanding of topics such as Web spidering would find these tools difficult to use due to different issues in terms of user interface, performance, and reliability. In view of these issues, we presented a tool called SpidersRUs to strike a balance between usability and functionality. On one hand, beginners should be able to operate the tool by using the basic functions needed to build a search engine. On the other, advanced users should be given the options to exert a higher level of customization while working on the tool. To study the interface design of SpidersRUs, we compared its usability and functionality from the users’ perspective with two other development tools, namely Alkaline and Greenstone. in an evaluation study. Our study showed that SpidersRUs was preferred over the other two. particularly in areas of screen layout and sequence, terminology and system information. and learning to use the system. (c) 2009 Elsevier B.V. All rights reserved.",
        "keywords": "Web search; Search engine development tools; User evaluation",
        "released": 2010,
        "link": "https://doi.org/10.1016/j.dss.2009.10.001"
    },
    {
        "title": "WIDE: Centralized and collaborative approach to teaching web development",
        "abstract": "Teaching Web development is an increasingly important and complex task due to multiple technologies that students have to master and implement in each particular solution. This fact imposes the specific learning approach and development environment as a collaborative learning tool. It is particularly important in the learning scenarios that include large groups of students and computers that are not intended solely for teaching Web development as well. The authors of this paper find, based on their past experiences, that a disproportionately large amount of time is expended on troubleshooting infrastructure problems, and that collaboration amongst students is unsatisfactory. This paper presents a solution for centralized and collaborative work on learning Web development, as well as observations made during the course of its development and the first year of deployment.",
        "keywords": "e Web development; On-line IDE; Education; Collaboration",
        "released": 2018,
        "link": "https://doi.org/10.3966/160792642018081904004"
    },
    {
        "title": "An intelligent optimization method for optimal virtual machine allocation in cloud data centers",
        "abstract": "A cloud computing paradigm has quickly developed and been applied widely for more than ten years. In a cloud data center, cloud service providers offer many kinds of cloud services, such as virtual machines (VMs), to users. How to achieve the optimized allocation of VMs for users to satisfy the requirements of both users and providers is an important problem. To make full use of VMs for providers and ensure low makespan of user tasks, we formulate an optimal allocation model of VMs and develop an improved differential evolution (IDE) method to solve this optimization problem, given a batch of user tasks. We compare the proposed method with several existing methods, such as round-robin (RR), min-min, and differential evolution. The experimental results show that it can more efficiently decrease the cost of cloud service providers while achieving lower makespan of user tasks than its three peers. Note to Practitioners-VM allocation is one of the challenging problems in cloud computing systems, especially when user task makespan and cost of cloud service providers have to be considered together. We propose an IDE approach to solve this problem. To show its performance, this article compares the commonly used methods, i.e., RR and min-min, as well as the classic differential evolution method. A cloud simulation platform called CloudSim is used to test these methods. The experimental results show that the proposed one can well outperform its compared ones, and its VM allocation results can achieve the highest satisfaction of both users and providers. The proposed method can be readily applicable to industrial cloud computing systems.",
        "keywords": "Resource management; Cloud computing; Task analysis; Dynamic scheduling; Virtual machining; Data centers; Optimization; Cloud computing; improved differential evolution (IDE); intelligent optimization; virtual machine allocation",
        "released": 2020,
        "link": "https://doi.org/10.1109/TASE.2020.2975225"
    },
    {
        "title": "Tau prolog: A prolog interpreter for the web",
        "abstract": "Tau Prolog is a client-side Prolog interpreter fully implemented in JavaScript, which aims at implementing the ISO Prolog Standard. Tau Prolog has been developed to be used with either Node.js or a browser seamlessly, and therefore, it has been developed following a non-blocking, callback-based approach to avoid blocking web browsers. Taking the best from JavaScript and Prolog, Tau Prolog allows the programmer to handle browser events and manipulate the Document Object Model (DOM) of a web using Prolog predicates. In this paper we describe the architecture of Tau Prolog and its main packages for interacting with the Web, and we present its programming environment.",
        "keywords": "Tau Prolog; logic programming; Prolog interpreter; JavaScript",
        "released": 2024,
        "link": "https://doi.org/10.1017/S1471068423000352"
    },
    {
        "title": "A web-based environment to improve teaching and learning of computer programming in distance education",
        "abstract": "Learning computer programming is not an easy task. Students need to spend hours doing practical activities in order to comprehend the techniques of writing computer programs and beginners usually face a number of obstacles associated with installing and using a compiler or integrated development environment. This paper introduces an online web-based system that provides an interactive integrated environment for students doing programming activities and coursework in a distance learning institution. The interactive system provides students with timely and effective feedback about programming activities without the need to have instructors and students meet at the same time and the same place. The web-based system provides students with an editing, compiling, testing and debugging environment for learning computer programming on the web. Instructors can monitor the learning progress of students, compile the student’s program and view the error messages through the student’s workplace in the online system.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000231036300028"
    },
    {
        "title": "Creating sophisticated development tools with OmniBrowser",
        "abstract": "Smalltalk is not only an object-oriented programming language; it is also known for its extensive integrated development environment supporting interactive and dynamic programming. While the default tools are adequate for browsing the code and developing applications, it is often cumbersome to extend the environment to support new language constructs or to build additional tools supporting new ways of navigating and presenting source code. In this paper, we present the OmniBrowser, a browser framework that supports the definition of browsers based on an explicit metamodel. With OmniBrowser a domain model is described in a graph and the navigation in this graph is specified in its associated metagraph. We present how new browsers are built from predefined parts and how new tools are easily described. The browser framework is implemented in the Squeak Smalltalk environment. This paper shows several concrete instantiations of the framework: a remake of the ubiquitous Smalltalk system browser, a coverage browser, the Duo Browser and the dynamic protocols browser. (c) 2007 Elsevier Ltd. All rights reserved.",
        "keywords": "programming environment; meta-description; graphical user interface; domain modelling",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.cl.2007.05.005"
    },
    {
        "title": "A development environment for knowledge-based medical applications on the world-wide web",
        "abstract": "The World-Wide Web (WWW) is increasingly being used as a platform to develop distributed applications, particularly in contexts, such as medical ones, where high usability and availability are required. In this paper we propose a methodology for the development of knowledge-based medical applications on the web, based on the use of an explicit domain ontology to automatically generate parts of the system. We describe a development environment. centred on the LISPWEB Common Lisp HTTP server, that supports this methodology, and we show how it facilitates the creation of complex web-based applications, by overcoming the limitations that normally affect the adequacy of the web for this purpose. Finally, we present an outline of a system for the management of diabetic patients built using the LISPWEB environment. (C) 1998 Elsevier Science B.V. All rights reserved.",
        "keywords": "Web-based applications; knowledge-based systems; insulin-dependent diabetes mellitus",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0933-3657(98)00038-4"
    },
    {
        "title": "Development iterations based on web augmentation and context tasks",
        "abstract": "The use of prototypes in requirements engineering has widely known benefits since they actively involve the stakeholders in the development process. Web Augmentation techniques make it possible to build prototypes relying on existing web applications. Thus, high fidelity mockups can be quickly generated. One of the most critical activities is dividing requirements into tasks and managing them through the development process. This paper proposes an approach that includes high fidelity mockups into the Task-oriented Development approach. The proposed approach consists of the following steps: (i) end-users specifies requirements, (ii) a product owner verifies and prioritizes the requirements, (iii) tasks are defined and included in a kanban board, (iv) developers should provide the functionality, and (v) the product owner should approved the functionality. The main contribution of this approach is to integrate the requirements specified through web augmentation mockups, into the development environment via a task-oriented development approach. Thus, developers will have a rich context that facilitates the understanding of the requirements. At the same time, the management of the development process will have benefits because of the traceability between tasks and requirements. This paper describes the approach proposed, called “WAMRI”, and an application of its usage, as well as a tool to support the application.",
        "keywords": "Web engineering; Web augmentation; Requirements engineering; Software development environments; Task contexts; Software tools",
        "released": 2023,
        "link": "https://doi.org/10.1007/s11042-022-13694-2"
    },
    {
        "title": "Modeling semantic web services with the web service modeling toolkit",
        "abstract": "The lack of any methodology for modeling Semantic Web Services means that developers wishing to utilize technologies like the Web Service Modeling Ontology, the Web Service Modeling Language, and the Web Service Execution Environment are lost in a Semantic wilderness with no road signs to guide them on their way. This paper presents an initial guide for developers wishing to model Semantic Web Services, along with a description of the Web Service Modeling Toolkit that provides tool support for the activities that must be conducted by the developer in this process.",
        "keywords": "Semantic Web Services; Web Service Modeling Ontology; Integrated Development Environment; Web Service Modeling Toolkit",
        "released": 2009,
        "link": "https://doi.org/10.1007/s10922-009-9130-8"
    },
    {
        "title": "PipelineDog: A simple and flexible graphic pipeline construction and maintenance tool",
        "abstract": "A Summary: Analysis pipelines are an essential part of bioinformatics research, and ad hoc pipelines are frequently created by researchers for prototyping and proof-of-concept purposes. However, most existing pipeline management system or workflow engines are too complex for rapid prototyping or learning the pipeline concept. A lightweight, user-friendly and flexible solution is thus desirable. In this study, we developed a new pipeline construction and maintenance tool, PipelineDog. This is a web-based integrated development environment with a modern web graphical user interface. It offers cross-platform compatibility, project management capabilities, code formatting and error checking functions and an online repository. It uses an easy-to-read/write script system that encourages code reuse. With the online repository, it also encourages sharing of pipelines, which enhances analysis reproducibility and accountability. For most users, PipelineDog requires no software installation. Overall, this web application provides a way to rapidly create and easily manage pipelines.",
        "keywords": "",
        "released": 2018,
        "link": "https://doi.org/10.1093/bioinformatics/btx759"
    },
    {
        "title": "A survey of neural network accelerator with software development environments",
        "abstract": "Recent years, the deep learning algorithm has been widely deployed from cloud servers to terminal units. And researchers proposed various neural network accelerators and software development environments. In this article, we have reviewed the representative neural network accelerators. As an entirety, the corresponding software stack must consider the hardware architecture of the specific accelerator to enhance the end-to-end performance. And we summarize the programming environments of neural network accelerators and optimizations in software stack. Finally, we comment the future trend of neural network accelerator and programming environments.",
        "keywords": "neural network accelerator; compiling optimization; programming environments",
        "released": 2020,
        "link": "https://doi.org/10.1088/1674-4926/41/2/021403"
    },
    {
        "title": "WEBMAKE - INTEGRATING DISTRIBUTED SOFTWARE-DEVELOPMENT IN a STRUCTURE-ENHANCED WEB",
        "abstract": "In this paper, a technique for structuring large amounts of interdependent data is presented. This approach which facilitates graph-based hierarchical structuring and allows for the definition of arbitrary views on graph structures can be applied to a broad range of very different application areas. Based on this concept we implemented a distributed software development environment supporting cooperative work on top of the World-Wide Web. In general, the approach is intended to serve as a basis for decentralized efforts to tame the immense and hardly manageable collection of data accessible in the Web.",
        "keywords": "SOFTWARE DEVELOPMENT ENVIRONMENT; COMPUTER SUPPORTED COOPERATIVE WORK; WORLDWIDE WEB; STRUCTURING APPROACH",
        "released": 1995,
        "link": "https://doi.org/10.1016/0169-7552(95)00019-4"
    },
    {
        "title": "How end-user programmers debug visual web-based programs: An information foraging theory perspective",
        "abstract": "Web-active end-user programmers squander much of their time foraging for bugs and related information in mashup programming environments as well as on the web. To analyze this foraging behavior while debugging, we utilize an Information Foraging Theory perspective. Information Foraging Theory models the human (predator) behavior to forage for specific information (prey) in the webpages or programming IDEs (patches) by following the information features (cues) in the environment. We qualitatively studied the debugging behavior of 16 web-active end users. Our results show that end-user programmers spend substantial amounts (73%) of their time just foraging. Further, our study reveals new cue types and foraging strategies framed in terms of Information Foraging Theory, and it uncovers which of these helped end-user programmers succeed in their debugging efforts.",
        "keywords": "Information Foraging Theory; End-user programming; End-user software engineering; Visual programming language; Debugging",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.cola.2019.04.003"
    },
    {
        "title": "Reconfigurable wireless sensor node remote laboratory platform with cloud connectivity",
        "abstract": "Thanks to the recent rapid technological advancement in IoT usage, there is a need for students to learn IoT-based concepts using a dedicated experimental platform. Furthermore, being forced into remote learning due to the ongoing COVID-19 pandemic, there is an urgent need for innovative learning methods. From our perspective, a learning platform should be reconfigurable to accommodate multiple applications and remotely accessible at any time, from anywhere, and on any connected device. Considering that many of the university courses are now held online, the reliability and scalability of the system become critical. This paper presents the design and development of a wireless configurable myRIO-based sensor node that connects to SystemLink Cloud. The sensors that were used are for ambient light, temperature, and proximity. A graphical programming environment (G-LabVIEW) and related APIs were used for rapid concept-to-development process. Distinct applications have been developed for the instructor and students, respectively. The students can select which sensor and application to run on the system and observe the measurements on the local student’s application or the cloud platform at a specific moment. They can also read the data on the cloud platform and use them in their LabVIEW application. In the context of remote education, we strongly believe that this platform is and will be suitable for the COVID and Post-COVID eras as well because it creates a much better remote laboratory experience for students. In conclusion, the system that was developed is innovative because it is software reconfigurable from the device, from the instructor’s application and cloud via a web browser, it is intuitive, and it has a user-friendly interface. It meets most of the necessary requirements in the current era, being also highly available and scalable in the cloud.",
        "keywords": "cloud; reconfigurable; tags; pmod sensors; myRIO; SystemLink Cloud",
        "released": 2021,
        "link": "https://doi.org/10.3390/s21196405"
    },
    {
        "title": "Parallel programming and performance evaluation with the URSA tool family",
        "abstract": "This paper contributes to the solution of several open problems with parallel programming tools and their integration with performance evaluation environments. First, we propose interactive compilation scenarios instead of the usual black-box-oriented use of compiler tools. In such scenarios, information gathered by the compiler and the compiler’s reasoning are presented to the user in meaningful ways and on-demand. Second, a tight integration of compilation and performance analysis tools is advocated. Many of the existing, advanced instruments for gathering performance results are being used in the presented environment and their results are combined in integrated views with compiler information and data from other tools. Initial instruments that assist users in “data mining” this information are presented and the need for much stronger facilities is explained. The URSA Family provides two tools addressing these issues. URSA MINOR supports a group of users at a specific site, such as a research or development project. URSA MAJOR complements this tool by making available the gathered results to the user community at large via the World-wide Web. This paper presents objectives, functionality, experience, and next development steps of the URSA tool family. Two case studies are presented that illustrate the use of the tools for developing and studying parallel applications and for evaluating parallelizing compilers.",
        "keywords": "parallel programming; programming environment; performance evaluation; program optimization; web-based tool",
        "released": 1998,
        "link": "https://doi.org/10.1023/A:1018720530812"
    },
    {
        "title": "Online gait generation for an exoskeleton used in lower limb rehabilitation",
        "abstract": "Providing walking assistance for mild hemiplegia patients with lower limb rehabilitation exoskeleton has gained considerable attention. The patients’ motion characteristics should be emphasized, meanwhile the planning gait should always maintain the balance ability in the walking procedure. In this paper, an online balance gait generation strategy is proposed for an exoskeleton used in lower limb rehabilitation. In the present research, the online gait generation strategy is explained and consists of a gait planning and a gait control strategy. In the gait planning strategy, the healthy swing foot trajectory has been learned, modelled and modified for next cycles’ exoskeleton motion by applying improved differential evolution Adaptive Hopf (iDE AHopt) oscillators. Meanwhile to illustrate the human motion intention, the dynamic step length estimation method is utilized to estimate the step length. The discrete step planner determines the motion of the exoskeleton’s centre of gravity (CoG). In the gait control strategy, the online balance gait regulation could regulate the exoskeleton joint reference adaptively to confirm the maximum zero-moment-point (ZMP) walking stability according to the PID controller feedback joint signal. Finally, the corresponding simulation results demonstrate the effectiveness of the proposed strategy by verifying the gait learning rate and ZMP stability margin.",
        "keywords": "Online gait generation; Lower limb exoskeleton; iDE AHopf oscillators learning; ZMP adaption",
        "released": 2020,
        "link": "https://doi.org/10.24846/v29i2y202006"
    },
    {
        "title": "Understanding the impact of involuntary discoveries of nonsuicidal self-injury: A thematic analysis",
        "abstract": "Growing research has examined instances of voluntarily disclosed nonsuicidal self-injury (NSSI), including how people with lived experience are impacted when they choose to share their NSSI with others. Voluntary disclosure, however, represents just one way that NSSI experiences become known to others; NSSI can also be discovered involuntarily, yet little to no research has explored the impact of these experiences. To understand the impact of these Involuntary Discovery Experiences (IDEs) the present study recruited 139 university students (Mage = 19.13, SD = 2.12; nfemale = 121) with lived experience of NSSI and who reported having a past IDE. Participants took part in an online study involving a series of open-ended questions concerning their past IDEs. A thematic analysis of their responses pointed to three overarching psychological impacts of IDEs: I felt Stigmatized and Marginalized, Things did not go well, and I No Longer felt Alone in my experience. These findings offer initial insights into the ways people with lived experience of NSSI may be impacted by IDEs and point to several new important research avenues. The current findings also suggest that clinicians may need to ask clients about any potential IDEs and their impact in order to best support clients who self-injure.",
        "keywords": "Nonsuicidal self-injury; Involuntary discovery experience; Self-harm; Voluntary Disclosure; Lived experience; Thematic analysis",
        "released": 2023,
        "link": "https://doi.org/10.1080/09515070.2023.2297883"
    },
    {
        "title": "Support for full life cycle cloud-native application management: Dynamic TOSCA and SWITCH IDE",
        "abstract": "Motivated by the complexity and difficulty of engineering time-critical cloud applications, we investigated the methodology and software workbench to provide full application life cycle support to the software engineer. For this purpose we present a novel high-level concept that is concentrated on exchanging the complex, dynamic data using OASIS TOSCA standard, mainly used in static contexts. This methodology enables specifying an application logic, provisioning, deploying, monitoring, and dynamical adaptation to changing conditions. It can be applied effectively to the interchange of dynamically changing data in order to maintain Quality of Service. By extending TOSCA with dynamic mapping of Qualify of Service and runtime attributes related to the application and underlying architecture we show that TOSCA can be used to exchange data and reconfigure on-demand compute resources whilst the application is in the process of design, infrastructure provisioning, runtime or adaptation. To demonstrate that our novel research contributions can be realised, we implemented SWITCH IDE, which showcases this concept through its development workflow which offers application and underlying infrastructure description, manipulation and reconfiguration of QoS and runtime attributes. Our approach differs in that it supports the concept of dynamic TOSCA by directly enabling application runtime to be reconfigured on the fly. (C) 2019 Elsevier B.V. All rights reserved.",
        "keywords": "Graph-based service modelling; TOSCA; Orchestration; Time-critical cloud application; Co-programming; Quality of service",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.future.2019.07.027"
    },
    {
        "title": "Research on user programming environment in grid",
        "abstract": "Principal among the technical obstacles when using the grid environment is usability. Owing to the inherently complex, programs in grid will reflect some of the complexity. To make grid resources useful and accessible effectively to the users, as we think, it requires new software or environment to build and program applications. The goal of the user programming environment of Vega Grid is to simplify the programming of the grid in the same way that the Web simplified information sharing over the Internet. To the end, it: is easier for ordinary users to develop, execute, and tune applications on the grid. In this paper, we firstly analyse the properties and capabilities grid programming tools should possess; and then describe our research on end-user programming environment when developing Vega Grid. We are now going to further develop our Grid End-user Programming Environment Prototype.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000221608900122"
    },
    {
        "title": "Investigating developers prevalent copy and paste activities and validating associated cloning patterns",
        "abstract": "To reuse available functionality, software developers usually accomplish copy and pasting of the source code. Empirical evaluations have established this activity as a prevalent cloning activity that acts as the main cause of the presence of clones into the software system. Analysis of these software systems has revealed various frequent cloning patterns, mainly classified into four categories viz. templating, forking, customization, and exact match. There are various tools that deal with clones proactively within the Integrated Development Environments. For a tool to be effective, requires the capturing of developers intentions. In this paper, we attempt to identify the hidden intentions of the software developers behind cloning practices via an online industrial survey involving professional software developers. This work will shed a light on what computer programmers are doing while reusing code and why. This study uncovers various intentions, extent, source etc. of the copy and paste activity done by the software developer. The observational determinations from this survey were then employed to validate different cloning patterns based on the developers perspective. Finally, based on the survey results, the solution to these cloning practices is discussed. Results depicted in this paper suggest an incorporation of these developers behavioural inference into the existing IDE based clone management systems.",
        "keywords": "Code Clones; Copy and Paste Activity; Developer Intention; Cloning Patterns",
        "released": 2019,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000477621400004"
    },
    {
        "title": "Measuring program comprehension: A large-scale field study with professionals",
        "abstract": "During software development and maintenance, developers spend a considerable amount of time on program comprehension activities. Previous studies show that program comprehension takes up as much as half of a developer’s time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers’ program comprehension activities go well beyond their IDE interactions. In this paper, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We follow Minelli et al.’s approach to assign developers’ activities into four categories: navigation, editing, comprehension, and other. We then measure the comprehension time by calculating the time that developers spend on program comprehension, e.g., inspecting console and breakpoints in IDE, or reading and understanding tutorials in web browsers. Using this approach, we can perform a more realistic investigation of program comprehension activities, through a field study of program comprehension in practice across a total of seven real projects, on 78 professional developers, and amounting to 3,148 working hours. Our study leverages interaction data that is collected across many applications by the developers. Our study finds that on average developers spend similar to 58 percent of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers’ experience, and project phase on the time that is spent on program comprehension, and we find senior developers spend significantly less percentages of time on program comprehension than junior developers. Our study also highlights the importance of several research directions needed to reduce program comprehension time, e.g., building automatic detection and improvement of low quality code and documentation, construction of software-engineering-specific search engines, designing better IDEs that help developers navigate code and browse information more efficiently, etc.",
        "keywords": "Program comprehension; field study; inference model",
        "released": 2018,
        "link": "https://doi.org/10.1109/TSE.2017.2734091"
    },
    {
        "title": "Plagiarism detection in computer programming using feature extraction from ultra-fine-grained repositories",
        "abstract": "Detecting instances of plagiarism in student homework, especially programming homework, is an important issue for practitioners. In the past decades, several tools have emerged that are able to effectively compare large corpora of homeworks and sort pairs by degree of similarity. However, those tools are available to students as well, allowing them to experiment and develop elaborate methods for evading detection. Also, such tools are unable to detect instances of & x201C;external plagiarism & x201D; where students obtained unethical help from sources not among other students of the same course. One way to battle this problem is to monitor student activity while solving their homeworks using a cloud-based integrated development environment (IDE) and detect suspicious behaviours. Each editing event in program source can be stored as a new commit to create a form of ultra-fine-grained source code repository. In this paper, the authors propose several new features that can be extracted from such repositories with the purpose of building a comprehensive profile of each individual developer. Machine learning techniques were used to detect suspicious behaviours, which allowed the authors to significantly improve upon the performance of more traditional plagiarism detection tools.",
        "keywords": "Plagiarism; Tools; Feature extraction; Programming; Data mining; Cloning; Machine learning; Repository mining; plagiarism detection; ultra-fine-grained repositories; machine learning; feature extraction",
        "released": 2020,
        "link": "https://doi.org/10.1109/ACCESS.2020.2996146"
    },
    {
        "title": "Prompter",
        "abstract": "Developers often require knowledge beyond the one they possess, which boils down to asking co-workers for help or consulting additional sources of information, such as Application Programming Interfaces (API) documentation, forums, and Q&A websites. However, it requires time and energy to formulate one’s problem, peruse and process the results. We propose a novel approach that, given a context in the Integrated Development Environment (IDE), automatically retrieves pertinent discussions from Stack Overflow, evaluates their relevance using a multi-faceted ranking model, and, if a given confidence threshold is surpassed, notifies the developer. We have implemented our approach in Prompter, an Eclipse plug-in. Prompter was evaluated in two empirical studies. The first study was aimed at evaluatingPrompter’s ranking model and involved 33 participants. The second study was conducted with 12 participants and aimed at evaluating Prompter’s usefulness when supporting developers during development and maintenance tasks. Since Prompter uses “volatile information” crawled from the web, we also replicated Study I after one year to assess the impact of such a “volatility” on recommenders like Prompter. Our results indicate that (i) Prompter recommendations were positively evaluated in 74 % of the cases on average, (ii) Prompter significantly helps developers to improve the correctness of their tasks by 24 % on average, but also (iii) 78 % of the provided recommendations are “volatile” and can change at one year of distance. While Prompter revealed to be effective, our studies also point out issues when building recommenders based on information available on online forums.",
        "keywords": "Recommenders; Mining software repositories; Stack overflow; Empirical studies",
        "released": 2016,
        "link": "https://doi.org/10.1007/s10664-015-9397-1"
    },
    {
        "title": "meSchup: A platform for programming interconnected smart things",
        "abstract": "In a future where hundreds of smart networked devices will be embedded in our everyday environments, the question of how to program the world around us arises. An abstraction layer and web-based integrated development environment can interweave available smart things’ capabilities into a collective orchestration of smart behavior.",
        "keywords": "Programming the World; meSchup; smart devices; networked devices; networking; pervasive computing; embedded systems; Internet of Things; IoT; sensors; actuators; context-aware devices; mobile computing",
        "released": 2017,
        "link": "https://doi.org/10.1109/MC.2017.4041350"
    },
    {
        "title": "Developing a learning data collection platform for learning analytics in online education",
        "abstract": "During the COVID-19 pandemic, most education has been conducted through online classes. While feedback and interaction between students and instructors are significant in programming education or engineering practice, online education today cannot satisfy these aspects of learning. Therefore, this study proposes a learning support system for programming education and presents the results of designing and implementing this system. The proposed system consists of an online development environment module, a learning monitoring module, and a learning support module. It also provides a web-based programming environment, real-time chat and code mirroring, error guide messages and related lectures, e-learning quizzes, and learning activity analysis features. The system standardizes the development environment between the instructor and students, helps students take the initiative in solving errors, and enables code-oriented interactions between the instructor and students. It also collects data from all learning situations in the database. Conducting a big data analysis with the collected data will enable individual guidance for students by finding errors that frequently occur in programming and recommending learning materials to solve them.",
        "keywords": "online education; learning analytics; programming education; data collection; platform; learning data",
        "released": 2022,
        "link": "https://doi.org/10.3390/app12115412"
    },
    {
        "title": "Interactive web-based tools for an introductory course in crystallography",
        "abstract": "The new possibilities offered by the Java programming environment combined with the accessibility of the World Wide Web present new and interesting perspectives. It is thus now possible to perform simulations directly, using a Web browser, independently of the computer platform being used. Basic concepts of crystallography, Le. crystal structures, point- and space-group symmetry, lattices, reciprocal lattices and diffraction principles can be illustrated utilizing interactive simulations written in Java. A number of such applications have been developed with the aim to facilitate the understanding of these crystallographic concepts to the newcomers in the field.",
        "keywords": "",
        "released": 1999,
        "link": "https://doi.org/10.1107/S0021889899011152"
    },
    {
        "title": "Managers don’t code: Making web services middleware applicable for end-users",
        "abstract": "Today’s web-pages are primarily designed for occasional usage. Professional users therefore use special application, that use Web Services increasingly. As the number of internet-users grows we argue that there is a disregarded growing gap between these professional- and occasional-users we refer to as experienced users. For this group of users with little or no programming-skills web-pages are inefficient but professional applications would be inexpedient. In this paper we describe how to make Web Services applicable for experienced web users. To support single Web Service calls efficiently we present a keyboard-controlled browser-embedded console with command auto-completion that wraps Web Services. To support multiple calls and automation we present a web-based IDE that allows visual composition of Web Service calls and simple control-structures that can be used on demand without installation and programming-skills.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000224322700011"
    },
    {
        "title": "Transformation architecture for multi-layered WebApp source code generation",
        "abstract": "The evolution of Web technologies leads to software premature obsolescence requiring technology-independent representations to increase the reuse rates during the development process. They also require integration into service-oriented architectures to exchange information with different Web systems supporting runtime interoperability. Web Applications (WebApps) run on devices with different capabilities and limitations increasing the complexity of the development process. To address these challenges, different proposals have emerged to facilitate the development of WebApps, which is still an open research field with many challenges to address. This paper presents a model transformation architecture based on software standards to automatically generate full stack multi-layered WebApps covering Persistence, Service, and Presentation layers. This transformation architecture also generates the set of test cases to test WebApp business logic. The proposed transformation architecture only requires a UML platform-independent class model as an input to generate fully functional Web applications in a three-tier architecture including the three layers, while most proposals focus on the generation of the Presentation layer. In addition, this architecture employs software industry standards to enable an easy integration into third-party tools and development environments. The transformation Architecture proposed has been empirically validated on the case study of a fully functional travel management WebApp that is generated using a UML class diagram employing a third-party tool integrated into the same integrated development environment.",
        "keywords": "Unified modeling language; Proposals; Codes; Service-oriented architecture; Computer architecture; Web services; Standards; Software product lines; computer-aided software engineering; client-server systems",
        "released": 2022,
        "link": "https://doi.org/10.1109/ACCESS.2022.3141702"
    },
    {
        "title": "Web-based expert geographical information system for advanced transportation management systems",
        "abstract": "The Internet is fast becoming the standard environment for client-server applications that involve multiple users. The proliferation of Internet-based application development tools opens new doors to transportation researchers who work in real-time decision support system development In the 1990s, one of the most important problems in advanced transportation management systems research was the development of better incident management systems. Although the incident management process has been well studied, the development of real-time decision support systems that can be used by all the involved agencies remains a challenging area of transportation engineering research. Existing incident management systems are developed on various traditional computing platforms, Including UNIX and Windows. However, with the advent of the World Wide Web and Internet-based programming tools such as Java, it is possible to develop platform independent decision support tools for the incident management agencies. Web-based support tools offer an invaluable opportunity to develop next-generation online decision support tools for real-time traffic management. The applicability of Web-based tools to the development of online decision support systems for incident management Is explored and demonstrated, and a prototype incident management decision support system (DSS) that has most of the capabilities of similar UNIX-based DSS support systems is developed and tested. Briefly described are the implementation and development of a prototype wide-area incident management system using Web-based tools.",
        "keywords": "",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000176425400026"
    },
    {
        "title": "Collaborative gaming and competition for CS-STEM education using SPHERES zero robotics",
        "abstract": "There is widespread investment of resources in the fields of Computer Science, Science, Technology, Engineering, Mathematics (CS-STEM) education to improve STEM interests and skills. This paper addresses the goal of revolutionizing student education using collaborative gaming and competition, both in virtual simulation environments and on real hardware in space. The concept is demonstrated using the SPHERES Zero Robotics (ZR) Program which is a robotics programming competition. The robots are miniature satellites called SPHERES-an experimental test bed developed by the MIT SSL on the International Space Station (ISS) to test navigation, formation flight and control algorithms in microgravity. The participants compete to win a technically challenging game by programming their strategies into the SPHERES satellites, completely from a web browser. The programs are demonstrated in simulation, on ground hardware and then in a final competition when an astronaut runs the student software aboard the ISS. ZR had a pilot event in 2009 with 10 High School (HS) students, a nationwide pilot tournament in 2010 with over 200 HS students from 19 US states, a summer tournament in 2010 with similar to 150 middle school students and an open-registration tournament in 2011 with over 1000 HS students from USA and Europe. The influence of collaboration was investigated by (1) building new web infrastructure and an Integrated Development Environment where intensive inter-participant collaboration is possible, (2) designing and programming a game to solve a relevant formation flight problem, collaborative in nature-and (3) structuring a tournament such that inter-team collaboration is mandated. This paper introduces the ZR web tools, assesses the educational value delivered by the program using space and games and evaluates the utility of collaborative gaming within this framework. There were three types of collaborations as variables within matches (to achieve game objectives), inter-team alliances and unstructured communication on online forums. Simulation competition scores, website usage statistics and post-competition surveys are used to evaluate educational impact and the effect of collaboration. (C) 2012 Elsevier Ltd. All rights reserved.",
        "keywords": "Collaborative gaming; Educational Robotics; Programming competitions; Social science research",
        "released": 2013,
        "link": "https://doi.org/10.1016/j.actaastro.2012.09.006"
    },
    {
        "title": "WebProtege: A collaborative ontology editor and knowledge acquisition tool for the web",
        "abstract": "In this paper, we present WebProtege-a lightweight ontology editor and knowledge acquisition tool for the Web. With the wide adoption of Web 2.0 platforms and the gradual adoption of ontologies and Semantic Web technologies in the real world, we need ontology-development tools that are better suited for the novel ways of interacting, constructing and consuming knowledge. Users today take Web-based content creation and online collaboration for granted. WebProtege integrates these features as part of the ontology development process itself. We tried to lower the entry barrier to ontology development by providing a tool that is accessible from any Web browser, has extensive support for collaboration, and a highly customizable and pluggable user interface that can be adapted to any level of user expertise. The declarative user interface enabled us to create custom knowledge-acquisition forms tailored for domain experts. We built WebProtege using the existing Protege infrastructure, which supports collaboration on the back end side, and the Google Web Toolkit for the front end. The generic and extensible infrastructure allowed us to easily deploy WebProtege in production settings for several projects. We present the main features of WebProtege and its architecture and describe briefly some of its uses for real-world projects. WebProtege is free and open source. An online demo is available at http://webprotege.stanford.edu.",
        "keywords": "Web-based ontology editing; knowledge acquisition; collaboration; Protege; Semantic Web",
        "released": 2013,
        "link": "https://doi.org/10.3233/SW-2012-0057"
    },
    {
        "title": "Native client: A sandbox for portable, untrusted x86 native code",
        "abstract": "Native Client is a sandbox for untrusted x86 native code. It aims to give browser-based applications the computational performance of native applications without compromising safety. Native Client uses software fault isolation and a secure runtime to direct system interaction and side effects through interfaces it controls. It further provides operating system portability for binary code while supporting performance-oriented features generally absent from Web application programming environments, such as thread support, instruction set extensions such as SSE, and use of compiler intrinsics and hand-coded assembler. We combine these properties in an open architecture that encourages community review and third-party tools.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1145/1629175.1629203"
    },
    {
        "title": "The image data explorer: Interactive exploration of image-derived data",
        "abstract": "Many bioimage analysis projects produce quantitative descriptors of regions of interest in images. Associating these descriptors with visual characteristics of the objects they describe is a key step in understanding the data at hand. However, as many bioimage data and their analysis workflows are moving to the cloud, addressing interactive data exploration in remote environments has become a pressing issue. To address it, we developed the Image Data Explorer (IDE) as a web application that integrates interactive linked visualization of images and derived data points with exploratory data analysis methods, annotation, classification and feature selection functionalities. The IDE is written in R using the shiny framework. It can be easily deployed on a remote server or on a local computer. The IDE is available at https://git.embl.de/heriche/image-data-explorer and a cloud deployment is accessible at https://shiny-portal.embl.de/shinyapps/app/01_ image-data-explorer.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1371/journal.pone.0273698"
    },
    {
        "title": "Digital trail making test-black and white: Normal vs MCI",
        "abstract": "Objective Trail Making Test-Black and White (TMT-B&W) was developed to assess the cognition of patients with mild cognitive impairment (MCI), dementia, and Alzheimer’s disease. Collection and analysis of test results have been limited due to scoring time and efforts required from both administrators and patients during and after taking the test. To increase efficiency and reducing scoring time, a computer version touchscreen-based digital trail making test-black and white (dTMT-B&W) was developed on Android and it was administered on MCI versus cognitively normal controls (NC) participants. The current study examines the sensitivity of newly developed computer version dTMT-B&W on NC and MCI subjects. Method dTMT-B&W was developed using MIT app inventor software, a web-based integrated development environment (IDE) with the Android development tools that are used to build fully functional applications for smartphones and tablets. A total of 44 participants were included, comprised of 22 NC and 22 MCI. The dTMT-TMT-B&W was administered to all NC and MCI subjects. Result dTMT-B&W was designed to be as consistent with the pen-paper TMT-B&W (ppTMT-B&W) where the application is a standalone installation. dTMT-B&W is divided into two parts (Part-A and Part-B), in which the subject attempts to connect black and white numbered circles sequentially as quickly as possible, while still maintaining accuracy. Similarly, the paper-based TMT-B&W requires the subject to connect black and white numbered circles in ascending order, except on a sheet of paper rather than a tablet. dTMT-B&W successfully distinguished NC from MCI subjects. Conclusion dTMT-B&W is an Android application that was successfully developed to be as consistent as possible with the original pen-paper TMT-B&W to establish equal concurrent validity, with some improved features embedded into the design and dTMT-B&W revealed a significant correlation with frontal executive function and this can help in early diagnosing subjects with MCI among NC subjects.",
        "keywords": "Design; mild cognitive impairment; mobile application; Trail Making Test",
        "released": 2022,
        "link": "https://doi.org/10.1080/23279095.2021.1871615"
    },
    {
        "title": "Intelligent program correction and evaluation system",
        "abstract": "The growing popularity of computing applications has sparked the interest of students in computer programming languages. Minor mistakes are prevalent while writing small code blocks due to the coder’s lack of knowledge and carelessness. Instead of merely providing syntax warnings, it would be better to offer developers with an Integrated Development Environment (IDE) that can automatically correct short code blocks containing mistakes. This makes code composition easier and more time-efficient, thus improving the efficacy of large-scale development environments. Because Python is becoming more popular, the goal of our study is to enhance the efficiency of writing Python code by offering an automatic code-correcting approach. Furthermore, automatic program evaluation has been performed to assist in the debugging of small code blocks, which will ultimately be employed in the creation of real-time computer applications. The proposed technique would be useful for new learners who wish to create small Python code blocks for ease of writing and debugging on online platforms (like edX, Coursera, and Udacity). One of the major contributions of the project is to create an erroneous dataset of Python coding that contains all potential forms of probable syntax errors. The dataset induces variety through the use of multiple coding templates and is used to train deep learning models. We used the state-of-the-art text-to-text T5 transformer network model to automatically repair and evaluate the incorrect code. The outcomes of auto-correction are examined using the ROUGE and BLEU scores, as well as accuracy. The model corrects Python code with single, double, and the multiple number of errors with greater than 80% accuracy. Similarly, the performance of the basic T5 transformer network for program auto-evaluation with and without mistakes has been examined, and the model achieves greater than 65% accuracy in both cases. The proposed T5 base transformer outperforms the SOTA auto-correction models in terms of accuracy, according to a comparison study of the proposed method with the earlier techniques for auto-correcting codes.",
        "keywords": "Python coding; automatic correction of code; automatic code evaluation; T5 transformer network; eLearning; deep learning",
        "released": 2024,
        "link": "https://doi.org/10.1142/S0218213023500719"
    },
    {
        "title": "LEARNING WEB DEVELOPMENT USING GITHUB COPILOT IN AND OUTSIDE ACADEMIA: A BLESSING OR a CURSE?",
        "abstract": "This article investigates the usage of GitHub Copilot, an artificial intelligence-powered coding assistant owned by Microsoft and GitHub, in the process of learning and teaching web development both in formal academic, and informal settings. We dive into the idea behind utilizing GitHub Copilot and highlight its most common and relevant use cases which can be used to learn Web Development. Drawing from existing scientific literature and online statements from software professionals, we present an overview of the current situation with artificial intelligence-assisted programming tools such as GitHub Copilot and its impact and irrelevance on Web Development education especially for the early learning stages. Professionals both in and outside academia agree that usage of artificial intelligence Pair Programming tools such as GitHub Copilot is neither recommended nor essential when learning or teaching Web Development.",
        "keywords": "AI pair programming; GitHub Copilot; Web Development; VS Code",
        "released": 2024,
        "link": "https://doi.org/10.7906/indecs.22.3.10"
    },
    {
        "title": "Parallelising semantic checking in an IDE: A way toward improving profits and sustainability, while maintaining high-quality software development",
        "abstract": "After recent improvements brought the incremental compilation of large industrial test suites down to a few seconds, the first semantic checking of a project became one of the longest-running processes. As multi-core systems are now the standard, we derived a parallelisation using software engineering laws to improve the performance of semantic checking.Our measurements show that even an outdated laptop is fast enough for daily use. The performance improvements came without performance regressions, and we can’t expect additional massive benefits even from infinitely scaling Cloud resources.Companies should utilise cheaper machines that still o er enough performance for longer. This approach can help businesses increase profits, reduce electronic waste and promote sustainability while maintaining high-quality software development practices.",
        "keywords": "parallel computing; cloud computing; semantic checking; integrated development environment; software development tools; software engineering laws; TTCN-3; performance improvement; sustainability; cost reduction; profit increase",
        "released": 2023,
        "link": "https://doi.org/10.2478/ausi-2023-0016"
    },
    {
        "title": "<I>SoEasy</i>: A software framework for easy hardware control programming for diverse IoT platforms",
        "abstract": "Many Internet of Things (IoT) applications are emerging and evolving rapidly thanks to widespread open-source hardware platforms. Most of the high-end open-source IoT platforms include built-in peripherals, such as the universal asynchronous receiver and transmitter (UART), pulse width modulation (PWM), general purpose input output (GPIO) ports and timers, and have enough computation power to run embedded operating systems such as Linux. However, each IoT platform has its own way of configuring peripherals, and it is difficult for programmers or users to configure the same peripheral on a different platform. Although diverse open-source IoT platforms are widespread, the difficulty in programming those platforms hinders the growth of IoT applications. Therefore, we propose an easy and convenient way to program and configure the operation of each peripheral using a user-friendly Web-based software framework. Through the implementation of the software framework and the real mobile robot application development along with it, we show the feasibility of the proposed software framework, named SoEasy.",
        "keywords": "visual programming tool; Internet of Things; Web of Things; IoT development tool",
        "released": 2018,
        "link": "https://doi.org/10.3390/s18072162"
    },
    {
        "title": "Unified programming concepts for unobtrusive integration of cloud-based and local parallel computing",
        "abstract": "The growth in the data and computation need of today’s operations has led to technical solutions that distribute workload over several entities for better performance. To facilitate such a paradigm, research studies have been investigating efficient approaches for combining high performance computing in shared-memory with distributed-memory environments. Meanwhile, the benefits of cloud computing and its modern enhancements have created potentials for applications to leverage the powerful, ubiquitous and cheap resources of cloud infrastructures. Yet, a small portion of the work in this area addresses the programming aspects of cloud-related technologies. Despite the extensive improvements in the fundamental mechanisms of this realm, programming environments offer little support for incorporating the high performance mechanisms of shared-memory computing with cloud computing. This study proposes a solution for an unobtrusive definition and integration of cloud-based and shared memory parallel computing, in order to further simplify the application of cloud capabilities in local systems. It does so by implementing the proposed concepts in @PT (Annotation Parallel Task), a parallel-programming environment that utilizes native Java annotations as its language constructs. The experimental evaluations discussed here demonstrate that the proposed approach facilitates achieving the potential benefits of cloud computing for performance and energy consumption in local devices. (C) 2020 Elsevier B.V. All rights reserved.",
        "keywords": "Computation offloading; Cloud computing; Java annotations; @PT; Unobtrusive parallelization; Remote method invocation",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.future.2020.09.024"
    },
    {
        "title": "FQN inference in partial code by prompt-tuned language model of code",
        "abstract": "Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this article, we propose using a prompt-tuned code masked language model (MLM) as a neural knowledge base for type inference, called POME, which is lightweight and has minimal requirements on code compilation. Unlike the existing symbol name and context matching for type inference, POME infers the FQNs syntax and usage knowledge encapsulated in prompt-tuned code MLM through a colze-style fill-in-blank strategy. POME is integrated as a plug-in into web and integrated development environments (IDE) to assist developers in inferring FQNs in the real world. We systematically evaluate POME on a large amount of source code from GitHub and Stack Overflow, and explore its generalization and hybrid capability. The results validate the effectiveness of the POME design and its applicability for partial code type inference, and they can be easily extended to different programming languages (PL). POME can also be used to generate a PL-hybrid type inference model for providing a one-for-all solution. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.",
        "keywords": "Type inference; fully qualified names; code masked language model; neural knowledge base",
        "released": 2024,
        "link": "https://doi.org/10.1145/3617174"
    },
    {
        "title": "Approximation-free state-feedback backstepping controller for uncertain pure-feedback nonautonomous nonlinear systems based on time-derivative estimator",
        "abstract": "A novel backstepping controller for uncertain single-input single-output pure-feedback non-affine and nonautonomous nonlinear systems based on the time-derivative estimator( IDE) is proposed. Using IDEs, time-derivatives of error signals used in virtual control terms are directly estimated in every backstepping design steps. As a result, the control law has a relatively simple form. In addition, convergence of tracking error to a small neighborhood of origin is guaranteed regardless of unstructured uncertainties or unmatched disturbances in the controlled system. It does not require separate adaptive schemes or universal approximators such as neural networks or fuzzy logic systems adaptively tuned online to cope with system uncertainties. Simulation results demonstrated the simplicity and good performance of the proposed approximation-free controller.",
        "keywords": "Pure-feedback; time-derivative estimator; approximation-free; uncertain nonlinear system",
        "released": 2019,
        "link": "https://doi.org/10.1109/ACCESS.2019.2938595"
    },
    {
        "title": "Semantic web logic programming tools",
        "abstract": "The last two decades of research in Logic Programming, both at the theoretical and practical levels, have addressed several topics highly relevant for the Semantic Web effort, providing very concrete answers to some open questions. This paper describes succinctly the contributions from the Logic Programming group of Centro de Inteligencia Artificial (CENTRIA) of Universidade Nova de Lisboa, as a prelude to a description of our recent efforts to develop integrated standard tools for disseminating this research throughout the interested Web communities. The paper does not intended to be a survey of logic programming techniques applicable to the Semantic Web, and so the interested reader should try to obtain the missing information in the logic programming journals and conferences.",
        "keywords": "",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000188097100002"
    },
    {
        "title": "BioRuby: Bioinformatics software for the ruby programming language",
        "abstract": "The BioRuby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology, written in the Ruby programming language. BioRuby has components for sequence analysis, pathway analysis, protein modelling and phylogenetic analysis; it supports many widely used data formats and provides easy access to databases, external programs and public web services, including BLAST, KEGG, GenBank, MEDLINE and GO. BioRuby comes with a tutorial, documentation and an interactive environment, which can be used in the shell, and in the web browser.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1093/bioinformatics/btq475"
    },
    {
        "title": "THE ACCEPTANCE OF AN EDUCATIONAL INTEGRATED DEVELOPMENT ENVIRONMENT TO LEARN PROGRAMMING FUNDAMENTALS",
        "abstract": "Programming is an important course for any IT or engineering-related course. However, previous research shows that students face difficulties in learning programming due to its abstract concepts. This study aims to evaluate the acceptance of a developed Integrated Development Environment (IDE), namely C-SOLVIS which is a web-based application that specifically intends to facilitate the teaching and learning of the C programming fundamentals in Malaysian tertiary education. The C-SOLVIS integrates problem-solving into a program development environment for the C language. The goal is to guide the users in problem-solving and help them write C programs based on problem-solving algorithms. The Rapid Application Development (RAD) Model was employed in the C-SOLVIS development process. Based on this model, the requirement planning phase was carried out through the triangulation technique by applying qualitative approaches comprising a literature review supported by semi-structured interviews, document reviews, and content validation by expert programming lecturers. Subsequently, the design of the application was accomplished through the iterative prototyping process which was then followed by the application construction. Then, the C-SOLVIS is deployed to be used by several programming lecturers to evaluate its usability by adopting a quantitative method using the System Usability Scale (SUS) questionnaire. The study has discovered several suitable techniques and designs for the problem-solving and program development environment. For the problem -solving environment, the Computational Thinking (CT) concepts were applied which were supported by the Input-Process-Output (IPO) Model through Scientific Instructions and Inquiries. Meanwhile, the program development environment was designed and developed based on frame -based programming using a set of developed Code Patterns. The C-SOLVIS evaluation using the SUS instrument has yielded a SUS mean score of 86.07. This score is interpreted by SUS as an A grade that indicates C-SOLVIS as a highly usable application and thus is accepted for C programming learning. Hence, the development process of the C-SOLVIS can be used as a guideline for educational software development, especially in the field of programming education.",
        "keywords": "IDE; educational software; usability; programming",
        "released": 2023,
        "link": "https://doi.org/10.33407/itlt.v93i1.5102"
    },
    {
        "title": "WebFlow - a visual programming paradigm for web/java based coarse grain distributed computing",
        "abstract": "We present here recent work at NPAC aimed at developing WebFlow - a general purpose Web-based visual interactive programming environment far coarse grain distributed computing, We follow the 3-tier architecture with the central control and integration WebVM layer in tier-2, interacting with the visual graph editor applets in tier-i (front-end) and the legacy systems in tier-3, WebVM is given by a mesh of Java Web servers such as Jeeves from JavaSoft or Jigsaw from MIT/W3C, All system control structures are implemented as URL-addressable servlets which enable Web browser-based authoring, monitoring, publication, documentation and software distribution tools for distributed computing, We view WebFlow/WEbVM as a promising programming paradigm and co-ordination model for the exploding volume of Web/Java software, and we illustrate it in a set of ongoing application development activities. (C) 1997 by John Wiley & Sons, Ltd.",
        "keywords": "",
        "released": 1997,
        "link": "https://doi.org/10.1002/(SICI)1096-9128(199706)9:6<555::AID-CPE308>3.0.CO;2-X"
    },
    {
        "title": "OAM: An ontology application management framework for simplifying ontology-based semantic web application development",
        "abstract": "Although the Semantic Web data standards are established, ontology-based applications built on the standards are relatively limited. This is partly due to high learning curve and efforts demanded in building ontology-based Semantic Web applications. In this paper, we describe an ontology application management (OAM) framework that aims to simplify creation and adoption of ontology-based application that is based on the Semantic Web technology. OAM introduces an intermediate layer between user application and programming and development environment in order to support ontology-based data publishing and access, abstraction and interoperability. The framework focuses on providing reusable and configurable data and application templates, which allow the users to create the applications without programming skill required. Three forms of templates are introduced: database to ontology mapping configuration, recommendation rule and application templates. We describe two case studies that adopted the framework: activity recognition in smart home domain and thalassemia clinical support system, and how the framework was used in simplifying development in both projects. In addition, we provide some performance evaluation results to show that, by limiting expressiveness of the rule language, a specialized form of recommendation processor can be developed for more efficient performance. Some advantages and limitations of the application framework in ontology-based applications are also discussed.",
        "keywords": "Semantic Web application framework; ontology application framework; ontologybased data publishing and access; knowledge-based application development tools",
        "released": 2016,
        "link": "https://doi.org/10.1142/S0218194016500066"
    },
    {
        "title": "Equipping IDEs with XML-path reasoning capabilities",
        "abstract": "One of the challenges in Web development is to achieve a good level of quality in terms of code size and runtime performance for popular domain-specific languages such as XQuery, XSLT, and XML Schema. We present the first IDE augmented with static detection of inconsistent XPath expressions that assists the programmer with simplifying development and debugging of any application involving XPath expressions. The tool is based on newly developed formal verification techniques based on expressive modal logics, which are now mature enough to be introduced in the process of software development. We further develop this idea in the context of XQuery for which we introduce an analysis for identifying and eliminating dead code automatically. This proof of concept aims at illustrating the benefits of equipping modern IDEs with reasoning capabilities.",
        "keywords": "Languages; Standardization; Verification; Programming; environment; compile time; analysis; reasoning; XML; schema; query; path",
        "released": 2014,
        "link": "https://doi.org/10.1145/2602573"
    },
    {
        "title": "Advances in testing JavaScript-based web applications",
        "abstract": "JavaScript is a flexible and expressive prototype-based scripting language that is used by developers to create interactive web applications. The language is interpreted, dynamic, weakly typed, and has first-class functions. It also interacts extensively with other web languages such as CSS and HTML at runtime. All these characteristics make JavaScript code particularly error-prone and challenging to analyze and test. In this chapter, we explore recent advances made in analysis and testing techniques geared toward JavaScript-based web applications. In particular, we look at recent empirical studies, testing techniques, test oracle automation approaches, test adequacy assessment methods, fault localization and repair, and Integrated Development Environment support to help programmers write better JavaScript code.",
        "keywords": "",
        "released": 2015,
        "link": "https://doi.org/10.1016/bs.adcom.2014.12.003"
    },
    {
        "title": "WWW access to legacy client/server applications",
        "abstract": "We describe a method for accessing Client/Server applications from standard World Wide Web browsers. An existing client for the system is modified to perform HTTP Proxy duties. Web browser users simply configure their browsers to use this HTTP Proxy, and can then access the system via specially encoded URLs that the HTTP Proxy intercepts and sends to the legacy server system. An example implementation using the Oz Process Centered Software Development Environment is presented.",
        "keywords": "HTTP; client; server; legacy; Proxy; Oz",
        "released": 1996,
        "link": "https://doi.org/10.1016/0169-7552(96)00023-2"
    },
    {
        "title": "Enabling ordinary users mobile development with web components",
        "abstract": "The rapid progress of the mobile internet has been promoting the popularity of mobile devices, and mobile application development is getting more pervasive. However, the state of the art development environments has a high learning barrier for users’ lack of programming experience. In this paper, instead of traditional programming environments, we take consideration of ordinary users’ requirements and propose a WYSIWYG cross-platform web-component-based mobile application creation environment for ordinary users. This environment has a visual editor with a drag-and-drop web component. A web component library model is proposed to standardize customized libraries. A cross-platform application model based on web components is implemented to build applications rapidly. It helps ordinary users generate installing packages within simple operations for multiple platforms. A native plugin model is proposed to assist web components to invoke native functionalities. The experiment result shows that ordinary users could quickly start to create mobile applications in our environment.",
        "keywords": "Mobile service; web components; cross-platform; ordinary users; visual development",
        "released": 2020,
        "link": "https://doi.org/10.1109/ACCESS.2019.2962393"
    },
    {
        "title": "SqueakJS a modern and practical smalltalk that runs in any browser",
        "abstract": "We report our experience in implementing SqueakJS, a bitcompatible implementation of Squeak/Smalltalk written in pure JavaScript. SqueakJS runs entirely in theWeb browser with a virtual file system that can be directed to a server or client-side storage. Our implementation is notable for simplicity and performance gained through adaptation to the host object memory and deployment leverage gained through the Lively Web development environment. We present several novel techniques as well as performance measurements for the resulting virtual machine. Much of this experience is potentially relevant to preserving other dynamic language systems and making them available in a browser-based environment.",
        "keywords": "Smalltalk; Squeak; Web browsers; JavaScript",
        "released": 2015,
        "link": "https://doi.org/10.1145/2661088.2661100"
    },
    {
        "title": "The DOG gateway: Enabling ontology-based intelligent domotic environments",
        "abstract": "This paper moves a first step towards the creation of Intelligent Domotic Environments (IDE) in real-life home-living. A new Domotic OSGi Gateway (DOG) is presented, able to manage different domotic networks as a single, technology neutral, home automation system. The adoption of a standard framework such as OSGi, and of sophisticated modeling techniques stemming from the Semantic Web research community, allows DOG to go beyond simple automation and to support reasoning-based intelligence inside home environments. By exploiting automatic device generalization, syntactic and semantic command validation, and internet-work scenario definition, DOG provides the building blocks for supporting the evolution of current, isolated, home automation plants into IDEs, where heterogeneous devices and domotic systems are coordinated to behave as a single, intelligent, proactive system. The paper introduces the DOG architecture and the underlying ontology modeling. A case stud), is also illustrated, where DOG controls a laboratory reconstruction of a simple domotic environment.",
        "keywords": "System architectures; integration and modeling; Ubiquitous computing; Rule-based processing; Knowledge modeling",
        "released": 2008,
        "link": "https://doi.org/10.1109/TCE.2008.4711217"
    },
    {
        "title": "WebSphere studio overview",
        "abstract": "In this paper we provide an overview of IBM WebSphere Studio, a family of tools for servers for state-of-the-art information technology systems. In today’s business environment such systems are complex, comprise multiple platforms, and make use of a wide range of technologies and standards. Through a representative development scenario we illustrate the way WebSphere Studio satisfies the challenging requirements for a modern integrated development environment. The scenario covers a variety of technologies and standards, including database access, Web services standards, Enterprise JavaBeans(TM) implementation, integrated application testing, Web page design, and performance optimization. We also describe the Eclipse Modeling Framework, the open source technology base on which WebSphere Studio is built.",
        "keywords": "",
        "released": 2004,
        "link": "https://doi.org/10.1147/sj.432.0384"
    },
    {
        "title": "Swoop: A web ontology editing browser",
        "abstract": "In this paper, we describe Swoop, a hypermedia inspired Ontology Browser and Editor based on OWL, the recently standardized Web-oriented ontology language. After discussing the design rationale and architecture of Swoop, we focus mainly on its features, using illustrative examples to highlight its use. We demonstrate that with its Web-metaphor, adherence to OWL recommendations and key unique features, such as Collaborative Annotation using Annotea, Swoop acts as a useful and efficient Web Ontology development tool. We conclude with a list of future plans for Swoop, that should further increase its overall appeal and accessibility. (c) 2005 Elsevier B.V. All rights reserved.",
        "keywords": "OWL; ontology engineering; systems",
        "released": 2006,
        "link": "https://doi.org/10.1016/j.websem.2005.10.001"
    },
    {
        "title": "QRTEngine: An easy solution for running online reaction time experiments using qualtrics",
        "abstract": "Performing online behavioral research is gaining increased popularity among researchers in psychological and cognitive science. However, the currently available methods for conducting online reaction time experiments are often complicated and typically require advanced technical skills. In this article, we introduce the Qualtrics Reaction Time Engine (QRTEngine), an open-source JavaScript engine that can be embedded in the online survey development environment Qualtrics. The QRTEngine can be used to easily develop browser-based online reaction time experiments with accurate timing within current browser capabilities, and it requires only minimal programming skills. After introducing the QRTEngine, we briefly discuss how to create and distribute a Stroop task. Next, we describe a study in which we investigated the timing accuracy of the engine under different processor loads using external chronometry. Finally, we show that the QRTEngine can be used to reproduce classic behavioral effects in three reaction time paradigms: a Stroop task, an attentional blink task, and a masked-priming task. These findings demonstrate that QRTEngine can be used as a tool for conducting online behavioral research even when this requires accurate stimulus presentation times.",
        "keywords": "Online experiments; Qualtrics; JavaScript; Amazon Mechanical Turk; Open-source",
        "released": 2015,
        "link": "https://doi.org/10.3758/s13428-014-0530-7"
    },
    {
        "title": "Blockchain-based auctioning for energy storage sharing in a smart community",
        "abstract": "The increasing prevalence of renewable energy resources introduces a high variability that complicates the task of energy management in modern power grids. Among other technologies, batteries have proven effective in managing power imbalances in such grids. However, the high cost of large-scale batteries, coupled with their enormous space requirements, could deter their adoption by large consumers such as shared facility controllers. The aggregation of residential energy storage units offers shared facility controllers (SFCs) an alternative way to leverage storage; however, a secure scheme that promotes fairness and transparency in the selection and compensation of shared storage unit owners is needed. To this end, an Ethereum smart contract that makes residential storage capacities available to SFCs via a double auction mechanism is proposed. The contract is written with solidity and deployed in the browser-based Remix-integrated development environment. Scenario tests prove the effectiveness of the smart contract in selecting and compensating the owners of shared storage capacities, according to predefined auction rules.",
        "keywords": "blockchain; smart contract; peer-to-peer trading; storage sharing",
        "released": 2022,
        "link": "https://doi.org/10.3390/en15061954"
    },
    {
        "title": "Development of a smartphone app for a genetics website: The amyotrophic lateral sclerosis online genetics database (ALSoD)",
        "abstract": "Background: The ALS Online Genetics Database (ALSoD) website holds mutation, geographical, and phenotype data on genes implicated in amyotrophic lateral sclerosis (ALS) and links to bioinformatics resources, publications, and tools for analysis. On average, there are 300 unique visits per day, suggesting a high demand from the research community. To enable wider access, we developed a mobile-friendly version of the website and a smartphone app. Objective: We sought to compare data traffic before and after implementation of a mobile version of the website to assess utility. Methods: We identified the most frequently viewed pages using Google Analytics and our in-house analytic monitoring. For these, we optimized the content layout of the screen, reduced image sizes, and summarized available information. We used the Microsoft. NET framework mobile detection property (HttpRequest. IsMobileDevice in the Request. Browser object in conjunction with HttpRequest. UserAgent), which returns a true value if the browser is a recognized mobile device. For app development, we used the Eclipse integrated development environment with Android plug-ins. We wrapped the mobile website version with the WebView object in Android. Simulators were downloaded to test and debug the applications. Results: The website automatically detects access from a mobile phone and redirects pages to fit the smaller screen. Because the amount of data stored on ALSoD is very large, the available information for display using smartphone access is deliberately restricted to improve usability. Visits to the website increased from 2231 to 2820, yielding a 26% increase from the pre-mobile to post-mobile period and an increase from 103 to 340 visits (230%) using mobile devices (including tablets). The smartphone app is currently available on BlackBerry and Android devices and will be available shortly on iOS as well. Conclusions: Further development of the ALSoD website has allowed access through smartphones and tablets, either through the website or directly through a mobile app, making genetic data stored on the database readily accessible to researchers and patients across multiple devices.",
        "keywords": "ALSoD; amyotrophic lateral sclerosis; frontotemporal dementia; Web-bases; database; genetics; bioinformatics; mobile website; app",
        "released": 2013,
        "link": "https://doi.org/10.2196/mhealth.2706"
    },
    {
        "title": "Using Camlp4 for presenting dynamic mathematics on the web: DynaMoW, an OCaml language extension for the run-time generation of mathematical contents and their presentation on the web an experience report",
        "abstract": "We report on the design and implementation of a programming tool, DynaMoW, to control interactive and incremental mathematical calculations to be presented on the web. This tool is implemented as a language extension of OCaml using Camlp4. Fragments of mathematical code written for a computer-algebra system as well as fragments of mathematical web documents are embedded directly and naturally inside OCaml code. A DynaMoW-based application is made of independent web services, whose parameter types are checked by the OCaml extension. The approach is illustrated by two implementations of online mathematical encyclopedias on top of DynaMoW.",
        "keywords": "Languages; Web Applications; Computer Algebra; Symbolic Computation; mathematical encyclopedias; quotations; antiquotations; metaprogramming; web services",
        "released": 2011,
        "link": "https://doi.org/10.1145/2034574.2034809"
    },
    {
        "title": "Easy-to-use cloud computing for teaching data science",
        "abstract": "One of the biggest hurdles of teaching data science and programming techniques to beginners is simply getting started with the technology. With multiple versions of the same coding language available (e.g., Python 2 and Python 3), various additional libraries and packages to install, as well as integrated development environments to navigate, the first step can be the most daunting. We show the advantages of using cloud computing to solve this issue and demonstrate one way of implementing it to allow beginners to get started with coding immediately. Using user-friendly Jupyter notebooks along with the interactive capabilities possible through Binder, we provide introductory Python and SQL material that students can access without downloading anything. This lets students to get started with coding right away without getting frustrated figuring out what to install. Example introductory modules on using Python and SQL for data analysis are provided through GitHub at and .",
        "keywords": "Education; Jupyter; Python; SQL; Training",
        "released": 2021,
        "link": "https://doi.org/10.1080/10691898.2020.1860726"
    },
    {
        "title": "An eFPGA generation suite with customizable architecture and IDE",
        "abstract": "From edge devices to cloud servers, providing optimized hardware acceleration for specific applications has become a key approach to improve the efficiency of computer systems. Traditionally, many systems employ commercial field-programmable gate arrays (FPGAs) to implement dedicated hardware accelerator as the CPU’s co-processor. However, com-mercial FPGAs are designed in generic architectures and are provided in the form of discrete chips, which makes it difficult to meet increasingly diver-sified market needs, such as balancing reconfigurable hardware resources for a specific application, or to be integrated into a customer’s system-on -a-chip (SoC) in the form of embedded FPGA (eFPGA). In this paper, we propose an eFPGA generation suite with customizable architecture and in-tegrated development environment (IDE), which covers the entire eFPGA design generation, testing, and utilization stages. For the eFPGA design generation, our intellectual property (IP) generation flow can explore the optimal logic cell, routing, and array structures for given target applica-tions. For the testability, we employ a previously proposed shipping test method that is 100% accurate at detecting all stuck-at faults in the entire FPGA-IP. In addition, we propose a user-friendly and customizable Web -based IDE framework for the generated eFPGA based on the NODE-RED development framework. In the case study, we show an eFPGA architecture exploration example for a differential privacy encryption application using the proposed suite. Then we show the implementation and evaluation of the eFPGA prototype with a 55 nm test element group chip design.",
        "keywords": "FPGA-IP; eFPGA; FPGA CAD; FPGA architecture; application-specific acceleration",
        "released": 2023,
        "link": "https://doi.org/10.1587/transfun.2022VLP0008"
    },
    {
        "title": "Leveraging declarative languages in web application development",
        "abstract": "Web Applications have become an omnipresent part of our daily lives. They are easy to use, but hard to develop. WYSIWYG editors, form builders, mashup editors, and markup authoring tools ease the development of Web Applications. However, more advanced Web Applications require servers-side programming, which is beyond the skills of end-user developers. In this paper, we discuss how declarative languages can simplify Web Application development and empower end-users as Web developers. We first identify nine end-user Web Application development levels ranging from simple visual customization to advanced three-tier programming. Then, we propose expanding the presentation tier to support all aspects of Web Application development. We introduce a unified XForms-based framework-called XFormsDB-that supports both client-side and server-side Web Application development. Furthermore, we make a language extension proposal-called XFormsRTC-for adding true real-time communication capabilities to XForms. We also present XFormsDB Integrated Development Environment (XIDE), which assists end-users in authoring highly interactive data-driven Web Applications. XIDE supports all Web Application development levels and, especially, promotes the transition from markup authoring and snippet programming to single and unified language programming.",
        "keywords": "Web framework; Web application; Web development; End-user development; Declarative language; Real-time communication",
        "released": 2016,
        "link": "https://doi.org/10.1007/s11280-015-0339-z"
    },
    {
        "title": "Mobile app design for teaching and learning: Educators’ experiences in an online graduate course",
        "abstract": "This research explored how educators with limited programming experiences learned to design mobile apps through peer support and instructor guidance. Educators were positive about the sense of community in this online course. They also considered App Inventor a great web-based visual programming tool for developing useful and fully functioning mobile apps. They had great sense of empowerment through developing unique apps by using App Inventor. They felt their own design work and creative problem solving were inspired by the customized mobile apps shared by peers. The learning activities, including sharing customized apps, providing peer feedback, composing design proposals, and keeping design journals (blogging), complemented each other to support a positive sense of community and form a strong virtual community of learning mobile app design. This study helped reveal the educational value of mobile app design activities and the web-based visual programming tool, and the possibility of teaching/learning mobile app design online. The findings can also encourage educators to explore and experiment on the potential of incorporating these design learning activities in their respective settings, and to develop mobile apps for their diverse needs in teaching and learning.",
        "keywords": "Online learning; mobile app design; programming; App Inventor (AI); virtual learning community (VLC)",
        "released": 2013,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000328538300009"
    },
    {
        "title": "Paggr: Linked data widgets and dashboards",
        "abstract": "The Linked Data effort has led to a large number of interconnected and reusable datasets with standardized RDF interfaces. Paggr is a novel Web system that simplifies personalized aggregation and interaction with these structured information sources through SPARQL-driven widgets and dashboards. Paggr utilizes a URI-based drag and drop mechanism for end-users, and a collaborative, browser-based development environment for widget creators. (C) 2009 Elsevier B.V. All rights reserved.",
        "keywords": "Linked Data; User interaction; Widget; Drag and drop; Personalization; Semantic web",
        "released": 2009,
        "link": "https://doi.org/10.1016/j.websem.2009.09.005"
    },
    {
        "title": "CRITERIA FOR THE SELECTION OF EDUCATIONAL PROGRAMMING ENVIRONMENT FOR THE FORMATION OF PROSPECTIVE PRIMARY SCHOOL TEACHERS’ ICT COMPETENCE",
        "abstract": "One of the main tasks of institutions of higher pedagogical education is the teacher training for life in the information society and for computer technology and software application in professional activity. The article reveals the conditions for choosing programming environment as a means of developing ICT competence of future primary school teachers. The analysis of Ukrainian and foreign scientists’ works shows that block visual programming languages have significant advantages over the text ones at the initial stage of programming training. The groups of criteria that influence the choice of visual programming environment are defined. The availability of support for performing basic mathematical and logical operations, the possibility to write formulas in mathematical form as well as the availability and support of the basic algorithmic constructions refer to the group of criteria related to the features of the programming environment. The criteria related to the possibility of using the environment at the initial stage of programming languages training contain the simplicity and convenience of the interface, the availability of instructions and manuals, the versions of programs that can be used free of charge. The technological criteria include the following indicators: a crossover form, the support for robotic designers, the availability of mobile and online versions, a license type, the support and the development of the environment. The paper presents a comparative analysis of present-day popular visual programming environments by all criteria and indicators (Alice, Google Blockly, Kodu, Scratch, Snap!). It has been determined that according to the mentioned above criteria and indicators, the most expedient, suitable and effective visual block programming environment for the formation and the development of ICT competence of future primary school teachers, despite some certain shortcomings, is Scratch and Scratch-like environments.",
        "keywords": "a programming language; programming environment; block programming environment; selection criteria; a primary school teacher",
        "released": 2020,
        "link": "https://doi.org/10.33407/itlt.v78i4.2912"
    },
    {
        "title": "RUCA:: A system for astronomical imaging",
        "abstract": "An optical imaging system for the Observatorio Astronomico Nacional at San Pedro Martir is described. The system includes a fixed wheel with 4 polarizers, a sliding bench for focal reducers, an interchangeable wheel for up to 8 interference filters and a rotating flange. Focal reducers are a couple of triplets optimized to work in the blue and red ranges, increasing the plate scale 1.6 times, yielding unvigneted fields of view of 14.9’ and 8.4’ for the 84 cm and 1.5 m telescopes, respectively. Control is carried out through an embedded IBM-PC compatible computer. The interface card is connected through the IDE port of the PC. The control and user interface programs are both written in C++. Two user interfaces were developed: one runs under WINDOWS, the other is an HTML page accesible from any web browser.",
        "keywords": "instrumentation",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000166277700007"
    },
    {
        "title": "Evolution of knowledge management towards wisdom management",
        "abstract": "To provide solutions to the world’s global challenges, there is an urgent demand for wise organisations, wise leadership, wise workers, and most importantly, for wise actions. Since the mid-1990s, Knowledge Management (KM) as a discipline and practice has emerged internationally, and it has passed through several phases of development. Simultaneously, in the last four decades we have experienced a growth of wisdom research and intensified discourses about Wisdom Management (WM) as a possible venue for dealing with wicked problems. The dilemma is whether the current sixth phase of KM is able to address the global problems of the world. Therefore, this conceptual paper seeks to answer the question if WM will complement or replace KM. This paper explores the evolutionary phases of KM; it presents the discourses about WM, and it discovers the presence of wisdom in the KM literature (1995-2022). The research approach is explorative and inductive. Methods of Computational Social Sciences and RStudio as an Integrated Development Environment (IDE) are utilised for analysing, visualising, and interpreting the data. This paper analyses 255 wisdom-related keywords that emerged from 39 sources written by 50 authors. The three data analyses methods are: word cloud data analysis; dictionary-based data analysis, and bipartite network analysis. The findings are presented in word clouds; keywords frequency; bipartite network, and in a synthesised framework to show the similar and different concepts of KM and WM. The novelty value of this paper is the result of the bipartite network analysis that allows to identify how authors and wisdom-related keywords are connected with the same word nodes. This paper contributes to KM because it is the first study to explore and illustrate the presence of wisdom attributes in the KM literature.",
        "keywords": "Knowledge Management (KM); wisdom management (WM); evolution of KM; Computational Social Sciences (CSS); RStudio; bipartite network analysis; dictionary-based data analysis",
        "released": 2023,
        "link": "https://doi.org/10.1142/S021964922350051X"
    },
    {
        "title": "A web-based distributed programming environment",
        "abstract": "A Java-based system called the GcoJAVA System was introduced in [1]. This system allows a user to remotely compile his/her own C/C++ programs and execute them for visualization among a group of remote users. DISPE, which stands for DIStributed Programming Environment, expands on the GcoJAVA System by allowing the resulting executables to be run on systems other than the host on which they were compiled, thus making the system more versatile. DISPE uses Common Object Request Broker (CORBA) services to enable executables compiled on this system to invoke methods in libraries on remote sites in an architecturally heterogeneous environment. Not only does this allow users to compile and execute their programs remotely, but the maintenance and duplication of Libraries is lowered since agents are used to search for symbols in libraries located remotely and to compile them with the user’s source code, hs long as there is an Internet connection between the hosts on which these libraries reside, the agents can search and compile with these libraries.",
        "keywords": "",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000166853100024"
    },
    {
        "title": "A visual environment for developing defeasible rule bases tor the semantic web",
        "abstract": "Defeasible reasoning is a rule-based approach for efficient reasoning with incomplete and inconsistent information. Such reasoning is useful for many applications in the Semantic Web. However, the RuleML syntax of defeasible logic may appear too complex for many users. Furthermore, the interplay between various technologies and languages, such as defeasible reasoning, RuleML, and RDF impose a demand for using multiple, diverse tools for building rule-based applications for the Semantic Web. In this paper we present VDR-Dcvice, a visual integrated development environment for developing and using defeasible logic rule bases on top of RDF ontologies. VDR-Device integrates in a user-friendly graphical shell, a visual RuleML-compliant rule editor that constrains the allowed vocabulary through analysis of the input RDF ontologies and a defeasible reasoning system that processes RDF data and RDF Schema ontologies.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000234124600014"
    },
    {
        "title": "Web applications:: A simple pluggable architecture for business rich clients",
        "abstract": "During the past decade we have been witnesses of the rise of the Web Application with a browser based client. This brought us ubiquitous access and centralized administration and deployment, but the inherent limitations of the approach however, and the availability of new technologies like XML and Web Services has made people start building rich clients as business applications front ends. But very often these applications are tied to the development tools and very hard to extend. We propose a clean and elegant architecture which considers a plugin based approach as a general solution to the extensibility problem. The approach is demonstrated by refactoring a simple application taken from a public forum into the proposed architecture including two new extensions that are implemented as plugins.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000231171300059"
    },
    {
        "title": "Enterprise web development environments",
        "abstract": "",
        "keywords": "",
        "released": 1997,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:A1997WX23600052"
    },
    {
        "title": "A browser for incremental programming",
        "abstract": "Much of the elegance and power of Smalltalk comes from its programming environment and tools. First introduced more than 20 years ago, the Smalltalk browser enables programmers to “home in” on particular methods using a hierarchy of manually-defined classifications. By its nature, this classification scheme says a lot about the desired state of the code, but little about the actual state of the code as it is being developed. We have extended the Smalltalk browser with dynamically computed virtual categories that dramatically improve the browser’s support for incremental programming. We illustrate these improvements by example, and describe the algorithms used to compute the virtual categories efficiently. (C) 2003 Elsevier Ltd. All rights reserved.",
        "keywords": "smalltalk browser; incremental programming; intentional programming; method reachability; requires set",
        "released": 2004,
        "link": "https://doi.org/10.1016/j.cl.2003.09.004"
    },
    {
        "title": "Web-based UMLS concept retrieval by automatic text scanning: A comparison of two methods",
        "abstract": "The Web is increasingly the medium of choice for multi-user application program delivery. Yet selection of an appropriate programming environment for rapid prototyping, code portability, and maintainability remain issues. We summarize our experience on the conversion of a LISP Web application, Search/SR to a new, functionally identical application, Search/SR-ASP using a relational database and active server pages (ASP) technology. Our results indicate that provision of easy access to database engines and external objects is almost essential for a development environment to be considered viable for rapid and robust application delivery. While LISP itself is a robust language, its use in Web applications may be hard to justify given that current vendor implementations do not provide such functionality. Alternative, currently available scripting environments for Web development appear to have most of LISP’s advantages and few of its disadvantages. (C) 2001 Elsevier Science Ireland Ltd. All rights reserved.",
        "keywords": "computing methodologies; internet; programming languages; LISP; active server pages; VBScript",
        "released": 2001,
        "link": "https://doi.org/10.1016/S0169-2607(00)00092-4"
    },
    {
        "title": "Process-context aware matchmaking for web service composition",
        "abstract": "Web service composition can help software developer design more powerful and flexible applications according to requirements of enterprise. But during compositing, how to discover suitable web services is a critical problem in design and implementing application-oriented web service technologies. The traditional keyword-based matchmaking approach is difficult to help developer to find suitable service. Current researches find that to attaching semantics to each registered service can help improve the precision of matchmaking. The improvement can help developer find more suitable service for business process. This paper proposes a novel approach of semantics-based matchmaking, which is named process-context aware matchmaking. The process-context aware matchmaking discovers the suitable service during web service composite modeling. During matchmaking, the approach utilizes not only semantics of technical process but also that of business process of a registered service, thus further improving the precision of matchmaking. We integrate the process-context aware matchmaking, with business-process-driven web service composition in an integrated development environment based on Eclipse. The performance evaluation shows that performance overhead of this novel approach is acceptable. (C) 2007 Elsevier Ltd. All rights reserved.",
        "keywords": "Web service matchmaking; Process-context aware; Web service composition",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.jnca.2007.11.008"
    },
    {
        "title": "Stochastic simulation service: Bridging the gap between the computational expert and the biologist",
        "abstract": "We present StochSS: Stochastic Simulation as a Service, an integrated development environment for modeling and simulation of both deterministic and discrete stochastic biochemical systems in up to three dimensions. An easy to use graphical user interface enables researchers to quickly develop and simulate a biological model on a desktop or laptop, which can then be expanded to incorporate increasing levels of complexity. StochSS features state-of-the-art simulation engines. As the demand for computational power increases, StochSS can seamlessly scale computing resources in the cloud. In addition, StochSS can be deployed as a multi-user software environment where collaborators share computational resources and exchange models via a public model repository. We demonstrate the capabilities and ease of use of StochSS with an example of model development and simulation at increasing levels of complexity.",
        "keywords": "",
        "released": 2016,
        "link": "https://doi.org/10.1371/journal.pcbi.1005220"
    },
    {
        "title": "Formal semantics of OMG’s interaction flow modeling language (IFML) for mobile and rich-client application model driven development",
        "abstract": "Model Driven Engineering relies on the availability of software models and of development tools supporting the transition from models to code. The generation of code from models requires the unambiguous interpretation of the semantics of the modeling languages used to specify the application. This paper presents the formalization of the semantics of the Interaction Flow Modeling Language (IFML), a recent 0MG MDA standard conceived for the specification of the front-end part of interactive applications. IFML constructs are mapped to equivalent structures of Place Chart Nets (PCN), which allows their precise interpretation. The defined semantic mapping is implemented in an online Model-driven development environment that enables the creation and inspection of PCNs from IFML, the analysis of the behavior of IFML specifications via PCN simulation, and the generation of code for mobile and web-based architectures. (C) 2017 Elsevier Inc. All rights reserved.",
        "keywords": "Mobile applications; Rich-client applications; Model-driven development; Translational semantics",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.jss.2017.11.067"
    },
    {
        "title": "Cloud-based applications for accessing satellite earth observations to support malaria early warning",
        "abstract": "Malaria epidemics can be triggered by fluctuations in temperature and precipitation that influence vector mosquitoes and the malaria parasite. Identifying and monitoring environmental risk factors can thus provide early warning of future outbreaks. Satellite Earth observations provide relevant measurements, but obtaining these data requires substantial expertise, computational resources, and internet bandwidth. To support malaria forecasting in Ethiopia, we developed software for Retrieving Environmental Analytics for Climate and Health (REACH). REACH is a cloud-based application for accessing data on land surface temperature, spectral indices, and precipitation using the Google Earth Engine (GEE) platform. REACH can be implemented using the GEE code editor and JavaScript API, as a standalone web app, or as package with the Python API. Users provide a date range and data for 852 districts in Ethiopia are automatically summarized and downloaded as tables. REACH was successfully used in Ethiopia to support a pilot malaria early warning project in the Amhara region. The software can be extended to new locations and modified to access other environmental datasets through GEE.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1038/s41597-022-01337-y"
    },
    {
        "title": "Electrical design platform on engineering cloud",
        "abstract": "The conventional requirements of monozukuri (manufacturing) included needs for high performance, miniaturization, low costs and fast development. In addition to these, product development has come to require businesses to fulfill their social responsibilities in areas such as business continuity and consideration for the environment and safety. In order to respond to these changes in technologies and the business environment, it is essential to introduce development processes and a development environment that allow for effective governance of development overall. Fujitsu has integrated the know-how of monozukuri cultivated over many years, and built Flexible Technical Computing Platform (FTCP) as the integrated design development environment. It has provided it to customers in development sections, manufacturing sections and repair sections as “Engineering Cloud.” This paper describes an overview of FTCP. It also covers the features of and linkage between CAD for designing printed circuit boards, a key component of the electrical design platform; a simulation environment for analyzing noise and such like; and a standard component database which supports them. Moreover, it also describes the merits of providing FTCP in a cloud environment by using Engineering Cloud, and the shift of customers’ existing development environments to the cloud.",
        "keywords": "",
        "released": 2012,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000310171300005"
    },
    {
        "title": "DEVELOPMENT OF THE “STRENGTH TRAINING RECORDS” MOBILE APPLICATION",
        "abstract": "Aim. To develop a mobile application to save the data obtained during strength training, to monitor training progress, and to analyze changes in strength levels. Materials and methods. The study took place at the Institute of Sport, Tourism and Service of South Ural State University (National Research University) and the Physical Education and Sport Club. The sample involved cross -country skiers, trackand-field athletes, and orienteers (n = 50), ages 17-25. The application was created in an integrated development environment (Android Studio) with the Kotlin programming language. Results. A mobile cloud application for the Android OS was developed to provide prospects for simultaneous training record monitoring for both a coach and an athlete. The application also allows making and amending training records and training plans, as well as monitoring automatically formed records. Conclusion. The application allows athletes and coaches to form their training plans by choosing physical activities that are adequate with respect to their muscle groups and physical activity levels. Moreover, it facilitates coach-athlete interaction due to the cloud storage of training data.",
        "keywords": "mobile application; training records; strength training; android; kotlin",
        "released": 2023,
        "link": "https://doi.org/10.14529/hsm230412"
    },
    {
        "title": "EXTENDING THE PROGRAMMING ENVIRONMENT - a GET AND SET BUILDER FOR THE BROWSER",
        "abstract": "",
        "keywords": "",
        "released": 1994,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:A1994NK22400011"
    },
    {
        "title": "A service level agreement aware online algorithm for virtual machine migration",
        "abstract": "The demand for cloud computing has increased manifold in the recent past. More specifically, on-demand computing has seen a rapid rise as organizations rely mostly on cloud service providers for their day-to-day computing needs. The cloud service provider fulfills different user requirements using virtualization - where a single physical machine can host multiple Virtual Machines. Each virtual machine potentially represents a different user environment such as operating system, programming environment, and applications. However, these cloud services use a large amount of electrical energy and produce greenhouse gases. To reduce the electricity cost and greenhouse gases, energy efficient algorithms must be designed. One specific area where energy efficient algorithms are required is virtual machine consolidation. With virtual machine consolidation, the objective is to utilize the minimum possible number of hosts to accommodate the required virtual machines, keeping in mind the service level agreement requirements. This research work formulates the virtual machine migration as an online problem and develops optimal offline and online algorithms for the single host virtual machine migration problem under a service level agreement constraint for an over-utilized host. The online algorithm is analyzed using a competitive analysis approach. In addition, an experimental analysis of the proposed algorithm on real-world data is conducted to showcase the improved performance of the proposed algorithm against the benchmark algorithms. Our proposed online algorithm consumed 25% less energy and performed 43% fewer migrations than the benchmark algorithms.",
        "keywords": "Cloud computing; green computing; online algorithms; virtual machine migration",
        "released": 2023,
        "link": "https://doi.org/10.32604/cmc.2023.031344"
    },
    {
        "title": "PROGRAMMERS WORK FASTER WITH ADRS ONLINE PROGRAMMING TOOLS",
        "abstract": "",
        "keywords": "",
        "released": 1984,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:A1984SW53200019"
    },
    {
        "title": "A bespoke target selection tool to guide biomarker discovery in tubo-ovarian cancer",
        "abstract": "Introduction: Cancers presenting at advanced stages inherently have poor prognosis. High grade serous carcinoma (HGSC) is the most common and aggressive form of tubo-ovarian cancer. Clinical tests to accurately diagnose and monitor this condition are lacking. Hence, development of disease-specific tests are urgently required. Methods: The molecular profile of HGSC during disease progression was investigated in a unique patient cohort. A bespoke data browser was developed to analyse gene expression and DNA methylation datasets for biomarker discovery. The Ovarian Cancer Data Browser (OCDB) is built in C# with a.NET framework using an integrated development environment of Microsoft Visual Studio and fast access files (.faf). The graphical user interface is easy to navigate between four analytical modes (gene expression; methylation; combined gene expression and methylation data; methylation clusters), with a rapid query response time. A user should first define a disease progression trend for prioritising results. Single or multiomics data are then mined to identify probes, genes and methylation clusters that exhibit the desired trend. A unique scoring system based on the percentage change in expression/methylation between disease stages is used. Results are filtered and ranked using weighting and penalties.Results: The OCDB’s utility for biomarker discovery is demonstrated with the identified target OSR2. Trends in OSR2 repression and hypermethylation with HGSC disease progression were confirmed in the browser samples and an independent cohort using bioassays. The OSR2 methylation biomarker could discriminate HGSC with high specificity (95%) and sensitivity (93.18%).Conclusions: The OCDB has been refined and validated to be an integral part of a unique biomarker discovery pipeline. It may also be used independently to aid identification of novel targets. It carries the potential to identify further biomarker assays that can reduce type I and II errors within clinical diagnostics.Crown Copyright (c) 2022 Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",
        "keywords": "Tubo-ovarian cancer; High grade serous carcinoma; Carcinoogenesis; Molecular profiling; Methylation profiling; Interactive data browser; Biomarkers; Integromics",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.csbj.2022.06.016"
    },
    {
        "title": "Web development tools: A survey",
        "abstract": "In this paper we review, classify, and assess 33 existing commercial products in the light of evaluation criteria based on the impact on the application development process, architecture, and user-perceived quality. From this review we draw requirements for new-generation development tools and show how current solutions match different application domains. (C) 1998 Published by Elsevier Science B.V. All rights reserved.",
        "keywords": "Web development; CASE tools; authoring tools; database gateways",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0169-7552(98)00021-X"
    },
    {
        "title": "Simpy simulation of patient priority-based cloud healthcare system",
        "abstract": "The recent outbreak of Coronavirus (COVID-19) has exposed the enormous scope of upgradation of our current healthcare facilities. People were forced to isolate themselves from others mainly to break the chain of virus transmission within society. Unfortunately, the patients, who were remotely located, could not even avail of healthcare facilities due to insufficient logistic and workforce support. This incident had led to the untimely death of many covid and non-covid patients, whose lives could have been saved with the timely delivery of healthcare facilities. As a result, we have realized the importance of prompt delivery of healthcare services to patients through a secured and virtual medium like the cloud. For this purpose, the authors have proposed a Healthcare Integrated Development Environment, where patients may avail multivariate healthcare facilities using a smart card-based single-window interface, namely, Multipurpose Electronic Card. Considering the installation and maintenance cost of a cloud-based electronic service delivery system, it would be a better idea to simulate its performance to verify its output gained with reference to the investment of resources. To achieve this objective, in this paper, the authors have performed an open-source Simpy library-based simulation of Cloud Healthcare System using Multipurpose Electronic Card.",
        "keywords": "Cloud Healthcare; Data traffic; End-user; Internet of things; Simpy simulation",
        "released": 2023,
        "link": "https://doi.org/10.1080/03772063.2022.2055661"
    },
    {
        "title": "Block-based abstractions and expansive services to make advanced computing concepts accessible to novices",
        "abstract": "Many block-based programming environments have proven to be effective at engaging novices in learning programming. However, most offer only restricted access to the outside world, limiting learners to commands and computing resources built in to the environment. Some allow learners to drag and drop files, connect to sensors and robots locally or issue HTTP requests. But in a world where most of the applications in our daily lives are distributed (i.e., their functionality depends on communicating with other computers or accessing resources and data on the internet), the limited support for beginners to envision and create such distributed programs is a lost opportunity. We argue that it is feasible to create environments with simple yet powerful abstractions that open up distributed computing and other widely-used but advanced computing concepts including networking, the Internet of Things, and cybersecurity to novices. The paper presents the architecture of and design decisions behind NetsBlox, a programming environment that supports these ideas. We show how NetsBlox expands opportunities for learning considerably: NetsBlox projects can access a wealth of online data and web services, and they can communicate with other projects. Moreover, the tool infrastructure enables young learners to collaborate with each other during program construction, whether they share their physical location or study remotely. Importantly, providing access to the wider world will also help counter widespread student perceptions that block-based environments are mere toys, and show that they are capable of creating compelling applications. In this way, NetsBlox offers an illuminating example of how tools can be designed to democratize access to powerful ideas in computing.",
        "keywords": "Block based programming; NetsBlox; Distributed computing; Message passing; Remote procedure calls",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.cola.2022.101156"
    },
    {
        "title": "A visual programming tool for mobile web augmentation",
        "abstract": "The use of mobile devices for web browsing has increased extraordinarily in recent years, becoming the main source of information. Unfortunately, developers cannot meet the needs of all users. As a result, users have been forced to adapt web content to their needs. Additionally, users face more costly interactions than on desktop computers due to the smaller screen size of cell phones. These interactions include URL typing, scrolling, taping, branching and tab-switching. If the number of web resources is large, the activity becomes unpleasant. Furthermore, should the quantity of interactions be extensive, the web experience also becomes unpleasant. Moreover, all these interactions lead to higher costs in terms of phone time and battery life, as well as more costly navigation. Web augmentation enhances the user web experience by adding content from different resources, modifying the original layout, or reducing interactions during web searches, among other benefits. To reduce user interactions and satisfy their information necessities, we propose MAWA, an extension for Firefox mobile that allows users to adapt any web page to their needs through web augmentation techniques. To validate our proposal, an evaluation of the tool has been carried out. The article presents the benefits introduced by MAWA as a response to the inconveniences experienced by users when performing their activities in the mobile browser. The results obtained show that MAWA is able to reduce the number of interactions significantly, as well as to reduce battery consumption and the time needed to obtain the desired information. In addition, users perceive that navigation is less cluttered.",
        "keywords": "Human-computer interaction; End-user development; Web augmentation; Customization; Mobile browser",
        "released": 2024,
        "link": "https://doi.org/10.1007/s10115-023-02039-6"
    },
    {
        "title": "THE IMPLEMENTATION OF OBJECTMATH - a HIGH-LEVEL PROGRAMMING ENVIRONMENT FOR SCIENTIFIC COMPUTING",
        "abstract": "We present the design and implementation of ObjectMath, a language and environment for high-level equation-based modeling and analysis in scientific computing. The ObjectMath language integrates object-oriented modeling with mathematical language features that make it possible to express mathematics in a natural and consistent way. The implemented programming environment includes a graphical browser for visualizing and editing inheritance hierarchies, an application oriented editor for editing ObjectMath equations and formulae, a computer algebra system for doing symbolic computations, support for generation of numerical code from equations, mid routines for graphical presentation. This programming environment has been successfully used in modeling and analyzing two different problems from the application domain of machine element analysis in an industrial environment.",
        "keywords": "",
        "released": 1992,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:A1992LF70000029"
    },
    {
        "title": "A proposed architecture for local-host and AWS with multiagent system",
        "abstract": "Cloud computing has become one of the leading technologies in the world today. The benefits of cloud computing affect end users directly. There are several cloud computing frameworks, and each has ways of monitoring and providing resources. Cloud computing eliminates customer requirements such as expensive system configuration and massive infrastructure while improving dependability and scalability. From the user’s perspective, cloud computing makes it easy to upload multiagents and operate on different web services. In this paper, the authors used a restful web service and an agent system to discuss, deployments, and analysis of load performance parameters like memory use, cen-tral processing unit (CPU) utilization, network latency, etc., both on localhost and an Amazon Web Service Elastic Cloud Computing (AWS-EC2) server. The Java Agent Development Environment (JADE) tool has been used to propose an archi-tecture and conduct a comparative study on both local and remote servers. JADE is an open-source tool for maintaining applications on AWS infrastructure. The focus of the study should be to reduce the complexity and time of load perfor-mance parameters by using an agent system on a cloud server instead of establish-ing a massive infrastructure on a local system, even for a small application.",
        "keywords": "Amazon web service; jade; local host; rest web service",
        "released": 2023,
        "link": "https://doi.org/10.32604/iasc.2023.034775"
    },
    {
        "title": "Cloud manufacturing service composition and formal verification based on extended process calculus",
        "abstract": "Cloud manufacturing is an emerging service-oriented model to solve existing problems in manufacturing. This study proposes a process calculus-based approach to formally model cloud manufacturing service composition that is composed of description model, interaction scenario model, and composition process formal model, in which the semantics of process calculus to describe the quality of service (QoS) information of service composition is extended, and an intelligent service composition method is put forward based on the extended process calculus. Then, a platform architecture for implementing the proposed approach is addressed. The integrated development environment for the platform is set up with selection of free software tools. Thus, a prototype of the platform is developed. Finally, automated guided vehicle processing service composition and the open integrated manufacturing system service composition are taken as examples to illustrate the application of the proposed service composition method. The main attributes of the formal models for the application cases are verified. Case study results show that the proposed service composition and formal verification method based on the extended process calculus is feasible. Compared with other approaches for service composition, the proposed intelligent service composition method meets the most requirements for service composition.",
        "keywords": "Cloud manufacturing; service composition; process calculus; formal verification; prototype platform",
        "released": 2018,
        "link": "https://doi.org/10.1177/1687814018781287"
    },
    {
        "title": "Deploying defeasible logic rule bases for the semantic web",
        "abstract": "Logic is currently the target of the majority of the upcoming efforts towards the realization of the Semantic Web vision, namely making the content of the Web accessible not only to humans, as it is today, but to machines as well. Defeasible reasoning, a rule-based approach to reasoning with incomplete and conflicting information, is a powerful tool in many Semantic Web applications. Despite its strong mathematical background, logic, in general, and defeasible logic, in particular, may overload the user with tons of additional complex semantic relationships among data and metadata of the Semantic Web. To this end, a comprehensible, visual representation of these semantic relationships (rules) would help users understand them and make more use of them. This paper presents VDR-DEVICE, a defeasible reasoning system, designed specifically for the Semantic Web environment. VDR-DEVICE is an integrated development environment for deploying and visualizing defeasible logic rule bases on top of RDF Schema ontologies. The system consists of a number of sub-components, which, though developed autonomously, are combined efficiently, forming a flexible framework. The system employs a defeasible reasoning system that supports direct importing and processing of RDF data and RDF Schema ontologies as well as a number of user-friendly rule base and ontology visualization modules. (C) 2008 Elsevier B.V. All rights reserved.",
        "keywords": "semantic web; reasoning; defeasible logic; visual representation; rule editor; rule mark-up languages",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.datak.2008.02.005"
    },
    {
        "title": "Using JavaScript and WebCL for numerical computations: A comparative study of native and web technologies",
        "abstract": "From its modest beginnings as a tool to validate forms, JavaScript is now an industrial-strength language used to power online applications such as spreadsheets, IDEs, image editors and even 3D games. Since all modern web browsers support JavaScript, it provides a medium that is both easy to distribute for developers and easy to access for users. This paper provides empirical data to answer the question: Is JavaScript fast enough for numerical computations? By measuring and comparing the runtime performance of benchmarks representative of a wide variety of scientific applications, we show that sequential JavaScript is within a factor of 2 of native code. Parallel code usingWebCL shows speed improvements of up to 2.28 over JavaScript for the majority of the benchmarks.",
        "keywords": "Benchmark; Numerical Computation; Web Browser; JavaScript; WebCL; OpenCL; C; Parallelism; Computational Dwarfs",
        "released": 2015,
        "link": "https://doi.org/10.1145/2661088.2661090"
    },
    {
        "title": "Understanding mashup development",
        "abstract": "Web mashups are Web applications developed using contents and services available online. Despite rapidly increasing interest in mashups over the past two years, comprehensive development tools and frameworks are lacking, and in most cases mashing up a new application implies a significant manual programming effort. This article overviews current tools, frameworks, and trends that aim to facilitate mashup development. The authors use a set of characteristic dimensions to highlight the strengths and weaknesses of some representative approaches.",
        "keywords": "",
        "released": 2008,
        "link": "https://doi.org/10.1109/MIC.2008.114"
    },
    {
        "title": "Fine-grained recording of student programming sessions to improve teaching and time estimations",
        "abstract": "It is not possible to directly observe how students work in an online programming course. This makes it harder for teachers to help struggling students. By using an online programming environment, we have the opportunity to record what the students actually do to solve an assignment. These recordings can be analyzed to provide teachers with valuable information. We developed such an online programming tool with fine-grained event logging and used it to observe how our students solve problems. Our tool provides descriptive statistics and accurate replays of a student’s programming sessions, including mouse movements. We used the tool in a course and collected 1028 detailed recordings. In this article, we compare fine-grained logging to existing coarse-grained logging solutions to estimate assignment-solving time. We find that time aggregations are improved by including time for active reading and navigation, both enabled by the increased granularity. We also divide the time users spent into editing (on average 14.8%), active use (on average 37.8%), passive use (on average 29.0%), and estimate time used for breaks (on average 18.2%). There is a correlation between assignment solving time for students who pass assignments early and students that pass later but also a case where the times differ significantly. Our tool can help improve computer engineering education by providing insights into how students solve programming assignments and thus enable teachers to target their teaching and/or improve instructions and assignments.",
        "keywords": "computer science education; learning analytics; educational data mining; computer engineering education",
        "released": 2016,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000378700600003"
    },
    {
        "title": "TermGenie - a web-application for pattern-based ontology class generation",
        "abstract": "Background: Biological ontologies are continually growing and improving from requests for new classes (terms) by biocurators. These ontology requests can frequently create bottlenecks in the biocuration process, as ontology developers struggle to keep up, while manually processing these requests and create classes. Results: TermGenie allows biocurators to generate new classes based on formally specified design patterns or templates. The system is web-based and can be accessed by any authorized curator through a web browser. Automated rules and reasoning engines are used to ensure validity, uniqueness and relationship to pre-existing classes. In the last 4 years the Gene Ontology TermGenie generated 4715 new classes, about 51.4% of all new classes created. The immediate generation of permanent identifiers proved not to be an issue with only 70 (1.4%) obsoleted classes. Conclusion: TermGenie is a web-based class-generation system that complements traditional ontology development tools. All classes added through pre-defined templates are guaranteed to have OWL equivalence axioms that are used for automatic classification and in some cases inter-ontology linkage. At the same time, the system is simple and intuitive and can be used by most biocurators without extensive training.",
        "keywords": "Ontology; Class generation",
        "released": 2014,
        "link": "https://doi.org/10.1186/2041-1480-5-48"
    },
    {
        "title": "The international documentation and evaluation system IDES: A single center observational case series for development of an ankle prosthesis documentation questionnaire and study of its feasibility and face validity",
        "abstract": "Background: The number of implanted total ankle replacements is increasing and most articles present short- and mid-term results. Comparison of outcomes is difficult because of inconsistent terminology and different use of parameters. Materials and methods: We created a module for total ankle prostheses in the framework of the International Documentation and Evaluation System (IDES). Content development was conducted with an iterative process based on a single surgeon series of 74 HINTEGRA (c) total ankle replacements and expert opinions. Results: The IDES ankle module comprises three forms A, B and C for recording of primary (A), revision (B) and followup (C) procedures. 74 primary interventions, 28 revisions and 92 followups could be documented in detail with the final version of the questionnaires. Conclusion: The IDES-forms facilitate a structured and standardized data collection for total ankle arthroplasties. Implemented on the academic MEMdoc portal http://www.memdoc.org of the University of Bern, all registered users can make use of IDES in its online or paper based versions.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1186/1757-1146-3-4"
    },
    {
        "title": "Quest markup for developing FAIR questionnaire modules for epidemiologic studies",
        "abstract": "BackgroundOnline questionnaires are commonly used to collect information from participants in epidemiological studies. This requires building questionnaires using machine-readable formats that can be delivered to study participants using web-based technologies such as progressive web applications. However, the paucity of open-source markup standards with support for complex logic make collaborative development of web-based questionnaire modules difficult. This often prevents interoperability and reusability of questionnaire modules across epidemiological studies.ResultsWe developed an open-source markup language for presentation of questionnaire content and logic, Quest, within a real-time renderer that enables the user to test logic (e.g., skip patterns) and view the structure of data collection. We provide the Quest markup language, an in-browser markup rendering tool, questionnaire development tool and an example web application that embeds the renderer, developed for The Connect for Cancer Prevention Study.ConclusionA markup language can specify both the content and logic of a questionnaire as plain text. Questionnaire markup, such as Quest, can become a standard format for storing questionnaires or sharing questionnaires across the web. Quest is a step towards generation of FAIR data in epidemiological studies by facilitating reusability of questionnaires and data interoperability using open-source tools.",
        "keywords": "Surveys and questionnaires; Data collection; Data commons; Data science; Epidemiologic methods",
        "released": 2023,
        "link": "https://doi.org/10.1186/s12911-023-02338-6"
    },
    {
        "title": "Coping with denial of service due to malicious java applets",
        "abstract": "A monitoring application, called (Signed) Applet Watch-Dog, is proposed to control the execution of malicious Java applets that users can (download and) execute during Web surfing. Typical security attacks that the monitor can stop are related to denial-of-service and antagonism (e.g. obscuring the screen), at the price of a modest degradation of the performance of the Web browser. The Applet Watch-Dog described here is the second release, improving over the original version presented in R. Gorrieri, G. Marchetti (Applet Watch-Dog: a monitor controlling the execution of Java applets, in: G. Papp, R. Posch (Eds.), Proceedings of Fourteenth IFIP International Information Security Conference (SEC’98), Chapman Be Hall, London, September 1998). The application is a signed Java applet, to be executed outside of the sandbox, simple to use and easily configurable by the user, because it works like a user interface. Signed Applet Watch-Dog seems also a necessary tool for software development environments for Java applets. (C) 2000 Elsevier Science B.V. All rights reserved.",
        "keywords": "Applet Watch-Dog; denial-of-service; antagonism",
        "released": 2000,
        "link": "https://doi.org/10.1016/S0140-3664(00)00251-6"
    },
    {
        "title": "Perceived effectiveness of students programming indigenous symbols in ghanaian context",
        "abstract": "There has been minimal investigation into students programming their indigenous projects in relation to blocks of code and mainstream languages in computing education. This study examines high school students’ experiences of programming non-western symbols using visual and text-based learning technologies. The high school students were exposed to a contextualised teaching strategy whereby cultural symbols were created using a web-based platform and an integrated development environment. In the end, 30 (N = 30) students responded to the questionnaires. The quantitative data revealed three key findings that were preferred to learning environments: 1) students’ realisations of computers and perceived usefulness in cultural contexts; 2) students’ motivations to succeed in computing education; 3) students’ gradual assimilations of basic programming concepts. These findings seem to suggest that cultural referents stimulate students’ interests to create indigenous projects in computing education that are also worthwhile outside the school walls.",
        "keywords": "computer attitudes; cultural symbols; two mode courses; computing education; block of codes; general-purpose language",
        "released": 2019,
        "link": "https://doi.org/10.1504/IJLT.2019.105708"
    },
    {
        "title": "The building blocks of a PaaS",
        "abstract": "Traditional cloud computing providers enable developers to program and deploy applications in the cloud by using platform as a service (PaaS) providers. Despite the benefits of such an approach, this service model usually comes with a high level of the lock in effect into the service provider. The lock in effect occurs when a software developer needs to change its cloud provider. When this happens, it usually means a major application rewrite, specific for the new PaaS. This paper details the initial results of a project whose goal is to build a PaaS where vendor lock in does not occur. With this PaaS, developers wishing to deploy their applications into the cloud may continue to program using their usual development environments. There are no necessary changes required to make the application PaaS compatible. As a proof of concept, we developed an open source PaaS management application as a regular web application and then deployed it on the cloud.",
        "keywords": "Platform as a service; Cloud computing; Performance testing",
        "released": 2014,
        "link": "https://doi.org/10.1007/s10922-012-9260-2"
    },
    {
        "title": "Web enabled and weather based forewarning of yellow stem borer [<i>scirpophaga incertulas</i> (walker)] and leaf folder [<i>cnaphalocrcis medinalis</i> (guenee)] for different rice growing locations of india",
        "abstract": "Loss in yield and quality of crop produce due to pest infestation could be reduced considerably if the pest occurrence is known in advance and timely remedial measures are taken. Weather plays an important role in pest development. Therefore, weather based models can be an effective scientific tool for forewarning pests in advance. In this study, weather based forewarning models have been developed for yellow stem borer [Scirpophaga incertulas(Walker)] and leaf folder [Cnaphalocrocis medinalis(Guenee)]of rice for different locations, viz., Aduthurai (Tamil Nadu), Chinsurah (West Bengal), Karjat (Maharashtra), Mandya (Karnataka), Ludhiana (Punjab) and Raipur (Chhattisgarh). The pest data comprised of population of yellow stem borer and leaf folder moths caught in light-trap per week for different locations. Weather data relating to maximum and minimum temperature (degrees C), morning and evening relative humidity (%) and rainfall (mm) in respect of the locations were obtained from the meteorological observatories of the locations per se. Data of pest and weather on weekly basis in respect of Kharif and Rabi seasons of 11 years (2000-2010) for all locations, and of 16 years (1995-2010) for Mandya (KA) were used for developing the forewarning models. Weather of six lag weeks from week of forecast were used for development of weather indices. These weather indices were used as independent variables in model building against the pest population as dependent variables. Stepwise regression models for predicting the yellow stem borer population for peak periods of occurrence during Kharif [Aduthurai (TN), Karjat MH) & Raipur (CG)]and Rabi [Chinsurah (WB) & Mandya (KA)]were developed with R-2 >= 0.9.Prediction of leaf folder for different weeks of Kharif for Aduthurai (TN) (32-35 SMW) and Ludhiana (PB) (32-36 SMW) and of Rabi for Aduthurai (TN) (44-47 SMW) gave R-2 between 0.6 and 0.8, respectively indicating better leaf folder prediction for Rabi over Kharif season at Aduthurai (TN). Validation of the models for subsequent years (2011) has been done for all cases. These developed models were converted into web-based forewarning system using 3-tier architecture. Net Beans 8.0.1 IDE (Integrated Development Environment), MS SQL Server, Java Server Pages (JSP) technologies have been used for the development of the web enabled forecasting of the two rice pests.",
        "keywords": "Yellow stem borer; Leaf folder; Weather; Forewarning; Rice; Kharif; Rabi",
        "released": 2016,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000389624800008"
    },
    {
        "title": "On examples of e-education environment for distance graduate studies",
        "abstract": "This article deals with the development of a Web-based integrated system for distance graduate studies of optimization models, methods, and applications. The aim is to support the curriculum and free choice courses and to provide students with hands-on experience on the effective use and development of software tools. The system is to stimulate and develop creative abilities of work as independent researchers by investigating of on-line optimization models on the web. The idea is to optimize heuristics of different discrete optimization models by the unified Bayesian heuristic approach. The software is regularly updated in response to new programming tools and students reports. The general structure of the website remains: http://pilis.if.ktu.lt/similar to jmockus. 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 18: 331-343. 2010; Published online in Wiley Inter Science (www.interscience.wiley.com): DOI 10.1002/cae.20264",
        "keywords": "graduate studies; Bayesian approach; tuning heuristics; global optimization",
        "released": 2010,
        "link": "https://doi.org/10.1002/cae.20264"
    },
    {
        "title": "JASPAR:: An open-access database for eukaryotic transcription factor binding profiles",
        "abstract": "The analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. The preferred models for representation of transcription factor binding specificity have been termed position-specific scoring matrices. JASPAR is an open-access database of annotated, high-quality, matrix-based transcription factor binding site profiles for multicellular eukaryotes. The profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. The database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genomewide and comparative genomic analysis of regulatory regions. JASPAR is available at http://jaspar. cgb.ki.se.",
        "keywords": "",
        "released": 2004,
        "link": "https://doi.org/10.1093/nar/gkh012"
    },
    {
        "title": "Golden seed breeding cloud platform for the management of crop breeding material and genealogical tracking",
        "abstract": "Effectively identifying and analyzing materials are key procedures for breeding novel crop varieties because of the large quantities of materials and their combinations. This paper presents an approach for crop breeding material management, improvement of crop breeding efficiency, and genealogical tracking through the golden seed breeding cloud platform (GSBCP). The system was developed with Java on an Eclipse integrated development environment. To support a large number of users, security, credibility, and expandability, we selected a cloud-based architecture for the system, in which material attribute, pedigree, and trait information were customized by a template. An intelligent retrieval function was provided for the rapid search of breeding materials, and material trait information, historical data, genealogical information, trial data analysis results and material images can be retrieved in the system. For the analysis of the materials’ genealogy, the rules of genealogy were regulated. The genealogy of the breeding materials was automatically generated on the basis of these rules, which support many breeding methods, such as selfing, hybridization, double cross, three-way cross, and backcross. The developed system has been successfully applied in many Chinese breeding enterprises and scientific research institutions. The system provided a low-cost and highly efficient solution for crop material information management and genealogical tracking.",
        "keywords": "Crop material information management; Genealogical tracking; Breeding informatization; Crop breeding information management system",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.compag.2018.07.015"
    },
    {
        "title": "Adapting EcoCyc for use on the world wide web",
        "abstract": "The World Wide Web (WWW) offers the potential to deliver specialized information to an audience of unprecedented size. Along with this exciting new opportunity comes a challenge for software developers: instead of rewriting our software applications to operate over the WWW, how can we maximize software reuse by retrofitting existing applications? We have developed a Web server tool, written in Common Lisp, that allows existing graphical user interface applications written using the Common Lisp Interface Manager (CLIM) to hook easily into the WWW. This tool - CWEST (CLIM-WEb Server Tool, pronounced “’quest”’) - was developed to operate with EcoCyc, an electronic encyclopedia of the genes and metabolism of the bacterium E. coli. EcoCyc consists of a database of objects relevant to E. coli biochemistry and a user interface, implemented in CLIM, that runs on the X-window system and generates graphical displays appropriate to biological objects. Each query to the EcoCyc WWW server is treated as a command to the EcoCyc program, which dynamically generates an appropriate CLIM drawing. CWEST translates that drawing, which can be a mixture of text and graphics, into the HyperText Markup Language (HTML) and/or the Graphics Interchange Format (GIF), which are returned to the client. Sensitive regions embedded in the CLIM drawing are converted to hyperlinks with Universal Resource Locators (URLs) that generate further EcoCyc queries. This tight coupling of CLIM output with Web output makes CLIM a powerful high-level programming tool for Web applications. The flexibility of Common Lisp and CLIM made implementation of the server tool surprisingly easy, requiring few changes to the existing EcoCyc program. The results can be seen at URL http://www.ai.sri.com/ecocyc/browser.html. We have made CWEST available to the CLIM community at large, with the hope that it will spur other software developers to make their CLIM applications available over the WWW.",
        "keywords": "World Wide Web; user interface; software/tools",
        "released": 1996,
        "link": "https://doi.org/10.1016/0378-1119(96)00269-7"
    },
    {
        "title": "Cloud refactoring: Automated transitioning to cloud-based services",
        "abstract": "Using cloud-based services can improve the performance, reliability, and scalability of a software application. However, transitioning an application to use cloud-based services is difficult, costly, and error-prone. The required re-engineering effort includes migrating to the cloud the functionality to be accessed as remote cloud-based services and re-targeting the client code accordingly. In addition, the client must be able to detect and handle the faults raised in the process of invoking the services. As a means of streamlining this transitioning, we developed a set of refactoring techniques-automated, IDE-assisted program transformations that eliminate the need to change programs by hand. In particular, we show how a programmer can extract services, add fault tolerance functionality, and adapt client code to invoke cloud services via refactorings integrated with a modern IDE. As a validation, we have applied our approach to automatically transform two third-party Java applications to use cloud-based services. We have also applied our approach to re-engineer a suite of services operated by General Electric to use cloud-based resources to better satisfy the GE business requirements.",
        "keywords": "Cloud computing; Services; Refactoring; Service extraction; Fault-tolerance; Program transformation",
        "released": 2014,
        "link": "https://doi.org/10.1007/s10515-013-0136-9"
    },
    {
        "title": "A meta-analysis of nucleos(t)ide analogues in patients with decompensated cirrhosis due to hepatitis b",
        "abstract": "The effect of nucleos(t)ide analogues therapy in patients with decompensated cirrhosis remains unclear. The purpose of this study was to evaluate the effect of nucleos(t)ide analogues on decompensated cirrhotic patients. An online search within PubMed, Web of Science, Embase, Cochrane Central of Register of Controlled Trials and China Biology Medicine disc from 1998-01-01 to 2011-09-05 was conducted. A meta-analysis was performed. Relative risks of mortality rate, Child-Pugh-Turcotte score and hepatitis B e-antigen (HBeAg) seroconversion of the decompensated patients were studied. Eight studies involving 511 patients were included. Data showed that lamivudine and telbivudine significantly decreased the mortality rate (relative risk 0.36, 95 % confidence interval 0.25-0.54), improved the Child-Pugh-Turcotte scores (mean difference -3.23, 95 % confidence interval -3.98 to -2.48) and promoted HBeAg seroconversion (relative risk 7.48, 95 % confidence interval 2.31-24.20). For patients with decompensated cirrhosis, lamivudine and telbivudine significantly decrease the mortality rate and disease severity. Also, they promote their HBeAg seroconversion.",
        "keywords": "Hepatitis B; Chronic; Liver cirrhosis; Decompensated; Nucleos(t)ide analogues",
        "released": 2013,
        "link": "https://doi.org/10.1007/s10620-012-2414-y"
    },
    {
        "title": "Design and evaluation of a block-based environment with a data science context",
        "abstract": "As computing becomes pervasive across fields, introductory computing curricula needs new tools to motivate and educate the influx of learners with little prior background and divergent goals. We seek to improve curricula by enriching it with authentic, real-world contexts and powerful scaffolds that can guide learners to success using automated tools, thereby reducing the strain on limited human instructional resources. To address these issues, we have created the BlockPy programming environment, a web-based, open-access, open-source platform for introductory computing students (https://www.blockpy.com). BlockPy has an embedded data science context that allows learners to connect the educational content with real-world scenarios through meaningful problems. The environment is block-based and gives guiding feedback to learners as they complete problems, but also mediates transfer to more sophisticated programming environments by supporting bidirectional, seamless transitions between block and text programming. Although it can be used as a stand-alone application, the environment has first-class support for the latest Learning Tools Interoperability standards, so that instructors can embed the environment directly within their Learning Management System. In this paper, we describe interesting design issues that we encountered during the development of BlockPy, an evaluation of the environment from fine-grained logs, and our future plans for the environment.",
        "keywords": "Data science; Tools; Programming environments; Programming profession; Engineering profession; Games; Computer science education; introductory computing; block-based programming; data science; automatic guidance",
        "released": 2020,
        "link": "https://doi.org/10.1109/TETC.2017.2729585"
    },
    {
        "title": "Adapting EcoCyC for use on the world wide web (reprinted from gene-combis, vol 172, pg GC43-GC50, 1996)",
        "abstract": "The World Wide Web (WWW) offers the potential to deliver specialized information to an audience of unprecedented size. Along with this exciting new opportunity comes a challenge for software developers: instead of rewriting our software applications to operate over the WWW, how can we maximize software reuse by retrofitting existing applications? We have developed a Web server tool, written in Common Lisp, that allows existing graphical user interface applications written using the Common Lisp Interface Manager (CLIM) to hook easily into the WWW. This tool - CWEST (CLIM-WEb Server Tool, pronounced “’quest”’) - was developed to operate with EcoCyc, an electronic encyclopedia of the genes and metabolism of the bacterium E. coli. EcoCyc consists of a database of objects relevant to E. coli biochemistry and a user interface, implemented in CLIM, that runs on the X-window system and generates graphical displays appropriate to biological objects. Each query to the EcoCyc WWW server is treated as a command to the EcoCyc program, which dynamically generates an appropriate CLIM drawing. CWEST translates that drawing, which can be a mixture of text and graphics, into the HyperText Markup Language (HTML) and/or the Graphics Interchange Format (GIF), which are returned to the client. Sensitive regions embedded in the CLIM drawing are converted to hyperlinks with Universal Resource Locators (URLs) that generate further EcoCyc queries. This tight coupling of CLIM output with Web output makes CLIM a powerful high-level programming tool for Web applications. The flexibility of Common Lisp and CLIM made implementation of the server tool surprisingly easy, requiring few changes to the existing EcoCyc program. The results can be seen at URL http://www.ai.sri.com/ecocyc/browser.html. We have made CWEST available to the CLIM community at large, with the hope that it will spur other software developers to make their CLIM applications available over the WWW.",
        "keywords": "World Wide Web; user interface; software/tools",
        "released": 1996,
        "link": "https://doi.org/10.1016/0378-1119(96)00269-7"
    },
    {
        "title": "StructuRly: A novel shiny app to produce comprehensive, detailed and interactive plots for population genetic analysis",
        "abstract": "Population genetics focuses on the analysis of genetic differences within and between-group of individuals and the inference of the populations’ structure. These analyses are usually carried out using Bayesian clustering or maximum likelihood estimation algorithms that assign individuals to a given population depending on specific genetic patterns. Although several tools were developed to perform population genetics analysis, their standard graphical outputs may not be sufficiently informative for users lacking interactivity and complete information. StructuRly aims to resolve this problem by offering a complete environment for population analysis. In particular, StructuRly combines the statistical power of the R language with the friendly interfaces implemented using the shiny libraries to provide a novel tool for performing population clustering, evaluating several genetic indexes, and comparing results. Moreover, graphical representations are interactive and can be easily personalized. StructuRly is available either as R package on GitHub, with detailed information for its installation and use and as shinyapps.io servers for those users who are not familiar with R and the RStudio IDE. The application has been tested on Linux, macOS and Windows operative systems and can be launched as a shiny app in every web browser.",
        "keywords": "",
        "released": 2020,
        "link": "https://doi.org/10.1371/journal.pone.0229330"
    },
    {
        "title": "Interactive programming paradigm for real-time experimentation with remote living matter",
        "abstract": "Recent advancements in life-science instrumentation and automation enable entirely new modes of human interaction with microbiological processes and corresponding applications for science and education through biology cloud laboratories. A critical barrier for remote and on-site life-science experimentation (for both experts and nonexperts alike) is the absence of suitable abstractions and interfaces for programming living matter. To this end we conceptualize a programming paradigm that provides stimulus and sensor control functions for real-time manipulation of physical biological matter. Additionally, a simulation mode facilitates higher user throughput, program debugging, and biophysical modeling. To evaluate this paradigm, we implemented a JavaScript-based web toolkit, “Bioty,” that supports real-time interaction with swarms of phototactic Euglena cells hosted on a cloud laboratory. Studies with remote and on-site users demonstrate that individuals with little to no biology knowledge and intermediate programming knowledge were able to successfully create and use scientific applications and games. This work informs the design of programming environments for controlling living matter in general, for living material microfabrication and swarm robotics applications, and for lowering the access barriers to the life sciences for professional and citizen scientists, learners, and the lay public.",
        "keywords": "human-computer interaction; cloud laboratory; augmented reality; swarm programming; interactive biotechnology",
        "released": 2019,
        "link": "https://doi.org/10.1073/pnas.1815367116"
    },
    {
        "title": "“Designing for the web” revisited: A survey of informal and experienced web developers",
        "abstract": "We report a subset of findings from a survey of over 300 web developers - a mixture of professional and more casual developers - targeted at understanding the needs, problems and the processes that developers follow and the tools they use. The prototypical web developer from our sample is meticulous about the quality of the web sites she produces, considers usability issues but neglects accessibility concerns. Web developers have many similar interests regarding web applications or features such as authentication, databases, online surveys or forms. They value ease of use as the most important property of a web development tool but mention many other needs such as integration with other tools, strong code editing features, or WYSIWYG facilities. This report details findings regarding process, tools, quality control, and leaming.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000231171300062"
    },
    {
        "title": "Teaching statistical concepts and modern data analysis with a computing-integrated learning environment",
        "abstract": "Revisiting the seminal 2010 Nolan and Temple Lang article on the role of computing in the statistics curricula, we discuss several trends that have emerged over the last ten years. The rise of data science has coincided with a broadening audience for learning statistics and using computational packages and tools. It has also increased the need for communication skills. We argue that, for most of this audience, instruction should focus on foundational concepts and the early introduction of different types of data and modern methods through the use of interactive learning environments without programming prerequisites. We then describe Integrated Statistics Learning Environment (ISLE), a web-based e-learning platform and lesson authoring framework for teaching statistics. Built on top of computing and peer-to-peer technology, the platform allows collaborative data analysis and real-time interactions in the classroom. We describe how ISLE contributes to the three key Nolan and Temple Lang components: broadening the statistical computing curriculum, deepening computational reasoning and literacy, and expanding computing with data in the practice of statistics. We then present several advantages of using a computing-integrated environment such as ISLE, including promoting (cross-disciplinary) communication, supporting classroom-focused integrated development environments, and advancing the science of data science.",
        "keywords": "Computing; Data science; E-learning; Integrated Statistics Learning Environment; Statistics curriculum",
        "released": 2021,
        "link": "https://doi.org/10.1080/10691898.2020.1854637"
    },
    {
        "title": "Combining interactivity and improved layout while creating educational software for the web",
        "abstract": "Two major attributes influence a project aimed at developing educational software: user interactivity and graphic design. These two attributes are not easily found within the same development tool. Moreover, sometimes the development tool is either very expensive to acquire or too difficult to use. This article aims to present a solution to the creation of educational development tools which emphasize the conjunction between interactivity and graphic design. The authors propose a methodology based on Web technology and an association between the Java programming language and the authoring tool Flash. In addition to fostering interactivity, Java offers development packages at no cost. The authoring software Flash is inexpensive and allows the creation of finely crafted Web pages with graphic animations and sound editing. This article describes the methodology that enables the association between Java and Flash components. This methodology was conceived during the development of a structural analysis educational software product specially geared to undergraduate engineering students. In order to ensure the understanding of our proposal, a specific example was created, along with sample pages that illustrate this methodology. (C) 2002 Elsevier Science Ltd. All rights reserved.",
        "keywords": "human-computer interface; interactive learning environments; media in education; programming and programming languages; authoring tools and methods",
        "released": 2003,
        "link": "https://doi.org/10.1016/S0360-1315(02)00131-8"
    },
    {
        "title": "RETRACTED: Research and development of an IoT-based remote asthma patient monitoring system (retracted article)",
        "abstract": "This paper presents the research and development of an Internet of Things- (IoT-) based remote health monitoring system for asthmatic patients. Asthma is an inflammatory disease. Asthma causes the lungs to swell and get narrower, making it difficult to carry air in and out of the lungs. This situation makes breathing very difficult. Remote patient monitoring (RPM) is a method of collecting health-related data from patients who are in a remote location and electronically transmitting it to healthcare providers for evaluation and consultation. The aim of this study is to design a monitoring system that allows doctors to monitor asthmatic patients from a remote area. The proposed system will allow patients to measure oxygen saturation (SpO2), heart rate, body temperature, humidity, volatile gases, room temperature, and electrocardiogram (ECG) using various sensors, which will be displayed in an application. This data is then sent to the doctor to monitor the patient’s condition and suggest appropriate actions. Overall, the system consists of an Android application, a website, and various sensors. The Android studio and Java programming language were used to develop the application. For the frontend, the website was built using Hypertext Markup Language (HTML), Cascading Style Sheets (CSS), JavaScript, and jQuery. The system also uses Django, a Python-based open-source web framework, for the backend. The system developed the various sensors using an ESP8266 microcontroller compatible with the Arduino Integrated Development Environment (IDE). The system uses a MAX30100 pulse oximeter and heart rate sensor, a GY-906 MLX90614 noncontact precision thermometer, a DHT11 humidity and temperature sensor, a MQ-135 gas and air quality sensor, and an AD8232 ECG sensor for collecting various parameters that may trigger asthma attacks. Finally, the system developed the Asthma Tracker app and the Asthma Tracker website for remote health monitoring. The system was initially tested on demo patients and later deployed and tested on seven real human test subjects. Overall, the monitoring system produced satisfactory results. The data acquired by the sensors has a high level of accuracy. The system also maintained user-friendliness and low cost.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1155/2021/2192913"
    },
    {
        "title": "A development environment for customer-oriented internet business: eBizBench",
        "abstract": "An important challenge in the age of Internet business is the proper alignment of customers’ needs with the business system. In order to improve an Internet business system according to customers’ needs continuously, reuse of design results is of particular importance. This paper proposes a development environment, called eBizBench, which provides support for this alignment and reusability. The eBizBench includes seven functions: (1) project management, (2) customer analysis, (3) value analysis, (4) web design, (5) implementation design, (6) repository management, and (7) repository. The eBizBench can help develop and maintain Internet business systems in a systematic fashion. The repository is useful for the conversion of design results among development environments. A real-life web site is illustrated to demonstrate the usefulness of the eBizBench. (C) 2003 Elsevier Inc. All rights reserved.",
        "keywords": "Internet business; development environment; repository; scenario; customer orientation",
        "released": 2004,
        "link": "https://doi.org/10.1016/S0164-1212(03)00218-8"
    },
    {
        "title": "Kinetically controlled glass transition measurement of organic aerosol thin films using broadband dielectric spectroscopy",
        "abstract": "Glass transitions from liquid to semi-solid and solid phase states have important implications for reactivity, growth, and cloud-forming (cloud condensation nuclei and ice nucleation) capabilities of secondary organic aerosols (SOAs). The small size and relatively low mass concentration of SOAs in the atmosphere make it difficult to measure atmospheric SOA glass transitions using conventional methods. To circumvent these difficulties, we have adapted a new technique for measuring glass-forming properties of atmospherically relevant organic aerosols. Aerosol particles to be studied are deposited in the form of a thin film onto an interdigitated electrode (IDE) using electrostatic precipitation. Dielectric spectroscopy provides dipole relaxation rates for organic aerosols as a function of temperature (373 to 233 K) that are used to calculate the glass transition temperatures for several cooling or heating rates. IDE-enabled broadband dielectric spectroscopy (BDS) was successfully used to measure the kinetically controlled glass transition temperatures of aerosols consisting of glycerol and four other compounds with selected cooling and heating rates. The glass transition results agree well with available literature data for these five compounds. The results indicate that the IDE-BDS method can provide accurate glass transition data for organic aerosols under atmospheric conditions. The BDS data obtained with the IDE-BDS technique can be used to characterize glass transitions for both simulated and ambient organic aerosols and to model their climate effects.",
        "keywords": "",
        "released": 2018,
        "link": "https://doi.org/10.5194/amt-11-3479-2018"
    },
    {
        "title": "Improved differential evolution-based elman neural network controller for squirrel-cage induction generator system",
        "abstract": "An improved differential evolution (IDE) algorithm-based Elman neural network (ENN) controller is proposed to control a squirrel-cage induction generator (SCIG) system for grid-connected wind power applications. First, the control characteristics of a wind turbine emulator are introduced. Then, an AC/DC converter and a DC/AC inverter are developed to convert the electric power generated by a three-phase SCIG to the grid. Moreover, the dynamic model of the SCIG system is derived for the control of the square of DC-link voltage according to the principle of power balance. Furthermore, in order to improve the transient and steady-state responses of the square of DC-link voltage of the SCIG system, an IDE-based ENN controller is proposed for the control of the SCIG system. In addition, the network structure and the online learning algorithm of the ENN are described in detail. Additionally, according to the different wind speed variations, a lookup table built offline by the dynamic model of the SCIG system using the IDE is provided for the optimisation of the learning rates of ENN. Finally, to verify the control performance, some experimental results are provided to verify the feasibility and the effectiveness of the proposed SCIG system for grid-connected wind power applications.",
        "keywords": "evolutionary computation; neurocontrollers; asynchronous generators; machine control; power grids; wind power plants; wind turbines; power generation control; invertors; voltage control; transient response; learning (artificial intelligence); AC-DC power convertors; improved differential evolution-based Elman neural network controller; squirrel-cage induction generator system; three-phase SCIG system; grid-connected wind power applications; wind turbine emulator control characteristics; AC-DC power converter; DC-AC power inverter; electric power conversion; DC-link voltage square control; power balance principle; steady-state responses; transient responses; IDE-based ENN intelligent controller algorithm; network structure; online learning algorithm; wind speed variations; lookup table; learning rate optimization; power grid",
        "released": 2016,
        "link": "https://doi.org/10.1049/iet-rpg.2015.0329"
    },
    {
        "title": "osFP: A web server for predicting the oligomeric states of fluorescent proteins",
        "abstract": "Background: Currently, monomeric fluorescent proteins (FP) are ideal markers for protein tagging. The prediction of oligomeric states is helpful for enhancing live biomedical imaging. Computational prediction of FP oligomeric states can accelerate the effort of protein engineering efforts of creating monomeric FPs. To the best of our knowledge, this study represents the first computational model for predicting and analyzing FP oligomerization directly from the amino acid sequence. Results: After data curation, an exhaustive data set consisting of 397 non-redundant FP oligomeric states was compiled from the literature. Results from benchmarking of the protein descriptors revealed that the model built with amino acid composition descriptors was the top performing model with accuracy, sensitivity and specificity in excess of 80% and MCC greater than 0.6 for all three data subsets (e.g. training, tenfold cross-validation and external sets). The model provided insights on the important residues governing the oligomerization of FP. To maximize the benefit of the generated predictive model, it was implemented as a web server under the R programming environment. Conclusion: osFP affords a user-friendly interface that can be used to predict the oligomeric state of FP using the protein sequence. The advantage of osFP is that it is platform-independent meaning that it can be accessed via a web browser on any operating system and device. osFP is freely accessible at http://codes.bio/osfp/ while the source code and data set is provided on GitHub at https://github.com/chaninn/osFP/.",
        "keywords": "Fluorescent protein; FP; Green fluorescent protein; GFP; Oligomeric state; Data mining; Web server",
        "released": 2016,
        "link": "https://doi.org/10.1186/s13321-016-0185-8"
    },
    {
        "title": "Development of an IoT tool for optimizing humidity control in cocoa cultivation.",
        "abstract": ". -The following document presents the development of a humidity sensor for cocoa crops using IoT and Arduino. Its purpose is to showcase the various applications that new technologies can offer, particularly in the cocoa industry. It involves designing a humidity sensor node to measure changes in humidity and temperature using the electronic design tool Proteus. The measurement module is programmed in Arduino IDE and connected to the cloud via IoT, incorporating Node-RED as a means to remotely visualize real-time results. This is achieved through a presentation dashboard as the final output of the measurement process, collecting data on climate changes received by the transducer and comparing them with data obtained from the web over a specified period. This process verifies the sensor’s efficiency and data accuracy, setting a precedent for future research in applied electronics, IoT, and telecommunications.",
        "keywords": "1; IoT 2; Arduino IDE 3; Proteus 4; Node-RED 5; Transducer",
        "released": 2023,
        "link": "https://doi.org/10.36561/ING.25.14"
    },
    {
        "title": "The world wide web leads a revolution in ATE programming environments",
        "abstract": "New software technologies, including the World Wide Web, may seem far removed from the tasks facing test program set (TPS) developers, but they promise to revolutionize the way TPS data is organized, presented, and used. This paper will describe how an integrated TPS development and execution environment can capitalize on these new technologies to improve test programming efficiency.",
        "keywords": "",
        "released": 1998,
        "link": "https://doi.org/10.1109/62.683729"
    },
    {
        "title": "RETRACTED: IoT-based real-time patients vital physiological parameters monitoring system using smart wearable sensors (retracted article)",
        "abstract": "Health care is one of the least funded sectors in Bangladesh and many other similar developing countries. People living in rural and remote areas do not have access to proper health care, and when they do, it is too expensive. This research aimed to develop a real-time health monitoring system that is cheap, easy to use, and accessible by doctors and patients. The system consists of several Internet of Things (IoT)-based sensors connected to an Arduino microprocessor, which thus measures the vital body signs of the patients. The measured readings are then transmitted to an Android application on a smartphone via a Bluetooth module. The sensors are connected to analog inputs. These sensors measure analog data, which is amplified by the microprocessor after being sorted. Doctors can also carry out the diagnosis of ailments using the data collected remotely from the patient. An Android-based mobile application that interfaces with a web-based application is implemented for efficient patients-doctors dual real-time communication. The Android application, which is connected to a MySQL database, updates the said database, which updates and displays the readings on a website accessible by both doctors and patients. Initially, the health monitor was tested using an Arduino Integrated Development Environment (IDE) monitor and a single user. Once initial simulations were successful, the proposed system was tested on five different real-human test subjects. The testing of the wireless health monitor produced successful results that measured patient vitals with a high level of accuracy. The proposed IoT-based system monitors vital signs such as the patient’s body temperature, heart rate, ECG, SpO2 levels, blood pressure, and glucose levels. This system also includes a medical treatment plan by the doctors. The proposed system is novel as it integrates the IoT-based patient monitoring system with telemedicine. This proposed system has different sensors for real-time measuring the vital signs of the human body. A mobile and web application have also been integrated with this system for real-time remote patient monitoring and treatment plan. There are now systems available that only offer a telemedicine facility, where patients and doctors can have discussions, but do not have an IoT-based patient vital sign monitoring system integrated with telemedicine. The proposed system in this paper has the facility of IoT-based patient vital signs monitoring integrated with telemedicine, which makes this research work novel. The proposed system will increase the life expectancy of people throughout the world.",
        "keywords": "Internet of things; Wireless health monitoring; Health care; Remote health care; Android application; Patient monitoring; Real-time; Wearable sensors; Smart",
        "released": 2023,
        "link": "https://doi.org/10.1007/s00521-022-07090-y"
    },
    {
        "title": "On the benefits of providing versioning support for end users: An empirical study",
        "abstract": "End users with little formal programming background are creating software in many different forms, including spreadsheets, web macros, and web mashups. Web mashups are particularly popular because they are relatively easy to create, and because many programming environments that support their creation are available. These programming environments, however, provide no support for tracking versions or provenance of mashups. We believe that versioning support can help end users create, understand, and debug mashups. To investigate this belief, we have added versioning support to a popular wire-oriented mashup environment, Yahoo! Pipes. Our enhanced environment, which we call “Pipes Plumber,” automatically retains versions of pipes and provides an interface with which pipe programmers can browse histories of pipes and retrieve specific versions. We have conducted two studies of this environment: an exploratory study and a larger controlled experiment. Our results provide evidence that versioning helps pipe programmers create and debug mashups. Subsequent qualitative results provide further insights into the barriers faced by pipe programmers, the support for reuse provided by our approach, and the support for debugging provided.",
        "keywords": "Human Factors; End-user software engineering; versioning; Mashups; Yahoo! Pipes; reuse; debugging; programming barriers",
        "released": 2014,
        "link": "https://doi.org/10.1145/2560016"
    },
    {
        "title": "Free development tools online for embedded windows",
        "abstract": "",
        "keywords": "",
        "released": 1999,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000083708400006"
    },
    {
        "title": "Automatically categorizing software technologies",
        "abstract": "Informal language and the absence of a standard taxonomy for software technologies make it difficult to reliably analyze technology trends on discussion forums and other on-line venues. We propose an automated approach called $\\mathrm{Witt}$ Witt for the categorization of software technologies (an expanded version of the hypernym discovery problem). $\\mathrm{Witt}$ Witt takes as input a phrase describing a software technology or concept and returns a general category that describes it (e.g., integrated development environment), along with attributes that further qualify it (commercial, php, etc.). By extension, the approach enables the dynamic creation of lists of all technologies of a given type (e.g., web application frameworks). Our approach relies on Stack Overflow and Wikipedia, and involves numerous original domain adaptations and a new solution to the problem of normalizing automatically-detected hypernyms. We compared $\\mathrm{Witt}$ Witt with six independent taxonomy tools and found that, when applied to software terms, $\\mathrm{Witt}$ Witt demonstrated better coverage than all evaluated alternative solutions, without a corresponding degradation in false positive rate.",
        "keywords": "Taxonomy; information retrieval; natural language processing; Wikipedia; tagging",
        "released": 2020,
        "link": "https://doi.org/10.1109/TSE.2018.2836450"
    },
    {
        "title": "Analytical and comparison study of main web programming languages - ASP and PHP",
        "abstract": "Web applications are growing and websites numbers are increasing, as well. At the same time the requirements on quality and performance are also increasing on the web application development. Every day new programming tools are appearing in the market. All of these issues are affecting the software development including web and mobile applications. This paper tries to provide helpful information for the web developers by means of making analytical comparison between two of the most important web programming languages - PHP and ASP as dynamic server-side scripting languages.",
        "keywords": "ASP; PHP; Programming languages; Web Application",
        "released": 2019,
        "link": "https://doi.org/10.18421/TEM84-58"
    },
    {
        "title": "Developing the online survey",
        "abstract": "Institutions of higher education are now using Internet-based technology tools to conduct surveys for data collection. Research shows that the type and quality of responses one receives with online surveys are comparable with what one receives in paper-based surveys. Data collection can take place on Web-based surveys, e-mail-based surveys, and personal digital assistants/Smartphone devices. Web surveys can be subscription templates, software packages installed on one’s own server, or created from scratch using Web programming development tools. All of these approaches have their advantages and disadvantages. The survey owner must make informed decisions as to the right technology to implement. The correct choice can save hours of work in sorting, organizing, and analyzing data.",
        "keywords": "Email data collection; Web-based survey; Personal digital assistant",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.cnur.2008.06.011"
    },
    {
        "title": "A declarative enhancement of JavaScript programs by leveraging the java metadata infrastructure",
        "abstract": "Web browsers have evolved into indispensable software platforms for modern applications. Although JavaScript has become a de-facto lingua franca for developing web applications, the JavaScript software development tools and methodologies lag behind those of languages such as Java and C#. In particular, to enhance JavaScript programs with non-functional concerns (e.g., persistence, security, transactions, and logging) JavaScript developers need to modify the source code by hand. Developers then often end up having to maintain the resulting modified version separately for reasons that include poor design, the complexities of program evolution, and fundamental language limitations. The necessity to modify code by hand could be avoided if JavaScript had metadata facilities for developers to specify non-functional concerns declaratively. To address this problem, we present a novel declarative approach, Transparent Automated Enhancement for JavaScript (TAE-JS) that enhances JavaScript programs with the ability to use declarative metadata. The metadata is expressed by means of Java annotations. We have implemented our technique in an open-source plug-in for the Eclipse IDE. This plugin allows a developer to enhance the original version of a JavaScript program by specifying concerns declaratively using Java annotations. It then analyzes an original version of the program to automatically generate enhanced program versions by adding the declared concerns. We evaluated TAE-JS with four practical examples that enhance JavaScript programs by declaratively adding concerns such as persistence, security, transactions, and logging. In a user study with ten student developers and three professional engineers at PayPal, the participants viewed favorably TAE-JS’s flexible declarative enhancement capabilities, seeing them as a valuable mechanism for implementing non-functional concerns in JavaScript programs. Our evaluation results are promising in demonstrating the potential benefits of our approach to complement existing JavaScript development tools. (C) 2019 Elsevier B.V. All rights reserved.",
        "keywords": "Software reuse; Code enhancement; Metadata; Annotations",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.scico.2019.05.005"
    },
    {
        "title": "rMyCoPortal - an r package to interface with the mycology collections portal",
        "abstract": "The understanding of the biodiversity and biogeographical distribution of fungi is still limited. The small number of online databases and the large effort required to access existing data have prevented their use in research articles. The Mycology Collections Portal was established in 2012 to help alleviate these issues and currently serves data online for over 4.3 million fungal records. However, the current process for accessing the data through the web interface is manual, therefore slow, and precludes the extensive use of the existing datasets. Here we introduce the software package rMyCoPortal, which allows users rapid, automated access to the data. rMyCoPortal makes data readily available for further computations and analyses in the open source statistical programming environment R. We will demonstrate the core functions of the package, and how rMyCoPortal can be employed to obtain fungal data that can be used to address basic research questions. rMyCoPortal is a free and open-source R package, available via GitHub.",
        "keywords": "data portal; database; fungaria; fungi; georeferencing; natural history collections; specimens; MyCoPortal; Symbiota; R package",
        "released": 2019,
        "link": "https://doi.org/10.3897/BDJ.7.e31511"
    },
    {
        "title": "Intelligent data post and read data system like to feed for IoT sensors",
        "abstract": "Nowadays, estimating the environmental weather condition is a pretty challenging task. In this paper, the proposed weather tracking system can solve the challenges arising during real-time monitoring. The proposed system uses an IoT sensor-based architecture to determine environmental parameters such as temperature, humidity, pressure, raindrops. The acquired sensor data is transmitted to the node MCU controller, whereas the Arduino IDE is utilized in posting the data. The serial monitor is placed as a gateway linking IoT sensors with cloud servers. The data is transmitted from IoT sensors to a serial monitor wherein the corresponding IP address is monitored. Finally, the data is read from the webserver using the HTTP protocol. The environmental parameter data are shown on a website to monitor weather data. Anyone can check the weather’s state from anyplace utilizing a web server without relying webpage since the sensed data is provided in a public mode.",
        "keywords": "Node MCU controller; IoT sensor; Weather tracking system; Environmental parameters",
        "released": 2022,
        "link": "https://doi.org/10.1007/s13198-022-01683-5"
    },
    {
        "title": "Remote laboratory for e-learning of systems on chip and their applications to nuclear and scientific instrumentation",
        "abstract": "Configuring and setting up a remote access laboratory for an advanced online school on fully programmable System-on-Chip (SoC) proved to be an outstanding challenge. The school, jointly organized by the International Centre for Theoretical Physics (ICTP) and the International Atomic Energy Agency (IAEA), focused on SoC and its applications to nuclear and scientific instrumentation and was mainly addressed to physicists, computer scientists and engineers from developing countries. The use of e-learning tools, which some of them adopted and others developed, allowed the school participants to directly access both integrated development environment software and programmable SoC platforms. This facilitated the follow-up of all proposed exercises and the final project. During the four weeks of the training activity, we faced and overcame different technology and communication challenges, whose solutions we describe in detail together with dedicated tools and design methodology. We finally present a summary of the gained experience and an assessment of the results we achieved, addressed to those who foresee to organize similar initiatives using e-learning for advanced training with remote access to SoC platforms.",
        "keywords": "e-learning; SoC-FPGA; remote laboratory; nuclear instrumentation; scientific instrumentation; embedded processor",
        "released": 2021,
        "link": "https://doi.org/10.3390/electronics10182191"
    },
    {
        "title": "Mango: Combining and analyzing heterogeneous biological networks",
        "abstract": "Background: Heterogeneous biological data such as sequence matches, gene expression correlations, protein-protein interactions, and biochemical pathways can be merged and analyzed via graphs, or networks. Existing software for network analysis has limited scalability to large data sets or is only accessible to software developers as libraries. In addition, the polymorphic nature of the data sets requires a more standardized method for integration and exploration. Results: Mango facilitates large network analyses with its Graph Exploration Language, automatic graph attribute handling, and real-time 3-dimensional visualization. On a personal computer Mango can load, merge, and analyze networks with millions of links and can connect to online databases to fetch and merge biological pathways. Conclusions: Mango is written in C++ and runs on Mac OS, Windows, and Linux. The stand-alone distributions, including the Graph Exploration Language integrated development environment, are freely available for download from http://www.complex.iastate.edu/download/Mango. The Mango User Guide listing all features can be found at http://www.gitbook.com/book/j23414/mango-user-guide.",
        "keywords": "Systems biology; Heterogeneous data integration; Biological pathway analysis; 3D visualization; Graph mathematics",
        "released": 2016,
        "link": "https://doi.org/10.1186/s13040-016-0105-5"
    },
    {
        "title": "Integration of LabVIEW with IoT solutions",
        "abstract": "The article discusses selected programming tools that can be useful in integrating software created in the LabVIEW environment with IoT (Intenet of Things). The development of web applications and containerization using Docker were discussed. (Integratyion of LabViewn with IoT devices)",
        "keywords": "LabVIEW; Web Services; REST API; Docker",
        "released": 2023,
        "link": "https://doi.org/10.15199/48.2023.10.43"
    },
    {
        "title": "Buyer’s guide: Development tools for web applications",
        "abstract": "",
        "keywords": "",
        "released": 1998,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000073585700045"
    },
    {
        "title": "Multi-criteria decision-making approach for container-based cloud applications: The SWITCH and ENTICE workbenches",
        "abstract": "Many emerging smart applications rely on the Internet of Things (IoT) to provide solutions to time-critical problems. When building such applications, a software engineer must address multiple Non-Functional Requirements (NFRs), including requirements for fast response time, low communication latency, high throughput, high energy efficiency, low operational cost and similar. Existing modern container-based software engineering approaches promise to improve the software lifecycle; however, they fail short of tools and mechanisms for NFRs management and optimisation. Our work addresses this problem with a new decision-making approach based on a Pareto Multi-Criteria optimisation. By using different instance configurations on various geo-locations, we demonstrate the suitability of our method, which narrows the search space to only optimal instances for the deployment of the containerised microservice. This solution is included in two advanced software engineering environments, the SWITCH workbench, which includes an Interactive Development Environment (IDE) and the ENTICE Virtual Machine and container images portal. The developed approach is particularly useful when building, deploying and orchestrating IoT applications across multiple computing tiers, from Edge-Cloudlet to Fog-Cloud data centres.",
        "keywords": "Cloud; Edge; Fog; Internet of Things; Non-Functional Requirements; Software Engineering; Quality of Service",
        "released": 2020,
        "link": "https://doi.org/10.17559/TV-20190105200327"
    },
    {
        "title": "ESTIMATION OF DEBRIS CLOUD TEMPORAL CHARACTERISTICS AND ORBITAL ELEMENTS",
        "abstract": "The Interplanetary Dust Experiment (IDE), one of the 56 experiments carried by LDEF, was, designed to detect impacts of extra-terrestrial particles and orbital debris. The IDE detectors (which covered about one square meter of the surface of LDEF) were sensitive to particles ranging in size from about 0.2 mum to 100 mum. The IDE experiment recorded the location in orbit and approximate origin direction for each impact. The resulting dataset represents perhaps the most extensive record ever gathered of the number and location of impacts due to small particles on a spacecraft in Earth orbit. The IDE dataset shows that impacts often occurred in “’bursts”’ reoccurring each orbit. Such events, which we have designated Multiple orbit Events, can occur only if the impacting particles are in an Earth orbit that intersects that of LDEF. We have examined several such event sequences in the LDEF IDE dataset and have found orbital debris concentrations distributed in both moderate inclination (ca. 25-degrees-35-degrees) and high inclination (> 60-degrees) orbits. A peak flux of at least 12 impacts/second/sq. meter was detected in one such cloud. This is nearly 4 orders of magnitude above the mean flux observed over the LDEF 5.77 year mission duration.",
        "keywords": "",
        "released": 1993,
        "link": "https://doi.org/10.1016/0273-1177(93)90573-T"
    },
    {
        "title": "A study of dynamic design dualities in a web-supported community of practice for teachers",
        "abstract": "The concept of a community of practice (CoP) is prevalent in several venues for teachers’ professional development, especially in online environments. However, there are few descriptive accounts that effectively represent a CoP in a manner that will be of use to other designers. In order to illuminate potential difficulties which may arise when attempting to design a framework to characterize or to build a CoP, this study describes the dynamics of five dualities (specific areas of tension) that were identified during the design and testing period of the Inquiry Learning Forum (ILF), a Web-based community for teachers’ professional development. During the three-year design trajectory of the ILF, these five dualities emerged from and characterized the interactions between the participating teachers and the site designers. As part of the data collection for this study, we conducted document analyses, interviews with designers, researchers, and teachers, and observations of online and face-to-face meetings. The findings of this study are intended to help future Web-designers both to better realize the full potential of online professional development environments and to avoid potential design development issues which may hamper the utility or participation rates in newly created CoPs.",
        "keywords": "community of practice; online community; teachers’ professional development; design dualities",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000233138300015"
    },
    {
        "title": "Tools fair: Untangling the web with web and client/server development tools",
        "abstract": "Vendors have recently marketed many new and upgraded tools that help developers create client/server and Internet applications. The authors provide a sampling of what’s available along with tips for picking those best suited to your needs.",
        "keywords": "",
        "released": 1996,
        "link": "https://doi.org/10.1109/52.536460"
    },
    {
        "title": "Cloud computing as an innovation: Percepetion, attitude, and adoption",
        "abstract": "Cloud computing is a current trend that reveals the next-generation application architecture and it is estimated that by 2013 the cloud market will have reached $8.1bn. While cloud services such as web-mail, Flickr and YouTube have been widely used by individuals for some time, it not until relatively recently that organisations have began to use cloud services as a tool for meeting their IT needs. This study aims to investigate how cloud computing is understood by IT professionals and the concerns that IT professionals have in regard to the adoption of cloud services. The study was carried out in Taiwan and used a survey by interview approach to understand IT professionals’ understandings and concerns about cloud computing. The findings of the study suggest that while the benefits of cloud computing such as its computational power and ability to help companies save costs are often mentioned in the literature, the primary concerns that IT managers and software engineers have are compatibility of the cloud with companies’ policy, IS development environment, and business needs: and relative advantages of adopting cloud solutions. The findings also suggest that most IT companies in Taiwan will not adopt cloud computing until the uncertainties associated with cloud computing, e.g. security and standardisation are reduced and successful business models have emerged. (C) 2012 Elsevier Ltd. All rights reserved.",
        "keywords": "Cloud computing; Innovation; Adoption; Business model; Compatibility",
        "released": 2012,
        "link": "https://doi.org/10.1016/j.ijinfomgt.2012.04.001"
    },
    {
        "title": "Effectiveness of tenofovir and entecavir in nucleos(t)ide analogue-naive chronic hepatitis b a protocol for systematic review and meta-analysis",
        "abstract": "Background: Chronic hepatitis b (CHB) is a serious problem worldwide. Tenofovir disoproxil fumarate (TDF) and entecavir (ETV) both are first-line drugs for CHB, but there is debate about which is more appropriate in nucleos(t)ide analogue-naive CHB. Objective: To systematically evaluate the effectiveness and safety of tenofovir and ETV in nucleos(t)ide analogue-naive CHB. Methods: The Web of Science, PubMed, The Cochrane Library, EMBASE, Clinical Trials, and China National Knowledge Infrastructure databases will be electronically searched to collect randomized controlled trials regarding the comparison between tenofovir and ETV in nucleos(t) ide analogue-naive CHB since the date of database inception to July 2019. Two researchers independently screened and evaluated the obtained studies and extracted the outcome indexes. RevMan 5.3 software will be used for the meta-analysis. Result: We will provide practical and targeted results assessing the effectiveness and safety of TDF and ETV for nucleos(t) ide analogue-naive CHB patients, try to compare the advantages of TDF and ETV. Conclusion: The stronger evidence about the effectiveness and safety of TDF and ETV for nucleos(t) ide analogue-naive CHB patients will be provided for clinicians. Protocol registration number: PROSPERO CRD42019134194.",
        "keywords": "chronic hepatitis B; entecavir; meta-analysis; nucleos(t)ide analogue-naive; tenofovir",
        "released": 2019,
        "link": "https://doi.org/10.1097/MD.0000000000016943"
    },
    {
        "title": "<I>whatsapp</i> use among african international distance education (IDE) students: Transferring, translating and transforming educational experiences",
        "abstract": "Much of the research on how social media is embedded into the educational practices of higher education students has a Western orientation. In concentrating on a case study of the varied ways in which African International Distance Education (IDE) students actively use social media to shape their learning experiences, we discuss an under-researched group. The paper draws on analysis of 1295 online questionnaires and 165 in-depth interviews with IDE students at UNISA, South Africa, one of the largest providers of IDE globally. WhatsApp emerges as “the” key social media tool that opens up opportunities for IDE students to transfer, translate and transform their educational journey when studying “at a distance”. Although WhatsApp does provide a “space of opportunity” for some students, this is framed through socio-technical marginalisation, itself a reflection of demographic legacies of inequality. Exploring social media practices though the case of African IDE students places these students centre stage and adds to the awareness of the multiple centres from which international education is practiced.",
        "keywords": "African international distance education; social media; WhatsApp; South Africa",
        "released": 2019,
        "link": "https://doi.org/10.1080/17439884.2019.1628048"
    },
    {
        "title": "Web based metacomputing",
        "abstract": "Programming tools that are simultaneously sustainable, highly functional, robust and easy to use have been hard to come by in the HPCC arena. This is partially due to the difficulty in developing sophisticated customized systems for what is a relatively small part of the worldwide computing enterprise. Thus, we have developed a new strategy - termed High Performance Commodity Computing (HPCC) [G. Fox, W. Furmanski, HPCC as high performance commodity computing, in: I. Foster, C. Kesselman (Eds.), Building National Grid, http://www.npac.syr.edu/users/gcf/HPcc/HPcc.html] - which builds HPCC programming tools on top of the remarkable new software infrastructure being built for the commercial web and distributed object areas. We add high performance to commodity systems using multi-tier architecture with Globus metacomputing toolkit as the backend of a middle-tier of commodity web and object servers. We have demonstrated the fully functional prototype of WebFlow during Alliance’98 meeting. (C) 1999 Elsevier Science B.V. All rights reserved.",
        "keywords": "WebFlow; metacomputing; globus; visual authoring tool; distributed object; Java; web server; three-tier architecture; HPCC",
        "released": 1999,
        "link": "https://doi.org/10.1016/S0167-739X(99)00023-0"
    },
    {
        "title": "Acceleration of neural network learning by GPGPU",
        "abstract": "Recently, graphic boards have come to have higher performance than CPUs as a result of the development of 3DCG and movie processing, and are now widely used due to progress in computer entertainment. Implementation of general-purpose computing on GPU (GPGPU) has become easier as a result of the integrated development environment CUDA distributed by NVIDIA. A GPU has dozens or hundreds of arithmetic circuits, whose allocations are controlled by CUDA. In prior research, the implementation of a neural network using GPGPU has been studied; however, the training of networks was not mentioned because the GPU performance is low in conditional processing but high in linear algebra processing. Therefore, we have proposed two methods. First, a whole network is implemented as a thread, and some networks are trained in parallel to shorten the time necessary to find the optimal weight coefficients. Second, this paper introduces parallelization in the neural network structure, in which the calculation of neurons in the same layers is parallelized. The processing to train the same network with different patterns is likewise independent. As a result, the second method is 20 times faster than the CPU, and 6 times as fast as the first proposed method. (c) 2013 Wiley Periodicals, Inc. Electron Comm Jpn, 96(8): 59-66, 2013; Published online in Wiley Online Library (wileyonlinelibrary.com). DOI 10.1002/ecj.11412",
        "keywords": "GPGPU; neural network; CUDA",
        "released": 2013,
        "link": "https://doi.org/10.1002/ecj.11412"
    },
    {
        "title": "A volunteer computing architecture for computational workflows on decentralized web",
        "abstract": "The amount of accessible computational devices over the Internet offers an enormous but latent computational power. Nonetheless, the complexity of orchestrating and managing such devices requires dedicated architectures and tools and hinders the exploitation of this vast processing capacity. Over the last years, the paradigm of (Browser-based) Volunteer Computing emerged as a unique approach to harnessing such computational capabilities, leveraging the idea of voluntarily offering resources. This article proposes VFuse, a groundbreaking architecture to exploit the Browser-based Volunteer Computing paradigm via a ready-to-access volunteer network. VFuse offers a modern multi-language programming environment for developing scientific workflows using WebAssembly technology without requiring the user any local installation or configuration. We equipped our architecture with a secure and transparent rewarding mechanism based on blockchain technology (Ethereum) and distributed P2P file system (IPFS). Further, the use of Non-Fungible Tokens provides a unique, secure, and transparent methodology for recognizing the users’ participation in the network. We developed a prototype of the proposed architecture and four example applications implemented with our system. All code and examples are publicly available on GitHub.",
        "keywords": "Scientific computing; Servers; Computer architecture; Browsers; Distributed processing; Computer applications; Blockchains; Web 3; 0; Parallel processing; Scientific computing; volunteer computing; browser-based volunteer computing; decentralized web; Web 3; 0; P2P; WebAssembly; distributed computing; parallel computing",
        "released": 2022,
        "link": "https://doi.org/10.1109/ACCESS.2022.3207167"
    },
    {
        "title": "Enabling user interface developers to experience accessibility limitations through visual, hearing, physical and cognitive impairment simulation",
        "abstract": "His paper presents a tool facilitating developers of user interfaces (UIs) to experience accessibility limitations that can be posed from various disabilities during the interaction of impaired users with their developments. In this respect, various aspects of visual, hearing, physical and cognitive impairments have been modelled through filters providing approximate, yet, realistic simulations over them. These filters have formed the basis for the developed tool, which can be used either on its own (as a standalone application), or be embedded in the NetBeans Integrated Development Environment. The tool, named DIAS, allows for impairment simulations to be performed over Java, mobile and web applications. Moreover, it integrates two of the most common assistive technologies (ATs), namely a screen reader and a magnifier. As a result, developers of UIs can not only experience how interaction would be affected from various impairments, but they can also understand how their developments would be perceived by impaired users through an AT. This work aims to provide an integrated, practical solution for impairment simulation, which could be easily adopted by developers, thus realistically increasing the possibilities for the future development of interactive applications that are more accessible to users with disabilities.",
        "keywords": "Human computer interaction; User interfaces; Accessibility; Simulation; User-centred software design",
        "released": 2014,
        "link": "https://doi.org/10.1007/s10209-013-0309-0"
    },
    {
        "title": "A middle-up approach with online capillary isoelectric focusing/mass spectrometry for in-depth characterization of cetuximab charge heterogeneity",
        "abstract": "Previously, we reported a new online capillary isoelectric focusing/mass spectrometric (CIEF/MS) method for intact monoclonal antibody (mAb) charge variant analysis that uses an electrokinetically pumped sheath-flow nanospray ion source on a time-of-flight (TOF) MS with a pressure-assisted chemical mobilization. The direct online CIEF/MS method exhibited excellent resolution of charge variants conforming to those of imaged capillary isoelectric focusing with ultraviolet detection (iCIEF/UV). However, for complex mAbs, CIEF/MS spectra of the intact charge variant peaks may be too convoluted to be effectively interpreted. In the present study, we implemented a middle-up approach to enhance the capability of the CIEF/MS method for characterizing complex mAb charge variants by reducing sample complexity. To demonstrate such a strategy, we fragmented cetuximab through IdeS enzymatic cleavage and dithiothreitol (DTT) reduction. For the first time, online CIEF/MS resolved the complex charge variants of cetuximab at subunit level, corroborating the profiles obtained by iCIEF/UV. Furthermore, high-resolution TOF mass spectra with high mass accuracy were obtained for the eight charge variants separated by CIEF/MS after IdeS cleavage and for the 11 charge variants after IdeS digestion with subsequent DTT reduction. In-depth analyses revealed the identities of all charge variants and pinpointed the causes of charge heterogeneity, which are in accord with those reported in the literature. The main sources of charge heterogeneity of cetuximab were identified as terminal lysine on the Fc domain (up to one on each single-chain Fc), glycolylneuraminic acid residues on the Fd’ domain (up to two on each Fd’), and likely several deamidation species on the Fd’ domain. No charge heterogeneity contribution was found from light chain. The in-depth characterization of complex charge variants for cetuximab demonstrates the remarkable capability of this middle up CIEF/MS approach. This novel workflow holds great potential for detecting and elucidating charge variants to help understand proteins with complex charge heterogeneity.",
        "keywords": "",
        "released": 2018,
        "link": "https://doi.org/10.1021/acs.analchem.8b04396"
    },
    {
        "title": "MTES: Visual programming environment for teaching and research in image processing",
        "abstract": "In this paper, we present a visual-programming environment for teaching and research referred to as “MTES”. MTES(Multimedia Education System) is the system designed to support both lecture and laboratory experiment simultaneously in one environment. It provides tools to prepare online teaching materials for the lecture and the experiment. It also provides a suitable teaching environment where lecturers can present the online teaching materials effectively and demonstrate new image processing concepts by showing real examples. In the same teaching environment, students can carry out experiments interactively by following the online instruction prepared by lecturer. In addition to support for the image processing education, MTES is also designed to assist research and application development with visual-programming environment. By supporting both teaching and research, MTES provides an easy bridge between learning and developing applications.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000230414400129"
    },
    {
        "title": "Online annotation tool for digital mammography",
        "abstract": "Rationale and Objectives. To develop a software tool for radiologists to annotate mammograms online. Materials and Methods. The tool is web-based with a Java development environment and composed of a Digital Image and Communication in Medicine (DICOM) image viewer, a clinical information collector, and a database. The use of the tool with a sample case is demonstrated in this article. Results. An online tool for the annotation of digital mammograms for teaching and research purposes has been developed. Conclusion. This annotation tool can be used to provide an annotated-case library on mammography education for residents and fellows. (C) AUR, 2004.",
        "keywords": "annotation; DICOM; online; digital mammography; teaching file",
        "released": 2004,
        "link": "https://doi.org/10.1016/S1076-6332(03)00726-8"
    },
    {
        "title": "RiceMapEngine: A google earth engine-based web application for fast paddy rice mapping",
        "abstract": "Mapping rice area is a critical resource planning task in many South Asia countries where rice is the primary crop. Remote-sensing-based methods typically rely on domain knowledge, such as crop calendar and crop phenology, and supervised classification with ground truth samples. Applying such methods on Google Earth engine (GEE) has been proven effective, especially at large scale owing to the comprehensive and up-to-date data catalog and massive server-side processing power. However, writing scripts through the code editor requires users to program in JavaScript and understand GEE application programming interface (API), which can be challenging for many researchers. Thus, this article presents a GEE-based web application that aims to eliminate the programming requirements for data selection, preprocessing, and visualizations so that users can easily produce rice maps and refine ground truth collections through intuitive graphical user interfaces (GUI). This software includes three submodule apps, namely the ground truth collection app, threshold-based rice mapping app, and classification-based rice mapping app. Users can customize data processing flow using GUI designed with Bootstrap, and the backend server uses GEE Python API, and a Google service account for authentication to execute the workflow on Google cloud servers. The experiment shows that both the overall accuracy and Kappa scores of the mapping result are higher than 0.9, which suggests that RiceMapEngine can significantly reduce the complexity and time costs it takes to produce the accurate rice area maps and meet the demands of real-world stakeholders.",
        "keywords": "Google Earth engine (GEE); ground truth; land use land cover; rice mapping",
        "released": 2023,
        "link": "https://doi.org/10.1109/JSTARS.2023.3290677"
    },
    {
        "title": "Translating statistical species-habitat models to interactive decision support tools",
        "abstract": "Understanding species-habitat relationships is vital to successful conservation, but the tools used to communicate species-habitat relationships are often poorly suited to the information needs of conservation practitioners. Here we present a novel method for translating a statistical species-habitat model, a regression analysis relating ring-necked pheasant abundance to landcover, into an interactive online tool. The Pheasant Habitat Simulator combines the analytical power of the R programming environment with the user-friendly Shiny web interface to create an online platform in which wildlife professionals can explore the effects of variation in local landcover on relative pheasant habitat suitability within spatial scales relevant to individual wildlife managers. Our tool allows users to virtually manipulate the landcover composition of a simulated space to explore how changes in landcover may affect pheasant relative habitat suitability, and guides users through the economic tradeoffs of landscape changes. We offer suggestions for development of similar interactive applications and demonstrate their potential as innovative science delivery tools for diverse professional and public audiences.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1371/journal.pone.0188244"
    },
    {
        "title": "TubeDB: An on-demand processing database system for climate station data",
        "abstract": "Geographers, ecologists, and other environmental scientists are typically required to utilise non-continuous measurements from various types of sensors as part of their research activities. However, data management and processing require advanced computer skills and specific knowledge of the measurement sensors. Here, we present the Tube Database (TubeDB), an easy-to-operate software system to archive, quality control, query, and further process time-series data in an efficient manner. Data are imported by loading any unprocessed raw formats as recorded by climate stations into TubeDB. When a user requests data, a query to an on-demand processing unit is created that builds an individual processing flow with the raw data as a source. The processing flow builds on the typical preparation steps for time-series data, such as temporal aggregation, quality checks, and the interpolation of missing values and transformations. The primary user interface is a web-based application to allow easy access, rapid visualisations of time-series within seconds, and powerful product-export functionality. We also provide an R package (rTubeDB) for seamless connection to the R programming environment. TubeDB enables data requesters to assemble individually processed sets of time-series within a web-browser without requiring sensor-specific knowledge or experience in time-series processing. This complements existing time-series databases, which miss climate data-specific processing and visualisation capabilities. As a self-contained application with an embedded database and webserver, TubeDB has both low hardware demands and low installation complexity. TubeDB enables easy access to time-series data for a wide range of non computer scientists and will be useful for many research activities.",
        "keywords": "Climatology; Data processing; Interpolation; Software engineering; Time-series database; Visualisation",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.cageo.2020.104641"
    },
    {
        "title": "VoiceXML and the voice/web environment - visual programming tools, for telephone application development",
        "abstract": "",
        "keywords": "",
        "released": 2001,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000170809000024"
    },
    {
        "title": "Networked control and supervision system based on LonWorks fieldbus and intranet/internet",
        "abstract": "A networked control and supervision system (NCSS) based on LonWorks fieldbus and Intranet/Internet was designed, which was composed of the universal intelligent control nodes (ICNs), the visual control and supervision configuration platforms (VCCP and VSCP) and an Intranet/Internet-based remote supervision platform (RSP). The ICNs were connected to field devices, such as sensors, actuators and controllers. The VCCP and VSCP were implemented by means of a graphical programming environment and network management so as to simplify the tasks of programming and maintaining the ICNs. The RSP was employed to perform the remote supervision function, which was based on a three-layer browser/server(B/S) structure mode. The validity of the NCSS was demonstrated by laboratory experiments.",
        "keywords": "fieldbus; LonWorks; networked control; visual control configuration; Web database",
        "released": 2007,
        "link": "https://doi.org/10.1007/s11771-007-0052-x"
    },
    {
        "title": "Terrain analysis in google earth engine: A method adapted for high-performance global-scale analysis",
        "abstract": "Terrain analysis is an important tool for modeling environmental systems. Aiming to use the cloud-based computing capabilities of Google Earth Engine (GEE), we customized an algorithm for calculating terrain attributes, such as slope, aspect, and curvatures, for different resolution and geographical extents. The calculation method is based on geometry and elevation values estimated within a 3 x 3 spheroidal window, and it does not rely on projected elevation data. Thus, partial derivatives of terrain are calculated considering the great circle distances of reference nodes of the topographic surface. The algorithm was developed using the JavaScript programming interface of the online code editor of GEE and can be loaded as a custom package. The algorithm also provides an additional feature for making the visualization of terrain maps with a dynamic legend scale, which is useful for mapping different extents: from local to global. We compared the consistency of the proposed method with an available but limited terrain analysis tool of GEE, which resulted in a correlation of 0.89 and 0.96 for aspect and slope over a near-global scale, respectively. In addition to this, we compared the slope, aspect, horizontal, and vertical curvature of a reference site (Mount Ararat) to their equivalent attributes estimated on the System for Automated Geospatial Analysis (SAGA), which achieved a correlation between 0.96 and 0.98. The visual correspondence of TAGEE and SAGA confirms its potential for terrain analysis. The proposed algorithm can be useful for making terrain analysis scalable and adapted to customized needs, benefiting from the high-performance interface of GEE.",
        "keywords": "topographic surface; terrain modeling; global terrain dataset",
        "released": 2020,
        "link": "https://doi.org/10.3390/ijgi9060400"
    },
    {
        "title": "FORTH - a PROGRAMMING ENVIRONMENT FOR ONLINE INSTRUMENTATION .1. UV-VIS SPECTROPHOTOMETER",
        "abstract": "",
        "keywords": "",
        "released": 1985,
        "link": "https://doi.org/10.1016/0165-9936(85)88023-7"
    },
    {
        "title": "From experience: Linking available resources and technologies to create a solution for document sharing - the early years of the WWW",
        "abstract": "We live and work in an era when seemingly every waking minute brings an invitation to visit yet another site on the World Wide Web. Inundated with calls to check out www.whatever.com, we can easily, lose sight of the fact that the birth of the Web is part of our very recent history. And notwithstanding the proliferation of personal and corporate vanity pages on the Web, this world of hyperlinks and hot spots has brought about dramatic changes in the means by which many individuals and organizations communicate, work, and trade. For new-products professionals interested in the processes that give rise to radical innovations, what lessons can be learned from the development of the World Wide Web? To gain insight into the process that resulted in the development of the first Web browser and Web sewer software, Ari-Pekka Hameri and Markus Nordberg examined project proposals, e-mail exchanges, and other documentation, and they interviewed key personnel who were involved in the process. From this research, they were able to document the process and the environmental constraints that shaped the development and diffusion of the tools and technologies that form the foundation of the World Wide Web. Although a staggeringly diverse range of Web-related services and applications have surfaced during the past few years, the original Web tools and technologies were targeted for a relatively small, focused community of researchers. Specifically, the origins of the World Wide Web are found in efforts aimed at meeting the information-sharing needs of researchers in the realm of high-energy physics. This far-flung group required a global network that could facilitate the interchange of documents stored in diverse formats on a wide variety of computing platforms. Meeting those needs required neither excessive R&D investments nor radically new core technologies. Instead, the solution involved the integration of existing technologies-networking tools and protocols, document formats, and desktop applications and development tools-by an innovator who had both the necessary vision and firsthand knowledge of the practical benefits that were needed. From a management perspective, perhaps the key lesson learned involves giving R&D personnel both the freedom and the support to initiate new projects and studies. (C) 1998 Elsevier Science Inc.",
        "keywords": "",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0737-6782(97)00083-0"
    },
    {
        "title": "Tool-supported interpreter-based user interface architecture for ubiquitous computing",
        "abstract": "With the upcoming era of Ubiquitous Computing (UbiComp) new demands on software engineering will arise. Fundamental needs for constructing user interfaces (UIs) in the context of UbiComp were identified and the subsumed results of a survey with special focus on model based user interface development environments (MB-UIDEs) are presented in this paper. It can be stated, that none of the examined systems is suitable for all the needs. Therefore a new architecture based on the Arch model is proposed, that supports the special UbiComp requirements. This layered architecture provides the desired flexibility with respect to different implementation techniques and UI modalities. It was implemented in a user interface development environment called Vesuf. Its usability was approved within the Global Info project [20], where heterogeneous services had to be integrated in a web portal.",
        "keywords": "",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000181620200007"
    },
    {
        "title": "An ontological analysis on recursive reusability of comprehensive web service in project management software",
        "abstract": "As information technology projects are creeping, it is essential to manage and maintain those projects using various project management tools. Most of the project management tools are designed to cater to the requirements which are laid down by the industry, but creating the project management tool from scratch is a cumbersome job for the IT industry thus most of the IT industries are now focusing of designing web services which can be consumed by the project management companies at large based on the requirement of the project selected web services can be consumed by the enterprise requesting the project management modules. As the number of projects is creeping and the same codes are being used in different projects, the code archive must be maintained, and the software should help the stakeholders using the software development tools to keep track of the reusability of the code. So that the efficiency of the project can be improved Some of the project management services such as task scheduling and resource allocation to the project can be recursively used and data repository can help in collection of relevant data items in the pool of the web services and segregate the information based on the project name and requirement of the project there is no need to rewrite these web services from scratch. Thus, such comprehensive web services can be grouped can and can be consumed by the enterprises as and when required for their project management activity. Archiving the project web services will help the project managers and the team members to run the web services live and will help the developers to contribute to development only on the required web service development environments.",
        "keywords": "PMS; WSDL; Web Services; SOAP; Project management and scheduling; project portfolio",
        "released": 2019,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000504422100017"
    },
    {
        "title": "A semantic web approach to simplifying trigger-action programming in the IoT",
        "abstract": "End-user programming environments for the IoT such as IFTTT rely on a multitude of low-level trigger-action rules that categorize devices and services by technology or brand. EUPont is a Semantic Web ontology that enables users to meet their needs with fewer, higher-level rules that can be adapted to different contextual situations and as-yet-unknown IoT devices and services.",
        "keywords": "end-user development; Internet of Things; IoT; Semantic Web; trigger-action programming; ontology; IFTTT; EUPont; semantic reasoning; Programming the World",
        "released": 2017,
        "link": "https://doi.org/10.1109/MC.2017.4041355"
    },
    {
        "title": "Studying the consistency of star ratings and reviews of popular free hybrid android and iOS apps",
        "abstract": "Nowadays, many developers make their mobile apps available on multiple platforms (e.g., Android and iOS). However, maintaining several versions of a cross-platform app that is built natively (i.e., using platform-specific tools) is a complicated task. Instead, developers can choose to use hybrid development tools, such as PhoneGap, to build hybrid apps. Hybrid apps are based on a single codebase across platforms. There exist two ways to use a hybrid development tool to build a hybrid app that runs on multiple platforms: (1) using web technologies (i.e., HTML, Javascript and CSS) and (2) in a common language, which is then converted to native code. We study whether these hybrid development tools achieve their main purpose: delivering an app that is perceived similarly by users across platforms. Prior studies show that users refer to star ratings and user reviews, when deciding to download an app. Given the importance of star ratings and user reviews, we study whether the usage of a hybrid development tool assists app developers in achieving consistency in the star ratings and user reviews across multiple platforms. We study 68 hybrid app-pairs, i.e., apps that exist both in the Google Play store and Apple App store. We find that 33 out of 68 hybrid apps do not receive consistent star ratings across platforms. We run Twitter-LDA on user reviews and find that the star ratings of the reviews that discuss the same topic could be up to three times as high across platforms. Our findings suggest that while hybrid apps are better at providing consistent star ratings and user reviews when compared to cross-platform apps that are built natively, hybrid apps do not guarantee such consistency. Hence, developers should not solely rely on hybrid development tools to achieve consistency in the star ratings and reviews that are given by users of their apps. In particular, developers should track closely the ratings and reviews of their apps across platforms, so that they can act accordingly when platform-specific issues arise.",
        "keywords": "Mobile apps; Star rating; User reviews; Twitter-LDA",
        "released": 2019,
        "link": "https://doi.org/10.1007/s10664-018-9617-6"
    },
    {
        "title": "Integrated development environment for EEG-driven cognitive-neuropsychological research",
        "abstract": "Background: EEG-driven research is paramount in cognitive-neuropsychological studies, as it provides a non-invasive window to the underlying neural mechanisms of cognition and behavior. A myriad collection of software and hardware frameworks has been developed to alleviate some of the technical barriers involved in EEG-driven research. Methods: we propose an integrated development environment which encompasses the entire technical “data-collection pipeline” of cognitive-neuropsychological research, including experiment design, data acquisition, data exploration and analysis in a state-of-the-art user interface. Our framework is based on a unique integration between a python-based web framework, time-oriented databases and object-based data schemes. Results: we demonstrated our framework with the recording and analysis of an n-Back task completed by 15 elderly (ages 50 to 80) participants. This case study demonstrates the highly utilized nature of our integrated framework with a challenging target population. Furthermore, our results may provide new insights into the correlation between brain activity and working memory performance in elderly people, who are prone to experience accelerated decline in executive prefrontal cortex functioning. Conclusion: our framework extends the range of EEG-driven experimental methods for assessing cognition available for cognitive-neuroscientists, allowing them to concentrate on the creative part of their work instead of technical aspects.",
        "keywords": "Emotiv; Python; working memory in elderly",
        "released": 2020,
        "link": "https://doi.org/10.1109/JTEHM.2020.2989768"
    },
    {
        "title": "MLweb: A toolkit for machine learning on the web",
        "abstract": "This paper describes MLweb, an open source software toolkit for machine learning on the web. The specificity of MLweb is that all computations are performed on the client side without the need to send data to a third-party server. MLweb includes three main components: a JavaScript API for scientific computing (LALOLib), an extension of this library with machine learning tools (ML.js) and an online development environment (LALOLab) with many examples. (C) 2017 Elsevier B.V. All rights reserved.",
        "keywords": "Machine learning; Classification; Regression; JavaScript",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.neucom.2017.11.069"
    },
    {
        "title": "Research and development of semantics-based sharable clinical pathway systems",
        "abstract": "The clinical pathway (CP) as a novel medical management schema is beneficial for reducing the length of stay, decreasing heath care costs, standardizing clinical activities, and improving medical quality. However, the practicability of CPs is limited by the complexity and expense of adding the standard functions of electronic CPs to existing electronic medical record (EMR) systems. The purpose of this study was to design and develop an independent clinical pathway (ICP) system that is sharable with different EMR systems. An innovative knowledge base pattern was designed with separate namespaces for global knowledge, local knowledge, and real-time instances. Semantic web technologies were introduced to support knowledge sharing and intelligent reasoning. The proposed system, which was developed in a Java integrated development environment, achieved standard functions of electronic CPs without modifying existing EMR systems and integration environments in hospitals. The interaction solution between the pathway system and the EMR system simplifies the integration procedures with other hospital information systems. Five categories of transmission information were summarized to ensure the interaction process. Detailed procedures for the application of CPs to patients and managing exceptional alerts are presented by explicit data flow analysis. Compared to embedded pathway systems, independent pathway systems feature greater feasibility and practicability and are more advantageous for achieving the normalized management of standard CPs.",
        "keywords": "Clinical pathways; Knowledge base; construction; Electronic medical record systems; Interactive process",
        "released": 2015,
        "link": "https://doi.org/10.1007/s10916-015-0257-8"
    },
    {
        "title": "Meta-analysis of the insulin degrading enzyme polymorphisms and susceptibility to alzheimer’s disease",
        "abstract": "The association between insulin degrading enzyme (IDE) gene polymorphisms and Alzheimer’s disease (AD) risk has been widely reported, but results were somewhat controversial. To assess the association between IDE polymorphisms and AD risk, a meta-analysis was performed. We systematically reviewed relevant studies retrieved by PubMed, Embase, AlzGene, CNKI and Web of Science. Finally, 8 articles were identified for rs3758505 polymorphism and 5 for rs1832196 polymorphism. The pooled ORs were performed for all the four genetic models. Subgroup analysis was also performed by ethnicity. Results suggested that rs3758505 polymorphism was unlikely to be associated with genetic susceptibility of AD based on the current published studies. However, for the rs1832196 polymorphism, significant association with AD was found by the dominant model in overall and subgroup analysis. However, larger scale association studies are necessary to further validate the association of IDE polymorphisms with sporadic AD risk and to define potential gene-gene interactions. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",
        "keywords": "Insulin degrading enzyme (IDE); Alzheimer’s disease (AD); Meta-analysis",
        "released": 2013,
        "link": "https://doi.org/10.1016/j.neulet.2013.01.051"
    },
    {
        "title": "An adaptive fault detector strategy for scientific workflow scheduling based on improved differential evolution algorithm in cloud",
        "abstract": "With the increasing popularity and acceptance of cloud computing, it is being applied in services like executing large-scale applications, where cloud environment is selected by the scientific associations to easily execute the computation intensive workflows. However, cloud computing can have higher failure rates due to the larger number of servers and components filled with the intensive workloads. These failures may lead to the unavailability of virtual machines (VMs) for computation. Hence, this issue of fault occurrences can be tolerated by adopting an effective and efficient fault tolerant strategy. The goal of our research in this paper is to develop an adaptive fault detector strategy based on Improved Differential Evolution (IDE) algorithm in cloud computing that can minimize the energy consumption, the makespan, the total cost and, at the same time, tolerate up faults when scheduling scientific workflows. This proposed work applies an adaptive network-based fuzzy inference system (ANFIS) prediction model to proactively control resource load fluctuation that increases the failure prediction accuracy before fault/failure occurrence. In addition, it applies a reactive fault tolerance technique for when a processor fails and the scheduler must allocate a new VM to execute the workflow tasks. The experimental results show that compared with existing techniques, the proposed approach significantly improves the overall scheduling performance, achieves a higher degree of fault tolerance with high HyperVolume (HV) compared with the ICFWS, IDE, and ACO algorithms, minimizes the makespan, the energy consumption and task fault ratio, and reduces the total cost. (C) 2020 Elsevier B.V. All rights reserved.",
        "keywords": "Cloud computing; Workflow scheduling; Fault tolerance; Migration; ANIFS",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.asoc.2020.106895"
    },
    {
        "title": "Wavelet fuzzy neural network with asymmetric membership function controller for electric power steering system via improved differential evolution",
        "abstract": "A wavelet fuzzy neural network using asymmetric membership function (WFNN-AMF) with improved differential evolution (IDE) algorithm is proposed in this study to control a six-phase permanent magnet synchronous motor (PMSM) for an electric power steering (EPS) system. First, the dynamics of a steer-by-wire EPS system and a six-phase PMSM drive system are described in detail. Moreover, the WFNN-AMF controller, which combines the advantages of wavelet decomposition, fuzzy logic system, and asymmetric membership function (AMF), is developed to achieve the required control performance of the EPS system for the improvement of stability of the vehicle and the comfort of the driver. Furthermore, the online learning algorithm of WFNN-AMF is derived using back-propagation method. However, degenerated or diverged responses will be resulted due to the inappropriate selection of small or large learning rates of the WFNN-AMF. Therefore, an IDE algorithm is proposed to online adapt the learning rates of WFNN-AMF. In addition, a 32-bit floating-point digital signal processor, TMS320F28335, is adopted for the implementation of the proposed intelligent controlled EPS system. Finally, the feasibility of the proposed WFNN-AMF controller with IDE for the EPS system is verified through experimental results.",
        "keywords": "Asymmetric membership function (AMF); differential evolution (DE); electric power steering (EPS); six-phase permanent magnet synchronous motor (PMSM); wavelet fuzzy neural network (WFNN)",
        "released": 2015,
        "link": "https://doi.org/10.1109/TPEL.2014.2327693"
    },
    {
        "title": "Upgrading a TEXTOR data acquisition system for remote participation using java and corba",
        "abstract": "The partners in the Trilateral Euregio Cluster (TEC) are implementing and developing Remote Participation technologies that are expected to support a joint research programme on the experimental facility TEXTOR-94. A common TEC architecture for our heterogeneous data acquisition and storage systems is seen to be one of the major issues. As a consequence, legacy systems will be affected and have to be upgraded for optimised wide area network communication, platform independent data access and display. The object oriented redesign of the system to be described follows theses guidelines. The architecture of the system under development uses Java as programming environment and CORBA as Client/Server communication standard. It is described in this paper, how an operational Data Acquisition CAMAC subsystem of TEXTOR-94 based on OpenVMS and Decnet communications could be redesigned into an open, object oriented architecture in a platform independent way. A suitable Web Browser is required on the client side without further installation of application software to run the server. CORBA static method invocations are used for the communication between the client and server. At the server side, there is only Java code on top of the existing commercial OpenVMS CAMAC device driver. A modular object oriented software design permitted to eliminate dependencies of the generic module levels from the underlying bus systems. Porting of the Java code to other platforms like Windows NT and Linux has proven to be successful. (C) 2000 Elsevier Science S.A. All rights reserved.",
        "keywords": "TEXTOR; data acquisition; remote participation",
        "released": 2000,
        "link": "https://doi.org/10.1016/S0920-3796(00)00138-1"
    },
    {
        "title": "Teaching object-oriented programming with AEIOU",
        "abstract": "AEIOU is an integrated programming environment developed to support the teaching-learning process of object-oriented programming in a gradual way. AEIOU includes three modules, one for each level of students’ expertise: novice, intermediate, and advanced. This article describes the features of each module of AEIOU and the experiences and results obtained using this programming environment to support the sequence of programming courses in two Mexican engineering schools. (c) 2011 Wiley Periodicals, Inc. Comput Appl Eng Educ 22:309-319, 2014; View this article online at ; DOI",
        "keywords": "teaching programming; programming environment; object-oriented programming",
        "released": 2014,
        "link": "https://doi.org/10.1002/cae.20556"
    },
    {
        "title": "Development of a 3D-LC/MS workflow for fast, automated, and effective characterization of glycosylation patterns of biotherapeutic products",
        "abstract": "Glycosylation is a common post-translational modification of therapeutic monoclonal antibodies produced in mammalian cells and is considered an important critical quality attribute (CQA), as it is known to impact efficacy, stability, half-life, and immunogenicity. For these reasons, glycosylation requires characterization and close monitoring during the manufacturing process. Due to the complexity of the glycosylation patterns, sophisticated analytical tools with high resolving power are required for the characterization of the glycoforms. This study describes, for the first time, the development and use of an online three-dimensional high-performance liquid chromatography/mass spectrometry (3D-HPLC/MS) approach for the monitoring of glycosylation patterns at the middle-up level. An immobilized IdeS-enzyme column was used in the first dimension for the digestion of mAbs in 10 min. Then, following an online reversed phase liquid chromatography (RPLC) column reduction, the approximate to 25 kDa proteolytic fragments were analyzed using hydrophilic interaction chromatography (HILIC) coupled to MS. This novel analytical workflow demonstrated the ability to accurately profile glycosylated variants within a total run time of 95 min. To compare the performance of this analytical strategy with a conventional offline procedure (IdeS digestion x reduction-HILIC/MS), a proof of concept study using two mAbs is described here.",
        "keywords": "",
        "released": 2020,
        "link": "https://doi.org/10.1021/acs.analchem.9b05193"
    },
    {
        "title": "Intelligent assessment and prediction system for somatic fitness and healthcare using machine learning",
        "abstract": "This study proposes a health assessment and predictive assistance system for intelligent health monitoring. Through machine learning, the tool features a customized set of quantitative measurements and web analysis systems for physical and mental fitness. The system replaces the manpower and time requirements of the past necessary to conduct interviews and keep paper records, allowing users to observe and analyze physical and mental fitness status through the webpage. To achieve this, ECG, EEG, and EMAS are used to follow physiological, psychological, and meridian energy states. ASP.NET software is used as a development tool for the system cloud page, which constructs, documents, evaluates, and predicts functions for the smart health assistance system. The measurement data is entered and recorded in the cloud database. The data is used to construct an assessment and prediction of the user’s state of mind and body through machine learning methods, as well as the individual’s physical and mental fitness.",
        "keywords": "Intelligent assessment; intelligent prediction; somatic fitness; healthcare; machine learning",
        "released": 2021,
        "link": "https://doi.org/10.3233/JIFS-189618"
    },
    {
        "title": "Using SWISH to realize interactive web-based tutorials for logic-based languages",
        "abstract": "Programming environments have evolved from purely text based to using graphical user interfaces, and now we see a move toward web-based interfaces, such as Jupyter. Web-based interfaces allow for the creation of interactive documents that consist of text and programs, as well as their output. The output can be rendered using web technology as, for example, text, tables, charts, or graphs. This approach is particularly suitable for capturing data analysis workflows and creating interactive educational material. This article describes SWISH, a web front-end for Prolog that consists of a web server implemented in SWI-Prolog and a client web application written in JavaScript. SWISH provides a web server where multiple users can manipulate and run the same material, and it can be adapted to support Prolog extensions. In this article we describe the architecture of SWISH, and describe two case studies of extensions of Prolog, namely Probabilistic Logic Programming and Logic Production System, which have used SWISH to provide tutorial sites.",
        "keywords": "Prolog; logic programming system; notebook interface; web",
        "released": 2019,
        "link": "https://doi.org/10.1017/S1471068418000522"
    },
    {
        "title": "JSPatcher, a visual programming environment for building high-performance web audio applications",
        "abstract": "Many visual programming languages (VPLs), which include Max or PureData, provide a graphic canvas for connecting between functions or data. This canvas, also called a patcher, is basically a graph meant to be interpreted as a dataflow computation by the system. Some VPLs are used for multimedia performance or content generation because the user interface system is generally a significant element of the language. This paper presents a Web-based VPL, JSPatcher, which allows users to build audio graphs using the Web Audio API. Users can use aWeb browser to graphically design and run digital signal processor algorithms using domain-specific languages for audio processing, such as FAUST or Gen, and execute them in a dedicated high-priority thread called AudioWorklet. The application can also be utilized to create interactive programs and shareable artworks online with other JavaScript language built-ins, Web APIs, Web-based audio plugins, or external JavaScript modules.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.17743/jaes.2022.0056"
    },
    {
        "title": "CESARSC: Framework for creating cultural entertainment systems with augmented reality in smart cities",
        "abstract": "The areas of application for augmented reality technology are heterogeneous but the content creation tools available are usually single-user desktop applications. Moreover, there is no online development tool that enables the creation of such digital content. This paper presents a framework for the creation of Cultural Entertainment Systems and Augmented Reality, employing cloud-based technologies and the interaction of heterogeneous mobile technology in real time in the field of mobile tourism. The proposed system allows players to carry out a series of games and challenges that will improve their tourism experience. The system has been evaluated in a real scenario, obtaining promising results.",
        "keywords": "Cultural Entertainment System; Augmented Reality; GPS locations",
        "released": 2016,
        "link": "https://doi.org/10.2298/CSIS150620006G"
    },
    {
        "title": "ALGORITHMIC LITERACY: Generative artificial intelligence technologies for data librarians",
        "abstract": "INTRODUCTION: Artificial intelligence (AI) is a novel type of library technology. AI technologies and the needs of data librarians are hybrid and symbiotic, because academic libraries must insert AI technologies into their information and data services. Library services need AI to interpret the context of big data. OBJECTIVES: In this context, we explore the use of the OpenAI Codex, a deep learning model trained on Python code from repositories, to generate code scripts for data librarians. This investigation examines the practices, models, and methodologies for obtaining code script insights from complex code environments linked to AI GPT technologies. METHODS: The proposed AI-powered method aims to assist data librarians in creating code scripts using Python libraries and plugins such as the integrated development environment PyCharm, with additional support from the Machinet AI and Bito AI plugins. The process involves collaboration between the data librarian and the AI agent, with the librarian providing a natural language description of the programming problem and the OpenAI Codex generating the solution code in Python.RESULTS: Five specific web-scraping problems are presented. The scripts demonstrate how to extract data, calculate metrics, and write the results to files. CONCLUSION: Overall, this study highlights the application of AI in assisting data librarians with code script creation for web scraping tasks. AI may be a valuable resource for data librarians dealing with big data challenges on the Web. The possibility of creating Python code with AI is of great value, as AI technologies can help data librarians work with various types of data sources. The Python code in Data Science web scraping projects uses a machine-learning model that can generate human-like code to help create and improve the library service for extracting data from a web collection. The ability of nonprogramming data librarians to use AI technologies facilitates their interactions with all types and data sources. The Python programming language has artificial intelligence modules, packages, and plugins such as the OpenAI Codex, which serialises automation and navigation in web browsers to simulate human behaviour on pages by entering passwords, selecting captcha options, collecting data, and creating different collections of datasets to be viewed.",
        "keywords": "Generative Pre-trained Transformer; Algorithmic Literacy; Python; Open AI; Data Librarian",
        "released": 2024,
        "link": "https://doi.org/10.4108/eetsis.4067"
    },
    {
        "title": "A PARTICULATE CONTRAST AGENT WITH POTENTIAL FOR ULTRASOUND IMAGING OF LIVER",
        "abstract": "Ultrasonic backscatter and attenuation coefficients of a medium can be increased by the addition of solid, micron sized inhomogeneities. A potentially useful agent for ultrasonic contrast of liver images has been identified. Iodipamide ethyl ester (IDE) particles can be produced in the form of dense, relatively incompressible solids with high impedance mismatch to water. The chemical, biochemical, and pharmacological properties of the small, uniform diameter IDE particles permit safe intravenous injection followed by rapid accumulation by reticuloendothelial (RE) cells of the liver and spleen, and later elimination from these organs. Since the particles are phagocytized by RE cells, present in normal liver but not in tumors and many lesions, the selective enhancement of ultrasonic backscatter should improve detectability of lesions which are hypo- or iso-echoic compared to surrounding tissue. The mechanisms of particle-ultrasound interactions may be described by relative motion attenuation, and scattering from a cloud of dense, incompressible spheres for the case of IDE particles in agar. Thus, values of attenuation and backscatter can be controlled by choice of ultrasound frequency and particle concentration and size. When the particles are accumulated in rat livers, additional mechanisms induce attenuation and backscatter in excess of that predicted by IDE in agar. This preliminary work demonstrates that solid, biocompatible particles may be useful as an ultrasonic contrast agent.",
        "keywords": "",
        "released": 1987,
        "link": "https://doi.org/10.1016/0301-5629(87)90181-5"
    },
    {
        "title": "Atmosphere, an open source measurement-oriented data framework for IoT",
        "abstract": "The ever more extensive data collection from Internet of Thing (IoT) devices stresses the need for efficient application development tools. State-of-the-art IoT cloud services are powerful, but the best solutions are proprietary, and there is a growing demand for interoperability and standardization. We have investigated how to develop a nonvendor-locked framework, which exploits state of the art data management technologies, and targets effective and efficient development in this article. Focusing on the concept of measurement, we abstracted an architecture that could be applied in a variety of domains and contexts. We tested the framework and its workflow in four use cases analyzing data and enabling new services in health, automotive, and instruction. Our experience showed the benefits of the development tool, which is not tied to a commercial platform, nor requires the huge set-up times needed to start a project from scratch. The tool is released open-source, particularly supporting collaborative research.",
        "keywords": "Cloud computing; Tools; Atmospheric modeling; Atmospheric measurements; Scalability; Databases; Application programming interface (API); autonomous driving; cloud; e-health; e-mobility; Internet of Thing (IoT); NoSQL database; open source; representational state transfer (REST)ful; smart city",
        "released": 2021,
        "link": "https://doi.org/10.1109/TII.2020.2994414"
    },
    {
        "title": "Integrated design of graduate education information system of universities in digital campus environment",
        "abstract": "This study takes the digital campus construction planning of the high school as an example and determines the requirements of the postgraduate management information system under the digital campus environment through the analysis of the overall framework and technology of the digital campus. Combining the current situation of computer technology, network technology, and the actual situation of our university, the current mainstream B/S three-layer architecture is adopted, the web adopts the current popular Java Server Pages technology, and the struts framework connects to the Oracle backend database through the Java Database Connectivity interface to design the browser-side and server-side programs. The struts framework connects to the Oracle backend database through the Java Database Connectivity interface to design browser-side and serverside programs. The functional model and data flow model of the system were established through a detailed and effective analysis of the entire workflow of postgraduate students’ training management during their school years. Then, the system analysis, design, and drawing of the swim lane diagram and data business flow diagram were carried out. The system was designed in detail in terms of system architecture, development tools, functional modules, and database design, and the core module of training program making in postgraduate training management was highlighted as an example to discuss the principles and methods in the construction of departmental business systems and informatization under the digital campus environment, and a flexible and efficient postgraduate management information system was realized. It standardizes the construction of data standardization in universities; does a good job of standardizing and normalizing information; improves the accuracy, validity, and real-time production of data collection and the real and safe unified management of historical data; and provides scientific and reasonable data support for the leadership to make relevant decisions.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1155/2021/8357488"
    },
    {
        "title": "Supporting the development of interactive applications in introductory programming courses",
        "abstract": "Extended Library for Visual Interactive Applications (ELVIA) is a programming tool developed and used by the two most important public universities in the Baja California peninsula in Mexico. ELVIA provides a Java class framework that helps novice students of programming to automatically generate the graphical user interface of interactive applications, focusing in the classes and objects that compose the applications. This paper describes ELVIA, some application examples and experiences applying this programming tool in introductory programming courses. (C) 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 20: 214220, 2012: 214-220, 2012; View this article online at wileyonlinelibrary.com/journal/cae; DOI 10.1002/cae.20387",
        "keywords": "graphical user interface; interactive application; introductory programming course",
        "released": 2012,
        "link": "https://doi.org/10.1002/cae.20387"
    },
    {
        "title": "As easy as “click”:: End-user web engineering",
        "abstract": "We are investigating the feasibility of end-user web engineering. The main target audience for this research is webmasters without programming experience - a group likely to be interested in building web applications. Our target domain is web-based data collection and management applications. As an instrument for studying the mental models of our audience and collecting requirements for an end-user web programming tool, we are developing Click, a proof-of-concept prototype. We discuss end-user related aspects of web engineering in general and describe the design rationale for Click. In particular, we elaborate on the need for supporting evolutionary prototyping and opportunistic and ad hoc development goals. We also discuss strategies for making end-user web engineering scalable and for encouraging end-user developers to continually increase their level of sophistication.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000231171300057"
    },
    {
        "title": "FORTH - a PROGRAMMING ENVIRONMENT FOR ONLINE INSTRUMENTATION .2. FLOATING POINT MATRIX PACKAGE",
        "abstract": "",
        "keywords": "",
        "released": 1985,
        "link": "https://doi.org/10.1016/0165-9936(85)85006-8"
    },
    {
        "title": "Communicating best environmental practices throughout an organization",
        "abstract": "BP Amoco’s Drilling Environmental Awareness Program, an intranet-based Web site, has been compiled to share current best environmental practices in drilling operations. The site pro,ides a single-point resource for related environmental guidelines and strategies to ensure information and technology transfer and enable knowledge sharing. It allows drilling engineers anywhere in the organization to access relevant information for their current operation and compare current or proposed operating practices with those from other similar areas to provide assurance and generate new options for improvement.",
        "keywords": "",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000089125600013"
    },
    {
        "title": "Exploring the mathematics of motion through construction and collaboration",
        "abstract": "In this paper, we give a detailed account of the design principles and construction of activities designed for learning about the relationships between position, velocity and acceleration, and corresponding kinematics graphs. Our approach is model-based, that is, it focuses attention on the idea that students constructed their own models-in the form of programs-to formalise and thus extend their existing knowledge. In these activities, students controlled the movement of objects in a programming environment, recording the motion data and plotting corresponding position-time and velocity-time graphs. They shared their findings on a specially designed Web-based collaboration system, and posted cross-site challenges to which others could react. We present learning episodes that provide evidence of students making discoveries about the relationships between different representations of motion. We conjecture that these discoveries arose from their activity in building models of motion and their participation in classroom and online communities.",
        "keywords": "communication; mathematics; modelling; programming; science",
        "released": 2006,
        "link": "https://doi.org/10.1111/j.1365-2729.2006.00164.x"
    },
    {
        "title": "Accelerating r-based analytics on the cloud",
        "abstract": "This paper addresses how the benefits of cloud-based infrastructure can be harnessed for analytical workloads. Often, the software handling analytical workloads is not developed by a professional programmer but on an ad hoc basis by analysts in high-level programming environments such as R or MATLAB. The goal of this research is to allow Analysts to take an analytical job that executes on their personal workstations and with minimum effort execute it on cloud infrastructure and manage both the resources and the data required by the job. If this can be facilitated gracefully, then the Analyst benefits from on-demand resources, low maintenance cost and scalability of computing resources, all of which are offered by the cloud. In this paper, a Platform for Parallel R-based Analytics on the Cloud (P2RAC) that is placed between an Analyst and a cloud infrastructure is proposed and implemented. P2RAC offers a set of command-line tools for managing the resources, such as instances and clusters, the data and the execution of the software on the Amazon Elastic Computing Cloud infrastructure. Experimental studies are pursued using two parallel problems and the results obtained confirm the feasibility of employing P2RAC for solving large-scale analytical problems on the cloud.Copyright (c) 2013 John Wiley & Sons, Ltd.",
        "keywords": "cloud computing; data analytics; R-script; catastrophe bonds",
        "released": 2016,
        "link": "https://doi.org/10.1002/cpe.3026"
    },
    {
        "title": "An optimization method for integrating educational information resources based on edge computing",
        "abstract": "This paper adopts an edge computing approach to conduct in-depth research and analysis on the optimization of educational information resource integration and constructs an integrated teaching resource design model concerning the integrated teaching model, human-centered mobile learning resource design, and the interdisciplinary concept of physics subject teaching method. The learning field constituted by the model is divided into two parts, the explicit field and the potential field and then designs a five-stage teaching resource based on the model. A five-stage teaching resource development path was designed based on the model. Based on the cloud service center model, we propose a hierarchical mechanism for sharing educational information resources and analyze how each hierarchical entity constructs and shares resources and the rights and responsibilities of each hierarchical entity; we explain the meaning and functions of the personalized educational resource integrated development environment provided by the cloud service center for users. The dynamic evaluation model of the value of educational information resources is summarized and proposed for the resource exchange behavior in the sharing of educational information resources, and the significance of the calculation method of resource value, parameter values, and resource value difference for resource sharing is introduced. Firstly, the mechanism for coconstruction of educational information resources at different levels is proposed, and the construction tasks of subjects at different levels in the process of coconstruction and sharing of resources are elaborated. The sharing mechanism of educational information resources covering the regularization system, evaluation mechanism, incentive mechanism, problem handling mechanism, and supporting service mechanism is proposed, and a dynamic evaluation model of the value of educational information resources is designed to improve the enthusiasm of the coconstruction and sharing subjects.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1155/2022/9891117"
    },
    {
        "title": "Taking control of your engineering tools",
        "abstract": "The authors’ experience moving development tools to the cloud highlights the importance of six key principles for a modern cloud-based engineering system: determinism and idempotence, environment independence, consistency, composability and self-description, scalability, and measurability and control.",
        "keywords": "",
        "released": 2013,
        "link": "https://doi.org/10.1109/MC.2013.337"
    },
    {
        "title": "External memory algorithms and data structures: Dealing with massive data",
        "abstract": "Data sets in large applications are often too massive to fit completely inside the computer’s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this article we survey the state of the art in the design and analysis of external memory (or EM) algorithms and data structures, where the goal is to exploit locality in order to reduce the I/O costs. We consider a variety of EM paradigms for solving hatched and online problems efficiently in external memory. For the batched problem of sorting and related problems such as permuting and fast Fourier transform, the key paradigms include distribution and merging. The paradigm of disk striping offers an elegant way to use multiple disks in parallel. For sorting, however, disk striping can be nonoptimal with respect to I/O, so to gain further improvements we discuss distribution and merging techniques for using the disks independently. We also consider useful techniques for batched EM problems involving matrices (such as matrix multiplication and transposition), geometric data (such as finding intersections and constructing convex hulls), and graphs (such as list ranking, connected components, topological sorting, and shortest paths). In the online domain, canonical EM applications include dictionary lookup and range searching. The two important classes of indexed data structures are based upon extendible hashing and B-trees. The paradigms of filtering and bootstrapping provide a convenient means in online data structures to make effective use of the data accessed from disk. We also reexamine some of the above EM problems in slightly different settings, such as when the data items are moving, when the data items are variable-length (e.g., text strings), or when the allocated amount of internal memory can change dynamically. Programming tools and environments are available for simplifying the EM programming task. During the course of the survey, we report on some experiments in the domain of spatial databases using the TPIE system (transparent parallel I/O programming environment). The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.",
        "keywords": "algorithms; design; experimentation; performance; theory; batched; block; B-tree; disk; dynamic; extendible hashing; external memory; hierarchical memory; I/O; multidimensional access methods; multilevel memory; online; out-of-core; secondary storage; sorting",
        "released": 2001,
        "link": "https://doi.org/10.1145/384192.384193"
    },
    {
        "title": "A transformational approach to managing data model evolution of web services",
        "abstract": "The communication of web services is typically organized through public APIs, which rely on a common data model shared among all system components. Over time, this data model must be changed in order to accommodate new or changing requirements, and the system components including the data they are operating on must be migrated. In practice, however, not all the affected components can be migrated instantly and at the same time. A common approach is to plan data model changes in a backward compatible fashion, which eventually causes serious maintenance problems and is a common cause of technical debt. In this paper, we propose an alternative solution to this problem by using a translation layer serving as a round-trip migration service which is responsible for the lossless forth-and-back translation of object-oriented data model instances of different versions. We present a framework which offers a version-aware interface definition language (IDL) for APIs, a typed JavaScript-based language for defining migration functions using the IDL definition, and a run-time environment for executing migrations. This is bundled into an integrated development environment assisting developers in implementing migration functions. From a methodological point of view, the development of round-trip migrations is supported by a catalog which comprises a set of typical data model evolution scenarios along with corresponding suitable round-trip migration strategies. We validate our framework by carrying out an extensive evaluation including a systematic assessment of expressiveness using our catalog, micro-benchmarking the performance of round-trip migrations, as well as a practical application in a case study of a real-world e-commerce web application obtained from an industrial partner.",
        "keywords": "Data models; Web services; Object oriented modeling; Unified modeling language; Distributed databases; Benchmark testing; Systematics; service data models; web API evolution; model transformations",
        "released": 2023,
        "link": "https://doi.org/10.1109/TSC.2022.3144613"
    },
    {
        "title": "Gird manufacturing: A new solution for cross-enterprise collaboration",
        "abstract": "Industry has put forward demands on cross enterprises collaboration including various resources sharing. In order to meet these demands, much has been done to realize cross-border collaboration. However, after analyzing the unique features of manufacturing resources and the limits of existing technologies like Internet, EC, ASP and Web-based design/manufacturing, which adopt many new Web technologies like Web mark-up languages, Web-based client-server programming tools and distributed object modeling methods, it is concluded that all these technologies could not meet industrial demands on dynamic sharing of various resources. Therefore grid manufacturing is put forward to meet these demands. According to the features of resources to be shared and the technologies to be used, grid manufacturing distinguishes itself from Web-based manufacturing by providing the transient services and by achieving the interoperability. The differences and relationship between grid manufacturing and grid computing are yielded. At last, a Web service-based architecture of grid manufacturing is proposed and the related technologies are summarized.",
        "keywords": "grid manufacturing; grid computing; web services; web-based manufacturing",
        "released": 2008,
        "link": "https://doi.org/10.1007/s00170-006-0832-8"
    },
    {
        "title": "Unifying computing resources and access interface to support parallel and distributed computing education",
        "abstract": "This article presents how various on-site and remote computing resources are combined into a framework to support teaching parallel and distributed computing (PDC) at the undergraduate level. The combination of these resources enables the delivery of PDC programming, system, and architectural concepts via a browser-based common interface (JupyterHub) and a single programming environment (Python and its supported libraries). This also allows lecturers and students to focus more on the principles of PDC and less on the technicalities of native languages for different platforms. We describe how this framework can support a comprehensive set of PDC course modules, including lectures, assignments, and projects, for a full semester junior-level class. Adoption of this framework in various teaching environments at Clemson University has received positive feedback from both instructors and participants. Published by Elsevier Inc.",
        "keywords": "Computing resources; MPI; MapReduce; Spark; Python; Distributed systems; Parallel computing education",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.jpdc.2018.02.020"
    },
    {
        "title": "Hybrid VNS-LP algorithm for online optimal coordination of directional overcurrent relays",
        "abstract": "Optimal coordination of directional overcurrent relays (DOCRs) in interconnected power systems is a highly constrained, non-convex and non-linear optimisation problem. In this study, use of a series of linear programming (LP) problems in a variable neighbourhood search (VNS) framework is proposed, in which pickup current settings can be considered either as continuous or discrete variables. Therefore, the proposed hybrid algorithm (VNS-LP) is suitable for coordination problem-solving in a network with different types of DOCRs. Furthermore, two versions of the VNS algorithm are proposed, the first of which is very easy to implement, while, the other is somewhat more sophisticated, and at the same time much faster and possibly suitable for online coordination in smart grids applications. One advantage of the second algorithm is that it relies only on elementary arithmetic and requires low RAM, which makes it implementable in any low-level programming environment. This, in turn, can increase the interoperability, which is of crucial importance when automating tasks. The efficiency of the proposed algorithm was investigated in several test systems. Numerical results indicate that the two proposed approaches outperform the previous methods. Moreover, the coordination problem is solved in a reasonably short time using a faster approach.",
        "keywords": "smart power grids; power system interconnection; optimisation; overcurrent protection; relay protection; search problems; power system protection; online optimal coordination; directional overcurrent relays; DOCR; interconnected power systems; nonconvex optimisation problem; nonlinear optimisation problem; linear programming problems; variable neighbourhood search framework; pickup current settings; continuous variables; discrete variables; coordination problem-solving; VNS algorithm; online coordination; smart grids applications; low-level programming environment; hybrid VNS-LP algorithm; VNS framework; elementary arithmetic; RAM",
        "released": 2020,
        "link": "https://doi.org/10.1049/iet-gtd.2019.1811"
    },
    {
        "title": "A GUI for jess",
        "abstract": "The paper describes JessGUI, a graphical user interface developed on top of the Jess expert system shell. The central idea of the JessGUI project was to make building, revising, updating, and testing Jess-based expert systems easier, more flexible, and more user friendly. There are many other expert system building tools providing a rich and comfortable integrated development environment to expert system builders. However, they are all either commercial or proprietary products. Jess and JessGUI are open-source freeware, and yet they are well suited for building even complex expert system applications, both stand-alone and Web-based ones. An important feature of JessGUI is its capability of saving knowledge bases in XML format (in addition to the original Jess format), thus making them potentially easy to interoperate with other knowledge bases on the Internet. Jess and JessGUI are also used as practical knowledge engineering tools to support both introductory and advanced university courses on expert systems. The paper presents design details of JessGUI, explains its links with the underlying Jess knowledge representation and reasoning tools, and shows examples of using JessGUI in expert system development. It also discusses some of the current efforts in extending Jess/JessGUI in order to provide intelligent features originally not supported in Jess. (C) 2004 Elsevier Ltd. All rights reserved.",
        "keywords": "expert systems; development tools; graphical user interface; knowledge base interoperability",
        "released": 2004,
        "link": "https://doi.org/10.1016/j.eswa.2003.12.012"
    },
    {
        "title": "ANALYSIS AND DESIGN OF PROGRAMMATIC INTERFACES FOR INTEGRATION OF DIVERSE WEB CONTENTS",
        "abstract": "The technology that integrates various types of Web contents to build a new Web application through end-user programming is widely used nowadays. However, the Web contents do not have a uniform interface for accessing the data and computation. Most of the general Web users access information on the Web through applications until now. Hence, designing a uniform and flexible programmatic interface for integration of different Web contents is unavoidable. In this paper, we propose an approach that can be used to analyze Web applications automatically and reuse the information of Web applications through the programmatic interface we designed. Our approach can support the flexible integration of Web applications, Web services and Web feeds. In our experiments, we use a large number of Web pages from different types of Web applications and achieve the integration by the proposed programmatic interfaces. The experimental results show that our approach brings to the end-users a flexible and user-friendly programming environment.",
        "keywords": "Programmatic interface; integration; Web application; Web service; partial information extraction; end-user programming",
        "released": 2013,
        "link": "https://doi.org/10.1142/S0218194013500472"
    },
    {
        "title": "The international association for mathematical geology WWW/FTP site: An analysis of the first five years and some thoughts for the future",
        "abstract": "The International Association for Mathematical Geology (IAMG) has been operating an Internet site since 1994. The site was initially started as an FTP service for computer program code published in Computers & Geosciences followed by the establishment of a WWW site. The establishment of the FTP site required consideration for operating system support and data archive standards support. The implementation of the WWW site enabled the IAMG to provide information and services for all Internet users. Analysis of the access logs for both the FTP and WWW services have shown that there has been a steady increase in access to the programs and IAMG information. Long-term archival issues include dealing with programs available only in binary (executable) form and long-term support of compression formats (i.e. zip, tar). As computer programming environments become increasingly sophisticated the journal is faced with the challenge of insuring that the computer program code is useful to geoscientists. The Internet programming environment is becoming increasingly popular as a means of creating program code that is independent of user platform type and also allows programmers to maintain control over the use and development of their code on one site. As this type of environment develops the IAMG FTP site has started to create URL links to the program source sites. Computers viruses are an issue which requires the attention of all IAMG users. The Computers & Geosciences Silver CD project has been started which will contain all program code (raster or ASCII text) form for volumes 1-25. Future services on the IAMG web site will include a searchable membership database, list servers for special interest groups within the IAMG and constant updating of useful services and links to the IAMG membership. (C) 2000 Published by Elsevier Science Ltd. All rights reserved.",
        "keywords": "Internet; WWW; FTP; access statistics; program archive; computer programs",
        "released": 2000,
        "link": "https://doi.org/10.1016/S0098-3004(99)00096-5"
    },
    {
        "title": "A grid middleware for aggregating scientific computing libraries and parallel programming environments",
        "abstract": "Grids collect the geographically distributed heterogeneous resources and use their aggregated power to solve large-scale scientific problems. In order to aggregate scientific computing libraries and parallel programming environments, a Grid computing agent is introduced. The agent acts as the middleware of grid computing. It coordinates several parallel programming environments to support the heterogeneous parallel computing and integrates scientific computing libraries to deliver the efficient solution of large problems. The agent consists of four modules: (1) parallel compilation module for compiling, debugging, and distributing codes. (2) parallel execution module for resource scheduling, application program execution and result redirection. (3) scientific computing module as a graphical user interface which is implemented by integrating Matlab and NetSolve [1]. (4) resource management module providing a single system image for the whole system. Based on the agent, a web portal is developed which provides a uniform service entry for end users. This approach improves the system usability significantly.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000189504900073"
    },
    {
        "title": "Generation of realistic navigation paths for web site testing using RNN and GAN",
        "abstract": "For applications that have not yet been launched, a reliable way for creating online navigation logs may be crucial, enabling developers to test their products as though they were being used by real users. This might lead to faster and lower-cost program testing and enhancement, especially in terms of usability and interaction. In this work we propose a method for using deep learning approaches such as recurrent neural networks (RNN) and generative adversarial neural networks (GANN) to produce high-quality weblogs. Eventually, we can utilize the created data for automated testing and improvement of Web sites prior to their release with the aid of model-driven development tools such as IFML Editor.",
        "keywords": "Web engineering; deep learning; data mining; generative adversarial networks; recurrent neural networks; testing",
        "released": 2021,
        "link": "https://doi.org/10.13052/jwe1540-9589.20816"
    },
    {
        "title": "JMS: An open source workflow management system and web-based cluster front-end for high performance computing",
        "abstract": "Complex computational pipelines are becoming a staple of modern scientific research. Often these pipelines are resource intensive and require days of computing time. In such cases, it makes sense to run them over high performance computing (HPC) clusters where they can take advantage of the aggregated resources of many powerful computers. In addition to this, researchers often want to integrate their workflows into their own web servers. In these cases, software is needed to manage the submission of jobs from the web interface to the cluster and then return the results once the job has finished executing. We have developed the Job Management System (JMS), a workflow management system and web interface for high performance computing (HPC). JMS provides users with a user-friendly web interface for creating complex workflows with multiple stages. It integrates this workflow functionality with the resource manager, a tool that is used to control and manage batch jobs on HPC clusters. As such, JMS combines workflow management functionality with cluster administration functionality. In addition, JMS provides developer tools including a code editor and the ability to version tools and scripts. JMS can be used by researchers from any field to build and run complex computational pipelines and provides functionality to include these pipelines in external interfaces. JMS is currently being used to house a number of bioinformatics pipelines at the Research Unit in Bioinformatics (RUBi) at Rhodes University. JMS is an open-source project and is freely available at https://github.com/RUBi-ZA/JMS.",
        "keywords": "",
        "released": 2015,
        "link": "https://doi.org/10.1371/journal.pone.0134273"
    },
    {
        "title": "A cloud-based data storage and visualization tool for smart city IoT: Flood warning as an example application",
        "abstract": "Collecting, storing, and providing access to Internet of Things (IoT) data are fundamental tasks to many smart city projects. However, developing and integrating IoT systems is still a significant barrier to entry. In this work, we share insights on the development of cloud data storage and visualization tools for IoT smart city applications using flood warning as an example application. The developed system incorporates scalable, autonomous, and inexpensive features that allow users to monitor real-time environmental conditions, and to create threshold-based alert notifications. Built in Amazon Web Services (AWS), the system leverages serverless technology for sensor data backup, a relational database for data management, and a graphical user interface (GUI) for data visualizations and alerts. A RESTful API allows for easy integration with web-based development environments, such as Jupyter notebooks, for advanced data analysis. The system can ingest data from LoRaWAN sensors deployed using The Things Network (TTN). A cost analysis can support users’ planning and decision-making when deploying the system for different use cases. A proof-of-concept demonstration of the system was built with river and weather sensors deployed in a flood prone suburban watershed in the city of Charlottesville, Virginia.",
        "keywords": "Internet of Things; smart cities; environmental monitoring; LoRaWAN; cloud computing; AWS; data management; cost analysis",
        "released": 2023,
        "link": "https://doi.org/10.3390/smartcities6030068"
    },
    {
        "title": "Linguistic analysis of internet media and social network data in the problems of social transformation assessment",
        "abstract": "A combined approach has been developed to estimate the effectiveness of social transformations as a measure of inconsistency between the actions of the authorities and the expectations of society and the synergy (social activity) of people, based on formalized coordination of the results obtained by the method of expert assessments and methods of sentiment analysis and intelligent analysis of text messages from online open sources and social networks. These methods are implemented as a set of web services and applications in the development environment of the Advanced Analytics integrated online platform of the World Data Center “Geoinformatics and Sustainable Development.” The effectiveness of the proposed approach is demonstrated by a quantitative assessment of the attitude of the population of Ukraine to the actions of the authorities concerned with the spread of the COVID-19 pandemic.",
        "keywords": "vector of government actions; vector of society&#8217; s expectations; vector of transformations (reforms); linguistic analysis; content analysis; linguistic sentiment analysis of Internet media data and social networks; open-source intelligence",
        "released": 2021,
        "link": "https://doi.org/10.1007/s10559-021-00348-8"
    },
    {
        "title": "BlockPy: An open access data-science environment for introductory programmers",
        "abstract": "Non-computer science majors often struggle to find relevance in traditional computing curricula that tend to emphasize abstract concepts, focus on nonpractical entertainment, or rely on decontextualized settings. BlockPy, a web-based, open access Python programming environment, supports introductory programmers in a data-science context through a dual block/text programming view.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1109/MC.2017.132"
    },
    {
        "title": "A metamodel-based definition of a conversion mechanism between SOAP and RESTful web services",
        "abstract": "Nowadays there are several frameworks that permit the conversion between SOAP and RESTful web services. However, none of these frameworks defines a high-level characterization of the interchange process, hindering full understanding of this process. This paper provides a metamodel-based approach that formalizes the conversion between SOAP and RESTful web services, clarifying this process. This approach can be used for guiding the ranslation process in forthcoming conversion frameworks and the publication of services in IDEs. In order to characterize the conversion mechanism three MOF metamodels are defined: SOAP, RESTful and intermediate SOA metamodels. This intermediate metamodel is used as a bridge between the other two metamodels. QVT Relations transformation rules between SOAP, RESTful and SOA metamodels are defined for a formal characterization of the transformation process. (C) 2016 Elsevier B.V. All rights reserved.",
        "keywords": "SOA; WSDL; WADL; MDA; MDD",
        "released": 2016,
        "link": "https://doi.org/10.1016/j.csi.2016.03.004"
    },
    {
        "title": "Solubility measurement of the solid solution containing the RbCl-CsCl-h<sub>2</sub>o system from <i>t</i>=298.15 to 348.15 k",
        "abstract": "The solubility of the RbCl-CsCl-H2O ternary system has been investigated at the temperature range from 298.15 to 348.15 K by two different methods, namely, the Flow-Cloud-Point (FCP) method and the Isothermal Dissolve Equilibrium (IDE) method. The measured solubility for this ternary system at 298.15 K by FCP method is consistent with the reported result in literature. The measured solubility results for this ternary system at 323.15 K by the FCP and IDE, two different methods that are consistent with each other, which indicated the results are more reliable. Meanwhile, the measured liquid-wet solid-solid lines proved that the equilibrium solid phases are two types of solid solutions (Rb,Cs)Cl-(ss) and (Cs,Rb)Cl-(ss) rather than pure salts (RbCl and CsCl) reported in literature. For the first time, the solubility of the RbCl-CsCl-H2O ternary system at 308.15 and 348.15 K has been measured by IDE method. The measured results shown that the equilibrated solid phases in this ternary system at 308.15 and 348.15 K are two types of solid solution (Rb,Cs)Cl-(ss) and (Cs,Rb)Cl-(ss), at RbCl rich side and CsCl rich side, respectively.",
        "keywords": "",
        "released": 2020,
        "link": "https://doi.org/10.1021/acs.jced.9b01045"
    },
    {
        "title": "MOBILE LEARNING SYSTEM FOR NUMERICAL ANALYSIS BY USING PHP",
        "abstract": "Programming tools are essential for students learning numerical analysis. It is troublesome to go to a laboratory where a computer is located after taking a lecture. Nowadays most students have mobile phones which can be used for programming practice through the Internet. PHP is a server-side scripting language designed for web development but also used as a general-purpose programming language. However, PHP has many inconveniences, such as adding a dollar symbol ($) to every varable. This paper introduces a slightly modified language, NAPHP, and a system which is designed for students to use their own mobile phone to write down the language NAPHP and run it on the web page. The system NAPHP-SYS is an educational tool that turns NAPHP into PHP, run PHP code and show the results on the web.",
        "keywords": "PHP; programming tool; Numercal Analysys",
        "released": 2019,
        "link": "https://doi.org/10.14317/jami.2019.157"
    },
    {
        "title": "Visual programming environments for end -user development of intelligent and social robots, a systematic review",
        "abstract": "Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot - based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fos- tering the so-called End -User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social re- search goals as well as to cater for professional needs of people not trained in more traditional text -based computer programming languages. This survey excludes interfaces aimed at supporting expert program- mers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two -fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component -Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working ?in -the -wild? rather than only in laboratories and structured settings.",
        "keywords": "Visual Programming Environment; End-User Development; Human-robot interaction; Social robot; Robotics",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.cola.2020.100970"
    },
    {
        "title": "A glimpse of hopjs",
        "abstract": "Hop. js is a multitier programming environment for JavaScript. It allows a single JavaScript program to describe the client-side and the server-side components of a web application. Its runtime environment ensures consistent executions of the application on the server and on the client. This paper overviews the Hop. js design. It shows the JavaScript extensions that makes it possible to conceive web applications globally. It presents how Hop. js interacts with the outside world. It also briefly presents the Hop. js implementation. It presents the Hop. js web server implementation, the handling of server-side parallelism, and the JavaScript and HTML compilers.",
        "keywords": "Web Programming; Functional Programming",
        "released": 2016,
        "link": "https://doi.org/10.1145/3022670.2951916"
    },
    {
        "title": "Adopting distributed pair programming as an effective team learning activity: A systematic review",
        "abstract": "As online learning has become an inevitable trend in the post-peak era of the COVID-19 pandemic, distributed pair programming (DPP) is gaining momentum in both education and industry. DDP serves as a collaborative programming approach and also benefits the development of computational thinking, a fundamental skill in today’s world. This study conducted a systematic review of studies on DPP published after 2010 to understand the themes and factors that impact the team effectiveness of DPP and thus inform future research and practices on how to better leverage this approach for teaching and learning. The results showed that individual characteristics attracted major investigations in the selected 23 studies, including prior programming experience, actual skill, perceived skill, gender, personality, time management, confidence, and self-esteem, with pair compatibility identified as a critical team design factor that significantly affects programmers’ satisfaction. Although the feel-good factor in the team process was investigated, no significant impact was found. Under the team environment theme, we compared different opinions on the orientation (e.g., scripted roles) and the use of technology (e.g., integrated development environment tools). Future research should investigate how task structure influences team effectiveness of DPP and relates to computational thinking education. Additionally, because most studies were conducted in higher education contexts, more research in primary and secondary educational contexts is also needed.",
        "keywords": "Distributed pair programming; Systematic review; Computational thinking; Team learning; Collaborative learning; STEM education",
        "released": 2024,
        "link": "https://doi.org/10.1007/s12528-023-09356-3"
    },
    {
        "title": "On distributed knowledge bases for robotized small-batch assembly",
        "abstract": "The flexibility demands in manufacturing are severe, e.g., for rapid-change-over to new product variants, while robots are flexible machines that potentially can be adapted to a large variety of production tasks. Task definitions such as explicit robot programs are hardly reusable from an application point-of-view. To improve the situation, a knowledge-based approach exploiting distributed declarative information and cloud computing offers many possibilities for knowledge exchange and reuse, and it has the potential to facilitate new business models for industrial solutions. However, there are many unresolved questions yet, e.g., those related to reliability, consistency, or legal responsibility. To investigate some of these issues, different knowledge-based architectures have been prototyped and evaluated by confronting the solution candidates with key application demands. The conclusion is that distributed cloud-based approaches offer many possibilities, but there is still a need for further research and better infrastructure before this approach can become industrially attractive. Note to Practitioners-It is possible to extend the capabilities of a robot system by providing online services. An example of such a service is an online library of robot applications, an app store, where robot programs can be downloaded and installed on the system with little effort from the user. Another is text analysis, that requires heavy computations but can provide a more natural user interaction. Experiences from several European projects with the goal to simplify robot usage/programming have resulted in a number of insights around the core challenges. For instance, providing application libraries in a traditional software sense is too limited when considering the necessary management of uncertainty (including human interaction, configuration of sensing, and learning). Robot programs are therefore better represented as distributable and compositional knowledge, together with supporting computational services. Since robot programming is time consuming and requires expertise, thus expensive, there is a need for good services that simplify this process. We see a need for coherent APIs in existing robot programming tools such that the desired app store can be added to the back-end of existing solutions.",
        "keywords": "Cognitive robotics; online knowledge bases; reuse of knowledge; robot programming; web services for robots",
        "released": 2015,
        "link": "https://doi.org/10.1109/TASE.2015.2408264"
    },
    {
        "title": "Physical education central: A possible online professional development tool",
        "abstract": "",
        "keywords": "",
        "released": 2015,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000359745700290"
    },
    {
        "title": "PAZAR: A framework for collection and dissemination of cis-regulatory sequence annotation",
        "abstract": "PAZAR is an open-access and open-source database of transcription factor and regulatory sequence annotation with associated web interface and programming tools for data submission and extraction. Curated boutique data collections can be maintained and disseminated through the unified schema of the mall-like PAZAR repository. The Pleiades Promoter Project collection of brain-linked regulatory sequences is introduced to demonstrate the depth of annotation possible within PAZAR. PAZAR, located at http://www.pazar.info, is open for business.",
        "keywords": "",
        "released": 2007,
        "link": "https://doi.org/10.1186/gb-2007-8-10-r207"
    },
    {
        "title": "A market-based framework for resource management in cloud federation",
        "abstract": "A federated cloud is a form of the inter-cloud environment in which cloud service providers cooperate for better scalability and service provisioning. By communicating with other cloud providers and sharing resources, federated members gain more advantages from utilizing a comprehensive resource management system. There are different setups for federated clouds, in terms of federation formation and member interactions. Consequently, difficulties arise since resource management must be dealt with as a general entity. The current paper introduces a generic resource management framework for inter-federation resource management that can be used for a wide range of cloud federation models. By considering the federated cloud as a market for trading resources, the present study proposes a market-based framework to manage the various types of federated clouds. The main components of the proposed framework are based on market management models. This generic framework is able to cover a variety of centralized cloud federation models that divide the federated cloud life cycle into time slots. Finally, the present work introduces a resource management model which is compatible with the proposed framework. The model is implemented with Java and NetBeans IDE 8.2 and evaluated using the FederatedCloudSim 2.0 toolkit. The results are discussed in order to evaluate the proposed framework and the selected market algorithms.",
        "keywords": "Federated cloud; Resource management framework; Market design; Centralized federation",
        "released": 2023,
        "link": "https://doi.org/10.1007/s10723-022-09635-w"
    },
    {
        "title": "Hadoop-based painting resource storage and retrieval platform construction and testing",
        "abstract": "This paper adopts Hadoop to build and test the storage and retrieval platform for painting resources. This paper adopts Hadoop as the platform and MapReduce as the computing framework and uses Hadoop Distributed Filesystem (HDFS) distributed file system to store massive log data, which solves the storage problem of massive data. According to the business requirements of the system, this paper designs the system according to the process of web text mining, mainly divided into log data preprocessing module, log data storage module, log data analysis module, and log data visualization module. The core part of the system is the log data analysis module. The analysis of search keywords ranking, Uniform Resource Locator (URL), and user click relationship, URL ranking, and other dimensions are realized through data statistical analysis, and Canopy coarse clustering is performed first according to search keywords, and then K-means clustering is used for the results after Canopy clustering, and the calculation of cosine similarity is adopted to realize the grouping of users and build user portrait. The Hadoop development environment is installed and deployed, and functional and performance tests are conducted on the contents implemented in this system. The constructed private cloud platform for remote sensing image data can realize online retrieval of remote sensing image metadata and fast download of remote sensing image data and solve the problems in storage, data sharing, and management of remote sensing image data to a certain extent.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1155/2021/9933330"
    },
    {
        "title": "Computer-based oral health records on the world wide web",
        "abstract": "Recently the World Wide Web has emerged as a platform for computer-based oral health records. Web-based patient records can make teledentistry an instant reality. Because an increasing number of dental care providers can access Web pages, traditional barriers to exchanging information are dropping. Web-based records also make cumulative, longitudinal patient records possible. Sophisticated security mechanisms can ensure the integrity and confidentiality of patient information. Because Web-based systems are simpler to install and configure, the cost of operating them may be reduced. However, their development is complex, difficult, and expensive because the Web was not developed as a programming environment Furthermore, the technologies underlying the Web are constantly evolving, forcing developers to continuously reengineer their systems. In addition, several policy questions, such as storage of and access to computer-based patient records, have to be answered. This article describes CMSWeb, a Web-based clinical information system implemented at Temple University School of Dentistry.",
        "keywords": "computer-based patient record; computer systems; dental informatics; dental record; World Wide Web",
        "released": 1999,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000081449900005"
    },
    {
        "title": "An environment for end-user development of web mashups",
        "abstract": "End-User Development aims to find novel ways that are suitable and intuitive for end users to create their own applications. We present a graphical environment in which users create new mashups by directly selecting interaction elements, content and functionalities from existing Web applications without requiring the intervention of expert developers. Then, users just need to exploit a copy paste metaphor to indicate how to compose the selected interactive content and functionalities in the new mashup. The environment is enabled by a Web-based platform accessible from any browser, and is suitable for users without particular programming skills. We describe the architecture of our platform and how it works, including its intelligent support, show example applications, and report the results of first user studies. (C) 2015 Elsevier Ltd. All rights reserved.",
        "keywords": "End User Development; Web mashups; User interface development tools",
        "released": 2016,
        "link": "https://doi.org/10.1016/j.ijhcs.2015.10.008"
    },
    {
        "title": "Dynamic soft sensor modeling method fusing process feature information based on an improved intelligent optimization algorithm",
        "abstract": "The lack of real-time and accurate melting index forecast model puts forward challenges to the online quality detection and control of the polypropylene industrial production. Meanwhile, the key process characteristics of dynamic and time delay are not fully and effectively integrated into the soft sensor model. Therefore, a novel dynamic soft sensing (DSS) modeling method using the orthogonal echo state network (OESN) based on the cumulative mutual information (CMI) threshold setting integrating an improved differential evolution algorithm (IDE) (OESN-CMI-IDE) is proposed. The mutual information of each candidate auxiliary variable relative to the dominant variable is calculated and sorted to determine the CMI threshold and select the corresponding auxiliary variable. Then an IDE algorithm is used to optimize the parameters of the OESN model, which has faster convergence, higher accuracy and stronger robustness than that of DE, self-adaptive DE (jDE), DE with time varying scale factor (DETVSF), line decreased inertia weight particle swarm optimization (LDPSO), flexible exponential inertia weight particle swarm optimization (FEPSO), and oscillating triangular inertia weight particle swarm optimization (OTPSO) in terms of evolutionary computing competition benchmark function tests. Finally, the proposed method is applied to predict the melt index in an actual polypropylene plant. The experimental results demonstrate that the prediction effect of the proposed method is better than that of the OESN model optimized by DE, jDE, DETVSF, LDPSO, FEPSO, OTPSO and the OESN model without optimization, which can escort the online monitoring and advanced control of the melt index in the propylene industry.",
        "keywords": "Dynamic soft sensor; Orthogonal echo state network; Cumulative mutual information; Improved differential evolution; Propylene industry",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.chemolab.2021.104415"
    },
    {
        "title": "MiniDeep: A standalone AI-edge platform with a deep learning-based MINI-PC and AI-QSR system",
        "abstract": "In this paper, we present a new AI (Artificial Intelligence) edge platform, called “MiniDeep”, which provides a standalone deep learning platform based on the cloud-edge architecture. This AI-Edge platform provides developers with a whole deep learning development environment to set up their deep learning life cycle processes, such as model training, model evaluation, model deployment, model inference, ground truth collecting, data pre-processing, and training data management. To the best of our knowledge, such a whole deep learning development environment has not been built before. MiniDeep uses Amazon Web Services (AWS) as the backend platform of a deep learning tuning management model. In the edge device, the OpenVino enables deep learning inference acceleration at the edge. To perform a deep learning life cycle job, MiniDeep proposes a mini deep life cycle (MDLC) system which is composed of several microservices from the cloud to the edge. MiniDeep provides Train Job Creator (TJC) for training dataset management and the models’ training schedule and Model Packager (MP) for model package management. All of them are based on several AWS cloud services. On the edge device, MiniDeep provides Inference Handler (IH) to handle deep learning inference by hosting RESTful API (Application Programming Interface) requests/responses from the end device. Data Provider (DP) is responsible for ground truth collection and dataset synchronization for the cloud. With the deep learning ability, this paper uses the MiniDeep platform to implement a recommendation system for AI-QSR (Quick Service Restaurant) KIOSK (interactive kiosk) application. AI-QSR uses the MiniDeep platform to train an LSTM (Long Short-Term Memory)-based recommendation system. The LSTM-based recommendation system converts KIOSK UI (User Interface) flow to the flow sequence and performs sequential recommendations with food suggestions. At the end of this paper, the efficiency of the proposed MiniDeep is verified through real experiments. The experiment results have demonstrated that the proposed LSTM-based scheme performs better than the rule-based scheme in terms of purchase hit accuracy, categorical cross-entropy, precision, recall, and F1 score.",
        "keywords": "MiniDeep; edge computing; deep learning; cloud computing; recommendation system",
        "released": 2022,
        "link": "https://doi.org/10.3390/s22165975"
    },
    {
        "title": "Special section: Software architectures and application development environments for cloud computing",
        "abstract": "",
        "keywords": "",
        "released": 2012,
        "link": "https://doi.org/10.1002/spe.1144"
    },
    {
        "title": "EFFICIENT DEVELOPMENT OF PROGRESSIVELY ENHANCED WEB APPLICATIONS BY SHARING PRESENTATION AND BUSINESS LOGIC BETWEEN SERVER AND CLIENT",
        "abstract": "A Web application’s codebase is typically divided into a server side and a client side with essential functionalities being implemented twice, such as validation or rendering. While developers can choose from a rich set of programming languages to implement a Web application’s server side, they are bound to JavaScript for the client side. Recent developments like Node.js allow using JavaScript in a simple and efficient way also on the server side, but lack offering a common codebase for the entire Web application. In this article, we present the SWAG approach that aims at reducing development efforts and minimizing coding errors in order to make creating Web applications more efficiently. Based on our approach, we created the SWAC framework. It enables establishing a unified Web application codebase that provides both dynamic functionality and progressive enhancement by taking characteristic differences between server and client into account.",
        "keywords": "Development Tools; HTML5 and Beyond; Web Standards and Protocols",
        "released": 2014,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000342116700004"
    },
    {
        "title": "Micro-IDE: A tool platform for generating efficient deployment alternatives based on microservices",
        "abstract": "Microservice architecture (MSA) is a paradigm to design and develop scalable distributed applications using loosely coupled, highly cohesive components that can be deployed independently. The applications that realize the MSA may contain thousands of services that together form the overall system. Microservices interact with each other by producing and consuming data. Deploying frequently communicating services to the same physical resource would reduce network utilization, which is vital for reducing costs and improving scalability. Since the physical resources have limited capacity, it is not always possible to deploy communicating services to the same resource. Therefore, automated efficient deployment alternatives need to be generated for MSA in the design phase. To address this problem, we proposed an algorithmic approach to generate efficient microservice deployment configurations to available cloud resources in our previous study. In this study, a tool (Micro-IDE) has been proposed to realize and evaluate this approach. The Micro-IDE tool has been validated using a case study inspired by the Spotify application.",
        "keywords": "automated deployment of microservices; cloud computing; microservice architectures; optimization algorithms; tool platform for deploying microservices",
        "released": 2022,
        "link": "https://doi.org/10.1002/spe.3088"
    },
    {
        "title": "Web-based three-dimensional geo-referenced visualization",
        "abstract": "This paper addresses several approaches to implementing web-based, three-dimensional (3-D), gee-referenced visualization. The discussion focuses on the relationship between multi-dimensional data sets and applications, as well as the thick/thin client and heavy/light server structure. Two models of data sets are addressed in this paper. One is the use of traditional 3-D data format such as 3-D Studio Max, Open Inventor 2.0, Vis5D and OBJ. The other is modelled by a web-based language such as VRML, Also, traditional languages such as C and C++, as well as web-based programming tools such as Java, Java3D and ActiveX, can be used for developing applications. The strengths and weaknesses of each approach are elaborated. Four practical solutions for using VRML and Java, Java and Java3D, VRML and ActiveX and Java wrapper classes (Java and C/C++), to develop applications are presented for web-based, real-time interactive and explorative visualization. (C) 1999 Elsevier Science Ltd. All rights reserved.",
        "keywords": "three-dimensional visualization; client/server structure; Java3D; VRML; ActiveX",
        "released": 1999,
        "link": "https://doi.org/10.1016/S0098-3004(99)00076-X"
    },
    {
        "title": "A theoretical framework for a structuration model of social issues in software development in information systems",
        "abstract": "This study looks at how human and social issues affect software developers’ work and the software they produce. It focuses on the development of software to support information systems, and therefore positions software development as an activity within Information Systems Development. Although some research in IS and software development address human and social issues they do so by considering either business organisations or software development environments separately but do not connect both contexts together. This research tries to build this connection through a model of social issues in software development. The model was generated from data collected from an interpretive online ethnography of virtual communities of software developers. The model of social issues complements Orlikowski’s structurational model of enactment of technologies-in-practice and suggests that human and social issues that affect the production of software emerge from three different contexts: software development environment, software development practices and complex business organisations. Copyright (c) 2008 John Wiley & Sons, Ltd.",
        "keywords": "information systems; software development; structuration theory; online ethnography",
        "released": 2008,
        "link": "https://doi.org/10.1002/sres.868"
    },
    {
        "title": "JOpera: A toolkit for efficient visual composition of web services",
        "abstract": "Web services are attracting attention because of their ability to provide standard interfaces to heterogeneous distributed services. Standard interfaces make it possible to compose more complex services out of basic ones. This paper tackles the problem of visual service composition and the efficient and scalable execution of the resulting composite services. The effort revolves around the JOpera system, which combines a visual programming environment for Web services with a flexible execution engine that interacts with Web services through the simple object access protocol (SOAP), described with Web services language description (WSDL) and registered with a universal description discovery and integration (UDDI) registry. The paper explains the syntax and implementation of the JOpera Visual Composition Language (JVCL) and its ability to provide different quality of service (QoS) levels in the execution of composite services.",
        "keywords": "JOpera; scalable process execution; visual programming languages; Web service composition",
        "released": 2004,
        "link": "https://doi.org/10.1080/10864415.2005.11044328"
    },
    {
        "title": "The analysis and implementation of film decision-making based on python",
        "abstract": "With the growing development of the era of big data, data acquisition and analysis have become hot spots, and Python-based crawler technology is one of the most widely used tools in data analysis work at present. In this paper, we apply Python crawler key technology to acquire data of movie list and hot movies on Cat’s Eye movie network, analyze data based on Python development environment Spyder, use the Numpy system to store and process large data, Chinese Jieba word separation tool to crawl data for word separation text processing, Snownlp library to process text sentiment, and finally by the word cloud map and web dynamic map display information such as viewers’ emotional tendency and movie rating statistics, and provide decision support for users’ movie viewing.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1155/2022/4131316"
    },
    {
        "title": "Web-centred end-user component modelling",
        "abstract": "This paper formally defines a web component model enabling end-user programmers to build component-based rich internet applications (RIAs) that are tailored to meet their particular needs. It is the product of a series of previously published papers. The formal definition in description logic verifies that the model is consistent and subsumes currently existing models. We demonstrate experimentally that it is more effective than the others. Current tools propose very disparate web component models, which are based on the appropriate invocation of service backends, overlooking user needs in order to exploit these services and resources in a friendly manner. We have proposed a web model based on a detailed study of existing tools, their pros and cons, limitations and key success factors that have enabled other web end-user development (WEUD) solutions to help end-user programmers to build software to support their needs. In this paper we have verified that the proposed model subsumes and is instantiated by the models of the other existing tools that we analysed, coming a step closer to the standardization of end-user centred RIAs and development environments. We have implemented a development tool, called EzWeb, to produce RIAs that implement the proposed model. This tool enables users to develop their application following the model’s component structure based on end-user programming success factors. We report a statistical experiment in which users develop increasingly complex web software using the EzWeb tool generating RIAs that conform to the proposed component model, and other WEUD tools generating RIAs that conform to other models. This experiment confirms the applicability of the proposed model and demonstrates that more enduser programmers (EUPs) (users concerned with programming primarily for personal rather public use) successfully develop web solutions for complex problems using the EzWeb tool that implements the model, which is more efficient than existing tools that implement other models. (c) 2015 Elsevier B.V. All rights reserved.",
        "keywords": "End-user programming; Web engineering; Component-based software; Human factors; Visual programming; Component modelling",
        "released": 2016,
        "link": "https://doi.org/10.1016/j.future.2015.07.002"
    },
    {
        "title": "Building software research environment using linux container and version control system",
        "abstract": "With advancements in software technology, more scientists and engineers are employing computer software and programming tools for research. However, several issues can arise in software-based research: environment setting, reproducibility, and loss of source codes. This study investigates the use of Linux containers and version control systems to prevent these problems. Managing research projects using a cloud source-code repository and building a research environment in a Linux container can prevent the abovementioned problems and make research collaboration easier. For researchers with no experience with Linux containers, a repository of project template containing shell scripts for building and running containers has been released.",
        "keywords": "software; programming; Linux container; version control",
        "released": 2021,
        "link": "https://doi.org/10.7582/GGE.2021.24.2.45"
    },
    {
        "title": "Grids challenged by a web 2.0 and multicore sandwich",
        "abstract": "We discuss the application of Web 2.0 to support scientific research (e-Science) and related “e-moreorlessanything” applications. Web 2.0 offers interesting technical approaches (protocols, message formats, and programming tools) to build core e-infrastructure (cyberinfrastructure) as well as many interesting services (Facebook, YouTube, Amazon S3/EC2, and Google maps) that can add value to e-infrastructure projects. We discuss why some of the original Grid goals of linking the world’s computer systems may not be so relevant today and that interoperability is needed at the data and not always at the infrastructure level. Web 2.0 may also support Parallel Programming 2.0-a better parallel computing software environment motivated by the need to run commodity applications on multicore chips. A “Grid on the chip” will be a common use of future chips with tens or hundreds of cores. Copyright (C) 2008 John Wiley & Sons, Ltd.",
        "keywords": "Grid; Web 2.0; web service; multicore; parallel programming",
        "released": 2009,
        "link": "https://doi.org/10.1002/cpe.1358"
    },
    {
        "title": "Integrated development environment for digital image computing and configuration management",
        "abstract": "In this paper, we present a system referred to as “Hello-Vision.” Hello-Vision is a software development environment that can be used in conjunction with research and application development in the area of image processing. It is an integrated environment that supports reusable image processing algorithm developments, systematic management of algorithms and related information. Hello-Vision’s function is to simplify image processing applications using the algorithms and to provide an ideal environment for the education of practical image processing engineers. Hello-Vision adopts a true object-oriented approach supporting well-separated data classes similar to WE (Image Understanding Environment) classes. This process allows algorithm to be written by their functions without user interface programming by separating the interface layer from data processing functions. The user-defined functions are easily registered in the online algorithm management systems and treated as a system command. Hello-Vision is equipped with a visual programming environment where a user can easily create image- processing applications by manipulating visual command icons and user-defined function icons that are created automatically when they are registered. Hello-Vision also provides an ideal environment where instructors can register their lecture materials for image processing theory, which allows hands-on experimentation with Hello-Vision through user interaction.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000229637900003"
    },
    {
        "title": "A LoRa sensor network for monitoring pastured livestock location and activity",
        "abstract": "Precision technologies for confinement animal agricultural systems have increased rapidly over the past decade, though precision technology solutions for pastured livestock remain limited. There are a number of reasons for this limited expansion of technologies for pastured animals, including networking availability and reliability, power requirements, and expense, among others. The objective of this work was to demonstrate a rapidly deployable long-range radio (LoRa) based, low-cost sensor suite that can be used to track location and activity of pastured livestock. The sensor is comprised of an inexpensive Arduino-compatible microprocessor, a generic MPU-9250 motion sensor which contains a 3-axis accelerometer, 3-axis magnetometer, and a 3-axis gyroscope, a generic GPS receiver, and a RFM95W generic LoRa radio. The microprocessor can be programmed flexibly using the open source Arduino IDE software to adjust the frequency of sampling, the data packet to send, and what conditions are needed to operate. The LoRa radio transmits to a Dragino LoRa gateway which can also be flexibly programmed through the Arduino IDE software to send data to local storage or, in cases where a web or cellular connection is available, to cloud storage. The sensor was powered using a USB cord connected to a 3,350 mAh lithium-ion battery pack. The Dragino gateway was programmed to upload data to the ThingSpeak IoT application programming interface for data storage, handling, and visualization. Evaluations showed minimal benefit associated with reducing sampling frequency as a strategy to preserve battery life. Packet loss ranged from 40% to 60%. In a 3 d evaluation on pastured sheep, the sensor suite was able to report GPS locations, inertial sensor readings, and temperature. Preliminary demonstrations of our system are satisfactory to detect animal location based on GPS data in real-time. This system has clear utility as a lower-cost strategy to deploy flexible, useful precision technologies for pasture-based livestock species.",
        "keywords": "connectivity; extensive system; intensive system; LoRa; networking; precision technology; sensor",
        "released": 2021,
        "link": "https://doi.org/10.1093/tas/txab010"
    },
    {
        "title": "Data workflows and visualization in support of surveillance practice",
        "abstract": "The Swedish National Veterinary Institute (SVA) is working on implementing reusable and adaptable workflows for epidemiological analysis and dynamic report generation to improve disease surveillance. Important components of this work include: data access, development environment, computational resources and cloud-based management. The development environment relies on Git for code collaboration and version control and the R language for statistical computing and data visualization. The computational resources include both local and cloud-based systems, with automatic workflows managed in the cloud. The workflows are designed to be flexible and adaptable to changing data sources and stakeholder demands, with the ultimate goal to create a robust infrastructure for the delivery of actionable epidemiological information.",
        "keywords": "animal health; epidemiology; data-driven; dashboards; digitalization; automation; reproducibility",
        "released": 2023,
        "link": "https://doi.org/10.3389/fvets.2023.1129863"
    },
    {
        "title": "Toward human-AI collaboration: A recommender system to support CS1 instructors to select problems for assignments and exams",
        "abstract": "Programming online judges (POJs) have been increasingly used in CS1 classes, as they allow students to practice and get quick feedback. For instructors, it is a useful tool for creating assignments and exams. However, selecting problems in POJs is time consuming. First, problems are generally not organized based on topics covered in the CS1 syllabus. Second, assessing whether problems require similar effort to be completed and map onto the same topic is a subjective and expert-dependent task. The difficulty increases if the instructor must create variations of these assessments, e.g., to avoid plagiarism. Thus, here, we research how to support CS1 instructors in the task of selecting problems, to compose one-size-fits-all or personalized assignments/exams. Our solution is to propose a novel intelligent recommender system, based on a fine-grained data-driven analysis of the students’ effort on solving problems in the integrated development environment of a POJ system, and automatic detection of topics for CS1 problems, based on problem descriptions. Data collected from 2714 students are processed to support, via our artificial intelligence (AI) method recommendations, the instructors’ decision-making process. We evaluated our method against the state of the art in a simple blind experiment with CS1 instructors (N < 35). Results show that our recommendations are 88% accurate, surpassing our baseline (p < 0.05). Finally, our work paves the way for novel POJ smart learning environments, wherein instructors define learning tasks (assignments/exams) supported by AI.",
        "keywords": "CS1 assessment; data driven; programming online judges (POJs); recommender systems; student effort; topic detection",
        "released": 2023,
        "link": "https://doi.org/10.1109/TLT.2022.3224121"
    },
    {
        "title": "Anesthesia decision analysis using a cloud-based big data platform",
        "abstract": "Big data technologies have proliferated since the dawn of the cloud-computing era. Traditional data storage, extraction, transformation, and analysis technologies have thus become unsuitable for the large volume, diversity, high processing speed, and low value density of big data in medical strategies, which require the development of novel big data application technologies. In this regard, we investigated the most recent big data platform breakthroughs in anesthesiology and designed an anesthesia decision model based on a cloud system for storing and analyzing massive amounts of data from anesthetic records. The presented Anesthesia Decision Analysis Platform performs distributed computing on medical records via several programming tools, and provides services such as keyword search, data filtering, and basic statistics to reduce inaccurate and subjective judgments by decision-makers. Importantly, it can potentially to improve anesthetic strategy and create individualized anesthesia decisions, lowering the likelihood of perioperative complications.",
        "keywords": "Anesthesia analysis; Decision-making; Big data; Cloud-based; Platform; Precision medicine",
        "released": 2024,
        "link": "https://doi.org/10.1186/s40001-024-01764-0"
    },
    {
        "title": "Secure deep learning framework for cloud to protect the virtual machine from malicious events",
        "abstract": "In recent decades, user communication has been digitalized with some advanced applications. However, securing the digital cloud system is complicated because of the vulnerability of large files and malicious events. Therefore, a present research study intended to design a novel Dragonfly-based Genetic Deep Belief Network (DGDBN) technique to protect the VM from malware activities in the cloud environment. Hence, to validate the presented model, the cloud user files data was considered and imported to the system as input. Then further processes such as preprocessing feature extraction, attack detection and classification were performed. Once the malicious event is predicted, it is neglected by the cloud user environment. Furthermore, implemented novel DGDBN model is tested in the MATLAB programming environment. Finally, the performance parameters like accuracy, precision, reconfiguration time, Recall, F-measure, and data overhead were measured and compared with associated approaches. The novel DGDBN scored the highest accuracy at 99.6%, reduced reconfiguration time at 320 ms and optimized data overhead at 24.2%. Hence, it is in optimal status compared to the existing models.",
        "keywords": "Virtual machine; Deep belief neural network; Cloud user; Malicious events; Optimization",
        "released": 2023,
        "link": "https://doi.org/10.1007/s11277-023-10524-y"
    },
    {
        "title": "Better leadership via a seven factor model on net profit - a case of facebook in USA",
        "abstract": "The story of Facebook and Mark is becoming successful lesson for many businesses which shows their ambitious plan and ides and working hard on it. We recognized leadership role of Facebook lying in online social media industry and network with a social site for may people to connect around the world. This paper will uses OLS method to estimate effects of Face book good management, via both micro and macro factors on net profit. Authors will analyze effects of Seven (7) micro and macroeconomic factors such as: stock price, net profit, lending rate, inflation, GPD growth, S&P 500, etc. on net profit of an online media company, Facebook in USA during 2014-2019 and make further analysis. Findings show that if inflation, GDP (increasing too much) there is significant effect on reducing Facebook net profit wand the next factor is decreasing SP500.",
        "keywords": "Facebook Net Profit; Leadership; Net Profit; GDP Growth; Inflationary; Market Interest Rate",
        "released": 2021,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000658894000114"
    },
    {
        "title": "Creation of web 2.0 tools ontology to improve learning",
        "abstract": "The aim of the paper is to present systematic review results on ontology development tools, to establish interconnections between learning styles, preferred learning activities and related Web 2.0 tools, and also to create Web 2.0 tools ontology to interconnect learning activities with relevant Web 2.0 tools. This ontology is necessary for learners to semantically search for suitable Web 2.0 tools while learning in virtual learning environments (VLEs). Suitability of Web 2.0 tools depends on preferred types of learning activities which in its turn depend on preferred learning styles. The research results include: (1) systematic review results on ontology development tools and ontology representation language/formats; (2) established interconnections between learning styles, preferred learning activities, and relevant Web 2.0 tools using sets portrait method, and (3) creating Web 2.0 tools ontology to interconnect preferred learning activities with relevant Web 2.0 tools in VLE. The research results will be implemented in iTEC - pan-European research and development project focused on the design of the future classroom funded by EU 7FP. The research results presented are absolutely novel in scientific literature, and this makes the current study distinct from all other works in the area. (C) 2014 Elsevier Ltd. All rights reserved.",
        "keywords": "Ontology; Web 2.0; Collaborative learning; Semantic search; Learning styles; Learning activities",
        "released": 2015,
        "link": "https://doi.org/10.1016/j.chb.2014.10.026"
    },
    {
        "title": "Declaratively programming the mobile web with mobl",
        "abstract": "A new generation of mobile touch devices, such as the iPhone, iPad and Android devices, are equipped with powerful, modern browsers. However, regular websites are not optimized for the specific features and constraints of these devices, such as limited screen estate, unreliable Internet access, touch-based interaction patterns, and features such as GPS. While recent advances in web technology enable web developers to build web applications that take advantage of the unique properties of mobile devices, developing such applications exposes a number of problems, specifically: developers are required to use many loosely coupled languages with limited tool support and application code is often verbose and imperative. We introduce mobl, a new language designed to declaratively construct mobile web applications. Mobl integrates languages for user interface design, styling, data modeling, querying and application logic into a single, unified language that is flexible, expressive, enables early detection of errors, and has good IDE support.",
        "keywords": "Design; Languages; Verification",
        "released": 2011,
        "link": "https://doi.org/10.1145/2076021.2048121"
    },
    {
        "title": "YNOVEL WEB SEARCH FOR DATA ACCESSIBILITY USING CONVOLUTION NEURAL NETWORK COMPARING WITH ARTIFICIAL NEURAL NETWORK",
        "abstract": "Aim: To improvise a web search for data accessibility through the neural network, using a convolution neural network compared with the artificial neural network. Materials and Methods : The implementation has been carried out for processing the data through Mysql and the algorithms are tested with Net Beans (IDE) applications. The convolutional neural network paved the way to achieve the enhanced web search for data accessibility. Results: With the sample size of 96 and tested around 21 number of times compared with artificial neural networks. The response time with the accuracy of 79.5% to enhanced web search for accessing the data using convolution neural networks, provides significantly better results compared to artificial neural networks. There was a statistical significance between convolution neural network and artificial neural network. Conclusion: To enhance the web search for data accessibility using a novel convolutional neural network provides more significant accuracy of 85 percent than artificial neural network by analysing the parameters of time with service name and id.",
        "keywords": "Artificial Intelligence; Convolutional Neural Network; Novel Web Search; Deep Learning; Artificial Neural Network",
        "released": 2022,
        "link": "https://doi.org/10.9756/INT-JECSE/V14I3.722"
    },
    {
        "title": "Remote multi-robot monitoring and control system based on MMS and web services",
        "abstract": "Purpose - This paper aims to develop a service-oriented distributed multi-robot system based on manufacturing message specification (MMS) and new-generation distributed object technology - web services for realizing remotely monitoring and controlling multiple heterogeneous robots in the internet environment. Design/methodology/approach - The study presents robot communication model and distributed multi-robot monitoring and control software structure based on MMS and web services. In particular, monitoring and control software design of MMS concepts in web services environment using Unified Modeling Language model is discussed in detail. In addition, to verify the validity of the proposed design method, a multi-robot prototype system for robot flexible assemble cell has been achieved. Its Server software is implemented in C+ + with Visual Studio.NET being the development environment and Client software is programmed in Java with Borland JBuilder 9 being the development tool. Findings - Finds that the communication structure following MMS can make the multi-robot monitoring and control system have perfect robustness, interoperability and reconfigurability. Besides, web services technology can conveniently realize MMS services, also can successfully resolve the remote multi-robot monitoring and control problem among cross-network, cross-platform and heterogeneous systems. Research limitations/implications - Provides an easy and low-cost method for realizing heterogeneous multi-robot remote driving. The web-based distribution of the presented system is critical in enabling capabilities such as e-manufacturing, e-diagnostics and e-maintenance. Practical implications - The proposed system can be seamlessly integrated into other automated manufacturing systems or management systems in plug-and-play fashion. The combination of MMS and web services is in favor of real manufacturing equipments being embedded in the network, so the presented systematic methodology can be a useful reference for constructing web-based reconfigurable manufacturing systems. Originality/value - Provides robot communication model based on MMS and web services and presents service-oriented distributed remote multirobot monitoring and control software architecture.",
        "keywords": "robotics; controllers; manufacturing resource planning; service operations",
        "released": 2007,
        "link": "https://doi.org/10.1108/01439910710738863"
    },
    {
        "title": "WEB-BASED SPATIAL DATA INFRASTRUCTURE: A SOLUTION FOR THE SUSTAINABLE MANAGEMENT OF THEMATIC INFORMATION SUPPORTED BY AERIAL ORTHOPHOTOGRAPHY",
        "abstract": "Under the framework of the Spanish National Plan for Aerial Orthophotography in Castilla-La Mancha, a web-based spatial data infrastructure has been developed that allows both the sustainable management of spatial and thematic information and efficient quality control of orthophoto production. The dissemination of thematic cartographic information by means of false-color infrared images and their physical parameters (reflectance and radiance) could allow its use in applications such as the extraction of biophysical parameters, forest coverage evolution and vegetative analysis of species. For wide accessibility, a geoportal that offers all the information related to the project and based on spatial data infrastructure technology has been created (http://ide.jccm.es/pnoa) that incorporates web map services.",
        "keywords": "Photogrammetric flight; False-color infrared images; Thematic information; Geographical Information System; Quality Control; Web Map Service",
        "released": 2013,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000319031800017"
    },
    {
        "title": "On line internet-based multi-location power system harmonics analysis and monitoring",
        "abstract": "Traditional methods to measure power system harmonics employ the power harmonic analyzer or a software package, such as Matlab or others, but they do have certain limitations in the graphical programming environment, applications for remote monitoring and control performance and for multi-point harmonic measurement. In this paper, a PC-based virtual instrument (VI) based on Fast Fourier Transform (FFT) to implement a remote multipoint power system harmonics analysis and monitoring using LabVIEW is proposed. The nearby PC (Server) can collect real-time waveform data from different multi-locations and transmit it to remote PCs (Clients) for on line harmonic analysis and monitoring via the Internet, The history of Total Harmonic Distortion (THD) in the waveform signals from remote locations can be also recorded and tracked in the data base. The strategies and guidelines are also developed to enhance teaching and/or learning the basics of operating Internet-based systems. Experimental results have testified its well performance and remote web-based capability. (C) 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 17: 241-252, 2009; Published online in Wiley InterScience (www.interscience.wiley.com); DOI 10.1002/cae.20175",
        "keywords": "FFT; TCP/IP; THD; VI; power system harmonics; Internet",
        "released": 2009,
        "link": "https://doi.org/10.1002/cae.20175"
    },
    {
        "title": "An optimal NIDS for VCN using feature selection and deep learning technique: IDS for VCN",
        "abstract": "In this modern era, due to demand for cloud environments in business, the size, complexity, and chance of attacks to virtual cloud network (VCN) are increased. The protection of VCN is required to maintain the faith of the cloud users. Intrusion detection is essential to secure any network. The existing approaches that use the conventional neural network cannot utilize all information for identifying the intrusions. In this paper, the anomaly-based NIDS for VCN is proposed. For feature selection, grey wolf optimization (GWO) is hybridized with a bald eagle search (BES) algorithm. For classification, a deep learning approach-deep sparse auto-encoder (DSAE)-is employed. In this way, this paper proposes a NIDS model for VCN named GWO-DES-DSAE. The proposed system is simulated in the python programming environment. The proposed NIDS model’s performance is compared with other recent approaches for both binary and multi-class classification on the considered datasets-NSL-KDD, UNSW-NB15, and CICIDS 2017-and found better than other methods.",
        "keywords": "Dataset; Deep Learning; Feature Selection; Intrusion Detection; Security",
        "released": 2021,
        "link": "https://doi.org/10.4018/IJDCF.20211101.oa10"
    },
    {
        "title": "D-GDM: A mobile diagnostic decision support system for gestational diabetes",
        "abstract": "Objective: The aim of the study is to describe a portable and convenient software to facilitate the diagnostics of gestational (GDM) and pre-gestational diabetes (PGDM). Materials and methods: An open source software, d-GDM, was developed in Java. The integrated development environment Android Studio was used as the Android operational system.The software for GDM diagnosis uses the criteria endorsed by the International Association of Diabetes and Pregnancy Study Group, modified by the World Health Organization. Results: GDM diagnosis criteria is not simple to follow, therefore, errors or inconsistencies in diagnosis are expected and could delay the appropriate treatment. The d-GDM, was developed to assist GDM diagnosis with precision and consistency diagnostic reports.The open source software can be manipulated conveniently.The operator requires information regarding the gestational period and selects the appropriate glycaemic marker options from the menu. During operation, pressing the button “diagnosticar” on the screen will present the diagnosis and information for the follow up. d-GDM is available in Portuguese or English and can be downloaded from the Google PlayStore. A responsive web version of d-GDM is also available.The usefulness and accuracy of d-GDM was verify by field tests involving 22 subjects and 5 mobile phone brands.The approval regards user-friendliness and efficiency were 95% or higher.The GDM diagnosis were 100% correct, in this pilot test. d-GDM is a user-friendly, free software for diagnosis that was developed for mobile devices. It has the potential to contribute and facilitate the diagnosis of gestational diabetes for healthcare professionals.",
        "keywords": "Gestational diabetes; medical informatics; software; decision support; mobile application",
        "released": 2019,
        "link": "https://doi.org/10.20945/2359-3997000000171"
    },
    {
        "title": "Robula plus : An algorithm for generating robust XPath locators for web testing",
        "abstract": "Automated test scripts are used with success in many web development projects, so as to automatically verify key functionalities of the web application under test, reveal possible regressions and run a large number of tests in short time. However, the adoption of automated web testing brings advantages but also novel problems, among which the test code fragility problem. During the evolution of the web application, existing test code may easily break and testers have to correct it. In the context of automated DOM-based web testing, one of the major costs for evolving the test code is the manual effort necessary to repair broken web page element locators - lines of source code identifying the web elements (e.g. form fields and buttons) to interact with. In this work, we present Robula+, a novel algorithm able to generate robust XPath-based locators - locators that are likely to work correctly on new releases of the web application. We compared Robula+ with several state of the practice/art XPath locator generator tools/algorithms. Results show that XPath locators produced by Robula+ are by far the most robust. Indeed, Robula+ reduces the locators’ fragility on average by 90% w.r.t. absolute locators and by 63% w.r.t. Selenium IDE locators. Copyright (c) 2016 John Wiley & Sons, Ltd.",
        "keywords": "web testing; test cases fragility; robust XPath locator; maintenance effort reduction; DOM selector",
        "released": 2016,
        "link": "https://doi.org/10.1002/smr.1771"
    },
    {
        "title": "New web application development tool and its MDA-based support methodology",
        "abstract": "Web applications are ubiquitous on the Internet, and almost every type of business now needs to be able to quickly develop their own Web applications. This paper introduces a technology developed by Fujitsu that reduces the development period for Web applications and also improves their quality. The paper discusses a development style that enables iterative refinement of a specification and specification checking with customers.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000222868500013"
    },
    {
        "title": "The OWL-s editor - a development tool for semantic web services",
        "abstract": "The power of Web Service (WS) technology lies in the fact that it establishes a common, vendor-neutral platform for integrating distributed computing applications, in intranets as well as the Internet at large. Semantic Web Services (SWSs) promise to provide solutions to the challenges associated with automated discovery, dynamic composition, enactment, and other tasks associated with managing and using service-based systems. One of the barriers to a wider adoption of SWS technology is the lack of tools for creating SWS specifications. OWL-S is one of the major SWS description languages. This paper presents an OWL-S Editor, whose objective is to allow easy, intuitive OWL-S service development and to provide a variety of special-purpose capabilities to facilitate SWS design. The editor is implemented as a plugin to the Protege OWL ontology editor, and is being developed as open-source software.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000230458500006"
    },
    {
        "title": "DIII-d tokamak control and neutral beam computer system upgrades",
        "abstract": "This paper covers recent computer system upgrades made to the DIII-D tokamak control and neutral beam computer systems. The systems responsible for monitoring and controlling the DIII-D tokamak and injecting neutral beam power have recently come online with new computing hardware and software. The new hardware and software have provided a number of significant improvements over the previous Modcomp AEG VME and accessware based systems. These improvements include the incorporation of faster, less expensive, and more readily available computing hardware which have provided performance increases of up to a factor 20 over the prior systems. A more modern graphical user interface with advanced plotting capabilities has improved feedback to users on the operating status of the tokamak and neutral beam systems. The elimination of aging and non supportable hardware and software has increased overall maintainability. The distinguishing characteristics of the new system include: (1) a PC based computer platform running the Redhat version of the Linux operating system; (2) a custom PCI CAMAC software driver developed by general atomics for the kinetic systems 2115 serial highway card; and (3) a custom developed supervisory control and data acquisition (SCADA) software package based on Kylix, an inexpensive interactive development environment (IDE) tool from borland corporation. This paper provides specific details of the upgraded computer systems. (C) 2004 Published by Elsevier B.V.",
        "keywords": "tokamak; neutral beams; control computers; CAMAC; linux",
        "released": 2004,
        "link": "https://doi.org/10.1016/j.fusengdes.2004.04.002"
    },
    {
        "title": "Nucleos(t)ide analogues causes HBV s gene mutations and carcinogenesis",
        "abstract": "BACKGROUND: The long-term use of nucleos(t)ide analogues causes drug resistance and mutations in the HBV reverse transcriptase (RT) region of the polymerase gene. The RT region overlaps the HBV surface gene (S gene) and therefore, the mutations in the RT region simultaneously modify S gene sequence. Certain mutations in the RT region bring about truncated S proteins because the corresponding changed S gene encodes a stop codon which results in the loss of a large portion of the C-terminal hydrophobic region of HBV surface protein. The rtA181T/sW172*, rtM204I/sW196* and rtV191I/sW182* are the most frequently reported drug-resistant mutations with C-terminal truncation, these mutations have oncogenic potential. DATA SOURCES: PubMed and Web of Science were searched using terms: “hepatitis B virus”, “HBV drug resistance mutation’; “HBV surface protein’; “HBV truncation; “hepatocellular carcinoma’; “rtA181T/sW172*’; “rtM204IfsW196*’; “rtV191I/sW182*’; and relevant articles published in English in the past decades were reviewed. RESULTS: The rtA181T/sW172* and rtV191I/sW182* mutants occurred more frequently than the rtM204I/sW196* mutant both in chronic hepatitis B patients and the HBV-related hepatocellular carcinoma tissues. Although these mutations occur naturally, nucleos(t)ide analogues therapy is the main driving force. These mutations may exist alone or coexist with other HBV mutations. All these three mutants impair the virion secretion and result in HBV surface protein retention and serum HBV DNA level reduction. These mutations possess potential carcinogenic properties. The three mutations are resistant to more than one nucleos(t)ide analogue and therefore, it is difficult to treat the patients with the truncated mutations. CONCLUSIONS: Nucleos(t)ide analogues induce drug resistance and HBV S gene truncated mutations. These mutations have potential carcinogenesis.",
        "keywords": "hepatitis B virus; drug resistance mutation; surface protein; C-terminal truncation; oncogenic potential; hepatocellular carcinoma",
        "released": 2016,
        "link": "https://doi.org/10.1016/S1499-3872(16)60064-4"
    },
    {
        "title": "CLOUDRB: A framework for scheduling and managing high-performance computing (HPC) applications in science cloud",
        "abstract": "In recent years, the Cloud environment has played a major role in running High-Performance Computing (HPC) applications, which are computationally intensive and data intensive in nature. The High-Performance Computing Cloud (HPCC) or Science Cloud (SC) provides the resources to these types of applications in an on demand and scalable manner. Scheduling of jobs or applications in a Cloud environment is NP-Complete and complex in nature due to the dynamicity of resources and on demand user application requirements. The main motivation behind this research study is to design and develop a CLOUD Resource Broker (CLOUDRB) for efficiently managing cloud resources and completing jobs for scientific applications within a user-specified deadline. It is implemented and integrated with a Deadline-based Job Scheduling and Particle Swarm Optimization (PSO)-based Resource Allocation mechanism. Our proposed approach intends to achieve the objectives of minimizing both execution time and cost based on the defined fitness function. It is simulated by modeling the HPC jobs and Cloud resources using the Matlab programming environment. The simulation results prove the effectiveness of the proposed research work by minimizing the completion time, cost and job rejection ratio and maximizing the number of jobs completing their applications within a deadline and meeting the user’s satisfaction. The proposed work has been tested in our Eucalyptus-based cloud environments by submitting real-world HPC applications and observed the improvements in performance. (C) 2013 Elsevier B.V. All rights reserved.",
        "keywords": "Cloud computing; High Performance Computing (HPC); CLOUD Resource Broker (CLOUDRB); Resource allocation; Job scheduling; Particle Swarm Optimization (PSO); Science cloud",
        "released": 2014,
        "link": "https://doi.org/10.1016/j.future.2013.12.024"
    },
    {
        "title": "Efficacy and safety of early chest tube removal after selective pulmonary resection with high-output drainage: A systematic review and meta-analysis",
        "abstract": "Background: There is controversy over the drainage threshold for removal of chest tubes in the absence of significant air leakage after selective pulmonary resection.Methods: A comprehensive search of online databases (PubMed, Web of Science, Embase, Cochrane Library, Scopus, Ovid, Elsevier, Ebsco, and Wiley) and clinical trial registries (WHO-ICTRP and ClinicalTrials.gov) was performed to investigate the efficacy and safety of early chest tube removal with high-output drainage. Primary outcome (postoperative hospital day) and secondary outcomes (30-day complications, rate of thoracentesis, and chest tube placement) were extracted and synthesized. Subgroup analysis, meta-regression, and sensitivity analysis were used to explore the potential heterogeneity. Study quality was assessed with the Newcastle-Ottawa Scale, and evidence was graded using the Grading of Recommendations Assessment, Development and Evaluation (GRADE) assessment by the online GRADEpro Guideline Development Tool.Results: Six cohort studies with a total of 1262 patients were included in the final analysis. The postoperative hospital stay in the high-output group was significantly shorter than in the conventional treatment group (weighted mean difference: -1.34 [-2.34 to -0.34] day, P = .009). While there was no significant difference between 2 groups in 30-day complications (relative ratio [RR]: 0.92 [0.77-1.11], P = .38), the rate of thoracentesis (RR: 1.93 [0.63-5.88], P = .25) and the rate of chest tube placement (RR: 1.00 [0.37-2.70], P = .99). According to the sensitivity analysis, the relative impacts of the 2 groups had already stabilized. Subgroup analysis revealed that postoperative hospital stay was modified by Newcastle-Ottawa Scale score. The online GRADEpro Guideline Development Tool presented very low quality of evidence for the available data.Conclusions: This meta-analysis revealed that it is feasible and safe to remove a chest tube with high-output drainage after pulmonary resection for selected patients.",
        "keywords": "chest tube removal; high-output drainage; selective pulmonary resection",
        "released": 2023,
        "link": "https://doi.org/10.1097/MD.0000000000033344"
    },
    {
        "title": "Model-driven web development with VisualWADE",
        "abstract": "VisualWADE(2) is a model-driven web development environment for web applications that includes model-based code generation techniques. This paper describes the capabilities of that environment and how Web applications can be generated in an automated way.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000223024600089"
    },
    {
        "title": "Systematic review with meta-analysis: Combination treatment of regimens based on pegylated interferon for chronic hepatitis b focusing on hepatitis b surface antigen clearance",
        "abstract": "Background: The seroclearance of hepatitis B surface antigen (HBsAg) in patients with chronic hepatitis B (CHB) is considered to be associated with favourable clinical outcomes. Aims: This meta-analysis was performed to establish the proportion of HBsAg loss rates among CHB patients who received combination treatment based on pegylated interferon (PegIFN). Four combination strategies have been studied with the aim of improving HBsAg loss: “de novo,” “NA-experienced,” “switch-to” and “add-on.” This meta-analysis was performed to determine which, if any, of these combination strategies was more effective. Methods: Medline, Web of Science and Embase databases were searched from inception to December 2017. The proportion of patients who achieved HBsAg loss after combination therapy was pooled using a random-effects model. Results: Twenty-four studies fulfilled the meta-analysis criteria. The overall pooled proportion suggested that the rate of HBsAg loss could be increased to 9% (95% CI: 7%-12%) based on the combination treatment in CHB patients. Compared with “de novo” strategy (8%, 95% CI: 6%-10%), the “nucleos(t)ide analogues-experienced” (11%, 95% CI: 8%-15%) was found to be more likely (P=0.036) to achieve a response. Compared with the “add-on” strategy (8%, 95% CI: 5%-13%), the “switch-to” (14%, 95% CI: 9%-20%) was found to be more likely (P=0.012) to achieve HBsAg loss. Conclusion: The “nucleos(t)ide analogues-experienced” strategy was more effective than the “De novo” strategy in achieving HBsAg loss for CHB patients. Combination treatment using regimens based on Peg-IFN may be useful to help nucleos(t)ide analogues-treated patients, who have experienced at least 48 weeks of nucleot(s)ide analogue, achieve HBsAg seroclearance.",
        "keywords": "",
        "released": 2018,
        "link": "https://doi.org/10.1111/apt.14629"
    },
    {
        "title": "AN ONLINE ASSISTANCE SYSTEM FOR THE SIMULATION-MODEL DEVELOPMENT ENVIRONMENT",
        "abstract": "",
        "keywords": "",
        "released": 1989,
        "link": "https://doi.org/10.1016/0020-7373(89)90022-9"
    },
    {
        "title": "Towards the novel reasoning among particles in PSO by the use of RDF and SPARQL",
        "abstract": "The significant development of the Internet has posed some new challenges and many new programming tools have been developed to address such challenges. Today, semantic web is a modern paradigm for representing and accessing knowledge data on the Internet. This paper tries to use the semantic tools such as resource definition framework (RDF) and RDF query language (SPARQL) for the optimization purpose. These tools are combined with particle swarm optimization (PSO) and the selection of the best solutions depends on its fitness. Instead of the local best solution, a neighborhood of solutions for each particle can be defined and used for the calculation of the new position, based on the key ideas from semantic web domain. The preliminary results by optimizing ten benchmark functions showed the promising results and thus this method should be investigated further.",
        "keywords": "",
        "released": 2014,
        "link": "https://doi.org/10.1155/2014/121782"
    },
    {
        "title": "Design of online professional development in science content and pedagogy: A pilot study in florida",
        "abstract": "The Exploring Florida Science project goals were: (1) increasing content knowledge of secondary science teachers, specifically in topics that are important to the future of the state, and (2) providing secondary science students with digital media for use in project-based learning. A team of instructional designers, educators, scientists, web designers and teacher educators designed an online professional development environment. Needs analysis included identification of the high incidence topics on the state science achievement test and topics of socio-scientific importance in the state. Development followed tenets of design-based research, and was guided by E-Learning for Educators standards and evaluated for content, pedagogy and usability using rubrics based on established guidelines. This paper details design, development, and evaluation frameworks, and summarizes pilot testing outcomes.",
        "keywords": "Professional development; Online; Secondary; Instructional design",
        "released": 2010,
        "link": "https://doi.org/10.1007/s10956-010-9210-2"
    },
    {
        "title": "A virtual laboratory for an online web-based course - “rapid e-business systems development”",
        "abstract": "This paper is to report a previous project on showing how physically classroom training, web-based course materials and web-based communication can help in teaching an e-business development course. The various planning and implementation issues in this project will also be discussed. The case addressed in this paper was about a virtual laboratory for an undergraduate course in City University of Hong Kong (CityU). One of the major objectives in this course is to design and develop an e-business application using existing programming tools and software packages. Students can learn the concepts of an Electronic Commerce (EC) website and follow the steps to build up a complete EC website. With the support of this virtual laboratory, the development and learning process can be carried out at any place of the world where Internet access is available.",
        "keywords": "",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000185886200010"
    },
    {
        "title": "Customization of library service in a cross-platform programming environment",
        "abstract": "This paper describes how library operations can be integrated in a cross-platform programming environment. It discusses the organizational and technical issues in planning and designing of macro applications from the technical services workstation. It then describes a variety of technical and public service macro applications. It shows the efficiency and ergonomic benefits of these customized programs. It concludes with an example of how PC-based macro-programs can populate Web-accessible server-side databases with ActiveX technologies.",
        "keywords": "",
        "released": 2001,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000167463200004"
    },
    {
        "title": "BioBlocks: Programming protocols in biology made easier",
        "abstract": "The methods to execute biological experiments are evolving. Affordable fluid handling robots and on-demand biology enterprises are making automating entire experiments a reality. Automation offers the benefit of high-throughput experimentation, rapid prototyping, and improved reproducibility of results. However, learning to automate and codify experiments is a difficult task as it requires programming expertise. Here, we present a web-based visual development environment called BioBlocks for describing experimental protocols in biology. It is based on Google’s Blockly and Scratch, and requires little or no experience in computer programming to automate the execution of experiments. The experiments can be specified, saved, modified, and shared between multiple users in an easy manner. BioBlocks is open-source and can be customized to execute protocols on local robotic platforms or remotely, that is, in the cloud. It aims to serve as a de facto open standard for programming protocols in Biology.",
        "keywords": "lab automation; rapid prototyping; reproducibility; Blockly; Scratch; high-level programming language",
        "released": 2017,
        "link": "https://doi.org/10.1021/acssynbio.6b00304"
    },
    {
        "title": "A visual programming environment for introducing distributed computing to secondary education",
        "abstract": "The paper introduces a visual programming language and corresponding web and cloud-based development environment called NetsBlox. NetsBlox is an extension of Snap! and builds upon its visual formalism as well as its open source code base. NetsBlox adds distributed programming capabilities by introducing two well-known abstractions to block-based programming: message passing and Remote Procedure Calls (RPC). Messages containing data can be exchanged by two or more NetsBlox programs running on different computers connected to the Internet. RPCs are called on a client program and are executed on the NetsBlox server. These two abstractions make it possible to create distributed programs such as multi-player games or client-server applications. We believe that NetsBlox not only teaches basic distributed programming concepts but also provides increased motivation for high-school students to become creators and not just consumers of technology. (C) 2018 Elsevier Inc. All rights reserved.",
        "keywords": "Visual programming; Distributed programming; Computer science education",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.jpdc.2018.02.021"
    },
    {
        "title": "Reducing the barriers to writing verified specifications",
        "abstract": "Formally verifying a program requires significant skill not only because of complex interactions between program sub-components, but also because of deficiencies in current verification interfaces. These skill barriers make verification economically unattractive by preventing the use of less-skilled (less-expensive) workers and distributed workflows (i.e., crowdsourcing). This paper presents VeriWeb, a web-based IDE for verification that decomposes the task of writing verifiable specifications into manageable subproblems. To overcome the information loss caused by task decomposition, and to reduce the skill required to verify a program, VeriWeb incorporates several innovative user interface features: drag and drop condition construction, concrete counterexamples, and specification inlining. To evaluate VeriWeb, we performed three experiments. First, we show that VeriWeb lowers the time and monetary cost of verification by performing a comparative study of VeriWeb and a traditional tool using 14 paid subjects contracted hourly from Exhedra Solution’s vWorker online marketplace. Second, we demonstrate the dearth and insufficiency of current ad-hoc labor marketplaces for verification by recruiting workers from Amazon’s Mechanical Turk to perform verification with VeriWeb. Finally, we characterize the minimal communication overhead incurred when VeriWeb is used collaboratively by observing two pairs of developers each use the tool simultaneously to verify a single program.",
        "keywords": "program verification; human factors; crowd-sourcing",
        "released": 2012,
        "link": "https://doi.org/10.1145/2398857.2384624"
    },
    {
        "title": "Polymerization of MIP-1 chemokine (CCL3 and CCL4) and clearance of MIP-1 by insulin-degrading enzyme",
        "abstract": "Macrophage inflammatory protein-1 (MIP-1), MIP-1 alpha (CCL3) and MIP-1 beta (CCL4) are chemokines crucial for immune responses towards infection and inflammation. Both MIP-1 alpha and MIP-1 beta form high-molecular-weight aggregates. Our crystal structures reveal that MIP-1 aggregation is a polymerization process and human MIP-1 alpha and MIP-1 beta form rod-shaped, double-helical polymers. Biophysical analyses and mathematical modelling show that MIP-1 reversibly forms a polydisperse distribution of rod-shaped polymers in solution. Polymerization buries receptor-binding sites of MIP-1 alpha, thus depolymerization mutations enhance MIP-1 alpha to arrest monocytes onto activated human endothelium. However, same depolymerization mutations render MIP-1 alpha ineffective in mouse peritoneal cell recruitment. Mathematical modelling reveals that, for a long-range chemotaxis of MIP-1, polymerization could protect MIP-1 from proteases that selectively degrade monomeric MIP-1. Insulin-degrading enzyme (IDE) is identified as such a protease and decreased expression of IDE leads to elevated MIP-1 levels in microglial cells. Our structural and proteomic studies offer a molecular basis for selective degradation of MIP-1. The regulated MIP-1 polymerization and selective inactivation of MIP-1 monomers by IDE could aid in controlling the MIP-1 chemotactic gradient for immune surveillance. The EMBO Journal (2010) 29, 3952-3966. doi:10.1038/emboj.2010.256; Published online 19 October 2010",
        "keywords": "chemokine; chemotactic gradient; insulin-degrading enzyme; MIP-1 polymerization; X-ray crystallography",
        "released": 2010,
        "link": "https://doi.org/10.1038/emboj.2010.256"
    },
    {
        "title": "Utilizing smartphones for approachable IoT education in k-12",
        "abstract": "Distributed computing, computer networking, and the Internet of Things (IoT) are all around us, yet only computer science and engineering majors learn the technologies that enable our modern lives. This paper introduces PhoneIoT, a mobile app that makes it possible to teach some of the basic concepts of distributed computation and networked sensing to novices. PhoneIoT turns mobile phones and tablets into IoT devices and makes it possible to create highly engaging projects through NetsBlox, an open-source block-based programming environment focused on teaching distributed computing at the high school level. PhoneIoT lets NetsBlox programs-running in the browser on the student’s computer-access available sensors. Since phones have touchscreens, PhoneIoT also allows building a Graphical User Interface (GUI) remotely from NetsBlox, which can be set to trigger custom code written by the student via NetsBlox’s message system. This approach enables students to create quite advanced distributed projects, such as turning their phone into a game controller or tracking their exercise on top of an interactive Google Maps background with just a few blocks of code.",
        "keywords": "IoT; mobile devices; sensors; user interaction; block-based programming",
        "released": 2022,
        "link": "https://doi.org/10.3390/s22249778"
    },
    {
        "title": "An operating system infrastructure for fault-tolerant reconfigurable networks",
        "abstract": "Dynamic hardware reconfiguration is becoming a key technology in embedded system design that offers among others new potentials in dependable computing. To make system designers benefit from this new technology, powerful infrastructures and programming environments are needed. In this paper, we will propose new concepts of an operating system (OS) infrastructure for reconfigurable networks that allow to efficiently design fault-tolerant systems. For this purpose, we consider a hardware/software solution that supports dynamic rerouting, hardware and software task migration, hardware/software task morphing, and online partitioning. Finally, we will present an implementation of such a reconfigurable network providing this OS infrastructure.",
        "keywords": "",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000236884200015"
    },
    {
        "title": "Autonomous monitoring system using wi-fi economic",
        "abstract": "In this project, it is presented the implementation of an autonomous monitoring system using solar panels and connecting to the network through Wi-Fi. The system will collect meteorological data and transmit in real-time to the web for the visualization and analysis of the results over temperature, humidity, and atmospheric pressure. The system will allow saving time and money, employing decision making and efficiency. For the development of this device, a small platform “Wemos D1” for the internet of things allows easy programming in the platform “Arduino IDE”.",
        "keywords": "Wemos d1 mini-skirt; Wi-Fi; sensor; internet",
        "released": 2019,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000485681800052"
    },
    {
        "title": "A method for performing an exhaustive evaluation of RDF(s) importers",
        "abstract": "Interoperability is one of the main quality criteria required for Semantic Web technology. In this paper we propose a method for defining benchmark suites for evaluating the RDF(S) importers of Semantic Web technology. We also show how this method was used for developing a benchmark suite that is being used for benchmarking the interoperability of ontology development tools.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000233807000021"
    },
    {
        "title": "Resource space grid: Model, method and platform",
        "abstract": "A Resource Space Grid is a virtual Grid that aims at effectively sharing, using and managing versatile resources across the Internet. The kernel of the Resource Space Grid includes a Resource Space Model (RSM) and a uniform Resource Using Mechanism (RUM). This paper presents the Resource Space Grid’s core scientific issues and methodology, architecture, model and theory, design criteria and method, and practice. A normal form theory is proposed to normalize the resource space-a coordinate system for uniformly specifying and organizing resources. The RUM provides not only the end-users with an operable resource browser to operate resources using the built-in Resource Operation Language (ROL), but also the application developers with the ROL-based programming environment. The prototype platform based on the proposed model and method has been implemented and used for sharing and managing resources in distributed research teams. Operations on Resource Spaces can constitute the virtual communities of Resource Space Grids-a platform independent resource sharing environment. Copyright (C) 2004 John Wiley Sons, Ltd.",
        "keywords": "resource sharing; query language; Semantic Grid; Semantic Web; Web",
        "released": 2004,
        "link": "https://doi.org/10.1002/cpe.867"
    },
    {
        "title": "Developing the TriLab, a triple access mode (hands-on, virtual, remote) laboratory, of a process control rig using LabVIEW and joomla",
        "abstract": "Laboratory education is a core part of engineering curricula; engineering students generally prefer to work on something real. The classical mode and the oldest form of laboratory education is the hands-on mode. Advances in information and communication technologies have contributed to the laboratory education by creating two new modes, the simulated (virtual) mode and the online controlled mode (remote). Recently, developments or utilizations of hybrid structures of two types (e.g., virtual and hands-on, or remote and hands-on) have been reported in the literature; however, until now there are no reports of hybrid structures of the three types together. This paper describes the technical development of a novel laboratory model that combines the three modes in one unifying software package, namely the TriLab, by using Laboratory Virtual Instrument Engineering Workbench (LabVIEW). It is shown that LabVIEW provides a single programming environment for developing all components of the TriLab. Furthermore, it is shown that the Joomla web content management system can be used as a solution for efficient deployment of a remote lab online portal on the top of the LabVIEW core software. The development of the TriLab using LabVIEW and Joomla for an Instrumentation and Control Engineering Laboratory rig is shown. The analysis of student survey is presented which has shown positive impact of the pedagogical utilization of the TriLab. This is the first paper which aims to provide engineering academics a generic architecture and software solutions to rapidly develop their own TriLab. (c) 2010 Wiley Periodicals, Inc. Comput Appl Eng Educ 21: 614-626, 2013",
        "keywords": "the TriLab; LabVIEW; Joomla; laboratory education; virtual and remote labs",
        "released": 2013,
        "link": "https://doi.org/10.1002/cae.20506"
    },
    {
        "title": "Contextual elements and conceptual components of information visibility on the web",
        "abstract": "Purpose - This paper aims to report the result of follow-up research oil end-users conceptions of information visibility oil the web and their conceptualizations of success and failure in web searching. Design/methodology/approach - The data were collected by a questionnaire followed by a brief interview with the participants. The questionnaire was developed based oil the information visibility model suggested by the author in the original study. Fifty-two library and information sciences, students from Tarbiat Mollem University (TMU) and Iran University of Medical Sciences (IUMS) in Tehran took part in the study. Findings - The model of information visibility can enable web users to gain a better understanding of their information seeking (IS) outcomes, and it can assist them to improve their information literacy skills. The model can provide a theoretical framework to investigate web users IS behavior and can be used as a diagnostic tool to explore the contextual mid conceptual elements affecting the visibility of information for end-users. Research limitations/implications - The paper suggests a visibility learning diary (VLD), which might be useful to measure the efficiency of information literacy training Courses. Originality/value - The contextual and conceptual approach of the paper provides ides a deeper insight into the issue of information visibility, which has received little attention by IS and information retrieval researchers until now.",
        "keywords": "Information searches; Internet; Information retrieval; Information literacy",
        "released": 2008,
        "link": "https://doi.org/10.1108/07378830810903355"
    },
    {
        "title": "An architecture for integrating OODBs with WWW",
        "abstract": "The main topic of this paper is how to structure information so that the view of the web, both within and across web pages, is dynamically customizable. We present an architecture that integrates Object-Oriented Databases with the World Wide Web to organize such dynamic structures. Different users, or the same user at different times, could have different views of the web. We discuss several architectural variants and implementation issues. Our chosen architecture provides high flexibility for a wide variety of applications, ranging from managed healthcare to software development environments, and has been realized in the dkweb system.",
        "keywords": "views; information structuring; meta information; embedded methods",
        "released": 1996,
        "link": "https://doi.org/10.1016/0169-7552(96)00046-3"
    },
    {
        "title": "Pair programming with generative AI",
        "abstract": "Generative AI based on large-language models is significantly impacting software development through IDE assistants, cloud-based APIs, and interactive chatbots for coding assistance. It excels in generating and translating code and data, navigating APIs, and creating boilerplate content, thereby enhancing productivity. However, it is prone to generating inaccurate information (”hallucinations”), erroneous code, and potentially introducing security vulnerabilities. To counter these risks, employing automated analysis tools, conducting rigorous testing, and maintaining a deep understanding of computer science concepts are essential. While generative AI can substantially aid development tasks it is not a replacement for human expertise, especially in understanding complex software, its requirements, and architecture.",
        "keywords": "",
        "released": 2024,
        "link": "https://doi.org/10.1109/MS.2024.3363848"
    },
    {
        "title": "Technological model for the implementation of a ubiquitous learning process on mobile cloud computing",
        "abstract": "This paper presents a proposal for a technological model that seeks to serve as a starting point for implementations of educational solutions that support training processes through ubiquitous education; This model was implemented on a technological platform based on Mobile Cloud Computing (MCC), which is supported on cloud computing technologies (Cloud Computing). To validate the application of the model, a case study was implemented, which is based on the topic of Mathematical Problems Resolution, an issue that is developed in subjects of first semesters at the university level. The implementation was carried out by developing an app (application for mobile devices) using the Microsoft Azure platform, as a Mobile Cloud Computing platform and the C # and ASP: NET development tools. The database manager used is MySQL.",
        "keywords": "ubiquitous learning; cloud computing; mobile cloud computing; problem-based learning; solving math problems",
        "released": 2020,
        "link": "https://doi.org/10.18273/revuin.v19n4-2020007"
    },
    {
        "title": "Mosaicode and the visual programming of web application for music and multimedia",
        "abstract": "The development of audio application demands a high knowledge about this application domain, traditional programming logic and programming languages. It is possible to use a Visual Programming Language to ease the application development, including experimentations and creative exploration of the language. In this paper we present a Visual Programming Environment to create Web Audio applications, called Mosaicode. Different from other audio creation platforms that use a visual approach, our environment is a source code generator based on code snippets to create complete applications.",
        "keywords": "Webaudio; Mosaicode; Visual Programming Language; Specific Domain (Programming) Languages; Graphical Programming Environment",
        "released": 2018,
        "link": "https://doi.org/10.5216/mh.v18i1.53577"
    },
    {
        "title": "Design of a low-cost portable electrocardiograph for telemedicine application",
        "abstract": "This paper presents the design of a portable electrocardiograph designed to provide community health care. The AD8232 main sensor has multiple options for displaying cardiac activity. The first option uses the serial plotter in the Arduino IDE, while the second employs LabVIEW, allowing additional observation of the patient’s blood pressure via block coding. In addition, the Arduino cloud is integrated to process the information captured by the ESP32, enabling visualization on any device with internet access. Through this platform, it is possible to download the studies performed in different periods (1 hour, 1 day, 7 days, and 15 days), with an efficiency percentage of 4.11%.",
        "keywords": "Portable Electrocardiograph; Healthcare; IoT; Community; Cardiac Activity",
        "released": 2024,
        "link": "https://doi.org/10.36561/ING.26.15"
    },
    {
        "title": "Extending movilog for supporting web services",
        "abstract": "Web Services enable computers to interact and exploit Web-accessible programs without human intervention. Despite researchers agree that mobile agent technology will obtain significant benefits from this line of research, the lack of proper development tools hinder the widespread adoption of mobile agent technology on the Web. This paper describes a novel programming language called WS-Log whose goal is to provide a tight integration between mobile agents and Web Services. Examples and experimental results showing some of the advantages of WS-Log are also reported. (c) 2006 Elsevier Ltd. All rights reserved.",
        "keywords": "mobile agents; logic programming; web services; intelligent agents",
        "released": 2007,
        "link": "https://doi.org/10.1016/j.cl.2006.02.001"
    },
    {
        "title": "PCONFIG: A web-based configuration tool for configure-to-order products",
        "abstract": "PCONFIG is a modern Web-based constraint system managing the complex configuration requirements of one product range from a specific computer manufacturer. The range spans multiple CPU types, Operating Systems, Option cards and all the diverse multi-way relationships found in the assembly of complex computers. A principled approach to configuration was adopted from the outset with special attention given to ongoing product enhancement. Computer Parts and Engineering Constraints are separated in this unique configuration engine allowing each to be updated independently; typically by different people. The complex modelling essential in capturing multi-way relationships is dealt with by coding configuration information using a tool boasting a patented pattern-matching algorithm. This highly versatile rules-based, object-orientated development tool encouraged the simplification of a potentially difficult and complex problem into a relatively straightforward and extensible system. This online configuration system goes live on the Digital customer Web pages early next year. Future enhancements to PCONFIG includes a parts editor, a constraints editor and ordering methods allowing users to place orders not only by part numbers, but also system functions and system benefits. (C) 1999 Elsevier Science B.V. All rights reserved.",
        "keywords": "configuration; Configure to Order; CTO; constraints : web-based tools; multiple configuration methods; Pconfig; modelling",
        "released": 1999,
        "link": "https://doi.org/10.1016/S0950-7051(99)00016-7"
    },
    {
        "title": "Risk-of-bias VISualization (robvis): An r package and shiny web app for visualizing risk-of-bias assessments",
        "abstract": "Despite a major increase in the range and number of software offerings now available to help researchers produce evidence syntheses, there is currently no generic tool for producing figures to display and explore the risk-of-bias assessments that routinely take place as part of systematic review. However, tools such as the R programming environment and Shiny (an R package for building interactive web apps) have made it straightforward to produce new tools to help in producing evidence syntheses. We present a new tool, robvis (Risk-Of-Bias VISualization), available as an R package and web app, which facilitates rapid production of publication-quality risk-of-bias assessment figures. We present a timeline of the tool’s development and its key functionality.",
        "keywords": "data visualization; evidence synthesis; R; risk of bias",
        "released": 2021,
        "link": "https://doi.org/10.1002/jrsm.1411"
    },
    {
        "title": "Automated cloud detection of satellite imagery using spatial modeler language and ERDAS macro language",
        "abstract": "Cloud detection from satellite imagery has been an important method of observing cloud covered areas. Presently, there are several algorithms for cloud detection, but there is no existing integrating frame and tools. The author used the software development tools within the commercial software ERDAS Imagine. The integrated software tools were the Imagine Developers’ Toolkit and the C programming language. ERDAS Imagine includes its own Graphical User Interface scripting language known as ERDAS Macro Language, and its own modeling language, known as Spatial Modeling Language. The novel module of software tools can convert raster data to brightness temperature and includes a special set time function, cloud top height, and cloud cover area. This study demonstrates successful cloud detection and classification from the Multi-functional Transport Satellite-2, which will be useful in Thailand’s efforts to forecast flood risks and provide early warnings of rain-causing clouds.",
        "keywords": "Automated cloud detection; Satellite imagery; Spatial modeler language; Erdas macro language",
        "released": 2013,
        "link": "https://doi.org/10.4103/0256-4602.113486"
    },
    {
        "title": "Heterogeneous parallel computing using java and WMPI",
        "abstract": "In this paper, we present briefly the implementation of a Java interface for WMPI, a Windows-based implementation of MPI, Then, we describe a system that is oriented for Web-based computing and present a solution to integrate WMPI with this tool by making use of a Java bridge component and the Java bindings for WMPI, This solution allows the execution of meta-applications over a mixed configuration of platforms, execution models and programming languages. The resulting system provides a way to solve the problem of heterogeneity and to unleash the potential of diverse computational resources and programming tools. Copyright (C) 2000 John Wiley & Sons, Ltd.",
        "keywords": "parallel computing; MPI; Java; cluster computing",
        "released": 2000,
        "link": "https://doi.org/10.1002/1096-9128(200009)12:11<1077::AID-CPE521>3.0.CO;2-\\#"
    },
    {
        "title": "Effects of intradialytic exercise on health-related quality of life in patients undergoing maintenance haemodialysis: A systematic review and meta-analysis",
        "abstract": "Purpose To summarize the current evidence regarding the effectiveness of intradialytic exercise (IDE) on the health-related quality of life (HRQOL) of patients undergoing maintenance haemodialysis. Methods Five English databases (PubMed, EMBASE, Cochrane Library, Web of Science, and ScienceDirect) and four Chinese databases (VIP, WAN FANG, CNKI, CBM) were comprehensively searched from their inception to 18 March 2021. This study was reported according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses statement. Two independent reviewers selected the papers and extracted the details of each study therein. Only randomized controlled trials were included. The risk of bias tool version 2.0 was used to evaluate the risk of bias of the included studies. A random-effects meta-analysis was conducted to pool the effect size. Results Thirty-three eligible studies with 1481 participants were included. For the generic HRQOL, assessed by the Medical Outcomes Study Short-Form survey, IDE significantly improved most domains and the physical component summary compared with the control group. Furthermore, aerobic exercise alone significantly improved more domains compared to resistance exercise, combined aerobic and resistance exercise, and other types of exercise. Regarding the kidney-specific HRQOL, IDE improved three of eleven domains, including the symptom/problem list, the effect of kidney disease, and the quality of social interaction. No significant effect was found on other domains of kidney-specific HRQOL. Conclusion Intradialytic exercise could benefit patients undergoing haemodialysis in improving most domains of generic HRQOL, but the effect on most domains of kidney-specific HRQOL is insufficient.",
        "keywords": "Intradialytic exercise; Health-related quality of life; Maintenance haemodialysis; Systematic review; Meta-analysis; Randomized controlled trials",
        "released": 2022,
        "link": "https://doi.org/10.1007/s11136-021-03025-7"
    },
    {
        "title": "WIRM: An open source toolkit for building biomedical web applications",
        "abstract": "This article describes an innovative software toolkit that allows the creation of web applications that facilitate the acquisition, integration, and dissemination of multimedia biomedical data over the web, thereby reducing the cost of knowledge sharing. There is a lack of highlevel web application development tools suitable for use by researchers, clinicians, and educators who are not skilled programmers. Our Web Interfacing Repository Manager (WIRM) is a software toolkit that reduces the complexity of building custom biomedical web applications. WIRM’s visual modeling tools enable domain experts to describe the structure of their knowledge, from which WIRM automatically generates full-featured, customizable content management systems.",
        "keywords": "",
        "released": 2002,
        "link": "https://doi.org/10.1197/jamia.M1138"
    },
    {
        "title": "Design and realisation of residential property management information system based on browser/server mode",
        "abstract": "The data filling, mathematical calculations and statistical operations for property management are realised by a system development tool that adopts the browser/server (B/S) architecture, Java, as the development language and a framework that adopts Spring MVC mode; additionally, the MySQL database is used, the source code and database interaction process use the Mybatis framework and the front-end display uses the VUE.js framework.. Functional modules include the following elements: owner information, real estate information, engineering equipment, personnel management and lease management. The system has passed the functional test, if it has been in trial operation for 1 year in multiple residential communities and has the characteristics of simple operation, stable operation and strong scalability, among others.",
        "keywords": "information system; management; browser/server; Spring MVC; MySQL database",
        "released": 2021,
        "link": "https://doi.org/10.2478/amns.2021.2.00046"
    },
    {
        "title": "Concepts of combinatorial chemistry and combinatorial technologies",
        "abstract": "A survey of basic concepts of combinatorial chemistry and combinatorial technologies and a great impact of this new approach on the traditional chemistry is presented. The main fields of application of CC/CT are reviewed and the reasons why CC/CT is so strongly needed and demanded are given. Be;ides obvious utilization of CC/CT in drug discovery, agrochemical research and research and development of new materials and catalysts also gain from this approach. The paper describes the origins and development of the technique, formed on the basis of probabilistic justifications. The applicability of combinatorial technologies and main combinatorial tools are described together with computer-assisted combinatorial chemistry, molecular design and biological methods of CC/CT. A Ist of important Web resources relevant to the topic is also presented.",
        "keywords": "combinatorial chemistry; combinatorial technologies",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000166201300006"
    },
    {
        "title": "Refiner: A problem-solving environment for scientific simulator creation",
        "abstract": "The science and engineering disciplines rely heavily on computer simulation as a tool for solving large, complex mathematical problems. The difficulty of creating scientific simulation code presents the need for a programming environment that is able to assist the programmer in the code development process. This paper describes a development methodology and a prototype implementation of a system that provides such assistance. Refiner, a programming environment for creating scientific simulators, provides expert users with online support for the entire development process, from mathematical modeling to low-level implementation details. Refiner is based on Posit, a high-level object-oriented modeling and programming language. Mathematical models are specified, and executable programs are then developed through the successive application of semantics-preserving program transformations. Code development is recorded in the form of a refinement tree structure where each path from root to leaf encodes a series of program transformations representing a single-solution strategy.",
        "keywords": "code generation; formal specification; program refinement; program transformation; PSE; scientific simulation",
        "released": 2002,
        "link": "https://doi.org/10.1177/0037549702078011002"
    },
    {
        "title": "Reusable software component retrieval system based on web",
        "abstract": "This study is to develop a web-based navigation tool to share various design objects generated in the whole development cycle of the software development environment based on UML, the previous stage of source code, to a version management system centering around an already existing source code between distributed development teams on the web. Furthermore, we provide a facet retrieval system based on which makes use of a object-oriented thesaurus, which supports an integrated environment through which all the project team members can share a lot of source codes and execution files as well as object files produced from the web-based collaborative development environment. Finally, we have designed and implemented a retrieval system for reusable software component that facilitates software developers to manage in their web-based search for the relationship of design information.",
        "keywords": "",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000184327300055"
    },
    {
        "title": "W-CoSIM (web-based hardware-software CoSimulator) - a design-time simulation tool for estimating the performance of online multimedia applications",
        "abstract": "Internet applications are increasingly being equipped with rich multimedia data. Most e-business development tools are quite good at authoring multimedia contents, however they lack explicit support for estimating the performance of the envisioned multimedia system at design time. W-CoSim (Web-based Hardware and Software Co-Simulator) is an EJB-based simulation tool used for validating the architecture and the performance of online multimedia systems at design time. W-CoSim has four components: Modeler, Translator, Engine and Scenario. Users start from Modeler to describe systems architecture by means of a UML(Unified Modeling Language) deployment diagram, and then specify hardware & software performance parameters such as the size of audio and video data, execution delay and network topology. All information specified in the Modeler are sent to the Translator, and then automatically converted to Java code. Engine and Scenario are responsible to run the Java code and produce results in the form of text and graphs.",
        "keywords": "",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000181472200119"
    },
    {
        "title": "Non-programmers identifying functionality in unfamiliar code: Strategies and barriers",
        "abstract": "Source code on the web is a widely available and potentially rich learning resource for non-programmers. However, unfamiliar code can be daunting to end-users without programming experience. This paper describes the results of an exploratory study in which we asked non-programmers to find and modify the code responsible for specific functionality within unfamiliar programs. We present two interacting models of how non-programmers approach this problem: the Task Process Model and the Landmark-mapping model. Using these models, we describe code search strategies non-programmers employed and the barriers they encountered. Finally, we propose guidelines for future programming environments that support non-programmers in finding functionality in unfamiliar programs. (C) 2010 Elsevier Ltd. All rights reserved.",
        "keywords": "Non-programmer; End-user; Code search; Strategy; Barrier; Comprehension; Navigation; Graphic output; Storytelling alice",
        "released": 2010,
        "link": "https://doi.org/10.1016/j.jvlc.2010.08.002"
    },
    {
        "title": "Evaluation of the use of an interactive, online resource for competency-based curriculum development",
        "abstract": "Purpose To evaluate pediatric educators’ use of and satisfaction with the Academic Pediatric Association’s Educational Guidelines for Pediatric Residency. Method The authors used customized programming to document all registered users and downloaded files from the Guidelines Web site for a 30-month period after site completion. An online survey of volunteer users was conducted. Results Between July 2005 and December 2007, 1,747 individuals registered on the Web site, and 8,754 files were downloaded. Registrants who downloaded files (n = 1,239) represented 97% of the pediatric residency programs in 2008 that were approved by the Accreditation Council for Graduate Medical Education. During 30 months, the frequency of downloads remained robust, peaking each spring. Curriculum-building tools were downloaded by 97% of programs using the site; the majority chose predesigned formats rather than self-selected lists of goals and objectives. Resident evaluation forms and tutorials were downloaded less frequently. A survey was completed by 111 site users, who indicated that the Guidelines tools were useful for Residency Review Committee site visit preparation. Most respondents said that the curriculum-building tools were easy to use, adaptable, and helpful in integration of competencies into residency programs. Respondents rated tutorials highly for educational content and clarity. Conclusions The data collection methods offer a practical strategy for evaluating access to online curriculum development tools. The majority of U.S. pediatric residency programs have accessed Guidelines’ resources for curriculum development; patterns of use have been sustained over time. Most users preferred the predesigned versions of the materials. Users surveyed found the tools useful for planning rotations and integrating competencies into their programs and reported high satisfaction with the Guidelines.",
        "keywords": "",
        "released": 2009,
        "link": "https://doi.org/10.1097/ACM.0b013e3181b18b21"
    },
    {
        "title": "What happened to my application? Helping end users comprehend evolution through variation management",
        "abstract": "Context: Millions of end users are creating software applications. These end users typically do not have clear requirements in mind; instead, they debug their programs into existence and reuse their own or other persons’ code. These behaviors often result in the creation of numerous variants of programs. Current end-user programming environments do not provide support for managing such variants. Objective: We wish to understand the variant creation behavior of end user programmers. Based on this understanding we wish to develop an automated system to help end user programmers efficiently manage variants. Method: We conducted an on-line survey to understand when and how end-user programmers create program variants and how they manage them. Our 124 survey respondents were recruited via email from among non computer science majors who had taken at least one course in the computer science department at our university; the respondents were involved in the Engineering, Sciences, Arts, and Management fields. Based on the results of this survey we identified a set of design requirements for providing variation management support for end users. We implemented variation management support in App Inventor - a drag and drop programming environment for creating mobile applications. Our support, AppinventorHelper, is meant to help end-user programmers visualize the provenance of and relationships among variants. We conducted a think-aloud study with 10 participants to evaluate the usability of AppinventorHelper. The participants were selected on a first come, first-served basis from those who responded to our recruitment email sent to list-servers. They were all end users majoring in electrical engineering, mechanical engineering, or physics. None had formal training in software engineering methods, but all had some experience with visual programming languages. Results: Our (user study) results indicate that AppinventorHelper can help end users navigate through variants and find variants that could be utilized cost-effectively as examples or actual code upon which to build new applications. For example, in one of our empirical studies end users explored variants of a paint application in order to find a variant that could easily be extended to incorporate a new feature. Conclusions: Our survey results show that end users do indeed reuse program variants and suggest that understanding the differences between variants is important. Further, end users prefer running code and looking at outputs, accessing source code and meta information such as filenames, referring to the creation and update dates of programs, and having information on the authors of code. When selecting variants users prefer to look at their major features such as correctness, similarity and authorship information. End users rely primarily on memory to track changes. They seldom make use of online or configuration management tools. Hence, integrated domain-specific variation management tools like AppInventorHelper can significantly help improve users’ interactions with the system. A key contribution of our work is a set of design requirements for end-user programming environments that facilitate the management and understanding of the provenance of program variants.",
        "keywords": "End-user programming; End-user software engineering; App inventor; Variation management",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.infsof.2018.06.008"
    },
    {
        "title": "Best practices for describing, consuming, and discovering web services: A comprehensive toolset",
        "abstract": "The service-oriented computing (SOC) paradigm has recently gained a lot of attention in the software industry because SOC represents a novel and a fresh way of architecting distributed applications. SOC is usually materialized via web services, which allows developers to structure applications exposing a clear, public interface to their capabilities. Although conceptually and technologically mature, SOC still lacks adequate development support from a methodological point of view. In this paper, we present the EasySOC project, a set of guidelines to simplify the development of service-oriented applications and services. EasySOC is a synthesized catalog of best SOC development practices that arise as a result of several years of research in fundamental Services Computing topics, that is, Web Service Description Language-based technical specification, Web Service discovery, and Web Service outsourcing. In addition, we describe a materialization of the guidelines for the Java language, which has been implemented as a plug-in for the Eclipse IDE. We believe that both the practical nature of the guidelines and the availability of this software that enforces them may help software practitioners to rapidly exploit our ideas for building real SOC applications. Copyright (c) 2012 John Wiley & Sons, Ltd.",
        "keywords": "service-oriented computing; service-oriented development guidelines; web services; WSDL anti-patterns; web service discovery; web service consumption",
        "released": 2013,
        "link": "https://doi.org/10.1002/spe.2123"
    },
    {
        "title": "BioLingua: A programmable knowledge environment for biologists",
        "abstract": "BioLingua is an interactive, web-based programming environment that enables biologists to analyze biological systems by combining knowledge and data through direct end-user programming. BioLingua embeds a mature symbolic programming language in a frame-based knowledge environment, integrating genomic and pathway knowledge about a class of similar organisms. The BioLingua language provides interfaces to numerous state-of-the-art bioinformatic tools, making these available as an integrated package through the novel use of web-based programmability and an integrated Wiki-based community code and data store. The pilot instantiation of BioLingua, which has been developed in collaboration with several cyanobacteriologists, integrates knowledge about a subset of cyanobacteria with the Gene Ontology, KEGG and BioCyc knowledge bases. We introduce the BioLingua concept, architecture and language, and give several examples of its use in complex analyses.",
        "keywords": "",
        "released": 2005,
        "link": "https://doi.org/10.1093/bioinformatics/bth465"
    },
    {
        "title": "BacAv, a new free online platform for clinical back-averaging",
        "abstract": "Objective: The back-average technique is very useful to study the relation between the activity in the cortex and the muscles. It has two main clinical applications, Bereitschaftspotential (BP) recording and myoclonus studies. The BP is a slow wave negativity originating in the supplementary motor cortex and premotor cortex that precedes voluntary movements. This wave also precedes involuntary movements in functional movement disorders (FMD), and it can be used as a helpful diagnostic tool. For the myoclonus studies, the back-average technique is very important to help localizing the source of the myoclonus. The hardware needed to do BP or myoclonus studies is standard and available in any electrophysiology lab, but there are not many software solutions to do the analysis. In this article together with describing the methodology that we use for recording clinical BPs and myoclonus, we present BacAv, an online free application that we developed for the purpose of doing back-average analysis. Methods: BacAv was developed in “R” language using Rstudio, a free integrated development environment. The recommended parameters for the data acquisition for BP recording and myoclonus studies are given in this section. Results: The platform was successfully developed, is able to read txt files, look for muscle bursts, segment the data, and plot the average. The parameters of the algorithm that look for the muscle bursts can be adapted according to the characteristics of the dataset. Conclusion: We have developed software for clinicians who do not have sophisticated equipment to do back-averaging. Significance: This tool will make this useful analysis method more available in a clinical environment. Published by Elsevier B.V. on behalf of International Federation of Clinical Neurophysiology.",
        "keywords": "Back-average; Bereitschaftspotential; Myoclonus",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.cnp.2019.12.001"
    },
    {
        "title": "Web-based environment for mechanism simulation integrated with CAD system",
        "abstract": "The present work describes the architecture of a new approach addressing the sharing of virtual prototypes over the Web with standard programming tools. The usage of standard Web-based technologies leads to easier, effective and more general applications suitable for small and medium size companies with limited resources in that field of research. Designers not only exchange just geometry data, but also more information about design knowledge and design solutions, and collaboration is performed on remote locations. In this work the framework is proposed to model a mechanical product focusing on behavioural criteria. In order to avoid time-consuming virtual model preparation, product variety is defined in single configuration file. The configuration file is covering all the variations, with formalisation of the assembly structure and linking the parts and attributes in uniform way. It enables visualisation of the models from different modelling systems and allows interactive changes and evaluation of the virtual prototype.",
        "keywords": "Kinematics simulation; Web services; XML; CAD",
        "released": 2010,
        "link": "https://doi.org/10.1007/s00366-009-0146-1"
    },
    {
        "title": "Towards an end-user development approach for web engineering methods",
        "abstract": "End-users who. are nonprogrammers create web applications by using advanced web development tools. However, these tools are not supported by any methodological process which produces that web applications are of low quality. This paper presents an approach to bring web engineering principles to the end-user community. We complement the web engineering method OOWS with tools that allow end-users to develop web applications by: (1) describing web applications in terms of the end-users’ knowledge about the application domain, (2) automatically obtaining a web application prototype by means of the OOWS code generation strategy, and (3) personalizing the web application look and feel by simply selecting a design template. To achieve this, an ontology-based strategy is introduced to support end-users throughout the web application development. We also introduce a strategy that allows us to define domain-independent presentation templates.",
        "keywords": "",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000238486900035"
    },
    {
        "title": "Flexible retrieval of web services",
        "abstract": "An important issue arising from Web Service applications is how to conveniently, accurately and efficiently retrieve services from large-scale and expanding service repositories. This paper proposes a flexible Web Service retrieval approach, which solves this issue by means of an orthogonal service space and establishing the multi-valued specialization relationships between services. The similarity degree between services is measured based on the specialization relationship between operations defined in services. An SQL-like flexible query language is used to support the flexible retrieval of services. The related programming environment and graphical user operation interface of the language have been implemented in the Service Grid environment. Compared with the current UDDI-based service retrieval approach, the proposed approach has the advantages of convenience, accuracy and efficiency. (C) 2003 Elsevier Inc. All rights reserved.",
        "keywords": "flexible retrieval; service grid; specialization; UDDI; web services",
        "released": 2004,
        "link": "https://doi.org/10.1016/S0164-1212(03)00003-7"
    },
    {
        "title": "Large-scale, multi-temporal remote sensing of palaeo-river networks: A case study from northwest india and its implications for the indus civilisation",
        "abstract": "Remote sensing has considerable potential to contribute to the identification and reconstruction of lost hydrological systems and networks. Remote sensing-based reconstructions of palaeo-river networks have commonly employed single or limited time-span imagery, which limits their capacity to identify features in complex and varied landscape contexts. This paper presents a seasonal multi-temporal approach to the detection of palaeo-rivers over large areas based on long-term vegetation dynamics and spectral decomposition techniques. Twenty-eight years of Landsat 5 data, a total of 1711 multi-spectral images, have been bulk processed using Google Earth Engine((C)) Code Editor and cloud computing infrastructure. The use of multi-temporal data has allowed us to overcome seasonal cultivation patterns and long-term visibility issues related to recent crop selection, extensive irrigation and land-use patterns. The application of this approach on the Sutlej-Yamuna interfluve (northwest India), a core area for the Bronze Age Indus Civilisation, has enabled the reconstruction of an unsuspectedly complex palaeo-river network comprising more than 8000 km of palaeo-channels. It has also enabled the definition of the morphology of these relict courses, which provides insights into the environmental conditions in which they operated. These new data will contribute to a better understanding of the settlement distribution and environmental settings in which this, often considered riverine, civilisation operated.",
        "keywords": "multi-temporal; seasonal; vegetation; palaeo-river; Indus Civilisation; archaeology",
        "released": 2017,
        "link": "https://doi.org/10.3390/rs9070735"
    },
    {
        "title": "INTERNET AS CLINICAL INFORMATION-SYSTEM - APPLICATION DEVELOPMENT USING THE WORLD-WIDE-WEB",
        "abstract": "Clinical computing application development at Columbia-Presbyterian Medical Center has been limited by the lack of a flexible programming environment that supports multiple client user platforms. The World Wide Web offers a potential solution, with its multifunction servers, multiplatform clients, and use of standard protocols for displaying information. The authors are now using the Web, coupled with their own local clinical data server and vocabulary server, to carry out rapid prototype development of clinical information systems. They have developed one such prototype system that can be run on most popular computing platforms from anywhere on the Internet. The Web paradigm allows easy integration of clinical information with other local and Internet-based information sources. The Web also simplifies many aspects of application design; for example, it includes facilities for the use of encryption to meet the authors’ security and confidentiality requirements. The prototype currently runs on only the Web server in the Department of Medical Informatics at Columbia University, but it could be run on other Web servers that access the authors’ clinical data and vocabulary servers. It could also be adapted to access clinical information from other systems with similar server capabilities. This approach may be adaptable for use in developing institution-independent standards for data and application sharing.",
        "keywords": "",
        "released": 1995,
        "link": "https://doi.org/10.1136/jamia.1995.96073829"
    },
    {
        "title": "Component-oriented application construction for a web service-based grid",
        "abstract": "We present the architecture and prototype implementation of a component-oriented programming environment for a Web service based computational Grid. As middleware, we utilize the Vienna Grid Environment (VGE), a framework that enables the provision of compute-intensive parallel applications as configurable, QoS-aware Grid services. Our component model follows the Common Component Architecture (CCA) and models application Web services as distributed components. We describe a component framework that integrates VGE services with a component model allowing to express and dynamically manage application and performance meta-data as well as dependencies on the infrastructure or other components. Furthermore, we show how the client programming interface is used to compose Grid applications from abstract application components that are mapped against available Grid services by the component framework at runtime. Copyright (c) 2006 John Wiley & Sons, Ltd.",
        "keywords": "component-based programming; compute-intensive Grid applications; performance related metadata; CCA; Web services",
        "released": 2007,
        "link": "https://doi.org/10.1002/cpe.1074"
    },
    {
        "title": "Enhancing students’ beliefs regarding programming self-efficacy and intrinsic value of an online distributed programming environment",
        "abstract": "Several studies have explored the factors that influence self-efficacy as well as its contribution to academic development in online learning environments in recent years. However, little research has investigated the effect of a web-based learning environment on enhancing students’ beliefs about self-efficacy for learning. This is especially noticeable in the field of online distributed programming. We need to design online learning environments for programming education that foster both students’ self-efficacy for programming learning and the added value that students perceive of the tool as a successful learning environment. To that end, we conducted a quantitative analysis to collect and analyze data of students using an online Distributed Systems Laboratory (DSLab) in an authentic, long-term online educational experience. The results indicate that (1) our distributed programming learning tool provides an environment that increases students’ belief of programming self-efficacy; (2) the students’ experience with the tool strengthens their belief in the intrinsic value of the tool; however (3) the relationship between students’ belief in the tool intrinsic value and their self-efficacy is inconclusive. This study provides relevant implications for online distributed (or general) programming course teachers who seek to increase students’ engagement, learning and performance in this field.",
        "keywords": "Online distributed programming learning; Programming self-efficacy; Intrinsic value",
        "released": 2022,
        "link": "https://doi.org/10.1007/s12528-022-09310-9"
    },
    {
        "title": "Rapid characterization of antibodies via automated flow injection coupled with online microdroplet reactions and native-pH mass spectrometry",
        "abstract": "Microdroplet reactions have aroused much interest due to significant reaction acceleration (e.g., ultrafast protein digestion in microdroplets could occur in less than 1 ms). This study integrated a microdroplet protein digestion technique with automated sample flow injection and online mass spectrometry (MS) analysis, to develop a rapid and robust method for structural characterization of monoclonal antibodies (mAbs) that is essential to assess the antibody drug’s safety and quality. Automated sequential aspiration and mixing of an antibody and an enzyme (IdeS or IgdE) enabled rapid analysis with high reproducibility (total analysis time: 2 min per sample; reproducibility: similar to 2% coefficient of variation). Spraying the sample in ammonium acetate buffer (pH 7) using a jet stream source allowed efficient digestion of antibodies and efficient ionization of resulting antibody subunits under native-pH conditions. Importantly, it also provided a platform to directly study specific binding of an antibody and an antigen (e.g., detecting the complexes mAb/RSFV antigen and F(ab ‘)2/RSVF in this study). Furthermore, subsequent tandem MS analysis of a resulting subunit from microdroplet digestion enabled localizing post-translational modifications on particular domains of a mAb in a rapid fashion. In combination with IdeS digestion of an antibody, additional tris(2-carboxyethyl)phosphine (TCEP) reduction and N-glycosidase F (PNGase F) deglycosylation reactions that facilitate antibody analysis could be realized in “one-pot” spraying. Interestingly, increased deglycosylation yield in microdroplets was found, simply by raising the sample temperature. We expect that our method would have a high impact for rapid characterization of monoclonal antibodies.",
        "keywords": "",
        "released": 2023,
        "link": "https://doi.org/10.1021/acs.analchem.2c04535"
    },
    {
        "title": "Integrating symbolic and numerical features for fault prediction and diagnosis by an expert system",
        "abstract": "This paper addresses the topic of a predictive task after integration of symbolic and numerical features by a hybrid diagnostic expert system. The online interaction of information and knowledge from various sources is achieved after successful combination of different development environments, tools and programs. The system infers using cooperatively dynamic modelling information, online sensor information, and stored knowledge in the knowledge base.",
        "keywords": "expert systems; model-based diagnosis; fault prediction; fault diagnosis; online process; hybrid intelligent systems",
        "released": 1999,
        "link": "https://doi.org/10.1111/1468-0394.00114"
    },
    {
        "title": "Web enabled client-server model for development environment of distributed image processing",
        "abstract": "Image processing applications (IPA) requirements can be best met by using the distributed environment. The authors had developed an environment over a network of VAX/VMS and Unix for distributed image processing. The efficiency was as high as 90-95%. This paper presents an augmentation and generalization of the environment using Java and web technology to make it truly system independent. Although the environment has been tested using image processing applications, the design and architecture is truly general so that it can be used for other applications, which require distributed processing.",
        "keywords": "DEDIP; parallel image processing; distributed image processing",
        "released": 2001,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000170924000013"
    },
    {
        "title": "PROGDOC - a new program documentation system",
        "abstract": "Though programming languages and programming styles evolve with remarkable speed today, there is no such evolution in the field of program documentation. And although there exist some popular approaches like Knuth’s literate programming system WEB [26], and nowadays JavaDoc [15] or Doxygen [16], tools for managing software development and documentation are not as widespread as desirable. This paper analyses a wide range of literate programming tools available during the past two decades and introduces PROGDOC, a new software documentation system. It is simple, language independent, and it keeps documentation and the documented software consistent. It uses LATEX for typesetting purposes, supports syntax highlighting for various languages, and produces output in Postscript, PDF or HTML format.",
        "keywords": "",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000189417700044"
    },
    {
        "title": "Security enhancement using blockchain based modified infinite chaotic elliptic cryptography in cloud",
        "abstract": "Cloud computing (CC) is an emerging field in the IT sector due to its prospective, progress and incredible impact on the development of that sector. CC is a centralized and distributed network comprised of interrelated and interconnected systems, and the whole universe uses the cloud for the ultimate portion of data transmission. However, the cloud cannot provide security assurance, which is one of the main drawbacks of the cloud. Blockchain is the fascinating massive attention in the cloud field because of its security features. It consists of a chain of several blocks composed of multiple transactions. These data available in blocks are encrypted and cannot be altered by any in the world. Several emerging applications are affected by insurance companies, anomalies, thieves, and intruders fetching individual data and disturbing the user’s routine. This paper develops a blockchain-based modified infinite chaotic elliptic cryptography (MICEC) to improve and tighten the security in the cloud. It contains three stages: protection of authentication, ownership protection, and identity mapping validation. At first, MICEC is employed for an authentication process that combines the infinite elliptic curve cryptography and modified chaotic neural network for pair of key generation and data encryption. In addition, the hash function is generated with the aid of the improved message digest 5 algorithm. Second, the improved Message Content Recommendation Algorithm with Latent Dirichlet Allocation is used to compute the score values. At last, cosine similarity matching criteria is computed in the validation based on the digests. The proposed work is simulated using the JAVA programming language with NetBeans IDE. The performance is evaluated and compared to other methods.",
        "keywords": "Blockchain; Cloud computing; Security; Privacy; Authentication; Hash function; Encryption",
        "released": 2023,
        "link": "https://doi.org/10.1007/s10586-022-03777-y"
    },
    {
        "title": "The protege OWL plugin: An open development environment for semantic web applications",
        "abstract": "We introduce the OWL Plugin, a Semantic Web extension of the Protege ontology development platform. The OWL Plugin can be used to edit ontologies in the Web Ontology Language (OWL), to access description logic reasoners, and to acquire instances for semantic markup. In many of these features, the OWL Plugin has created and facilitated new practices for building Semantic Web contents, often driven by the needs of and feedback from our users. Furthermore, Protege’s flexible open-source platform means that it is easy to integrate custom-tailored components to build real-world applications. This document describes the architecture of the OWL Plugin, walks through its most important features, and discusses some of our design decisions.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000225767000017"
    },
    {
        "title": "Tools and approaches for developing data-intensive web applications: A survey",
        "abstract": "The exponential growth and capillar diffusion of the Web are nurturing a novel generation of applications, characterized by a direct business-to-customer relationship. The development of such applications is a hybrid between traditional IS development and Hypermedia authoring, and challenges the existing tools and approaches for software production. This paper investigates the current situation of Web development tools, both in the commercial and research fields, by identifying and characterizing different categories of solutions, evaluating their adequacy to the requirements of Web application development, enlightening open problems, and exposing possible future trends.",
        "keywords": "design; experimentation; languages; reliability; application; development; HTML; intranet; WWW",
        "released": 1999,
        "link": "https://doi.org/10.1145/331499.331502"
    },
    {
        "title": "Commercial tools for the development of personalized web applications: A survey",
        "abstract": "In this paper we examine the state-of-the-practice of development tools for delivering personalized Web sites, Le Web-oriented applications that collect, elaborate and use information about the site’s users to better fulfill their mission. Personalization is at the same time one of the crucial success factors of B2C applications and one of the most significant cost factors in Web application development. In this paper, we classify the dimensions of personalized Web site development, review and classify 50 tools claiming to support such development, and motivate our conclusions on the need of a different approach to the personalization design and a novel generation of personalization tools.",
        "keywords": "",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000182748400011"
    },
    {
        "title": "Product development in a collaborative design environment",
        "abstract": "This paper reports the development of a collaborative product development environment using multidisciplinary analysis and design (MAD) technology and the information infrastructure of the WWW/Internet. The proposed product development environment provides a two level hierarchical setup for product development within the framework of a collaborative product development (CPD) system through the collocation of product designers (first level) and design services (second level). The prototype CPD system offers the following facilities for web based product development: (2) an environment for the Visual presentation, interactive refinement and specification of product specifications on VRML based sharable CAD models on the web, (2) web integrated product design data management system for supporting CPD tasks, and (3) web based design and manufacturability analysis. The prototype CPD system has been implemented using a variety of commercial packages such as I-DEAS and CastCHECK, and programming languages such as JAVA, C, PERL and FORTRAN77 and also encompasses a collection of web servers on different computing platforms such as SUN, DEC-Alpha and Windows NT workstations.",
        "keywords": "enterprise integration; collaborative product development; VRML based product model",
        "released": 1997,
        "link": "https://doi.org/10.1177/1063293X9700500407"
    },
    {
        "title": "CAPIRS: COVID-19-based application programming interface recommendation system for the developers",
        "abstract": "Context: From the past few years, Application Programming Interface (API) is widely used for mobile- and web-based application developments. Software developers can integrate third-party services into their projects to achieve their development goals efficiently using APIs; however, with the rapid increase in the number of APIs, the manual selection of Mashup-oriented API is becoming more difficult for the developer. Objective: In the COVID-19 pandemic, everyone wants an update about the latest Standard Operating Procedures (SOPs) and the latest information on COVID-19. Additionally, a software developer wants to develop an application that provides the SOPs and latest information of COVID-19; a developer can add these functionalities into an application using COVID-19-based APIs. Moreover, the current work aims at proposing a COVID-19-based API recommendation system for the developers. Method: In this study, we propose a COVID-19-based API recommendation system for developers. The recommendation system takes a developer query as input and recommends top-3 APIs and supported features, which help the developer during software development. Furthermore, the proposed COVID-19-based API recommendation system ensures the maximum participation of the developers by validating the recommended APIs and recommendation system from the expert developers using research questionnaires. Results: Additionally, the proposed COVID-19-based API recommendation system’s output is validated by expert developers and evaluated on 120 expert developers’ queries. In addition, experiment results show that single value decomposition achieves better prediction. Conclusion: We conclude that it is significantly important to recommend APIs along with supported features to the developer for project development, and future work is needed to take more developer’s queries also to build Integrated Development Environment for the developers.",
        "keywords": "Application Programming Interface; COVID-19; recommendation system; machine learning; expert developers",
        "released": 2022,
        "link": "https://doi.org/10.1142/S0219649222400044"
    },
    {
        "title": "RETRACTED: Engineering vehicle management system based on the internet of things (retracted article. See DEC, 2022)",
        "abstract": "To improve the efficiency of engineering vehicles to save cost and accelerate production operation, the Browser/Server (B/S) architecture is used, ASP.NET is used as a development tool, and Oracle is regarded as the background database management. The management system has good scalability and maintainability. The implementation of this system simplifies the engineering vehicle management process. The background and significance of the system development are described, and the technology used in the development system is explained. Moreover, the system is analysed and designed in detail, and the system is finally realized and tested. In the design and implementation of the system, seven functional modules of the system are completed, and the function of each functional module is designed and tested.",
        "keywords": "Transportation cost; Engineering vehicle; B; S architecture; Database management",
        "released": 2020,
        "link": "https://doi.org/10.1007/s00521-018-3781-x"
    },
    {
        "title": "Efficacy of nucleot(s) ide analogs therapy in patients with unresectable HBV-related hepatocellular carcinoma: A systematic review and meta-analysis",
        "abstract": "Aim. To determine whether nucleot(s) ide analogs therapy has survival benefit for patients with HBV-related HCC after unresectable treatment. Method. A systematic search was conducted through seven electronic databases including PubMed, OVID, EMBASE, Cochrane Databases, Elsevier, Wiley Online Library, and BMJ Best Practice. All studies comparing NA combined with unresectable treatment versus unresectable treatment alone were considered for inclusion. The primary outcome was the overall survival (OS) after unresectable treatment for patients with HBV-related HCC. The secondary outcome was the progression-free survival (PFS). Results were expressed as hazard ratio (HR) for survival with 95% confidence intervals. Results. We included six studies with 994 patients: 409 patients in nucleot(s) ide analogs therapy group and 585 patients without antiviral therapy in control group. There were significant improvements for the overall survival (HR = 0.57; 95% CI = 0.47-0.70; p < 0.001) and progression-free survival (HR = 0.84; 95% CI = 0.71-0.99; p = 0.034) in the NA-treated group compared with the control group. Funnel plot showed that there was no significant publication bias in these studies. When it comes to antiviral drugs and operation method, it also showed benefit in NA-treated group. At the same time, overall mortality as well as mortality secondary to liver failure in NA-treated group was obviously lesser. Sensitivity analyses confirmed the robustness of the results. Conclusions. Nucleot(s) ide analogs therapy after unresectable treatment has potential beneficial effects in terms of overall survival and progression- free survival. NA therapy should be considered in clinical practice.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1155/2017/7075935"
    },
    {
        "title": "PLA(id): A tool for organising and sharing on-line building product information",
        "abstract": "This paper outlines the objectives and issues of a research project about on-line product information used by architects, engineers and other design professionals. After identifying some key issues pertinent to Web-based product information, we introduce PLA(id), a tool for organising and sharing product information on the World Wide Web. The system has two different versions, each involving different design strategies and development tools. (C) 2002 Elsevier Science B.V. All rights reserved.",
        "keywords": "computer aided design; on-line product information; digital libraries; building materials; java shockwave applications",
        "released": 2002,
        "link": "https://doi.org/10.1016/S0926-5805(01)00068-1"
    },
    {
        "title": "Choosing a chatbot development tool",
        "abstract": "Chatbots are programs that supply services to users via conversation in natural language, acting as virtual assistants within social networks or web applications. Here, we review the most representative chatbot development tools with a focus on technical and managerial aspects.",
        "keywords": "Chatbot; Software tools; Computer applications; Companies; Social networking (online); Testing; Software Engineering; Chatbots; Natural Language Processing",
        "released": 2021,
        "link": "https://doi.org/10.1109/MS.2020.3030198"
    },
    {
        "title": "Unanticipated integration of development tools using the classification model",
        "abstract": "The increasing complexity of software development spawns lots of specialised tools to edit code, employ UML schemes, integrate documentation, and so on. The problem is that the tool builders themselves are responsible for making their tools interoperable with other tools or development environments. Because they cannot anticipate all other tools they can integrate with, a lot of tools cannot co-operate. This paper introduces the classification model, a lightweight integration medium that enables unrelated tools that were not meant to be integrated to cooperate easily. Moreover, the tool integration is done by a tool integrator, and not by the tool builder. To validate this claim, we show how to integrate several third-party tools using the classification model, and how it forms the foundation for the StarBrowser, a Smalltalk browser integrating different tools. (C) 2003 Elsevier Ltd. All rights reserved.",
        "keywords": "tool integration; software classifications; development environments",
        "released": 2004,
        "link": "https://doi.org/10.1016/j.cl.2003.08.003"
    },
    {
        "title": "The design and analysis of real-time systems using the ASTRAL software development environment",
        "abstract": "ASTRAL is a formal specification language for real-time systems. It is intended to support formal software development and, therefore, has been formally defined. The structuring mechanisms in ASTRAL allow one to build modularized specifications of complex systems with layering. A real-time system is modeled by a collection of state machine specifications and a single global specification. This paper discusses the ASTRAL Software Development Environment (SDE), which is an integrated set of design and analysis tools based on the ASTRAL formal framework. The tools that make up the support environment are a syntax-directed editor, a specification processor, a verification condition generator, a browser kit, a model checker, and a mechanical theorem prover.",
        "keywords": "",
        "released": 1999,
        "link": "https://doi.org/10.1023/A:1018934104631"
    },
    {
        "title": "Developing usable web sites - a review and model",
        "abstract": "The number and range of organisations developing a Web site is growing rapidly. Many of these Web sites are developed in-house, even though the skills and resources required for developing a successful site may not be available. It is argued that some of the limitations, in terms of resources and skills, inherent in the small-scale, in-house development environment can be overcome through the adoption of an informal Web site development model and suitable usability methods. Presents an informal development model synthesised from a review of development case studies and published Web research literature. This model identifies the main stages and tasks of development. A review of information gathering and usability methods currently being employed is integrated into the model. The importance of understanding user and information provider needs is discussed. A number of common usability methods are then examined in greater detail. The appropriateness of the model and methods for the small-scale, in-house development environment is considered.",
        "keywords": "World Wide Web; user studies; human-computer interaction",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000169652400010"
    },
    {
        "title": "Benchmark suites for improving the RDF(s) importers and exporters of ontology development tools",
        "abstract": "Interoperability is the ability of two or more systems to interchange information and to use the information that has been interchanged. Nowadays, interoperability between ontology development tools is low. Therefore, to assess and improve this interoperability, we propose to perform a benchmarking of the interoperability of ontology development tools using RDF(S) as the interchange language. This paper presents, on the one hand, the interoperability benchmarking that is currently in progress in Knowledge Web(1) and, on the other, the benchmark suites defined and used in this benchmarking.",
        "keywords": "",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000238574900011"
    },
    {
        "title": "Developing usable web sites - a review and model",
        "abstract": "The number and range of organisations developing a Web site is growing rapidly. Many of these Web sites are developed in-house, even though the skills and resources required for developing a successful site may not be available. It is argued that some of the limitations, in terms of resources and skills, inherent in the small-scale, in-house development environment can be overcome through the adoption of an informal Web site development model and suitable usability methods. Presents an informal development model synthesised from a review of development case studies and published Web research literature. This model identifies the main stages and tasks of development. A review of information gathering and usability methods currently being employed is integrated into the model. The importance of understanding user and information provider needs is discussed. A number of common usability methods are then examined in greater detail. The appropriateness of the model and methods for the small-scale, in-house development environment is considered.",
        "keywords": "World Wide Web; user studies; human-computer interaction",
        "released": 2000,
        "link": "https://doi.org/10.1108/10662240010342577"
    },
    {
        "title": "Optimal allocation of IaaS cloud resources through enhanced moth flame optimization (EMFO) algorithm",
        "abstract": "A new generation of computing resources is available to customers via IaaS, PaaS, and SaaS administrations, making cloud computing the most significant innovation in recent history for the general public. A virtual machine (VM) is configured, started, and maintained across numerous physical hosts using IaaS. In many cases, cloud providers (CPs) charge utility customers who have registered their premises with the utility registration authorities. Given the opposing aims of increasing customer demand fulfillment while decreasing costs and optimizing asset efficiency, efficient VM allocation is generally considered as one of the most difficult tasks for CPs to overcome. This paper proposes the Enhanced Moth Flame Optimization (EMFO) algorithm to provide a unique strategy for assigning virtual machines to suit customer requirements. The recommended approach is applied on Amazon’s EC2 after three distinct experiments are assumed. The utility of the proposed method is further shown by the use of well-known optimization techniques for effective VM allocation. The app was created using a Java-based programming language and then run on the Netbeans IDE 12.4 platform.",
        "keywords": "cloud computing; VM allocation; cloud providers; optimization; private cloud; external cloud",
        "released": 2022,
        "link": "https://doi.org/10.3390/electronics11071095"
    },
    {
        "title": "Effortless construction and management of program animations on the web",
        "abstract": "We describe an extension of a programming environment to generate web-based program animations. Emphasis is put on requiring little effort from the instructor that handles the system. User interaction is reduced to a minimum, mostly for customizing the animations. Both construction and maintenance are considered in order to guarantee low effort in an actual educational scenario. We describe several aspects of a program animation: the different kinds of information that compose it, its construction process, alternative graphical designs for web publishing, and its implementation as a package. In general, the instructor will wish to use the system to construct and handle a collection of animations for one or several courses. Therefore, we also consider the creation and management of collections of animations in a effortless way. Finally, we describe our experience as well as related work.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000231036300016"
    },
    {
        "title": "Cloud networked robotics",
        "abstract": "This article proposes a new field of research called Cloud Networked Robotics, which tackles the issues for supporting daily activity, especially for the elderly and the disabled, throughout various locations in a continuously and seamless manner by abstracting robotic devices and providing a means for utilizing them as a cloud of robots. With recent advances in robotic development environments and in integrated multi-robot systems, robots are acquiring richer functionalities and robotic systems are becoming much easier to develop. However, such stand-alone robotic services are not enough for continuously and seamlessly supporting daily activity. We examine the requirements in typical daily supporting services through example scenarios that target senior citizens and the disabled. Based on these requirements, we discuss the key research issues in cloud network robotics. As a case study, a field experiment in a shopping mall shows how our proposed prototype infrastructure of cloud networked robotics enables multi-location robotic services for life support.",
        "keywords": "",
        "released": 2012,
        "link": "https://doi.org/10.1109/MNET.2012.6201213"
    },
    {
        "title": "An information system for retrieving and reasoning about XML-based mathematical knowledge",
        "abstract": "XML has become a key language for information interchange and integration over the World Wide Web. Representing and storing mathematical achievements and knowledge in a self-describing, extensible and open manner facilitates Web information systems which simplify co-operation among mathematicians world wide. In this paper, we describe a knowledge base management system for XML-based mathematical knowledge. Our information system provides different types of information retrieval techniques. Digitally stored and edited mathematical knowledge can be accessed by applying a well-defined and structured access to the types and elements Of XML-documents. The information system transforms XML-based mathematical knowledge into a complex PROLOG-structure called field notation, which serves as the basis for building a digital library of fine-grained mathematical objects. Based on the field notation we provide a powerful and flexible declarative query language in a logic programming environment.",
        "keywords": "",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000186329400007"
    },
    {
        "title": "Haystack: A platform for authoring end user semantic web applications",
        "abstract": "The Semantic Web promises to open innumerable opportunities for automation and information retrieval by standardizing the protocols for metadata exchange. However, just as the success of the World Wide Web can be attributed to the ease of use and ubiquity of Web browsers, we believe that the unfolding of the Semantic Web vision depends on users getting powerful but easy-to-use tools for managing their information. But unlike HTML, which can be easily edited in any text editor, RDF is more complicated to author and does not have an obvious presentation mechanism. Previous work has concentrated on the ideas of generic RDF graph visualization and RDF Schema-based form generation. In this paper, we present a comprehensive platform for constructing end user applications that create, manipulate, and visualize arbitrary RDF-encoded information, adding another layer to the abstraction cake. We discuss a programming environment specifically designed for manipulating RDF and introduce user interface concepts on top that allow the developer to quickly assemble applications that are based on RDF data models. Also, because user interface specifications and program logic are themselves describable in RDF, applications built upon our framework enjoy properties such as network updatability, extensibility, and end user customizability - all desirable characteristics in the spirit of the Semantic Web.",
        "keywords": "",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000188096900047"
    },
    {
        "title": "Minimizing the ripple effect of web-centric software by using the pheromone extension",
        "abstract": "The ripple effect metric shows what impact changes to a software will likely have on the rest of the system. In web-based data analysis, it has become a widespread practice to deploy both the script and the program developed with a high-level programming language software tool. Due to different vendors and the diversified philosophies behind different software tools, it may be slightly difficult to cope with the ripple effect across them. This paper initiates an experimental idea to minimize the wrapper interface ripple for web-based script tools and high-level programming environments and also indicates many potential research directions for the development of computationally intelligent tools in the software engineering domain that demand lower cost and less complexity. This work incorporates Ant Colony Optimization (ACO) and its prime artifact pheromone, which has been modified as a pheromone extension module to minimize ripples when cross-coding. A standard benchmark data set has been taken to validate the performance of the proposed algorithm. (C) 2012 Elsevier Inc. All rights reserved.",
        "keywords": "Ripple-effect; Web-based software; Cross-code; Wrapper interface; Ant Colony Optimization; Pheromone",
        "released": 2012,
        "link": "https://doi.org/10.1016/j.ins.2012.01.007"
    },
    {
        "title": "WinHIPE: An IDE for functional programming based on rewriting and visualization",
        "abstract": "The article describes an IDE for functional programming, called WinHIPE. It provides an interactive and flexible tracer, as well as a powerful visualization and animation system. The former tool is based on the rewriting model of evaluation, and the latter provides automatic generation of visualizations and animations, friendly support for customization, maintenance and exportation of animations to the Web, and facilities to cope with large scale. Its main advantage over other visualization systems is an effortless approach to animation creation and maintenance, based on generating visualizations and animations automatically, as a side effect of program execution. Finally, we briefly describe our experience using the system during several years in educational settings.",
        "keywords": "functional programming; programming environments; expression evaluation; term rewriting; tracing; program visualization; program animation",
        "released": 2007,
        "link": "https://doi.org/10.1145/1273039.1273042"
    },
    {
        "title": "SimpleITK image-analysis notebooks: A collaborative environment for education and reproducible research",
        "abstract": "Modern scientific endeavors increasingly require team collaborations to construct and interpret complex computational workflows. This work describes an image-analysis environment that supports the use of computational tools that facilitate reproducible research and support scientists with varying levels of software development skills. The Jupyter notebook web application is the basis of an environment that enables flexible, well-documented, and reproducible workflows via literate programming. Image-analysis software development is made accessible to scientists with varying levels of programming experience via the use of the SimpleITK toolkit, a simplified interface to the Insight Segmentation and Registration Toolkit. Additional features of the development environment include user friendly data sharing using online data repositories and a testing framework that facilitates code maintenance. SimpleITK provides a large number of examples illustrating educational and research-oriented image analysis workflows for free download from GitHub under an Apache 2.0 license: github.com/InsightSoftwareConsortium/SimpleITK-Notebooks.",
        "keywords": "Image analysis; Open-source software; Registration; Segmentation; R; Python",
        "released": 2018,
        "link": "https://doi.org/10.1007/s10278-017-0037-8"
    },
    {
        "title": "REAL-TIME EXPERT-SYSTEM FOR FAULT-TOLERANT SUPERVISORY CONTROL",
        "abstract": "Many mechanical systems are sufficiently complex that it is impractical to describe their dynamics by exact mathematical models. In the presence of such modeling uncertainties, advanced controllers like adaptive controllers perform better than linear feedback controllers since they actively reduce the uncertainty by online parameter estimation. Unfortunately, the advanced control strategies, due to their lack of robustness, can become unstable in the presence of unpredictable external disturbances, and hence, there exists a need for a fault-tolerant approach to preserve the overall system integrity even at the cost of design performance. This motivated the research, presented in this paper, to investigate the suitability of the IDES (Influence Diagram Based Expert System) as an expert supervisory controller to predict incipient instability, a significant failure mode, and take corrective action in real-time when closed loop stability appears to be in danger. The expert supervisory control scheme is demonstrated on a model-referenced adaptive controller as applied to a robotic manipulator. The real-time expert system, with the information from sensors, dynamically optimizes the cost of control and as a result chooses between a robust auxiliary controller and the nonrobust adaptive controller depending on inferences made from the observable variables. IDES, as a real-time expert supervisory controller, preserves the stability of the system even under potentially destabilizing unexpected disturbances, exhibiting on demand a fault-tolerant behavior by trading design performance for overall system integrity. The results indicate the potential for influence diagram expert systems in monitoring and controlling mechanical systems where exact mathematical models are difficult or not practical to obtain.",
        "keywords": "",
        "released": 1993,
        "link": "https://doi.org/10.1115/1.2899025"
    },
    {
        "title": "Real-time and online monitoring of glucose contents by using molecular imprinted polymer-based IDEs sensor",
        "abstract": "A highly sensitive, selective, reversible, and reusable glucose sensor is developed by using molecularly imprinted polymer-based artificial receptors onto interdigital transducer. Sensor receptors were synthesized through bulk imprinting technology by using styrene as monomer, ethylene glycol dimethacrylate (EGDMA) as cross-linker, and AIBN as free radical initiator. Topography of the synthesized receptors was investigated by scanning electron microscopy (SEM). Fabricated sensor showed concentration-dependent linear and reversible response with lower limit of detection of 30 ppb and upper limit of detection similar to 500 ppm. Furthermore, newly fabricated sensor is highly selective towards its analyte of interest in the presence of other competing agents, and the regeneration of sensor response has been assessed with the percentage error of less than 2% under the period of 1 year at room temperature and pressure conditions. The reported sensor may have potential technological applications in the field of medical diagnostics, food, and pharmaceutical industry.",
        "keywords": "Molecular imprinted polymers; Electrochemical sensors; Glucose; Sensitivity; Selectivity; Limit of detection",
        "released": 2019,
        "link": "https://doi.org/10.1007/s12010-019-03049-3"
    },
    {
        "title": "Intelligent correction and monitoring of ship propulsion rotary device vibration",
        "abstract": "Field inspection is a traditional way to detect the problem of shaft imbalance or abnormal vibration in a ship propulsion system; however, the ship cannot execute any tasks or activities during calibration. This study develops a human-machine monitoring interface (HMMI) to estimate vibration abnormalities and implement an intelligent active balance correction to the propulsion system online. In this study, Arduino IDE, InduSoft, and LabVIEW are used to create a function monitored by HMMI. By comparing the abnormal vibration amplification of the moment of inertia, HMMI calculates the correct mass to reduce the vibration. The experimental results show that, after HMMI carries out continuous active balance correction online, the correction rate achieves 105.37%. This indicates that HMMI can calculate the amount of imbalance and phase angles and drive a counterweight to the correct balance position while the device is still operating.",
        "keywords": "Arduino IDE; InduSoft; HMMI; LabVIEW; active balance",
        "released": 2022,
        "link": "https://doi.org/10.46604/ijeti.2022.9151"
    },
    {
        "title": "EJB components from oracle platform in web applications",
        "abstract": "Oracle platform has been enriched in the past few years with many components needed for relative easiest building of some solid applications over the Web technology. Oracle JDeveloper 10g is a development environment with end-to-end support for modeling, developing, debugging, and deploying e-business applications and Web services based on Service Oriented Application. It allows developers to build Java 2 Enterprise Edition - J2EE applications and Web services either from scratch or by using a J2EE framework. JDeveloper provides a comprehensive set of integrated tools to support the complete development lifecycle: wizards, editors, visual design tools, drag and drop data binding to user interfaces, and deployment tools to create high-quality, standard J2EE components. JDeveloper also provides a public Extension SDK to extend and customize the development environment and to seamlessly integrate with external products.",
        "keywords": "database; Java platform; web technology; Oracle products",
        "released": 2008,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000258665600016"
    },
    {
        "title": "Impedance sensor for real-time ammonium detection based on MWCNT/ZnO nanocomposites",
        "abstract": "This work reports on the development of an impedance sensor-based real-time-field specific system to monitor aqueous Ammonium (NH4+). The sensing element was fabricated by modifying screen-printed interdigitated electrodes (IDEs) with a hybrid nanocomposite of Multi-Wall Carbon Nanotube (MWCNT) with Zinc Oxide (ZnO) nanocrystals. The NH4+ of the water was monitored, and it exhibited a sensitivity of 67.13 Omega/mM with average correlation coefficients of 0.80. The impedance magnitude (Omega) of the NH4+ sensor was unaffected by the presence of Fe2+, Ni2+, K+ and P+ interfering cations. The developed sensor was interfaced with an IoT-enabled NodeMCU microcontroller, enabling a direct method for continuous monitoring of NH4+ concentrations. This integrated system is interconnected to the field-deployed sensor nodes, which provide real-time NH4+ levels to the remote user through web applications.",
        "keywords": "Zinc oxide; II-VI semiconductor materials; Impedance; Nanocomposites; Surface impedance; Electrodes; Surface morphology; Screen printing; MWCNT/ZnO; NH4+ sensor; IoT",
        "released": 2023,
        "link": "https://doi.org/10.1109/TNB.2022.3166388"
    },
    {
        "title": "Designing interaction protocols using noughts and crosses type games",
        "abstract": "Interaction management is concerned with the protocols that govern structured interactive activities among multiple users or agents in networked collaborative environments. It is an important aspect of networked software in many application domains such as online meetings, online groupware and online games. However, there is limited support in most programming languages and programming environments for implementing interaction management. High-level features, such as interaction protocols and management policies, are usually hard coded by skilled network programmers, who are often scarce in many applications such as e-learning. In this paper, we present an abstraction of various collaborative applications in the form of the noughts and crosses game and its variations. We examine the needs in these games for programming interaction protocols, and propose a comprehensive collection of program constructs for supporting interaction. We report our efforts for incorporating these new constructs into JACIE (Java-based Authoring language for Collaborative Interactive Environments), an existing scripting language designed to support rapid prototyping and implementation of collaborative applications. We demonstrate, through variations of the noughts and crosses game and an on-line bridge game, the usefulness of these language constructs. (C) 2006 Elsevier Ltd. All rights reserved.",
        "keywords": "net-centric computing; collaborative applications; interaction protocols; rapid prototyping; authoring tools; scripting languages",
        "released": 2007,
        "link": "https://doi.org/10.1016/j.jnca.2006.01.002"
    },
    {
        "title": "WebEAV: Automatic metadata-driven generation of web interfaces to entity-attribute-value databases",
        "abstract": "The task of creating and maintaining a front end to a large institutional entity-attribute-value (EAV) database can be cumbersome when using traditional client-server technology. Switching to Web technology as a delivery vehicle solves some of these problems but introduces others. In particular, Web development environments tend to be primitive, and many features that client-server developers take for granted are missing. WebEAV is a generic framework for Web development that is intended to streamline the process of Web application development for databases having a significant EAV component. It also addresses some challenging user interface issues that arise when any complex system is created. The authors describe the architecture of WebEAV and provide an overview of its features with suitable examples.",
        "keywords": "",
        "released": 2000,
        "link": "https://doi.org/10.1136/jamia.2000.0070343"
    },
    {
        "title": "Generating and selecting resilient and maintainable locators for web automated testing",
        "abstract": "Web user interface (UI) test automation strategies have been dominated by programmable and record-playback approaches. Of these, record-playback allows creating automation tests easily and reduces the cost of test generation. However, this approach increases the cost of test maintenance due to its unstable generated locators for identifying UI objects during playback. In this paper, we propose a new approach to generating and selecting resilient and maintainable locators. Our approach consists of two parts, a new XPath construction method and selecting the best XPath to locate the target element. Our XPath construction method relies on semantic structures of Web pages to locate the target element using its neighbors. We conducted an experiment on 15 popular websites. The results show that our approach outperforms the state-of-the-practice/art Selenium IDE and Robula+ in locating target elements by effectively avoiding wrong locators. It also produces more readable XPaths (hence more maintainable tests) than do these approaches.",
        "keywords": "automated testing; web locators; web UI test automation; XPath",
        "released": 2021,
        "link": "https://doi.org/10.1002/stvr.1760"
    },
    {
        "title": "TopSuite web server: A meta-suite for deep-learning-based protein structure and quality prediction",
        "abstract": "Proteins carry out the most fundamental processes of life such as cellular metabolism, regulation, and communication. Understanding these processes at a molecular level requires knowledge of their three-dimensional structures. Experimental techniques such as X-ray crystallography, NMR spectroscopy, and cryogenic electron microscopy can resolve protein structures but are costly and time-consuming and do not work for all proteins. Computational protein structure prediction tries to overcome these problems by predicting the structure of a new protein using existing protein structures as a resource. Here we present TopSuite, a web server for protein model quality assessment (TopScore) and template-based protein structure prediction (TopModel). TopScore provides meta-predictions for global and residue-wise model quality estimation using deep neural networks. TopModel predicts protein structures using a top-down consensus approach to aid the template selection and subsequently uses TopScore to refine and assess the predicted structures. The TopSuite Web server is freely available at https://cpclab.uni-duesseidorf.ide/topsuite/.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1021/acs.jcim.0c01202"
    },
    {
        "title": "Integrated document browsing and data acquisition for building large ontologies",
        "abstract": "Named entities (e.g., “Kofi Annan”, “Coca-Cola”, “Second World War”) are ubiquitous in web pages and other types of document and often provide a simplified picture of the document’s content. We present an ontology currently containing 31,000 named entities in different languages from various domains such as history, geography, politics, sports, arts, etc., which is being developed at the University of Munich (LMU). The underlying graph data model is simple and yet extremely versatile in different application scenarios. We demonstrate a prototype of a graphical interface to both the ontology and to documents on the web or in a local document repository, with a tight interaction in both directions. Occurrences of concepts from the ontology are highlighted and hyperlinked in the documents. Unrecognized entities could be added to the database and related to other concepts in a semiautomatic process. The entity database can also be used for extending full-text queries on the web or the repository to semantically close documents, and for indexing different kinds of named entities in the document repository. Similar to a programming IDE, the system illustrates how integrated browsing, search and update functionality contributes to the construction of high-quality ontologies, fundamental to the vision of a truly “semantic” web.",
        "keywords": "n",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000242123100078"
    },
    {
        "title": "Variations in/nearby genes coding for <i>JAZF1</i>, <i>TSPAN8/LGR5</i> and <i>HHEX</i>-<i>IDE</i> and risk of type 2 diabetes in han chinese",
        "abstract": "Several genetic loci (JAZF1, CDC123/CAMK1D, TSPAN8/LGR5, ADAMTS9, VEGFA and HHEX-IDE) were identified to be significantly related to the risk of type 2 diabetes and quantitative metabolic traits in European populations. Here, we aimed to evaluate the impacts of these novel loci on type 2 diabetes risk in a population-based case-control study of Han Chinese (1912 cases and 2041 controls). We genotyped 13 single-nucleotide polymorphisms (SNPs) in/near these genes and examined the differences in allele/genotype frequency between cases and controls. We found that both IDE rs11187007 and HHEX rs1111875 were associated with type 2 diabetes risk (for both variants: odds ratio (OR)=1.15, 95% confidence interval (CI) 1.04-1.28, P=0.009). In a meta-analysis where we pooled our data with the three previous studies conducted in East Asians, we found that the variants of JAZF1 rs864745 (1.09 (1.03-1.16); P=3.49x10(-3)) and TSPAN8/LGR5 rs7961581 (1.11 (1.05-1.17); P=1.89x10(-4)) were significantly associated with type 2 diabetes risk. In addition, the meta-analysis (7207 cases and 8260 controls) also showed that HHEX rs1111875 did have effects on type 2 diabetes in Chinese population (OR=1.15(1.10-1.21); P=1.93x10(-8)). This large population-based study and meta-analysis further confirmed the modest effects of the JAZF1, TSPAN8/LGR5 and HHEX-IDE loci on type 2 diabetes in Chinese and other East Asians. Journal of Human Genetics (2010) 55, 810-815; doi:10.1038/jhg.2010.117; published online 7 October 2010",
        "keywords": "association study; Han Chinese population; HHEX; type 2 diabetes",
        "released": 2010,
        "link": "https://doi.org/10.1038/jhg.2010.117"
    },
    {
        "title": "Pangea: An MLOps tool for automatically generating infrastructure and deploying analytic pipelines in edge, fog and cloud layers",
        "abstract": "Development and operations (DevOps), artificial intelligence (AI), big data and edge-fog-cloud are disruptive technologies that may produce a radical transformation of the industry. Nevertheless, there are still major challenges to efficiently applying them in order to optimise productivity. Some of them are addressed in this article, concretely, with respect to the adequate management of information technology (IT) infrastructures for automated analysis processes in critical fields such as the mining industry. In this area, this paper presents a tool called Pangea aimed at automatically generating suitable execution environments for deploying analytic pipelines. These pipelines are decomposed into various steps to execute each one in the most suitable environment (edge, fog, cloud or on-premise) minimising latency and optimising the use of both hardware and software resources. Pangea is focused in three distinct objectives: (1) generating the required infrastructure if it does not previously exist; (2) provisioning it with the necessary requirements to run the pipelines (i.e., configuring each host operative system and software, install dependencies and download the code to execute); and (3) deploying the pipelines. In order to facilitate the use of the architecture, a representational state transfer application programming interface (REST API) is defined to interact with it. Therefore, in turn, a web client is proposed. Finally, it is worth noting that in addition to the production mode, a local development environment can be generated for testing and benchmarking purposes.",
        "keywords": "edge; cloud; analytic pipeline; MLOps; infrastructure; mine",
        "released": 2022,
        "link": "https://doi.org/10.3390/s22124425"
    },
    {
        "title": "Hypertext: The next maintenance mountain",
        "abstract": "For many organizations today, maintaining hypertext documents is becoming a burdensome task in its own right. The development process for hypertext documents is also starting to resemble that of software, with increasingly complex analysis of hypertext documents’ ability to meet an organization’s needs, well-planned design and implementation, and follow-up testing fur consistency and effectiveness. Software maintenance is a major problem, and hypertext documents share many of the characteristics of software. Yet according to the authors’ review of the summaries of 52 Web development tools, only six contained maintenance-related terms or features. The situation is made more critical because Web development has attracted a mixed community of professional and nonprofessional developers. In this article, the authors compare the characteristics of software and hypertext documents. They derive lessons from software development that can be applied to the creation and maintenance of hypertext documents, in terms of structure, reuse, testing, version management, development tools, and development processes.",
        "keywords": "",
        "released": 1998,
        "link": "https://doi.org/10.1109/2.735850"
    },
    {
        "title": "Privacy preservation in big data with data scalability and efficiency using efficient and secure data balanced scheduling algorithm",
        "abstract": "In the current circumstance, the dimension of information in many kind of cloud atmosphere increases enormously along with the trend of Big Data. Subsequently making, it is a test for common software programming tools to get, supervise, and huge scale process information inside an explicit spans. Thus, it is a test for previous anonymization ways to agreement protection preservation on credential datasets because of their insufficiency of adaptability. To overcome these issues, efficient and Secure data balanced scheduling (ESDBS) algorithm is introduced to data maintain large-scale credential data sets using the MapReduce framework in big data aspects. The proposed system designs efficient MapReduce tasks to concretely achieve the specialization computation in a scalable manner. Based on the experimental evaluation, proposed ESDBS algorithm reduces 20 seconds encryption time and improves 15% algorithm efficiency and 6% system throughput compare than convention techniques.",
        "keywords": "Big data privacy; Efficient job balanced scheduling; MapReduce; Merging; Level-level privacy; clustering efficiency; scalability",
        "released": 2019,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000499763600007"
    },
    {
        "title": "MoSCoE: An approach for composing web services through iterative reformulation of functional specifications",
        "abstract": "We propose a specification-driven approach to Web service composition. Our framework allows the users (or service developers) to start with a high-level, possibly incomplete specification of a desired (goal) service that is to be realized using a subset of the available component services. These services are represented using labeled transition systems augmented with guards over variables with infinite domains and are used to determine a strategy for their composition that would realize the goal service functionality. However, in the event the goal service cannot be realized using the available services, our approach identifies the cause(s) for such failure which can then be used by the developer to reformulate the goal specification. Thus, the technique supports Web service composition through iterative reformulation of the functional specification. We present a prototype implementation in a tabled-logic programming environment that illustrates the key features of the proposed approach.",
        "keywords": "service-oriented architectures; web services; composition; symbolic transition systems; tabled-logic programming",
        "released": 2008,
        "link": "https://doi.org/10.1142/S0218213008003807"
    },
    {
        "title": "Experimental evaluation of the virtual environment efficiency for distributed software development",
        "abstract": "At every software design stage nowadays, there is an acute need to solve the problem of effective choice of libraries, development technologies, data exchange formats, virtual environment systems, characteristics of virtual machines. Due to the spread of various kinds of devices and the popularity of Web platforms, lots of systems are developed not for the universal installation on a device (box version), but for a specific architecture with the subsequent provision of web services. Under these conditions, the only way for estimating the efficiency parameters at the design stage is to conduct various kinds of experiments to evaluate the parameters of a particular solution. Using the example of the Web platform of digital psychological tools, the methods for experimental parameter evaluation were developed in the article. The mechanisms and technologies for improving the efficiency of the Vagrant and Docker cloud virtual environment were also proposed in the paper. A set of basic criteria for evaluating the effectiveness of the configuration of the virtual development environment has been determined to be rapid deployment; increase in the speed and decrease in the volume of resources used; increase in the speed of data exchange between the host machine and the virtual machine. The results of experimental estimates of the parameters that define the formulated efficiency criteria are given as: processor utilization involved (percentage); the amount of RAM involved (GB); initialization time of virtual machines (seconds); time to assemble the component completely (Build) and to reassemble the component (Watch) (seconds). To improve the efficiency, a file system access driver based on the NFS protocol was studied in the paper.",
        "keywords": "Distributed software development; virtual development environment; increase development efficiency; virtual machines; vagrant; Docker; NFS; webpack",
        "released": 2019,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000476620600039"
    },
    {
        "title": "Comparative efficacy of tenofovir and entecavir in nucleos(t)ide analogue-naive chronic hepatitis b: A systematic review and meta-analysis",
        "abstract": "Objective To compare the efficacy of tenofovir and entecavir in nucleos(t)ide analogue-naive chronic hepatitis B. Methods The Web of Science, PubMed, Cochrane Library, EMBASE, Clinical Trials and China National Knowledge Infrastructure(CNKI) databases were electronically searched to collect randomized controlled trials (RCTs) regarding the comparison between tenofovir and entecavir in nucleos(t)ide analogue-naive chronic hepatitis B (CHB) since the date of database inception to July 2019. Two researchers independently screened and evaluated the obtained studies and extracted the outcome indexes. RevMan 5.3 software was used for the meta-analysis. Results Early on, tenofovir had a greater ability to inhibit the hepatitis B virus, I-2 = 0% [RR = 1.08, 95% CI (1.03, 1.13), P<0.01] (96 weeks). Entecavir can normalize the ALT levels earlier, I-2 = 0% [RR = 0.87, 95% CI (0.77, 0.98), P = 0.02] (48 weeks). However, there was no statistically significant difference between TDF and ETV at 144 weeks. Tenofovir was as effective as entecavir in terms of HBeAg clearance and HBeAg seroconversion, I-2 = 0% [RR = 1.05, 95% CI (0.68, 1.62), P = 0.82]; I-2 = 69% [RR = 0.93, 95% CI (0.54, 1.61), P = 0.80]. The difference in the incidence of elevated creatine kinase levels was not statistically significant I-2 = 0% [RR = 0.66, 95% CI (0.27, 1.60), P = 0.35]. Conclusions Tenofovir and entecavir were equally effective in the treatment of patients with nucleos(t)ide analogue-naive chronic hepatitis B. In addition, TDF has an advantage in the incidence of hepatocellular carcinoma. Additional RCTs and a large-sample prospective cohort study should be performed.",
        "keywords": "",
        "released": 2019,
        "link": "https://doi.org/10.1371/journal.pone.0224773"
    },
    {
        "title": "A pilot experience with software programming environments as a service for teaching activities",
        "abstract": "Software programming is one of the key abilities for the development of Computational Thinking (CT) skills in Science, Technology, Engineering and Mathematics (STEM). However, specific software tools to emulate realistic scenarios are required for effective teaching. Unfortunately, these tools have some limitations in educational environments due to the need of an adequate configuration and orchestration, which usually assumes an unaffordable work overload for teachers and is inaccessible for students outside the laboratories. To mitigate the aforementioned limitations, we rely on cloud solutions that automate the process of orchestration and configuration of software tools on top of cloud computing infrastructures. This way, the paper presents ACTaaS as a cloud-based educational resource that deploys and orchestrates a whole realistic software programming environment. ACTaaS provides a simple, fast and automatic way to set up a professional integrated environment without involving an overload to the teacher, and it provides an ubiquitous access to the environment. The solution has been tested in a pilot group of 28 students. Currently, there is no tool like ACTaaS that allows such a high grade of automation for the deployment of software production environments focused on educational activities supporting a wide range of cloud providers. Preliminary results through a pilot group predict its effectiveness due to the efficiency to set up a class environment in minutes without overloading the teachers, and providing ubiquitous access to students. In addition, the first student opinions about the experience were greatly positive.",
        "keywords": "cloud computing; software programming; STEM education; learning environments; software as a service",
        "released": 2021,
        "link": "https://doi.org/10.3390/app11010341"
    },
    {
        "title": "Accessibility and the next generation of web development tools - introduction to the special thematic session",
        "abstract": "This session introduces a series of papers that present new perspectives to support the industry in its quest to support a wider variety of users. Through them, we will learn how different research groups are approaching the topic, and will try to understand whether the research arena is tackling in a supportive way the needs of the industry.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000222635100044"
    },
    {
        "title": "Understanding the motivations, challenges and needs of blockchain software developers: A survey",
        "abstract": "The blockchain technology has potential applications in various areas such as smart-contracts, Internet of Things (IoT), land registry, supply chain management, storing medical data, and identity management. Although GitHub currently hosts more than six thousand active Blockchain software (BCS) projects, few software engineering researchers have investigated these projects and their contributors. Although the number of BCS projects is growing rapidly, the motivations, challenges, and needs of BCS developers remain a puzzle. Therefore, the primary objective of this study is to understand the motivations, challenges, and needs of BCS developers and analyze the differences between BCS and non-BCS development. On this goal, we sent an online survey to 1,604 active BCS developers identified by mining the GitHub repositories of 145 popular BCS projects. The survey received 156 responses that met our criteria for analysis. The results suggest that the majority of the BCS developers are experienced in non-BCS development and are primarily motivated by the ideology of creating a decentralized financial system. Although most of the BCS projects are Open Source Software (OSS) projects by nature, more than 93% of our respondents found BCS development somewhat different from a non-BCS development as BCS projects have higher emphasis on security and reliability than most of the non-BCS projects. Other differences include: higher costs of defects, decentralized and hostile environment, technological complexity, and difficulty in upgrading the software after release. These differences were also the primary sources of challenges to them. Software development tools that are tuned for non-BCS development are inadequate for BCS and the ecosystem needs an array of new or improved tools, such as: customized IDE for BCS development tasks, debuggers for smart-contracts, testing support, easily deployable simulators, and BCS domain specific design notations.",
        "keywords": "Blockchain; Cryptocurrency; Survey; Bitcoin; Ethereum; Motivation; Challenges",
        "released": 2019,
        "link": "https://doi.org/10.1007/s10664-019-09708-7"
    },
    {
        "title": "Design planning by end-user web developers",
        "abstract": "We report an exploratory research project that investigates the impacts of different forms of design planning on end users asked to develop a simple interactive web application. End users created their projects (a Ride Board application) using the CLICK end-user web development tool [J. Rode, User-centered design of end-user web development tool, Ph.D. Dissertation, Department of Computer Science, Virginia Tech, Blacksburg, VA, USA, 2005]. Some participants were asked to create a conceptual map to plan their projects and others to write user interaction scenarios; a third group was asked to do whatever they found useful. We describe the planning that each group underwent, how they approached the web development task, and their reactions to the experience afterwards. The overall pattern of results suggests that while the participants who planned using scenarios felt they better understood the web development task, it was the group who created concept maps that explored and incorporated more of the novel programming features of the CLICK tool. We also discuss the role of gender in the CLICK development task, noting that women were less likely to explore the tool’s novel features and perceived themselves as less successful in the task. We conclude with a discussion of design implications and future work. (C) 2008 Elsevier Ltd. All rights reserved.",
        "keywords": "end-user programming; web development; design; concept maps",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.jvlc.2008.03.001"
    },
    {
        "title": "Geoprocessing in cloud computing platforms - a comparative analysis",
        "abstract": "ting platforms; emphasizes the importance of virtualization; recommends applications of hybrid geoprocessing Clouds, and suggests an interoperable solution on geoprocessing Cloud services. The comparison allows one to selectively utilize Cloud Computing platforms or hybrid Cloud pattern, once it is understood that the current development of geoprocessing Cloud services is restricted to specific Cloud Computing platforms with certain kinds of technologies. The performance evaluation is also performed over geoprocessing services deployed in public Cloud platforms. The tested services are developed using geoprocessing algorithms from different vendors, GeoSurf and Java Topology Suite. The evaluation results provide a valuable reference on providing elastic and cost-effective geoprocessing Cloud services. The emergence of Cloud Computing technologies brings a new information infrastructure to users. Providing geoprocessing functions in Cloud Computing platforms can bring scalable, on-demand, and cost-effective geoprocessing services to geospatial users. This paper provides a comparative analysis of geoprocessing in Cloud Computing platforms - Microsoft Windows Azure and Google App Engine. The analysis compares differences in the data storage, architecture model, and development environment based on the experience to develop geoprocessing services in the two Cloud Compu",
        "keywords": "geoprocessing; Cloud computing; geospatial service; GIS; Microsoft Azure; Google App Engine",
        "released": 2013,
        "link": "https://doi.org/10.1080/17538947.2012.748847"
    },
    {
        "title": "ChIPpeakAnno: A bioconductor package to annotate ChIP-seq and ChIP-chip data",
        "abstract": "Background: Chromatin immunoprecipitation (ChIP) followed by high-throughput sequencing (ChIP-seq) or ChIP followed by genome tiling array analysis (ChIP-chip) have become standard technologies for genome-wide identification of DNA-binding protein target sites. A number of algorithms have been developed in parallel that allow identification of binding sites from ChIP-seq or ChIP-chip datasets and subsequent visualization in the University of California Santa Cruz (UCSC) Genome Browser as custom annotation tracks. However, summarizing these tracks can be a daunting task, particularly if there are a large number of binding sites or the binding sites are distributed widely across the genome. Results: We have developed ChIPpeakAnno as a Bioconductor package within the statistical programming environment R to facilitate batch annotation of enriched peaks identified from ChIP-seq, ChIP-chip, cap analysis of gene expression (CAGE) or any experiments resulting in a large number of enriched genomic regions. The binding sites annotated with ChIPpeakAnno can be viewed easily as a table, a pie chart or plotted in histogram form, i.e., the distribution of distances to the nearest genes for each set of peaks. In addition, we have implemented functionalities for determining the significance of overlap between replicates or binding sites among transcription factors within a complex, and for drawing Venn diagrams to visualize the extent of the overlap between replicates. Furthermore, the package includes functionalities to retrieve sequences flanking putative binding sites for PCR amplification, cloning, or motif discovery, and to identify Gene Ontology (GO) terms associated with adjacent genes. Conclusions: ChIPpeakAnno enables batch annotation of the binding sites identified from ChIP-seq, ChIP-chip, CAGE or any technology that results in a large number of enriched genomic regions within the statistical programming environment R. Allowing users to pass their own annotation data such as a different Chromatin immunoprecipitation (ChIP) preparation and a dataset from literature, or existing annotation packages, such as GenomicFeatures and BSgenome, provides flexibility. Tight integration to the biomaRt package enables up-to-date annotation retrieval from the BioMart database.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1186/1471-2105-11-237"
    },
    {
        "title": "PhenoDB: A new web-based tool for the collection, storage, and analysis of phenotypic features",
        "abstract": "To interpret whole exome/genome sequence data for clinical and research purposes, comprehensive phenotypic information, knowledge of pedigree structure, and results of previous clinical testing are essential. With these requirements in mind and to meet the needs of the Centers for Mendelian Genomics project, we have developed PhenoDB (http://phenodb.net), a secure, Web-based portal for entry, storage, and analysis of phenotypic and other clinical information. The phenotypic features are organized hierarchically according to the major headings and subheadings of the Online Mendelian Inheritance in Man (OMIM (R)) clinical synopses, with further subdivisions according to structure and function. Every string allows for a free-text entry. All of the approximately 2,900 features use the preferred term from Elements of Morphology and are fully searchable and mapped to the Human Phenotype Ontology and Elements of Morphology. The PhenoDB allows for ascertainment of relevant information from a case in a family or cohort, which is then searchable by family, OMIM number, phenotypic feature, mode of inheritance, genes screened, and so on. The database can also be used to format phenotypic data for submission to dbGaP for appropriately consented individuals. PhenoDB was built using Django, an open source Web development tool, and is freely available through the Johns Hopkins McKusick-Nathans Institute of Genetic Medicine (http://phenodb.net).",
        "keywords": "phenotyping; mendelian disorders; database; bioinformatics",
        "released": 2013,
        "link": "https://doi.org/10.1002/humu.22283"
    },
    {
        "title": "Efficacy and safety of the siRNAJNJ-73763989 and the capsid assembly modulator JNJ-56136379 (bersacapavir) with nucleos(t)ide analogues for the treatment of chronic hepatitis b virus infection (REEF-1): A multicentre, double-blind, active-controlled, randomised, phase 2btrial",
        "abstract": "Background JNJ-73763989 (JNJ-3989), a small interfering RNA, targets all hepatitis B virus (HBV) RNAs, reducing all HBV proteins. JNJ-56136379 (JNJ-6379; also known as bersacapavir), a capsid assembly modulator, inhibits HBV replication. We aimed to evaluate the efficacy (ie, antiviral activity) and safety of these therapeutics in combination with nucleos(t)ide analogues in patients with chronic hepatitis B. Methods The REEF-1 multicentre, double-blind, active-controlled, randomised, phase 2b study was done at 108 hospitals or outpatient centres across 19 countries in Asia, Europe, and North and South America. We included patients aged 18-65 years with chronic hepatitis B (defined as HBsAg positivity at screening and at least 6 months before screening or alternative markers of chronicity [eg, HBV DNA]), including those not currently treated, virologically suppressed, HBeAg positive, and HBeAg negative. Patients were randomly assigned (1:1:2:2:2:2) via permuted block randomisation according to a computer-generated schedule to receive oral nucleos(t)ide analogues once per day plus placebo (control group); oral JNJ-6379 250 mg daily plus nucleos(t)ide analogues (JNJ-6379 dual group); nucleos(t)ide analogues plus subcutaneously injected JNJ-3989 at doses of 40 mg (JNJ-3989 dual 40 mg group), 100 mg (JNJ-3989 dual 100 mg group), or 200 mg (JNJ-3989 dual 200 mg group) every 4 weeks; or JNJ-6379 250 mg plus JNJ-3989 100 mg every 4 weeks plus nucleos(t)ide analogues (triple group) for 48 weeks followed by a follow-up phase. An interactive web response system provided concealed treatment allocation, and investigators remained masked to the intervention groups until the primary analysis at week 48. The primary endpoint was the proportion of patients meeting predefined nucleos(t)ide analogue-stopping criteria (alanine aminotransferase <3 x upper limit of normal, HBV DNA below the lower limit of quantitation, HBeAg negative, and HBsAg <10 IU/mL) at week 48. All patients who received at least one dose of study drug were included in the analysis population used for primary efficacy assessment, excluding those who withdrew because of COVID-19-related reasons, withdrew before week 44, or had no efficacy data (ie, the modified intention-to-treat population). Safety was assessed in all participants who received at least one dose of study drugs. This trial is registered with ClinicalTrials.gov, NCT03982186. The study has been completed.Findings Between Aug 1, 2019, and April 26, 2022, 470 patients (310 [66%] male and 244 [52%] White) were randomly assigned: 45 to the control group, 48 to the JNJ-6379 dual group, 93 to the JNJ-3989 dual 40 mg group, 93 to the JNJ-3989 dual 100 mg group, 96 to the JNJ-3989 dual 200 mg group, and 95 to the triple group. At week 48, five (5%; 90% CI 2-11) of 91 patients in the JNJ-3989 dual 40 mg group, 15 (16%; 10-24) of 92 in the JNJ-3989 dual 100 mg group, 18 (19%; 13-27) of 94 in the JNJ-3989 dual 200 mg group, eight (9%; 4-15) of 94 in the triple group, and one (2%; 0-10) of 45 in the control group met nucleos(t)ide analogue stopping criteria. No patients in the JNJ-6379 dual group met stopping criteria. 38 (81%) patients who met nucleos(t)ide analogue-stopping criteria at week 48 were virologically suppressed and HBeAg negative at baseline. Ten (2%) of 470 patients had serious adverse events during the treatment phase, and two patients (one each from the JNJ-3989 dual 200 mg group [exercise-related rhabdomyolysis] and the triple group [increase in ALT or AST]) had serious adverse events related to study treatment. During follow-up, 12 (3%) of 460 patients had a serious adverse event; one (<1%), a gastric ulcer, was considered to be related to nucleos(t)ide analogues and occurred in a patient from the JNJ-3989 dual 200 mg group. 29 (6%) of 460 patients in the treatment phase and in ten (2%) of 460 patients in the follow-up phase had grade 3 or 4 adverse events. Five (1%) of 470 patients discontinued treatment due to adverse events, and there were no deaths.",
        "keywords": "",
        "released": 2023,
        "link": "https://doi.org/10.1016/S2468-1253(23)00148-6"
    },
    {
        "title": "X-ray imaging virtual online laboratory for engineering undergraduates",
        "abstract": "Distance learning engineering students (as well as those in face-to-face settings) should acquire a basic background in radiation-matter interaction physics (usually in the first semesters). Some students in this group may feel some degree of aversion towards these types of pure science-related subjects (mathematics, physics, chemistry, etc). In online learning scenarios, the average student is already an adult (37 years old or above) and may see no particular application of the aforementioned courses in their current or future professional life. Besides this, online institutions tend to lean too much on applet-based simulations. Although they may shed some light on the theory associated with the studied physical processes, these animated and interactive examples also seem to be “ stripped down” versions of the real events, and are felt to be disconnected from current scientific environments and engineering settings. For this reason, we describe a novel virtual lab approach to teach the basics of the low-energy interactions present in average x-ray settings. It combines real scientific simulation frameworks with modern computing techniques such as virtualization, cloud infrastructures, containers, networking and shared collaboration environments. It also fosters the use of hugely demanded development tools and programming languages and addresses the fundamentals of digital radiography and the linked electronic standards for image storage and transmission. With this mixed approach, blending scientific concepts, healthcare and state-of-the-art software solutions, our virtual labs have proven (over a period of five academic terms) to be both very attractive to and pedagogically successful (technically, and scientifically) for online engineering undergraduates. For the sake of completeness, we also propose a hands-on activity that mimics the geometrical peculiarities of x-ray rooms with the help of visible light and cheap materials.",
        "keywords": "x-ray physics; online learning; digital standards; collaborative environments; cloud technologies; virtual laboratory; containers",
        "released": 2020,
        "link": "https://doi.org/10.1088/1361-6404/ab5011"
    },
    {
        "title": "Enhancing team collaboration through integrating social interactions in a web-based development environment",
        "abstract": "This paper presents the design and evaluation of a Web-based collaborative learning environment called EduCo for learning and practicing team-based exercises in computer science and software engineering courses. EduCo’s defining characteristic is integrating a number of services for software development activities, for example, project management, requirements engineering, design, and programming into integrated shared workspaces with social-networking facilities to enhance collaboration among students and instructors. We conducted a controlled experiment to evaluate the effectiveness of the system in doing a three-iteration programming project. A total of 126 second-year students who attended an object-oriented programming course participated in the experiment. The results show that while the system was not found to affect student programming performance, it improved student engagement and satisfaction when working in teams. These results imply that the system’s shared workspaces and social-networking services have the potential to enhance learning outcomes of students working in teams when these features are integrated into collaborative learning environments. (c) 2016 Wiley Periodicals, Inc. Comput Appl Eng Educ 24:529-545, 2016; View this article online at ; DOI",
        "keywords": "social network; collaborative environment; social environment; programming language",
        "released": 2016,
        "link": "https://doi.org/10.1002/cae.21729"
    },
    {
        "title": "Agile software development using cloud computing: A case study",
        "abstract": "Agile software development is successful due to self-organizing teams, adaptive planning, a cooperative environment with respect to communication with clients and team members, small development cycles, continuous design improvements, continuous delivery and feedback of clients. Cloud computing helps to reduce cost, enables scalability and enhances communication through its services. A generic framework with the conjunction of Agile Development and Cloud Computing (ADCC) proposed in an earlier study is evaluated in this study. The Malaysia Research and education network (MyRen) cloud is utilized to implement the framework. A case study is conducted to evaluate the framework. Before conducting the case study, the participants are educated on the ADCC framework. The results of the case study show that the performance of agile methods is improved with the usage of the ADCC framework. The improvement is measured in terms of local and distributed agile development environments.",
        "keywords": "Agile development; case study; cloud-based agile tools; cloud computing",
        "released": 2020,
        "link": "https://doi.org/10.1109/ACCESS.2019.2962257"
    },
    {
        "title": "Evaluating software engineering techniques for developing complex systems with multiagent approaches",
        "abstract": "Context: Multiagent systems (MAS) allow complex systems to be developed in which autonomous and heterogeneous entities interact. Currently, there are a great number of methods and frameworks for developing MAS. The selection of one or another development environment is a crucial part of the development process. Therefore, the evaluation and comparison of MAS software engineering techniques is necessary in order to make the selection of the development environment easier. Objective: The main goal of this paper is to define an evaluation framework that will help in facilitating, standardizing, and simplifying the evaluation, analysis, and comparison of MAS development environments. Moreover, the final objective of the proposed tool is to provide a repository of the most commonly used MAS software engineering methods and tools. Method: The proposed framework analyzes methods and tools through a set of criteria that are related to both system engineering dimensions and MAS features. Also, the support for developing organizational and service-oriented MAS is studied. This framework is implemented as an online application to improve its accessibility. Results: In this paper, we present Masev, which is an evaluation framework for MAS software engineering. It allows MAS methods, techniques and environments to be analyzed and compared. A case study of the analysis of four methodologies is presented. Conclusion: It is concluded that Masev simplifies the evaluation and comparison task and summarizes the most important issues for developing MAS, organizational MAS, and service-oriented MAS. Therefore, it could help developers to select the most appropriate MAS method and tools for developing a specific system, and it could be used for MAS software engineering developers to detect and deficiencies in their methods and tools. Also, developers of new tools can understand this application as a way to publish their tools and demonstrate what their contributions are to the state of the art. (C) 2010 Elsevier B.V. All rights reserved.",
        "keywords": "Multiagent systems development tool; Methodology; Software engineering",
        "released": 2011,
        "link": "https://doi.org/10.1016/j.infsof.2010.12.012"
    },
    {
        "title": "Supporting dynamic, third-party code customizations in JavaScript using aspects",
        "abstract": "Web sites and web browsers have recently evolved into platforms on top of which entire applications are delivered dynamically, mostly as JavaScript source code. This delivery format has sparked extremely enthusiastic efforts to customize both individual web sites and entire browsers in ways the original authors never expected or accommodated. Such customizations take the form of yet more script dynamically injected into the application, and the current idioms to do so exploit arcane JavaScript features and are extremely brittle. In this work, we accept the popularity of extensions and seek better linguistic mechanisms to support them. We suggest adding to JavaScript aspect-oriented features that allow straightforward and declarative ways for customization code to modify the targeted application. Compared to most prior aspect-related research, our work has a different motivation and a different target programming environment, both of which lead to novel design and implementation techniques. Our aspect weaving is entirely integrated into a new dynamic JIT compiler, which lets us properly handle advice to first-class functions in the presence of arbitrary aliasing, without resorting to whole-program code transformations. Our prototype demonstrates that an aspect-oriented approach to web-application customization is often more efficient than current idioms while simplifying the entire process.",
        "keywords": "Languages; Design; Aspects; extensions; JavaScript",
        "released": 2010,
        "link": "https://doi.org/10.1145/1932682.1869490"
    },
    {
        "title": "An approach to build JSON-based domain specific languages solutions for web applications",
        "abstract": "Because of their level of abstraction, Domain-Specific Languages (DSLs) enable building applications that ease software implementation. In the context of web applications, we can find a lot of technologies and programming languages for server-side applications that provide fast, robust, and flexible solutions, whereas those for client-side applications are limited, and mostly restricted to directly use JavaScript, HTML5, CSS3, JSON and XML. This article presents a novel approach to creating DSL-based web applications using JSON grammar (JSON-DSL) for both, the server and client side. The approach includes an evaluation engine, a programming model and an integrated web development environment that support it. The evaluation engine allows the execution of the elements created with the programming model. For its part, the programming model allows the definition and specification of JSON-DSLs, the implementation of JavaScript components, the use of JavaScript templates provided by the engine, the use of link connectors to heterogeneous information sources, and the integration with other widgets, web components and JavaScript frameworks. To validate the strength and capacity of our approach, we have developed four case studies that use the integrated web development environment to apply the programming model and check the results within the evaluation engine.",
        "keywords": "Domain-Specific Languages; JavaScript; JSON; JSON-DSL; Web applications; Templates engine",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.cola.2023.101203"
    },
    {
        "title": "A review on effective approach to teaching computer programming to undergraduates in developing countries",
        "abstract": "Universities in developing countries and maybe a few in some developed countries are faced with the challenge of adopting an effective pedagogy for teaching Computer Programming Languages to students. This is due to lack of technological infrastructure and a number of peculiar limitations within these regions. The goal of this paper is twofold, conducting a systematic review of related literature in Programming Languages and proposing a model for teaching Programming Languages effectively given sparse resources. On the systematic review, a search for literature from 4 databases using appropriate inclusion and exclusion techniques yielded 18 relevant research articles. Based on gaps found in literature, we propose an effective teaching approach that integrates teaching basic foundational aspects of programming, teaching students how to represent computing problems using these foundational concepts and using mobile compilers to compile their codes on the event that systems are not available. A few initial empirical results showing the effect of the adoption of a mobile tool for teaching Programming to undergraduate students are presented. This research recommends the adoption of an Online Console Compiler as the mobile Integration Development Environment (IDE). It also recommends some Programming Languages that should be adopted in teaching students from first year to fourth year using the National University Commission’s (NUC) guidelines, course outlines from four Universities and data from Universities in developed countries. Care should be taken in the use of mobile phones as there are safety rules in some developed countries on the minimal set of acceptable systems for programmers. Recommendations for future research are discussed. (C) 2022 The Authors. Published by Elsevier B.V. on behalf of African Institute of Mathematical Sciences / Next Einstein Initiative.",
        "keywords": "Teaching Computer Programming; Programming Languages; Mobile compilers; Developing countries; National University Commission; Education",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.sciaf.2022.e01240"
    },
    {
        "title": "Ranking cloud computing criteria in developing electronic communications services using MCDM",
        "abstract": "The main purpose of this study was ranking cloud computing criteria in developing electronic communications services using multiple-criteria decision analysis methodology (MCDM). This correlational research was applied in terms of purpose. Statistical population of the study included ICT experts of the steel industry in Yazd province (industrial experts), among which, 312 individuals were selected using purposeful nonprobability (judgmental) sampling method. Considering the conducted investigations and critically reviewing the related books and articles, the variables, criteria, and scales were identified using cloud computing technology. To analyze the data, MCDM and fuzzy logic calculations were utilized in Expert Choice software. According to the results and considering fuzzy calculations related to the capabilities of cloud computing in developing electronic communications services, the most important criteria in the “IT management in steel industries” cluster having (A) network code was “communicating with steel industries’costumers” having (AB) network code and fuzzy network weight equal to 0.096; the most important criteria in “ cloud computing capabilities” cluster having (B) network code were “reducing steel industries’costs” having (BA) network code and fuzzy network weight equal to 0.191; and “providing rapid services to steel industries’costumers” having (BB) network code and fuzzy network weight equal to 0.120. on the other hand, the most important criteria in “developing electronic communications services” cluster having (C) network code was “storing the data in electronic communications services” having (CD) network code and fuzzy network weight equal to 0.123, since based on fuzzy logic calculation, they had the highest fuzzy rank in Matlab programming environment regarding cloud computing capabilities in developing electronic communications services.",
        "keywords": "Cloud computing; Electronic communications services; Multiple-criteria decision analysis methodology (MCDM); Fuzzy logic; Steel industries of Yazd province",
        "released": 2021,
        "link": "https://doi.org/10.20511/pyr2021.v9nSPE1.885"
    },
    {
        "title": "Web applications design recovery and evolution with RE-UWA",
        "abstract": "This paper presents a semi-automatic approach for the recovery and evolution of the design of existing Web applications. The proposed approach is structured in two main phases and is based on the Ubiquitous Web Applications (UWA) design framework, a methodology and a set of models and tools for the user-centered design of multichannel context-aware Web applications. In the first phase a representative set of the application’s front-end Web pages are analyzed to abstract the as-is’ design model of the application according to the UWA methodology. In the second phase, the recovered design model is evolved to define the to be’ version of it. This evolution activity considers the up-to-date requirements available for the application and UWA design guidelines to identify shortcomings and opportunities of improvement in the as-is’ design. The reverse modeling phase exploits clustering and clone detection techniques and is supported by the RE-UWA tool, an Eclipse IDE customized to implement the reverse engineering process defined to extract formal UWA models expressed as instances of a MOF metamodel. The forward design phase is supported by a set of UWA modeling tools that are built on top of the Eclipse Modeling Framework (EMF) and the Eclipse Graphical Modeling Framework (GMF). The proposed design recovery and evolution approach is applied to four real-world Web applications and the obtained results are also presented in the paper. Copyright (c) 2012 John Wiley & Sons, Ltd.",
        "keywords": "Web applications design recovery; Web systems evolution; UWA; RE-UWA; clustering; clone detection; Eclipse; GEF; GMF",
        "released": 2013,
        "link": "https://doi.org/10.1002/smr.1561"
    },
    {
        "title": "AgTC and AgETL: Open-source tools to enhance data collection and management for plant science research",
        "abstract": "Advancements in phenotyping technology have enabled plant science researchers to gather large volumes of information from their experiments, especially those that evaluate multiple genotypes. To fully leverage these complex and often heterogeneous data sets (i.e. those that differ in format and structure), scientists must invest considerable time in data processing, and data management has emerged as a considerable barrier for downstream application. Here, we propose a pipeline to enhance data collection, processing, and management from plant science studies comprising of two newly developed open-source programs. The first, called AgTC, is a series of programming functions that generates comma-separated values file templates to collect data in a standard format using either a lab-based computer or a mobile device. The second series of functions, AgETL, executes steps for an Extract-Transform-Load (ETL) data integration process where data are extracted from heterogeneously formatted files, transformed to meet standard criteria, and loaded into a database. There, data are stored and can be accessed for data analysis-related processes, including dynamic data visualization through web-based tools. Both AgTC and AgETL are flexible for application across plant science experiments without programming knowledge on the part of the domain scientist, and their functions are executed on Jupyter Notebook, a browser-based interactive development environment. Additionally, all parameters are easily customized from central configuration files written in the human-readable YAML format. Using three experiments from research laboratories in university and non-government organization (NGO) settings as test cases, we demonstrate the utility of AgTC and AgETL to streamline critical steps from data collection to analysis in the plant sciences.",
        "keywords": "data pipeline; extract-transform-load; database; data aggregation; data processing; plant phenotyping",
        "released": 2024,
        "link": "https://doi.org/10.3389/fpls.2024.1265073"
    },
    {
        "title": "A methodology for agent oriented web service engineering",
        "abstract": "Web Service (WS) has been widely accepted in recent years. It is becoming the next paradigm to deploy business services on the Web. However, building WS is not an easy task due to the complexity of various business processes and different communication protocols. In this paper, we propose a methodology to efficiently develop Web Services by plugging in software agents. This agent oriented method uses software agents as building blocks of WS, and exploits the commonly available agent development tools to accelerate the whole development cycle. Software agents in this methodology not only implement the business processes, but also enrich the functions of WS. Our approach provides a solution for Web Service engineering. A case study in loan application service is presented to show the benefits and advantages of this agent oriented service engineering methodology.",
        "keywords": "",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000239626100074"
    },
    {
        "title": "Adobe AIR: Desktop/web convergence",
        "abstract": "Over the last few years, web-based Rich Internet Applications (RIAs) have become increasingly sophisticated. They are leveraging technologies such as HTML, AJAX, Flash, and PDF to add depth and interactivity previously found only in desktop applications. At the same time, these technologies often allow for shorter development times than traditional desktop development environments such as C++.",
        "keywords": "",
        "released": 2007,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000250230500007"
    },
    {
        "title": "Object-oriented web application development",
        "abstract": "The Web has evolved into a global environment for delivering all kinds of applications, but their development andmaintenance remains largely ad hoc. The authors suggest that the Web’s roots as an information medium establish a resource-based implementation model that is semantically too weak to support the lif cycle of complex applications. They describe new developments in an ongoing project, called WebComposition, for imposing object-orientation on the Web and thus facilitating the use of well-known software development tools. WebComposition is a method based on object-oriented decomposition of Web applications into fine-grained components, which serve as units for composition, sharing, abstraction, reuse, and evolution of Web-based applications. The method uses a Component Store for persistent storage of the component model, and a Resource Generator for automated mapping to an implementation in the Web. WebComposition uses an XML-based language to describe a markup notation for WebComposition concepts.",
        "keywords": "",
        "released": 1999,
        "link": "https://doi.org/10.1109/4236.747323"
    },
    {
        "title": "Quantitative analysis of distance learning courseware",
        "abstract": "Web-based courseware are widely developed for distance learning programs in continue education, employee training center, and e-learning portals. Usually, a Web-based courseware contains course contents and on-line tests. However, most Web document development tools are not incorporated with a strategic evaluation mechanism, to allow a quantitative analysis of distance learning courseware. We propose an evaluation mechanism and a multimedia tool, based on our Courseware Diagram, to allow a quantitative justification of courseware. Courseware produced by our development system allows an instructor to choose different instruction sequences based on the outcomes of an exam. Alternatively, the courseware may allow self-guided study in a Web-based distance learning program. This paper explains the courseware diagram, the evaluation algorithm, and the design of our courseware development system.",
        "keywords": "distance learning; influence diagram; conceptual graph; automatic assessment; courseware diagram; virtual university",
        "released": 2003,
        "link": "https://doi.org/10.1023/A:1023470400109"
    },
    {
        "title": "Data descriptor: A longitudinal dataset of five years of public activity in the scratch online community",
        "abstract": "Scratch is a programming environment and an online community where young people can create, share, learn, and communicate. In collaboration with the Scratch Team at MIT, we created a longitudinal dataset of public activity in the Scratch online community during its first five years (2007-2012). The dataset comprises 32 tables with information on more than 1 million Scratch users, nearly 2 million Scratch projects, more than 10 million comments, more than 30 million visits to Scratch projects, and more. To help researchers understand this dataset, and to establish the validity of the data, we also include the source code of every version of the software that operated the website, as well as the software used to generate this dataset. We believe this is the largest and most comprehensive downloadable dataset of youth programming artifacts and communication.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1038/sdata.2017.2"
    },
    {
        "title": "A library to generate synthetic precipitation data",
        "abstract": "A critical issue in biophysical modeling projects is to develop a set of reusable libraries to support the development of future applications. This is true for weather modeling as well. Rain is a software component providing a collection of stochastic approaches to generate precipitation data on daily and subdaily time steps. Synthetic data generated on a daily time step consists of precipitation occurrence and amount. Subdaily generation includes time of peak, peak intensity, storm arrival, and duration. The software design allows for extension of the models implemented without recompiling the component. The component, inclusive of a hypertext help file and of documentation generated from source code comments, has been released as compiled NET and Java versions, allowing application development in either programming environment. Illustrative examples of Windows-based applications using Rain are provided as source code. A sample web service and a web application were also developed as possible use of the component.",
        "keywords": "",
        "released": 2006,
        "link": "https://doi.org/10.2134/agronj2005.0210"
    },
    {
        "title": "Data integration using web services",
        "abstract": "In this paper we examine the opportunities for data integration in the context of the emerging Web Services systems development paradigm. The paper introduces the programming standards associated with Web Services and provides. an example of how Web Services can be used to. unlock heterogeneous business systems to extract and integrate business data. We provide an introduction to the problems and research issues encountered when applying Web Services to data integration. We provide a formal definition of aggregation (as a type of data integration) and discuss the impact of Web Services on aggregation. We show that Web Services will make the development of systems for aggregation both faster and less expensive to develop. A system architecture for Web Services based aggregation is presented that is representative of products available from software vendors today. Finally, we highlight some of the challenges facing Web Services that are not currently being addressed by standards bodies or software vendors. These include context mediation, trusted intermediaries, quality and source selection, licensing and payment mechanisms, and systems development tools. We suggest some research directions for each of these challenges.",
        "keywords": "",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000182919100015"
    },
    {
        "title": "XGuide -: A practical guide to XML-based web engineering",
        "abstract": "Various approaches have been proposed in the field of Web engineering that attempt to exploit the advantages of XML/XSL technologies. Although a strict separation of presentation and content achieved through XML/XSL has many advantages, a considerable effort is involved in using these technologies to develop Web sites. The lack of experience in XML/XSL can be a major cause for the extra effort. In several XML/XSL-based Web projects, we felt the need for a methodology that systematically guides the developer in the field through the development process while taking into account the limitations and strengths of XML. In this paper, we present XGuide, a practical guide for XML-based Web Engineering that focuses on parallel development. XGuide is a methodology for XML/XSL-based Web development that is tool-independent and hence, can be used with a broad range of development tools. We are currently using the XGuide approach in several Web projects.",
        "keywords": "XML; Web Engineering; methodology; Web Service Life Cycle; parallel development",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000187251900009"
    },
    {
        "title": "Components of dynamic digital maps",
        "abstract": "The core of a dynamic digital map (DDM) is a program created to display a digital geologic map and its associated data set as a single integrated package. This DDM program core is authored using a high-level object oriented graphical programming environment, SuperCard, a Macintosh “xTalk” language. Some of the major philosophical underpinnings relevant to the job of creating DDMs in this programming environment include taking advantage of specific object’s attributes, how to organize data structure effectively for this job and keeping applicable code as transportable (generalized) as possible between objects. The core components of DDMs are parts of a SuperCard “template” or map display “container” into which the various externally created digital pieces (e.g. CAD produced geologic maps, spread sheet data, digital images, text files, movies) can be inserted for integrated display and digital publication. Components of DDMs consist of windows, cards within these windows, card fields (text and data containers) and a menu structure. The paper discusses each of these major components and how they fit into a DDM and gives examples of each topic in programs whose components demonstrate the concept. These programs are available from Web (see Appendix A); readers can refer to them as they examine these examples. (C) 1999 Elsevier Science Ltd. All rights reserved.",
        "keywords": "SuperCard; XTalk; digital geologic maps; desktop publishing",
        "released": 1999,
        "link": "https://doi.org/10.1016/S0098-3004(98)00156-3"
    },
    {
        "title": "A service relation model for web-based land cover change detection",
        "abstract": "Change detection with remotely sensed imagery is a critical step in land cover monitoring and updating. Although a variety of algorithms or models have been developed, none of them can be universal for all cases. The selection of appropriate algorithms and construction of processing workflows depend largely on the expertise of experts about the “algorithm-data” relations among change detection algorithms and the imagery data used. This paper presents a service relation model for land cover change detection by integrating the experts’ knowledge about the “algorithm-data” relations into the web-based geo-processing. The “algorithm-data” relations are mapped into a set of web service relations with the analysis of functional and non-functional service semantics. These service relations are further classified into three different levels, i.e., interface, behavior and execution levels. A service relation model is then established using the Object and Relation Diagram (ORD) approach to represent the multi-granularity services and their relations for change detection. A set of semantic matching rules are built and used for deriving on-demand change detection service chains from the service relation model. A web-based prototype system is developed in. NET development environment, which encapsulates nine change detection and pre-processing algorithms and represents their service relations as an ORD. Three test areas from Shandong and Hebei provinces, China with different imagery conditions are selected for online change detection experiments, and the results indicate that on-demand service chains can be generated according to different users’ demands. (C) 2017 Published by Elsevier B.V.",
        "keywords": "Land cover; Change detection; “Algorithm-data” relation; Service relation modeling; Semantic matching rule; Web-based system",
        "released": 2017,
        "link": "https://doi.org/10.1016/j.isprsjprs.2017.08.007"
    },
    {
        "title": "IDEAL: An integrated distributed environment for asynchronous learning",
        "abstract": "In this paper, we present the design and implementation of IDEAL, an Integrated Distributed Environment for Asynchronous Learning. The learning environment supports a Web-based distributed community for student-centered, self-paced, and highly interactive learning. IDEAL enables the students in the community to learn from each other and enhances their learning experience. Implemented using the prevalent Internet, Web, intelligent agent, and digital library technologies, IDEAL adopts an open architecture design and targets at large-scale, distributed operations. In the initial implementation, a number of prototypes using different Java-based software development environments have been developed.",
        "keywords": "",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000171881900020"
    },
    {
        "title": "A hybrid meta-heuristic for optimal load balancing in cloud computing",
        "abstract": "Nowadays, a trending technology that provides a virtualized computer resources based on the internet is named as cloud computing, these clouds performance mostly depends on the various factors among the load balancing. The allocation of the dynamic workload in between the cloud systems and equally shares the resources so that no database server is overloaded or under loaded is technically referred to as load balancing (LB). Therefore, in cloud an active load balancing scheme can perhaps enhance the reliability, services and the utilization of resources as well. In this manuscript, the benefits are integrated for Harries Hawks Optimization and Pigeon inspired Optimization Algorithm to create efficient load balancing scheme, which ensures the optimal resources utilizations with tasks response time. The proposed approach is implemented in JAVA Net beans IDE incorporated in the cloudsim framework that is analyzed based on different number of task in order to assess the performance. However, the simulation outcomes demonstrate that the proposed Hawks Optimization and Pigeon inspired Optimization algorithm based load balancing scheme is significantly balance the load optimally amid the Virtual Machines within a shorter period of time than the existing algorithms. The efficiency of the proposed method is 97% compared to the other existing methods. The computational time, cost, throughput analysis, make span, latency, execution time are determined and gets analysed, compared with the Harries Hawks Optimization, Spider Monkey Algorithm, Ant Colony Optimization and Honey Bee Optimization.",
        "keywords": "Load balancing; Virtual machines; Hawks optimization algorithm (HOA); Pigeon optimization algorithm (POA)",
        "released": 2021,
        "link": "https://doi.org/10.1007/s10723-021-09560-4"
    },
    {
        "title": "Web accessibility automatic evaluation tools: To what extent can they be automated?",
        "abstract": "Web accessibility automatic evaluation tools (WAET) are used to evaluate the conformance of the web content to the web content accessibility guidelines (WCAG) success criteria (SC). This paper aims to identify performance criteria that can be used to compare between automatic web accessibility evaluation tools (WAET), determine which SC can be automatically tested based on current technologies, which one requires more advanced technologies, and how can WAET reduce the number of mistakenly reported errors. WCAG 2.1 SC level-A, AA, and AAA are analyzed. The obtained results help in exploring new directions that can lead to more efficient and reliable automatic Web contents assessment as well as development tools. The outcome can help the developers of WAET to increase the number of SC checked in their tools by utilizing cutting-edge technologies. In addition, the presented performance indicators can help to identify how to measure the performance of WAET.",
        "keywords": "Web accessibility; Automatic evaluation tools; Performance indicators; Web content accessibility guidelines",
        "released": 2023,
        "link": "https://doi.org/10.1007/s42486-023-00127-8"
    },
    {
        "title": "<I>Sucre4Stem</i>: Collaborative projects based on IoT devices for students in secondary and pre-university education",
        "abstract": "This paper describes a new technological evolution of the Sucre project, which aims to foster a vocation for science and develop computational thinking and programming skills in pre-university students. This improved version is called Sucre4Stem and has been designed from the Internet of Things perspective. At a technological level, we differentiate two main tools, SucreCore andSucreCode. SucreCore provides a new, more compact design, encapsulates an advanced microcontroller and supports wireless connectivity with the ability to create online variables and functions. SucreCode, the block-based visual programming tool, has a revamped interface and allows wireless communication with SucreCore. At the pedagogical level, Sucre4Stem makes it easier to implement new group dynamics and to create novel types of collaborative projects between groups of students. In this article, we also explore how these collaborative projects can be carried out by taking advantage of’ the different types of communications between SucreCore and the server-side platform using shared online variables and functions.",
        "keywords": "Computational thinking; programming promotion; pre-university ages; Internet of Things",
        "released": 2022,
        "link": "https://doi.org/10.1109/RITA.2022.3166854"
    },
    {
        "title": "An efficient cloud-based healthcare services paradigm for chronic kidney disease prediction application using boosted support vector machine",
        "abstract": "Cloud computing is the on-demand access to computer resources such as applications, servers, data storage, development tools, networking capabilities, and other resources that are held in a remote data center maintained by a cloud services provider(CSP) and accessible over the internet. The limited resources and higher time consumption are the major issues faced by cloud users. The appropriate selection of virtual machines (VMs) is used to maximize cloud resource usage and reduce task execution time for the clients (patients, physicians, etc.). In this article, we have introduced an efficient cloud-based healthcare services paradigm (HCS). The opposition-based Laplacian equilibrium optimizer (O-LEO) algorithm is used to select optimal VMs in which the maximization of resource utilization and minimization of task execution time is performed. Additionally, the boosted support vector machine (SVM) effectively predicts chronic kidney disease (CKD) thereby ensuring better prediction results. The CloudSim platform is used as the implementation platform of the proposed method. The overall time taken by the O-LEO-based CloudSim is less than the standard Cloud Sim model to create three cloudlets which improve the system efficiency by 6%. When compared with the existing techniques, both the O-LEO and boosted SVM classifier outperforms superior performances.",
        "keywords": "boosted SVM; CKD diagnosis; health care services; opposition-based Laplacian equilibrium optimizer; task scheduling",
        "released": 2022,
        "link": "https://doi.org/10.1002/cpe.6722"
    },
    {
        "title": "Evaluating the use of search engine development tools in IT education",
        "abstract": "It is important for education in computer science and information systems to keep up to date with the latest development in technology. With the rapid development of the Internet and the Web, many schools have included Internet-related technologies, such as Web search engines and e-commerce, as part of their curricula. Previous research has shown that it is effective to use search engine development tools to facilitate students’ learning. However, the effectiveness of these tools in the classroom has not been evaluated. In this article, we review the design of three search engine development tools, SpidersRUs, Greenstone, and Alkaline, followed by an evaluation study that compared the three tools in the classroom. In the study, 33 students were divided into 13 groups and each group used the three tools to develop three independent search engines in a class project. Our evaluation results showed that SpidersRUs performed better than the two other tools in overall satisfaction and the level of knowledge gained in their learning experience when using the tools for a class project on Internet applications development.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1002/asi.21223"
    },
    {
        "title": "Intermural online research group meetings as professional development tools for undergraduate, graduate, and postdoctoral trainees",
        "abstract": "In academic research laboratories, well-organized group meetings are common training tools that can benefit individuals and the group as a whole. Owing to the COVID-19 pandemic, virtual meetings have become an increasingly important format for research group meetings. Virtual meeting formats also offer an important and underappreciated advantage: ease of collaboration with researchers at other (potentially distant) institutions. Herein, we describe the strategies we employed to facilitate engaging and productive online meetings and academic exchanges between environmental chemistry/engineering laboratories at a primarily undergraduate program and a doctoral research intensive institution. Over a period of 12 months, six intermural group meetings were held through videoconference. All meetings consisted of two segments: (1) a literature or research discussion and (2) a professional development session that emphasized topics such as navigating life after completing a bachelor’s degree, advice on securing graduate school admission and funding, and characteristics of effective instructors (from the undergraduate perspective). Student-led discussion of scientific literature and research is valuable in enhancing trainees’ communication skills, interdisciplinary perspectives, and critical reading of the literature. Professional development sessions facilitate unique opportunities for professional mentorship. Given the substantial pedagogical and other professional benefits of intermural group meetings, we recommend this meeting format as a useful training tool for research trainees even after the current pandemic wanes.",
        "keywords": "higher education; interdisciplinary training; collaborative learning; professional development",
        "released": 2022,
        "link": "https://doi.org/10.1089/ees.2021.0147"
    },
    {
        "title": "JChem: Java applets and modules supporting chemical database handling from web browsers",
        "abstract": "A Java based development tool for building portable chemical information systems is presented. The system contains applets for constructing web-based interfaces and classes that add structure handling to relational databases. Custom applications built with JChem can combine SQL and structural queries.",
        "keywords": "",
        "released": 2000,
        "link": "https://doi.org/10.1021/ci9902696"
    },
    {
        "title": "COMMUNICATING IN SOFTWARE ENGINEERING ENVIRONMENTS - THE GRASPIN PARADIGM",
        "abstract": "Programming environments need a specially tailored component which handles the interaction between the user and a variety of tools. In this paper, the principles that guided the implementation strategy for such a component in the development of the GRASPIN environment are reported. The functionalities of the User-Tool Interaction Subsystem are presented and the way they were incorporated in the overall architecture is described. In particular, focus is placed on the development of the user interface of a software engineering environment based on the properties of uniformity, consistency, and customizability. Our experience has shown that in addition to these properties, a proper mixture of “minor” facilities, such as history editing, online help, and command completion, when uniformly applied, can make the difference between a good and bad user interface.",
        "keywords": "",
        "released": 1991,
        "link": "https://doi.org/10.1016/0164-1212(91)90079-L"
    },
    {
        "title": "Activity theory for the analysis and design of multi-agent systems",
        "abstract": "Modeling a Multi-Agent System (MAS) involves a large number of entities and relationships. This implies the need for defining an organization, in order to structure and manage the complexity of the whole system. In this work we propose the use of human organization metaphors and their application to the verification of MAS specification. In concrete, we apply Activity Theory, which has its roots in Sociology, to study agent systems and obtain relationship patterns that can be applied to the analysis of MAS. These patterns guide analysis and design refinements, and help to detect inconsistencies. This technique has been implemented and integrated in the INGENIAS IDE platform, and proved with some case studies, in particular, for agent-based web applications.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000189407600008"
    },
    {
        "title": "Effectiveness and efficiency of a domain-specific language for high-performance marine ecosystem simulation: A controlled experiment",
        "abstract": "It is a long-standing hypothesis that the concise and customized notation of a DSL improves the performance of developers when compared with a GPL. For non-technical domains-e.g., science-, this hypothesis lacks empirical evidence. Given this lack of empirical evidence, we evaluate a DSL for ecological modeling designed and implemented by us with regard to performance improvements of developers as compared to a GPL. We conduct an online survey with embedded controlled experiments among ecologists to assess the correctness and time spent of the participants when using a DSL for ecosystem simulation specifications compared with a GPL-based solution. We observe that (1) solving tasks with the DSL, the participants’ correctness point score was -depending on the task- on average 61 % up to 63 % higher than with the GPL-based solution and their average time spent per task was reduced by 31 % up to 56 %; (2) the participants subjectively find it easier to work with the DSL, and (3) more than 90 % of the subjects are able to carry out basic maintenance tasks concerning the infrastructure of the DSL used in our treatment, which is based on another internal DSL embedded into Java. The tasks of our experiments are simplified and our web-based editor components do not offer full IDE-support. Our findings indicate that the development of further DSL for the specific needs of the ecological modeling community should be a worthwhile investment to increase its members’ productivity and to enhance the reliability of their scientific results.",
        "keywords": "Domain-specific languages (DSLs); Program comprehension; Computational science; Scientific software development",
        "released": 2017,
        "link": "https://doi.org/10.1007/s10664-016-9483-z"
    },
    {
        "title": "Increasing the interactivity and educational impact of a web-based radiology teaching file by using the java programming environment",
        "abstract": "",
        "keywords": "",
        "released": 1996,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:A1996VP84302717"
    },
    {
        "title": "JESSICA: An object-oriented hypermedia publishing processor",
        "abstract": "The lifecycle of Web applications covers the design, implementation, and maintenance of the services. First generation Web development tools just concentrated on the creation of single pages. Later Web engineering tools have been integrating the management of complete Web sites and the navigation model. But only few attempt to cover all the aspects of the lifecycle, and especially the maintenance task which is essential on a dynamic medium as is the case on the Web. To increase the manageability and introduce flexibility to large Web services we introduce the JESSICA engineering system that employs an object-oriented abstraction model for the hypermedia information. An object-oriented language describes components of the Web service that are easy to manage, reusable, highly dynamic and of polymorphic type, covering all elements of a complete Web site. The objects are accessible throughout the lifecycle for management and maintenance activities. A compiler maps the abstract service description to the file-based repository of a standard Web server. We demonstrate the feasibility of the engineering system on managing the Vienna International Festival Web site, a multilingual database Web application on culture and arts, containing 300+ static pages and several interactive services. (C) 1998 Published by Elsevier Science B.V. All rights reserved.",
        "keywords": "Web service engineering; object orientation",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0169-7552(98)00071-3"
    },
    {
        "title": "Towards quality of service for parallel computing: An overview of the MILAN project",
        "abstract": "Parallel computing is faced with many practical difficulties, e.g. the gap between simple, high-level programming models and complex, real execution environments like a cluster of workstations, and the unpredictability of program execution. The MILAN project addresses these problems and aims at increased Quality of Service (QoS) for parallel programs. This paper presents an overview of the current research results of MILAN. For clusters of workstations, the Calypso system provides a simple programming environment that: leverages theoretical results on fault-tolerant execution of parallel programs. A resource management system implements the necessary resource contracts for QoS, particularly for parallel applications. The concepts of Calypso have been applied to Web-based computing with the Charlotte system.",
        "keywords": "",
        "released": 1999,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000088252100085"
    },
    {
        "title": "Incorporating the ontology paradigm into a mainstream programming environment",
        "abstract": "The emergence of the Semantic Web have revived the interest in knowledge engineering and ontologies. Different paradigms often share challenges and solutions, and can complement and mutually improve each other. This paper presents a simple and agile integration of ontologies and programming on a small scale, and in a down-to-Earth manner by incorporating the ontology paradigm into a mainstream programming environment. The approach is based on metaprogramming, which has been used to internalize the ontology modeling paradigm into the Clojure language. The resulting DSL, Magic Potion, is implemented in Cojure and blends ontology, functional, object-oriented and concurrent paradigms, which is suitable for general-purpose domain modeling, from technology enhanced learning to business.",
        "keywords": "programming paradigms; multiparadigm languages; ontology paradigm; ontology languages; metaprogramming; domain-specific languages; programming languages; domain engineering; programming techniques",
        "released": 2012,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000306154900003"
    },
    {
        "title": "Cloud computing based deduplication using high-performance grade byte check and fuzzy search technique",
        "abstract": "Background: Data redundancy (DR) and data privacy (DP) is a critical issue that increases storage and security problems in cloud environments. Data de-duplication (DD) is one of the efficient backup storage techniques to reduce DR. The main problem with using cloud computing (CC) is more storage, the cost of deployment and maintenance. Objective: To minimize this problem, High-performance Grade Byte Check and Fuzzy search Techniques (HP-GBC-FST) based DD is proposed in this paper. Methods: The HP-GBC-FST is based on the pre-process of data by comparing their first byte and categorizing the byte based on the first byte. After DD, encryption has been processed on data to improve the data security in the cloud environment and then encrypted data is stored in the cloud. This HP-GBC-FST recognizes DR at the block level, reducing the redundancy of data more effectively. Then, HP-GBC-FST is created to detect and eliminate duplicates, improve security and storage efficiency (SE), reduce DD time and computation cost (CPC) in the DD verification and auditing phase. Result: The experiment has been conducted in an Intel I5 system and 500GB, 1Tb memory space and implemented in the Java programming environment. The results of the experiment reveal that the HP-GBC-FST improved the DD ratio and security by 3.7 and 97%, respectively, and reduced the DD time and CPC by 87% and 84.4%, respectively, over the existing technique. Conclusion: It concluded that the HP-GBC-FST has greater improvement overDDdata in the cloud. Finally, the performance analysis of the HP-GBC-FST achieves higher storage, both privacy and security attributes, and incurs minimal CPC, DD time compared with the state he art research.",
        "keywords": "Fuzzy search (FS); cloud computing (CC); data deduplication (DD); encryption; grade byte check (GBC)",
        "released": 2023,
        "link": "https://doi.org/10.3233/JIFS-220206"
    },
    {
        "title": "A delivery robot cloud platform based on microservice",
        "abstract": "Delivery robots face the problem of storage and computational stress when performing immediate tasks, exceeding the limits of on-board computing power. Based on cloud computing, robots can offload intensive tasks to the cloud and acquire massive data resources. With its distributed cluster architecture, the platform can help offload computing and improve the computing power of the control center, which can be considered the external “brain” of the robot. Although it expands the capabilities of the robot, cloud service deployment remains complex because most current cloud robot applications are based on monolithic architectures. Some scholars have proposed developing robot applications through the microservice development paradigm, but there is currently no unified microservice-based robot cloud platform. This paper proposes a delivery robot cloud platform based on microservice, providing dedicated services for autonomous driving of delivery robot. The microservice architecture is adopted to split the monomer robot application into multiple services and then implement automatic orchestration and deployment of services on the cloud platform based on components such as Kubernetes, Docker, and Jenkins. This enables containerized CI/CD (continuous integration, continuous deployment, and continuous delivery) for the cloud platform service, and the whole process can be visualized, repeatable, and traceable. The platform is prebuilt with development tools, and robot application developers can use these tools to develop in the cloud, without the need for any customization in the background, to achieve the rapid deployment and launch of robot cloud service. Through the cloud migration of traditional robot applications and the development of new APPs, the platform service capabilities are continuously improved. This paper verifies the feasibility of the platform architecture through the delivery scene experiment.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1155/2021/6656912"
    },
    {
        "title": "VPPE: A novel visual parallel programming environment",
        "abstract": "Parallel programming continues to be a challenging task despite the many advances in parallel architectures and their wide availability in the cloud. The need both to partition the workload among various processing elements and to specify communication between them to share code and data, and to coordinate their tasks, requires from the developer a deep understanding of the problem, the parallel architecture and the programming language used in order to develop efficient parallel applications. This problem can be reduced significantly through the use of visual programming languages to hide most aspects related to the specification of communication and processes management. This paper presents VPPE, a novel Visual Parallel Programming Environment that allows developers to program parallel applications through organising workflows of interconnected icons. VPPE is a cloud environment that supports icons for specifying: I/O operations, workflow organisation, communication, and processing. Processing computing patterns supported so far include Single Program Multiple Data, Multiple Program Multiple Data, Pipeline, and Master-Slave. The paper highlights the design of VPPE based on a context-free graph grammar, its current implementation based on Java-MPI, its use in developing various parallel applications, and its evaluation compared to Java-MPI text-based programming.",
        "keywords": "Parallel patterns; Workflow; Graph grammar; Hyperedge replacement grammar; Cloud computing",
        "released": 2019,
        "link": "https://doi.org/10.1007/s10766-019-00639-w"
    },
    {
        "title": "Integrating a case-based reasoning shell and web 2.0: Design recommendations and insights",
        "abstract": "The design and implementation of case-based reasoning (CBR) applications is time-consuming. To facilitate the development of CBR applications in various problem domains, the CBR community has created a number of CBR shells and software frameworks in the past twenty years. This paper provides a review of the state-of-the-art of CBR shells and software frameworks, highlights why the integration of Web 2.0 and CBR development tools is useful, and gives an example as to how we implement such integration. We use this example to illustrate how Web 2.0 features such as blogging functions can be integrated in a CBR system. Design recommendations and insights for implementing a Web 2.0-based CBR shell are also provided.",
        "keywords": "Case-based reasoning; CBR shell; Web 2.0; Case base; Case library",
        "released": 2016,
        "link": "https://doi.org/10.1007/s11280-015-0380-y"
    },
    {
        "title": "Electrolint and security of electron applications",
        "abstract": "JavaScript applications today are not limited to just client-side web applications and server-side code powered by Node.js. They became the standard for desktop application development with the emergence and popularity of the Electron framework. Combining the features of client-side and server-side applications, the Electron applications possess a completely different security posture. The attacks typical for front-end applications can now be escalated to the back-end attacks, for example, making a cross-site scripting result in a remote code execution on the user’s machine. The goal of our study is to analyze the typical security vulnerabilities of an Electron application, study common mitigation controls, and propose new remediation solutions that are easy to implement for developers. In this study we analyze security vulnerabilities in over a hundred open source Electron applications using automated and manual static analysis. We explore the mitigation controls existing in the Electron framework, and propose changes to the framework that will prevent many of the common vulnerabilities. Based on these results, we develop an IDE plugin for Electron applications that automatically suggests remediations to common security defects within a developer’s work environment, thus shifting the fixing of a vulnerability to earlier in the software development life cycle. We show the effectiveness of the IDE plugin by applying the plugin’s suggestions to the analyzed open source applications and demonstrating that they stop being exploitable after the applied fix.",
        "keywords": "JavaScript security; Web security; Desktop security; Framework analysis; Electron framework; Static analysis",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.hcc.2021.100032"
    },
    {
        "title": "Process aggregation using web services",
        "abstract": "This paper examines the opportunities and challenges related to data and process integration architectures in the context of Web Services. A primary goal of most enterprises in today’s economic environment is to improve productivity by streamlining and aggregating business processes. This paper illustrates how integration architectures based on Web Services offer new opportunities to improve productivity that are expedient and economical. First, the paper introduces the technical standards associated with Web Services and provides business example for illustration. Abstracting from this example, we introduce a concept we call Process Aggregation that incorporates data aggregation and workflow to improve productivity. We show that Web Services will have a major impact on Process Aggregation, making it both faster and less expensive to implement. Finally, we suggest some research directions relating to the Process Aggregation challenges facing Web Services that are not currently being addressed by standards bodies or software vendors. These include context mediation, trusted intermediaries, quality and source selection, licensing and payment mechanisms, and systems development tools.",
        "keywords": "",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000181619900002"
    },
    {
        "title": "RDF(s) INTEROPERABILITY RESULTS FOR SEMANTIC WEB TECHNOLOGIES",
        "abstract": "Interoperability among different development tools is not a straightforward task since ontology editors rely on specific internal knowledge models which are translated into common formats such as RDF(S). This paper addresses the urgent need for interoperability by providing an exhaustive set of benchmark suites for evaluating RDF(S) import, export and interoperability. It also demonstrates, in an extensive field study, the state-of-the-art of interoperability among six Semantic Web tools. From this field study we have compiled a comprehensive set of practices that may serve as recommendations for Semantic Web tool developers and ontology engineers.",
        "keywords": "RDF(S); interoperability; benchmarking; benchmark suite",
        "released": 2009,
        "link": "https://doi.org/10.1142/S0218194009004556"
    },
    {
        "title": "Deploying fault tolerant web service compositions",
        "abstract": "Many businesses are now moving towards the use of composite web services. These consist of a collection of web services working together to achieve an objective. Although they are becoming business-critical elements, current development tools do not provide a practical way to include fault tolerance characteristics in web services compositions. This paper proposes a mechanism that allows programmers to easily develop fault tolerant compositions using diverse services. The mechanism allows programmers to specify alternative web services for each operation and offers a set of artifacts that simplify the coding process, by automatically dealing with all aspects related to the redundant web services invocation and responses voting. The mechanism is also able to perform a continuous evaluation of the services based on their behavior during operation. The approach is illustrated using compositions based on services publicly available in the Internet anti on the services specified by the standard TPC-App performance benchmark.",
        "keywords": "web services; composition; fault tolerance; diversity",
        "released": 2008,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000264492300004"
    },
    {
        "title": "Simple software simulator for teaching embedded programming",
        "abstract": "This article presents simple software simulator of a microcontroller evaluation board FRDM-KL25Z. The simulator was developed to make it possible to teach our embedded systems course online during the COVID-19 pandemic. It is in principle a software library that handles function calls and register access from student’s program and displays the outputs in a console window of a stan-dard desktop application. It does not require any special hardware or software tools except an IDE capable of building C++ applications for the desktop com-puter. It can be easily modified for different microcontrollers and thus can be useful if existing lessons need to be switched from in-person to distance learning at a short notice.",
        "keywords": "-microcontroller; simulator; Kinetis; embedded programming",
        "released": 2022,
        "link": "https://doi.org/10.3991/ijep.v12i6.28193"
    },
    {
        "title": "Internet of things-enabled crop growth monitoring system for smart agriculture",
        "abstract": "In order to solve the problems of high cost and difficult management of traditional agricultural planting, internet of things (IoT) technology was applied to realize real-time detection and intelligent management of crop growth and remote control of equipment, and change the traditional agricultural planting mode. The research results show that in MyEclipse development environment, using B/S (Browser/Server) architecture, Java and JavaScript language to design, Tomcat built server to publish information and complete the function of data storage and query, users can access the monitoring center in the local area network (LAN). When the detected data exceed the set threshold range, the control instructions issued by the monitoring center are transmitted to the main control chip through ethernet, and then the switching operation of the relay is controlled. The real-time monitoring of crop growth environment can be realized.",
        "keywords": "Crop Growth Monitoring; Internet of Things; Smart Agriculture; Wireless Sensor Network",
        "released": 2021,
        "link": "https://doi.org/10.4018/IJAEIS.20210401.oa3"
    },
    {
        "title": "Argovis: A web application for fast delivery, visualization, and analysis of argo data",
        "abstract": "Since the mid-2000s, the Argo oceanographic observational network has provided near-real-time four-dimensional data for the global ocean for the first time in history. Internet (i.e., the “web”) applications that handle the more than two million Argo profiles of ocean temperature, salinity, and pressure are an active area of development. This paper introduces a new and efficient interactive Argo data visualization and delivery web application named Argovis that is built on a classic three-tier design consisting of a front end, back end, and database. Together these components allow users to navigate 4D data on a world map of Argo floats, with the option to select a custom region, depth range, and time period. Argovis’s back end sends data to users in a simple format, and the front end quickly renders web-quality figures. More advanced applications query Argovis from other programming environments, such as Python, R, and MATLAB. Our Argovis architecture allows expert data users to build their own functionality for specific applications, such as the creation of spatially gridded data for a given time and advanced time-frequency analysis for a space-time selection. Argovis is aimed to both scientists and the public, with tutorials and examples available on the website, describing how to use the Argovis data delivery system-for example, how to plot profiles in a region over time or to monitor profile metadata.",
        "keywords": "Ocean; Algorithms; Data processing; Databases; Profilers; oceanic",
        "released": 2020,
        "link": "https://doi.org/10.1175/JTECH-D-19-0041.1"
    },
    {
        "title": "Confidence polytopes for quantum process tomography",
        "abstract": "In the present work, we propose a generalization of the confidence polytopes approach for quantum state tomography (QST) to the case of quantum process tomography (QPT). Our approach allows obtaining a confidence region in the polytope form for a Choi matrix of an unknown quantum channel based on the measurement results of the corresponding QPT experiment. The method uses the improved version of the expression for confidence levels for the case of several positive operator-valued measures. We then demonstrate how confidence polytopes can be employed for calculating confidence intervals for affine functions of quantum states (Choi matrices), such as fidelities and observables mean values, which are used both in QST and QPT settings. As we propose, this problem can be efficiently solved using linear programming tools. We also study the performance and scalability of the developed approach on the basis of simulation and experimental data collected using IBM cloud quantum processor.",
        "keywords": "quantum state tomography; quantum process tomography; confidence regions; confidence intervals",
        "released": 2021,
        "link": "https://doi.org/10.1088/1367-2630/ac3cf7"
    },
    {
        "title": "Reusing petri nets through the semantic web",
        "abstract": "The paper presents the Petri net ontology that should enable sharing Petri nets on the Semantic Web. Previous work on formal methods for representing Petri nets mainly defines tool-specific Petri. net descriptions (i.e. metamodels) or formats for Petri net model interchange (i.e. syntax). However, such efforts do not provide a suitable model description for using Petri nets on the Semantic Web. This paper uses the Petri net UML model as a starting point for implementing the Petri net ontology. The UML model is then refined using the Protege ontology development tool and the Ontology UML profile. Resulting Petri net models are represented on the Semantic Web is using XML-based ontology representation languages, Resource Description Framework (RDF) and Web Ontology Language (OWL). We implemented a Petri net software tool as well as tools for the Petri net Semantic Web infrastructure.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000221526400020"
    },
    {
        "title": "Middle-up analysis of monoclonal antibodies after combined IgdE and IdeS hinge proteolysis: Investigation of free sulfhydryls",
        "abstract": "Despite significant analytical improvements during this last decade, characterizing the whole integrity of monoclonal antibodies during their bioproduction remains a challenge. In this study, we report a new analytical approach to evaluate the overall heterogeneity/integrity of mAbs by LC-MS after combined proteolysis at their lower- and upper-hinge sites using the immunoglobulin-degrading enzymes IdeS and IgdE respectively. The whole sample preparation did not use any harsh conditions such as low pH, high temperature or reductive conditions and enables the splitting of mAbs structure into three fragments, namely the hinge dimer, Fab and Fc/2. Using the NIST mAb reference material, this method was demonstrated to be particularly suited for the analysis of mAbs disulfide bridges. The three fragments as well as their corresponding free sulfhydryl forms were well separated by chromatography and identified online by mass spectrometry. The method was then successfully applied to several mAbs of variable hydrophobicities. (C) 2017 LFB BIOTECHNOLOGIES. Published by Elsevier B.V.",
        "keywords": "Disulfide bonds; Free sulfhydryls; IdeS; IgdE; Mass spectrometry; Monoclonal antibody",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.jpba.2017.11.046"
    },
    {
        "title": "Interoperability results for semantic web technologies using OWL as the interchange language",
        "abstract": "Using Semantic Web technologies in complex scenarios requires that such technologies correctly interoperate by interchanging ontologies using the RDF(S) and OWL languages. This interoperability is not straightforward because of the high heterogeneity in Semantic Web technologies and, while the number of such technologies grows, affordable mechanisms for evaluating Semantic Web technology interoperability are needed to comprehend the current and future interoperability of Semantic Web technologies. This paper presents the OWL Interoperability Benchmarking, an international benchmarking activity that involved the evaluation of the interoperability of different Semantic Web technologies using OWL as the interchange language. It describes the evaluation resources used in this benchmarking activity, the OWL Lite Import Benchmark Suite and the IBSE tool, and presents how to use them for evaluating the OWL interoperability of Semantic Web technologies. Moreover, the paper offers an overview of the OWL interoperability results of the eight tools participating in the benchmarking: one ontology-based annotation tool (GATE), three ontology frameworks (Jena, KAON2, and SWI-Prolog), and four ontology development tools (Protege Frames, Protege OWL, SemTalk, and WebODE). (C) 2010 Elsevier B. V. All rights reserved.",
        "keywords": "Semantic Web technologies; Interoperability; OWL; Benchmarking",
        "released": 2010,
        "link": "https://doi.org/10.1016/j.websem.2010.08.008"
    },
    {
        "title": "Vessel to shore data movement through the internet of floating things: A microservice platform at the edge",
        "abstract": "The rise of the Internet of Things has generated high expectations about the improvement in people’s lifestyles. In the last decade, we saw several examples of instrumented cities where different types of data were gathered, processed, and made available to inspire the next generation of scientists and engineers. In this framework, sensors and actuators became leading actors of technologically pervasive urban environments. However, in coastal areas, marine data crowdsourcing is difficult to apply due to the challenging operational conditions, extremely unstable network connectivity, and security issues in data movement. To fill this gap, we present a novel version of our DYNAMO transfer protocol (DTP), a platform-independent data mover framework where data collected on board of vessels are stored locally and then moved from the edge to the cloud when the operating conditions are favorable. We evaluate the performance of DTP in a controlled environment with a private cloud by measuring the time it takes for the clouds ide to process and store a fixed amount of data while varying the number of microservice instances. We show that the time decreases exponentially when the number of microservice instances goes from 1 to 16 and it remains constant above that number.",
        "keywords": "cloud database; data crowdsourcing; Internet of Floating Things; microservices; ocean data; security; transfer protocol",
        "released": 2021,
        "link": "https://doi.org/10.1002/cpe.5988"
    },
    {
        "title": "ProteoSign: An end-user online differential proteomics statistical analysis platform",
        "abstract": "Profiling of proteome dynamics is crucial for understanding cellular behavior in response to intrinsic and extrinsic stimuli and maintenance of homeostasis. Over the last 20 years, mass spectrometry (MS) has emerged as the most powerful tool for large-scale identification and characterization of proteins. Bottom-up proteomics, the most common MS-based proteomics approach, has always been challenging in terms of data management, processing, analysis and visualization, with modern instruments capable of producing several gigabytes of data out of a single experiment. Here, we present ProteoSign, a freely available web application, dedicated in allowing users to perform proteomics differential expression/abundance analysis in a userfriendly and self-explanatory way. Although several non-commercial standalone tools have been developed for post-quantification statistical analysis of proteomics data, most of them are not end-user appealing as they often require very stringent installation of programming environments, third-party software packages and sometimes further scripting or computer programming. To avoid this bottleneck, we have developed a user-friendly software platform accessible via a web interface in order to enable proteomics laboratories and core facilities to statistically analyse quantitative proteomics data sets in a resource-efficient manner. ProteoSign is available at http://bioinformatics.med.uoc.gr/ProteoSign and the source code at https://github.com/yorgodillo/ProteoSign.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1093/nar/gkx444"
    },
    {
        "title": "A web application system for structural modal identification",
        "abstract": "The increasing development of algorithms for the assessment of structural modal characteristics stems from continuous improvement in vibration tests and computational processing capabilities. However, the identification process of deformable dynamic systems occasionally demands high computational costs. Nevertheless, with the increasing expansion of high-speed broadband internet, the concept of web application becomes a prevailing alternative. This paper presents the free web application Midas (Modal Identification of Deformable Systems), which is implemented in Java and applied to structural modal identification. This procedure is performed via the internet using data gathered from dynamic measurements. Available technologies and web development tools such as JavaServer Pages and JavaServer Faces are used to implement Midas. For the dynamic treatment of the vibration data, two identification techniques are used - the random decrement method and the Ibrahim time domain method. In order to evaluate the developed web application, two experimental applications are explored.",
        "keywords": "bridges; dynamics; field testing & monitoring",
        "released": 2014,
        "link": "https://doi.org/10.1680/stbu.12.00020"
    },
    {
        "title": "SPHINX: A framework for creating personal, site-specific web crawlers",
        "abstract": "Crawlers, also called robots and spiders, are programs that browse the World Wide Web autonomously. This paper describes SPHINX, a Java toolkit and interactive development environment for Web crawlers. Unlike other crawler development systems, SPHINX is geared towards developing crawlers that are Web-site-specific, personally customized, and relocatable. SPHINX allows site-specific crawling rules to be encapsulated and reused in content analyzers, known as classifiers. Personal crawling tasks can be performed (often without programming) in the Crawler Workbench, an interactive environment for crawler development and testing. For efficiency, relocatable crawlers developed using SPHINX can be uploaded and executed on a remote Web server. (C) 1998 Published by Elsevier Science B.V. All rights reserved.",
        "keywords": "crawlers; robots; spiders; Web automation; Web searching; Java; end-user programming; mobile code",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0169-7552(98)00064-6"
    },
    {
        "title": "Exploration of the application and propagation characteristics of web design",
        "abstract": "Web advertising can be said to be today’s digital media under the rapid development of new products. Through the software on the digital visual layout planning, web designers have reached the purpose of network marketing. Graphic design is the basis for the development of web advertising, but compared to the former, the latter has a more clear purpose and benefits, the characteristics of the network media is also very prominent. In this paper, based on this network development environment, and based on the development of web advertising, content, features, its visual elements constitute and application patterns, propagation characteristics, and the actual case were analyzed, so the future development of the industry was expected.",
        "keywords": "Web advertising; visual design; advertising communication; propagation characteristics",
        "released": 2017,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000405993100359"
    },
    {
        "title": "Process simulation platform for virtual manufacturing systems evaluation",
        "abstract": "The paper presents the design of a cloud simulation platform and the associated services that will provide the computing resources and services for hybrid simulation of Virtual Manufacturing Systems. At the core of the research is the design of a framework defined as Virtual Development Environment (VDE). Associated to VDE is a set of block functions and algorithms defined as software assets, stored in a large repository implemented within an on-line cloud supported library. The platform offers flexibility concerning the continuous testing and updates regarding the algorithms in order to improve the existing ones. (C) 2018 Elsevier B.V. All rights reserved.",
        "keywords": "Virtual manufacturing systems; Hybrid process simulation; Context-aware systems; Cloud computing; Hardware-in-loop; Sensor-cloud infrastructure; Software reusability",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.compind.2018.09.008"
    },
    {
        "title": "HBVdb: A knowledge database for hepatitis b virus",
        "abstract": "We have developed a specialized database, HBVdb (http://hbvdb.ibcp.fr), allowing the researchers to investigate the genetic variability of Hepatitis B Virus (HBV) and viral resistance to treatment. HBV is a major health problem worldwide with more than 350 million individuals being chronically infected. HBV is an enveloped DNA virus that replicates by reverse transcription of an RNA intermediate. HBV genome is optimized, being circular and encoding four overlapping reading frames. Indeed, each nucleotide of the genome takes part in the coding of at least one protein. However, HBV shows some genome variability leading to at least eight different genotypes and recombinant forms. The main drugs used to treat infected patients are nucleos(t)ides analogs (reverse transcriptase inhibitors). Unfortunately, HBV mutants resistant to these drugs may be selected and be responsible for treatment failure. HBVdb contains a collection of computer-annotated sequences based on manually annotated reference genomes. The database can be accessed through a web interface that allows static and dynamic queries and offers integrated generic sequence analysis tools and specialized analysis tools (e.g. annotation, genotyping, drug resistance profiling).",
        "keywords": "",
        "released": 2013,
        "link": "https://doi.org/10.1093/nar/gks1022"
    },
    {
        "title": "An efficient multi-task PaaS cloud infrastructure based on docker and AWS ECS for application deployment",
        "abstract": "The setup environment and deployment of distributed applications is a human intensive and highly complex process that poses significant challenges. Nowadays many applications are developed in the cloud and existing applications are migrated to the cloud because of the promising advantages of cloud computing. Presenting two common serious challenging scenarios in the application development environment, we propose a multi-task PaaS cloud infrastructure using Docker and AWS services for application isolation, optimization and rapid deployment of distributed applications. We fully utilized Docker, a lightweight containerization technology that uses a host of the Linux kernel’s features such as namespaces and cgroup’s to sandbox processes into configurable virtual environments. The Amazon EC2 container service helps our container management framework. The cluster management framework uses optimistic, shared state scheduling to execute processes on EC2 instances using Docker containers. Several experimentations were carried out, one of the experimentation focused on a simulation of application deployment scheduling that shows our propose infrastructure is flexible, efficient and well optimized.",
        "keywords": "App deployment; Cloud computing; Virtualization; Hypervisors; Virtual machine (VM); Container; Clustering; Docker",
        "released": 2016,
        "link": "https://doi.org/10.1007/s10586-016-0599-0"
    },
    {
        "title": "The SPECCHIO spectral information system",
        "abstract": "Spectral Information Systems provide a framework to assemble, curate, and serve spectral data and their associated metadata. This article documents the evolution of the SPECCHIO system, devised to enable long-term usability and data-sharing of field spectroradiometer data. The new capabilities include a modern, web-based client-server architecture, a flexible metadata storage scheme for generic metadata handling, and a rich application programming interface, enabling scientists to directly access spectral data and metadata from their programming environment of choice. The SPECCHIO system source code has been moved into the open source domain to stimulate contributions from the spectroscopy community while binary distributions, including the SPECCHIO virtual machine, simplify the installation and use of the system for the end-users.",
        "keywords": "Iron; Servers; Frequency modulation; Licenses; Planning; Data systems; Java; Information systems; metadata; relational databases; spectroradiometers; spectroscopy",
        "released": 2020,
        "link": "https://doi.org/10.1109/JSTARS.2020.3025117"
    },
    {
        "title": "VIRTUAL LEARNING OBJECT BASED ON CUISENAIRE STRIPS TO DEVELOP LOGICAL-MATHEMATICAL THINKING",
        "abstract": "Cuisenaire’s Rullets is a didactic material widely used to make sense of mathematics and through manipulation to move from the abstract to the concrete and vice versa, but education in virtual environments makes it difficult to use didactic materials in a traditional way. With the objective of creating a virtual learning object with the incorporation of digital metaphors based on Cuisenaire’s rulers, three free software applications were analyzed. Using software development tools, a web page was generated through the combination of vector graphics and JavaScript programming, achieving a usable web application that fulfills the pedagogical purpose for students from first to fourth grade of elementary school.",
        "keywords": "Cuisenaire Rulers; virtual learning objects; mathematics learning; meaningful learning; web programming",
        "released": 2021,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000704915900011"
    },
    {
        "title": "Experience report: Evolution of a web-integrated software development and verification environment",
        "abstract": "This paper summarizes our experiences over the last 4years in creating a web-integrated software development and verification environment. The environment has been used for both research experimentation and education. It has been used in undergraduate computer science courses to teach modular software development and analytical reasoning principles at multiple institutions. In the process, the environment has undergone many refinements to meet demands for improved functionality and to leverage rapidly changing underlying technology for the improvements. The environment is tailored to present formal specifications and alternative implementations of components, and enable correctness checking through a server-side verifying compiler. This paper presents a detailed account of the development and evolution of the environmentits functionality, user interface, and underlying technologythat we hope will serve as a model for others, especially as the benefits of online learning systems are becoming increasingly obvious. Copyright (c) 2014 John Wiley & Sons, Ltd.",
        "keywords": "Education; formal methods; IDE; reusable components; software engineering; verification; web application",
        "released": 2015,
        "link": "https://doi.org/10.1002/spe.2259"
    },
    {
        "title": "A systematic review of the evaluation in k-12 artificial intelligence education from 2013 to 2022",
        "abstract": "This study reviewed evaluations of K-12 AI education from 2013 to 2022. The analysis encompassed 36 articles and focused on examining research methods, sample sizes, evaluation methods and types, learning outcomes, evaluation contexts, and primary findings. The results showed that most evaluations took a summative approach and relied on self-report surveys to evaluate cognitive learning outcomes related to machine learning (ML) concepts among middle and high school students in informal settings. The tools commonly employed for evaluation were AI-based programming tools and online platforms. These findings offer valuable insights for AI educators and researchers in K-12 education, highlighting the significance of introducing students to the assessment of AI in formal learning settings at an early age, utilizing thorough qualitative and mixed-methods techniques, and embracing diverse assessment approaches to evaluate cognitive learning outcomes beyond machine learning.",
        "keywords": "K-12 AI education; evaluation; evaluation method; machine learning; learning outcomes",
        "released": 2024,
        "link": "https://doi.org/10.1080/10494820.2024.2335499"
    },
    {
        "title": "A GA-based query optimization method for web information retrieval",
        "abstract": "By a different use of relevance feedback (the order in which the relevant documents are retrieved, the terms of the relevant documents, and the terms of the irrelevant documents) in the design of fitness function, and by introducing three different genetic operators, we have developed a new genetic algorithm-based query optimization method on relevance feedback for Web information retrieval. Based on three benchmark test collections Cranfield, Medline and CACM, experiments have been carried out to compare our method with three well-known query optimization methods on relevance feedback: the traditional Ide Dec-hi method, the Horng and Yeh’s GA-based method and the Lopez-Pujalte et al.’s GA-based method. The experiments show that our method can achieve better results. (c) 2006 Elsevier Inc. All rights reserved.",
        "keywords": "genetic algorithm; relevance feedback; information retrieval; query optimization; fitness function",
        "released": 2007,
        "link": "https://doi.org/10.1016/j.amc.2006.07.044"
    },
    {
        "title": "A hands-on guide to conducting psychological research on twitter",
        "abstract": "The rising popularity of social media has created valuable opportunities for researchers to quickly and cheaply tap large amounts of data on naturalistic social interactions. One of the most useful and accessible sources of social media data is Twitter. Although Twitter data are free to access, however, the programming tools required to collect and analyze these data are likely to create a barrier for many psychologists. The goal of this article is to reduce that barrier by explaining what data are available from Twitter and providing code and step-by-step instructions to retrieve them. I also review approaches to deriving psychological insight from these data, the accompanying challenges, and potential solutions, providing code to make these tasks easier. Particularly for researchers without access to large participant pools, overseas collaborators, or online panels, Twitter can be an important source of psychological insight.",
        "keywords": "Twitter; methods; language analysis; social networks",
        "released": 2017,
        "link": "https://doi.org/10.1177/1948550617697178"
    },
    {
        "title": "Adaptive retrofitting for industrial machines: Utilizing webassembly and peer-to-peer connectivity on the edge",
        "abstract": "Leveraging previously untapped data sources offers significant potential for value creation in the manufacturing sector. However, asset-heavy shop floors, extended machine replacement cycles, and equipment diversity necessitate considerable investments for achieving smart manufacturing, which can be particularly challenging for small businesses. Retrofitting presents a viable solution, enabling the integration of low-cost sensors and microcontrollers with older machines to collect and transmit data. In this paper, we introduce a concept and a prototype for retrofitting industrial environments using lightweight web technologies at the edge. Our approach employs WebAssembly as a novel bytecode standard, facilitating a consistent development environment from the cloud to the edge by operating on both browsers and bare-metal hardware. By attaining near-native performance and modularity reminiscent of container-based service architectures, we demonstrate the feasibility of our approach. Our prototype was evaluated with an actual industrial robot within a showcase factory, including measurements of data exchange with a cutting-edge data lake system. We further extended the prototype to incorporate a peer-to-peer network that facilitates message routing and WebAssembly software updates. Our technology establishes a foundational framework for the transition towards Industry 4.0. By integrating considerations of sustainability and human factors, it further extends this groundwork to facilitate progression into Industry 5.0.",
        "keywords": "Industry 4.0; Retrofitting; Edge computing; WebAssembly; Peer-to-Peer",
        "released": 2024,
        "link": "https://doi.org/10.1007/s11280-024-01237-8"
    },
    {
        "title": "Identifying classes in legacy JavaScript code",
        "abstract": "JavaScript is the most popular programming language for the Web. Although the language is prototype-based, developers can emulate class-based abstractions in JavaScript to master the increasing complexity of their applications. Identifying classes in legacy JavaScript code can support these developers at least in the following activities: (1) program comprehension; (2) migration to the new JavaScript syntax that supports classes; and (3) implementation of supporting tools, including IDEs with class-based views and reverse engineering tools. In this paper, we propose a strategy to detect class-based abstractions in the source code of legacy JavaScript systems. We report on a large and in-depth study to understand how class emulation is employed, using a dataset of 918 JavaScript applications available on GitHub. We found that almost 70% of the JavaScript systems we study make some usage of classes. We also performed a field study with the main developers of 60 popular JavaScript systems to validate our findings. The overall results range from 97% to 100% for precision, from 70% to 89% for recall, and from 82% to 94% for F-score.",
        "keywords": "JavaScript; Program comprehension; Reverse engineering",
        "released": 2017,
        "link": "https://doi.org/10.1002/smr.1864"
    },
    {
        "title": "Movie forecast guru: A web-based DSS for hollywood managers",
        "abstract": "Herein we describe a Web-based DSS to help Hollywood managers make better decisions on important movie characteristics, such as, genre, super stars, technical effects, release time, etc. These parameters are used to build prediction models to classify a movie in one of nine success categories, from a “flop” to a “blockbuster”. The system employs a number of traditional and non-traditional prediction models as distributed independent experts, implemented as Web services. The paper describes the purpose and the architecture of the system, the development environment, the user assessment results, and the lessons learned as they relate to Web-based DSS development. (C) 2005 Elsevier B.V. All rights reserved.",
        "keywords": "decision support systems; web-based; forecasting; box-office receipts; information fusion; sensitivity analysis; usability assessment",
        "released": 2007,
        "link": "https://doi.org/10.1016/j.dss.2005.07.005"
    },
    {
        "title": "A RESTful API for accessing microbial community data for MG-RAST",
        "abstract": "Metagenomic sequencing has produced significant amounts of data in recent years. For example, as of summer 2013, MG-RAST has been used to annotate over 110,000 data sets totaling over 43 Terabases. With metagenomic sequencing finding even wider adoption in the scientific community, the existing web-based analysis tools and infrastructure in MG-RAST provide limited capability for data retrieval and analysis, such as comparative analysis between multiple data sets. Moreover, although the system provides many analysis tools, it is not comprehensive. By opening MG-RAST up via a web services API (application programmers interface) we have greatly expanded access to MG-RAST data, as well as provided a mechanism for the use of third-party analysis tools with MG-RAST data. This RESTful API makes all data and data objects created by the MG-RAST pipeline accessible as JSON objects. As part of the DOE Systems Biology Knowledgebase project (KBase, http://kbase.us) we have implemented a web services API for MG-RAST. This API complements the existing MG-RAST web interface and constitutes the basis of KBase’s microbial community capabilities. In addition, the API exposes a comprehensive collection of data to programmers. This API, which uses a RESTful (Representational State Transfer) implementation, is compatible with most programming environments and should be easy to use for end users and third parties. It provides comprehensive access to sequence data, quality control results, annotations, and many other data types. Where feasible, we have used standards to expose data and metadata. Code examples are provided in a number of languages both to show the versatility of the API and to provide a starting point for users. We present an API that exposes the data in MG-RAST for consumption by our users, greatly enhancing the utility of the MG-RAST service.",
        "keywords": "",
        "released": 2015,
        "link": "https://doi.org/10.1371/journal.pcbi.1004008"
    },
    {
        "title": "Collect earth: Land use and land cover assessment through augmented visual interpretation",
        "abstract": "Collect Earth is a free and open source software for land monitoring developed by the Food and Agriculture Organization of the United Nations (FAO). Built on Google desktop and cloud computing technologies, Collect Earth facilitates access to multiple freely available archives of satellite imagery, including archives with very high spatial resolution imagery (Google Earth, Bing Maps) and those with very high temporal resolution imagery (e.g., Google Earth Engine, Google Earth Engine Code Editor). Collectively, these archives offer free access to an unparalleled amount of information on current and past land dynamics for any location in the world. Collect Earth draws upon these archives and the synergies of imagery of multiple resolutions to enable an innovative method for land monitoring that we present here: augmented visual interpretation. In this study, we provide a full overview of Collect Earth’s structure and functionality, and we present the methodology used to undertake land monitoring through augmented visual interpretation. To illustrate the application of the tool and its customization potential, an example of land monitoring in Papua New Guinea (PNG) is presented. The PNG example demonstrates that Collect Earth is a comprehensive and user-friendly tool for land monitoring and that it has the potential to be used to assess land use, land use change, natural disasters, sustainable management of scarce resources and ecosystem functioning. By enabling non-remote sensing experts to assess more than 100 sites per day, we believe that Collect Earth can be used to rapidly and sustainably build capacity for land monitoring and to substantively improve our collective understanding of the world’s land use and land cover.",
        "keywords": "land monitoring; augmented visual interpretation; assessment; land use; land use change; very high resolution imagery; open source; Google Earth; Collect Earth",
        "released": 2016,
        "link": "https://doi.org/10.3390/rs8100807"
    },
    {
        "title": "EQuaTE: Efficient quantum train engine for runtime dynamic analysis and visual feedback in autonomous driving",
        "abstract": "This article proposes an efficient quantum train engine (EQuaTE), a novel development tool for quantum neural network (QNN) autonomous driving software, which plots gradient variances to confirm whether the QNN falls into local minima situations (called barren plateaus). Based on this runtime visualization, the stability and feasibility of QNN-based software can be tested during runtime operations of autonomous driving functionalities. This software testing of a QNN via dynamic analysis is essentially required due to undetermined probabilistic qubit states during runtime operations. Furthermore, an EQuaTE is capable of visual feedback because the barren plateaus can be identified at local autonomous driving platforms, and the corresponding information will be visualized at remotely connected cloud. Based on this visualized information at the cloud, the QNN, which is also stored at cloud, should be automatically reorganized and retrained for eliminating barren plateaus. Then, the trained parameters can be downloaded into the QNN of autonomous driving platforms.",
        "keywords": "Autonomous vehicles; Visualization; Qubit; Training; Vehicle dynamics; Task analysis; Railway engineering; Runtime",
        "released": 2023,
        "link": "https://doi.org/10.1109/MIC.2023.3307395"
    },
    {
        "title": "Training business students to use online networking for self-development",
        "abstract": "This paper contributes to understanding opportunities to use social media for developing online networking skills that can be applied in entrepreneurial self-development. Analysis of business students’ changing priorities in finding and using online social networks during the years 2008-2019 demonstrated the dominance of Facebook and, more recently, Instagram, compared to LinkedIn and more specialized networks for entrepreneurs. Students involved in knowledge sharing for travelling, sports, music and online gaming have a more detailed understanding of online social media as entrepreneurial self-development tools. Recent years demonstrated some desire to integrate online communication and joint actions in physical locations. Students need a deeper understanding of how their online communication and networking priorities can be aligned with their entrepreneurial network development and knowledge sharing priorities through social media. Our longitudinal research of student priorities in online social network use demonstrates to educators the importance of understanding the self-development paths of students when improving their skills to use online networking. Higher education should guide students to use online media to trace new entrepreneurship opportunities, expand their contact network and prepare them to reflect critically on online information dissemination practices.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1111/ijtd.12269"
    },
    {
        "title": "The footprint database and web services of the herschel space observatory",
        "abstract": "Data from the Herschel Space Observatory is freely available to the public but no uniformly processed catalogue of the observations has been published so far. To date, the Herschel Science Archive does not contain the exact sky coverage (footprint) of individual observations and supports search for measurements based on bounding circles only. Drawing on previous experience in implementing footprint databases, we built the Herschel Footprint Database and Web Services for the Herschel Space Observatory to provide efficient search capabilities for typical astronomical queries. The database was designed with the following main goals in mind: (a) provide a unified data model for meta-data of all instruments and observational modes, (b) quickly find observations covering a selected object and its neighbourhood, (c) quickly find every observation in a larger area of the sky, (d) allow for finding solar system objects crossing observation fields. As a first step, we developed a unified data model of observations of all three Herschel instruments for all pointing and instrument modes. Then, using telescope pointing information and observational meta-data, we compiled a database of footprints. As opposed to methods using pixellation of the sphere, we represent sky coverage in an exact geometric form allowing for precise area calculations. For easier handling of Herschel observation footprints with rather complex shapes, two algorithms were implemented to reduce the outline. Furthermore, a new visualisation tool to plot footprints with various spherical projections was developed. Indexing of the footprints using Hierarchical Triangular Mesh makes it possible to quickly find observations based on sky coverage, time and meta-data. The database is accessible via a web site http://herschel.vo.elte.hu and also as a set of REST web service functions, which makes it readily usable from programming environments such as Python or IDL. The web service allows downloading footprint data in various formats including Virtual Observatory standards.",
        "keywords": "Astronomical data bases: miscellaneous; Instrumentation: detectors; Techniques: miscellaneous; Telescopes; Space vehicles: instruments; Virtual observatory tools",
        "released": 2016,
        "link": "https://doi.org/10.1007/s10686-016-9502-5"
    },
    {
        "title": "Adverse events following cervical disc arthroplasty: A systematic review",
        "abstract": "Study Design: Systematic review. Objectives: Cervical arthroplasty is an increasingly popular treatment of cervical radiculopathy and myelopathy. An understanding of the potential adverse events (AEs) is important to help both clinicians and patients. We sought to provide a comprehensive systematic review of the AEs reported in all randomized controlled trials (RCTs) of cervical disc arthroplasty in an attempt to characterize the quality of reporting. Methods: We conducted a systematic review of MEDLINE and Web of Science for RCTs of cervical disc arthroplasty reporting AEs. We reported the most frequently mentioned AEs, including dysphagia/dysphonia, vascular compromise, dural injury, and infections. We recorded the presence of industry funding and scored the quality of collection methods and reporting of AEs. Results: Of the 3734 identified articles, 29 articles met full inclusion criteria. The quality of AE reporting varied significantly between studies, and a combined meta-analysis was not feasible. The 29 articles covered separate 19 RCTs. Eight studies were US Food and Drug Administration (FDA) investigational device exemption (IDE) trials. Rates were recorded for the following AEs: dysphagia/dysphonia (range = 1.3% to 27.2%), vascular compromise (range = 1.1% to 2.4%), cervical wound infection (range = 1.2% to 22.5%), and cerebrospinal fluid leak (range = 0.8% to 7.1%). Conclusions: There is a lack of consistency in reporting of AEs among RCTs of cervical arthroplasty. FDA IDE trials scored better in AE event reporting compared to other studies. Standardized definitions for AEs and standardized data collection methodology are needed to improve future studies.",
        "keywords": "cervical disc arthroplasty; systematic review; adverse events; complications; industry funding",
        "released": 2018,
        "link": "https://doi.org/10.1177/2192568217720681"
    },
    {
        "title": "An innovative cloud-fog-based smart grid scheme for efficient resource utilization",
        "abstract": "Smart grids (SGs) enhance the effectiveness, reliability, resilience, and energy-efficient operation of electrical networks. Nonetheless, SGs suffer from big data transactions which limit their capabilities and can cause delays in the optimal operation and management tasks. Therefore, it is clear that a fast and reliable architecture is needed to make big data management in SGs more efficient. This paper assesses the optimal operation of the SGs using cloud computing (CC), fog computing, and resource allocation to enhance the management problem. Technically, big data management makes SG more efficient if cloud and fog computing (CFC) are integrated. The integration of fog computing (FC) with CC minimizes cloud burden and maximizes resource allocation. There are three key features for the proposed fog layer: awareness of position, short latency, and mobility. Moreover, a CFC-driven framework is proposed to manage data among different agents. In order to make the system more efficient, FC allocates virtual machines (VMs) according to load-balancing techniques. In addition, the present study proposes a hybrid gray wolf differential evolution optimization algorithm (HGWDE) that brings gray wolf optimization (GWO) and improved differential evolution (IDE) together. Simulation results conducted in MATLAB verify the efficiency of the suggested algorithm according to the high data transaction and computational time. According to the results, the response time of HGWDE is 54 ms, 82.1 ms, and 81.6 ms faster than particle swarm optimization (PSO), differential evolution (DE), and GWO. HGWDE’s processing time is 53 ms, 81.2 ms, and 80.6 ms faster than PSO, DE, and GWO. Although GWO is a bit more efficient than HGWDE, the difference is not very significant.",
        "keywords": "cloud computing; fog computing; improved differential evolution; gray wolf optimization; efficient resource utilization; smart grid",
        "released": 2023,
        "link": "https://doi.org/10.3390/s23041752"
    },
    {
        "title": "Telbivudine protects against renal impairment in patients with chronic hepatitis b infection",
        "abstract": "It has been reported that treatment with telbivudine might be associated with a significant improvement of renal function in patients with chronic hepatitis B. To confirm this finding, a systematic literature review was conducted. Four online databases (PubMed, CNKI/FMJS, Ovid and SpringerLink) were searched. Serum creatinine, glomerular filtration rate and estimated glomerular filtration rate changes from baseline to the end of the treatment were analyzed. A total of seven full-text articles and 13 abstracts with sample sizes ranging from 15 to 689 patients fulfilled the selection criteria for review. The results showed that long-term treatment with telbivudine, either as monotherapy or combined with other nucleos(t)ide analogs, can significantly improve renal function in chronic hepatitis B patients, particularly those at high risk of renal impairment.",
        "keywords": "chronic hepatitis B; glomerular filtration rate; nucleos(t)ide analogs; renal function; serum creatinine; telbivudine",
        "released": 2016,
        "link": "https://doi.org/10.2217/fvl-2015-0015"
    },
    {
        "title": "Orange4WS environment for service-oriented data mining",
        "abstract": "Novel data-mining tasks in e-science involve mining of distributed, highly heterogeneous data and knowledge sources. However, standard data mining platforms, such as Weka and Orange, involve only their own data mining algorithms in the process of knowledge discovery from local data sources. In contrast, next generation data mining technologies should enable processing of distributed data sources, the use of data mining algorithms implemented as web services, as well as the use of formal descriptions of data sources and knowledge discovery tools in the form of ontologies, enabling automated composition of complex knowledge discovery workflows for a given data mining task. This paper proposes a novel Service-oriented Knowledge Discovery framework and its implementation in a service-oriented data mining environment Orange4WS (Orange for Web Services), based on the existing Orange data mining toolbox and its visual programming environment, which enables manual composition of data mining workflows. The new service-oriented data mining environment Orange4WS includes the following new features: simple use of web services as remote components that can be included into a data mining workflow; simple incorporation of relational data mining algorithms; a knowledge discovery ontology to describe workflow components (data, knowledge and data mining services) in an abstract and machine-interpretable way, and its use by a planner that enables automated composition of data mining workflows. These new features are showcased in three real-world scenarios.",
        "keywords": "data mining; knowledge discovery; knowledge discovery ontology; e-science workflows; automated planning of data mining workflows",
        "released": 2012,
        "link": "https://doi.org/10.1093/comjnl/bxr077"
    },
    {
        "title": "Design and assessment of a mobile health care solution for the military pediatrician: The DHA pediatrics app",
        "abstract": "Introduction: Mobile health technology design and use by patients and clinicians have rapidly evolved in the past 20 years. Nevertheless, the technology has remained in silos of practices, patients, and individual institutions. Uptake across integrated health systems has lagged. Materials and Methods: In 2015, the authors designed a mobile health application (App) aimed at augmenting the capabilities of clinicians who care for children within the Military Health System (MHS). This App incorporated a curated, system-based collection of Clinical Practice Guidelines, access to emergency resuscitation cards, call buttons for local market subspecialty and inpatient teams, links to residency academic calendars, and other web-based resources. Over the next 5 years, three PlanDo-Study-Act cycles facilitated multiple enhancements for the App which eventually transitioned from the Android/iOS stores to a web browser. The “People At the Centre of Mobile Application Development” tool which has validity evidence captured user experience. The team assessed the App’s global effectiveness using Google Analytics. A speed test measured time saved and accuracy of task completion for clinicians using the App compared to non-users. Finally, MHS medical librarians critiqued the App using a questionnaire with validity evidence. The Walter Reed National Military Medical Center Institutional Review Board reviewed the study and deemed it exempt. Results: Clinician respondents (n= 68 complete responses across six MTFs, 51% graduate medical trainees representing a 7.4% response rate of active duty pediatrician forces) perceived the App to have appropriate qualities of efficiency, effectiveness, learnability, memorability, errors, satisfaction, and cognitive properties following App use in clinical practice. Google Analytics demonstrated more than 1,000 unique users on the App from May 1, 2020 to January 20, 2021. There were 746 instances (26% of all sessions) when a user navigated between more than one military treatment facility. App users were faster and more accurate at task completion during a digital scavenger hunt. Medical librarians measured the App to have acceptable usefulness, accuracy, authority, objectivity, timeliness, functionality, design, security, and value. Conclusions: The App appears to be an effective tool to extend a clinician’s capabilities and inter-professional communication between world-wide users and six MHS markets. This App was designed-and used-for a large health care network across a wide geographic footprint. Next steps are establishing an enduring chain of App champions for continued updates and sharing the App’s code with other military medical disciplines and interested civilian centers.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1093/milmed/usab204"
    },
    {
        "title": "Demographic, procedural and 30-day safety results from the WEB intra-saccular therapy study (WEB-IT)",
        "abstract": "Introduction The Woven EndoBridge (WEB) represents a novel intrasaccular therapeutic option for the treatment of intracranial wide-necked bifurcation aneurysms (WNBAs). The WEB-IT Study is a pivotal Investigational Device Exemption (IDE) study to determine the safety and effectiveness of the WEB device for the treatment of WNBAs located in the anterior and posterior intracranial circulations. We present the patient demographics, procedural characteristics, and 30-day adverse event data for the US WEB-IT study. Methods WEB-IT is a prospective multicenter single-arm interventional study conducted at 25 US and 6 international centers. The study enrolled 150 adults with WNBAs of the anterior and posterior intracranial circulations. All patients were intended to receive a WEB device delivered via standard endovascular neurosurgical embolization techniques. The study was conducted under Good Clinical Practices and included independent adjudication effectiveness outcomes and all adverse events. Results One hundred and fifty patients enrolled at 27 investigational sites underwent attempted treatment with the WEB. Mean age was 59 years (range 29-79) and 110 (73.3%) of the patients were female. Treated aneurysms were located at the basilar apex (n=59, 39.3%), middle cerebral artery bifurcation (n=45, 30%), anterior communicating artery (n=40, 26.7%), and internal carotid artery terminus (n=6, 4%). Average aneurysm size was 6.4 mm (range 3.6-11.4) with a mean neck size of 4.8 mm (range 2.0-8.2, mean dome to neck ratio 1.34). Nine patients presented with ruptured aneurysms. Of the enrolled patients, 98.7% were treated successfully with WEB devices. Mean +/- SD fluoroscopy time was 30.2 +/- 15.7 min. One primary safety event (PSE) (0.7%)-a delayed parenchymal hemorrhage 22 days after treatment-occurred between the index procedure and 30-day follow-up. In addition to the single PSE, there were seven (4.7%) minor ischemic strokes (5 resolved without sequelae and 2 had a modified Rankin Scale score of 1 at 30 days), five (2.7%) transient ischemic attacks, and two (1.3%) minor subarachnoid hemorrhages, which did not meet the prospectively established criteria for PSEs. Conclusions The WEB device can be used to treat WNBAs with a high level of procedural safety and a high degree of technical success.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1136/neurintsurg-2016-012841"
    },
    {
        "title": "Software design for oil industry metrology systems",
        "abstract": "This paper focuses on the integration of metrology principles in Enterprise Resource Planning (ERP) systems. The authors discuss the development methodology and the implementation of a metrology oriented software application for the calibration of oil reservoirs. The application is integrated with an existing Enterprise Resource Planning (ERP) system of a large oil and gas company. The software application was developed in the SAP (System and Applications Products) programming environment ABAP (Advanced Business Application Programming), using the latest Web Dynpro WD technology. The development methodology includes the analysis, implementation and testing of the software application. The application includes models: dispatcher model, body-leasing model, calibration given outside (fixed price). The metrology software application is presently being tested in a large oil and gas company and handles the management of tank oil calibration.",
        "keywords": "Enterprise Resource Planning; Metrology System; Cyber-Physical Systems",
        "released": 2014,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000346943300006"
    },
    {
        "title": "ROBOT-DRAW, an internet-based visualization tool for robotics education",
        "abstract": "Robot manipulators are described geometrically by their Denavit-Hartenberg parameters table. This article describes a package that combines recently developed internet-based programming tools to generate three-dimensional virtual models of robot manipulators from a DH parameter table. Robot-Draw combines hypertext markup language (HTML), practical extraction and report language (PERL), and virtual reality modeling language (VRML), Internet users can generate three-dimensional robot manipulator models on their computer screens, navigate around the robot model and examine it from any angle. The package was designed as a visualization aid in robotics education and allows educators and students to easily visualize robotic structures and directly evaluate the effect of a parameter variation on the overall robot. Robot-Draw can also be a useful tool in the structural design of robot manipulators. Users with Internet connections can use the University of West Florida Robot-Draw package at the UWF Electrical Engineering web server by connecting to http://uwf.edu/ece/robotdraw.htm.",
        "keywords": "DH-parameters; robot kinematics; robot manipulator; three-dimensional model; visualization; virtual reality modeling language (VRML)",
        "released": 2001,
        "link": "https://doi.org/10.1109/13.912707"
    },
    {
        "title": "MODELING a SMART IOT DEVICE FOR MONITORING INDOOR AND OUTDOOR ATMOSPHERIC POLLUTION",
        "abstract": "Air pollution is caused by chemical, physical, or biological components alters the fundamental properties of the atmosphere, and contaminates the interior or outdoor settings. With the rapid industrialization activity in recent years, there is an urgent need to monitor air quality. The proposed research provides a mechanism for monitoring air pollution in indoor and outdoor environments. The system consists of an IoT-based Arduino device. The Arduino IDE connects the Temperature and Humidity sensor, Grove light sensor, and air quality sensor to measure the air pollutants such as Carbon dioxide CO2, Nitrogen oxide (NOx), and Particulate Matter PM2.5. The sensors work efficiently and provide qualitative findings from the environment when they are exposed to CO2, gasoline, solvents, thinner, formaldehyde, and other harmful chemicals. The Wi-Fi module of the Blues Wireless Notehub is used for secure data routing to the IoT cloud. The Air Quality Index (AQI) measures provide information on whether there is something unsafe in indoor and outdoor environments. For collecting and analyzing the device data, the Notecard is intended to be used with a cloud storage service. Also, the indoor fire detector identifies the incident of fire intimates the users through the alarm, and measures the indoor pollutant at that time. The proposed smart IoT product model would be an excellent device for air quality monitoring because of its long-term consistency and low electricity consumption.",
        "keywords": "air pollution; air quality; Arduino; sensors; cloud; humidity; smart IoT; pollutants; temperature.",
        "released": 2024,
        "link": "https://doi.org/10.12694/scpe.v25i1.2359"
    },
    {
        "title": "Debugging unsatisfiable classes in OWL ontologies",
        "abstract": "As an increasingly large number of OWL ontologies become available on the Semantic Web and the descriptions in the ontologies become more complicated, finding the cause of errors becomes an extremely hard task even for experts. Existing ontology development environments provide some limited support, in conjunction with a reasoner, for reporting errors in OWL ontologies. Typically, these are restricted to the mere detection of, for example, unsatisfiable concepts. However, the diagnosis and resolution of the bug is not supported at all. For example, no explanation is given as to why the error occurs (e. g., by pinpointing the root clash, or axioms in the ontology responsible for the clash) or how dependencies between classes cause the error to propagate (i.e., by distinguishing root from derived unsatisfiable classes). In the former case, information from the internals of a description logic tableaux reasoner can be extracted and presented to the user (glass box approach); while in the latter case, the reasoner can be used as an oracle for a certain set of questions and the asserted structure of the ontology can be used to help isolate the source of the problems (black box approach). Based on the two approaches, we have integrated a number of debugging cues generated from our reasoner, Pellet, in our hypertextual ontology development environment, Swoop. A conducted usability evaluation demonstrates that these debugging cues significantly improve the OWL debugging experience, and point the way to more general improvements in the presentation of an ontology to users. (c) 2005 Elsevier B.V. All rights reserved.",
        "keywords": "OWL; ontology debugging; explanation; semantic web",
        "released": 2005,
        "link": "https://doi.org/10.1016/j.websem.2005.09.005"
    },
    {
        "title": "Biomedical sensing analyzer (BSA) for mobile-health (mHealth)-LTE",
        "abstract": "The rapid expansion of mobile-based systems, the capabilities of smartphone devices, as well as the radio access and cellular network technologies are the wind beneath the wing of mobile health (mHealth). In this paper, the concept of biomedical sensing analyzer (BSA) is presented, which is a novel framework, devised for sensor-based mHealth applications. The BSA is capable of formulating the Quality of Service (QoS) measurements in an end-to-end sense, covering the entire communication path (wearable sensors, link-technology, smartphone, cell-towers, mobile-cloud, and the end-users). The characterization and formulation of BSA depend on a number of factors, including the deployment of application-specific biomedical sensors, generic link-technologies, collection, aggregation, and prioritization of mHealth data, cellular network based on the Long-Term Evolution (LTE) access technology, and extensive multidimensional delay analyses. The results are studied and analyzed in a LabView 8.5 programming environment.",
        "keywords": "BSA; delay; LTE; mHealth; sensor",
        "released": 2014,
        "link": "https://doi.org/10.1109/JBHI.2013.2262076"
    },
    {
        "title": "A shelf browsing search system for marginalized user groups",
        "abstract": "As the number of participants and the level of standardization in the global information economy increase, it becomes more feasible to provide search tools that cater to segments of the user community who are outside the main stream. This paper describes a prototype for a shelf browsing search system that can be tailored to meet the needs of such marginalized user groups. Tn this approach, terms in an online thesaurus are linked to relevant classification numbers. A user can search the thesaurus, identify appropriate classification numbers, and use them as starting points in browsing the shelf lists of online catalogues. A variety of thesauri can be used to provide vocabularies appropriate to particular user populations. The classification systems inevitably impose biases and limitations, but these can be partially alleviated by supplying information regarding the goodness of the fit between a term and the linked classification number. In our prototype we have built links between a widely used feminist vocabulary, A Women’s Thesaurus, and the Dewey Decimal Classification. The enhanced thesaurus is constructed and maintained in a commercial thesaurus package and then exported to a relational database for searching. A hypertextual interface has been developed to search the database on a workstation, and a second interface has been built to permit searching through the World Wide Web. The aim is to provide a model for implementations that use minimal resources and are widely applicable, so we have worked in the Windows environment with inexpensive development tools like Visual Basic and Microsoft Access. The prototype has demonstrated that even such simple tools can provide an effective user interface and satisfactory performance.",
        "keywords": "",
        "released": 1998,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000077585300034"
    },
    {
        "title": "Plugging in and into code bubbles: The code bubbles architecture",
        "abstract": "Code Bubbles is an attempt to redefine the user interface for an integrated programming environment. As it represents a whole new user interface, implementing it as a plug-in is inherently difficult. We get around this difficulty by combining two different plug-in architectures, a standard one based on registrations and callbacks and a message-based one that puts the plug-in at arm’s length and defines a narrower two-way interface. This paper describes both how we have implemented Code Bubbles as a plug-in to Eclipse and how Code Bubbles itself is implemented as a set of plug-ins representing the different aspects of the environment, using both traditional and message-based plug-in architectures as appropriate. It also shows how the resultant architecture is flexible enough to support collaboration, different back ends, and a cloud-based environment. Copyright (c) 2013 John Wiley & Sons, Ltd.",
        "keywords": "programming environments; plug-in architectures; software development",
        "released": 2014,
        "link": "https://doi.org/10.1002/spe.2196"
    },
    {
        "title": "Programming cobots by voice: A pragmatic, web-based approach",
        "abstract": "This paper introduces a novel voice-based programming approach and software framework for collaborative robots (cobots) based on the Web Speech API, which is now supported by most modern browsers. The framework targets human programmable interfaces and human-machine interfaces, which can be used by people with little or no programming experience. The framework follows a meta-programming approach by enabling users to program cobots by voice in addition to using a mouse, tablet, or keyboard. Upon a voice instruction, the framework automates the manual tasks required to manipulate the vendor-provided interfaces. The main advantages of this approach are simplified, guided programming, which only requires the knowledge of 5-10 voice instructions; increased programming speed compared to the manual approach; and the possibility of sharing programs as videos. The approach is generalized to other kinds of robots and robot programming tools using so-called meta-controllers, which leverage the power of graphical user interface automation tools and techniques.",
        "keywords": "Voice-based programming; GUI automation; cobots; plugins; meta-programming; speech recognition",
        "released": 2022,
        "link": "https://doi.org/10.1080/0951192X.2022.2148754"
    },
    {
        "title": "Developing XML web services with WebSphere studio application developer",
        "abstract": "Web services have recently emerged as a powerful technology for integrating heterogeneous, applications over the Internet. The widespread adoption of Web services promises to usher in an exciting new generation of advanced distributed applications. These will support a new and growing set of specifications, such as Simple Object Access Protocol (SOAP), Web Services Description Language (WSDL), and Universal Description, Discovery, and Integration (UDDI). Extensible Markup Language (XML) and its associated, family of standards also play a central role in Web services by providing a data interchange, format that is independent of both programming , languages and operating systems. The application developer seeking to reap the benefits of Web services is therefore faced with a significant, and potentially steep, new learning curve. Clearly, application development tools that lower this barrier are crucial for the rapid and widespread adoption, of Web services. This paper discusses the development tasks associated with XML Web services and describes a new suite of tools that improve developer productivity, by reducing the, requirements for detailed: knowledge-of-the, underlying specifications and standards, and allow the developer to,focus on the business. problem domain. This suite of XML and Web services tools is part of IBM’s recently released, WebSphere((R)) Studio Application Developer product, which is based on the new-Eclipse open source tool integration platform.",
        "keywords": "",
        "released": 2002,
        "link": "https://doi.org/10.1147/sj.412.0178"
    },
    {
        "title": "LolliPop for learning resources: Information literacy staff training within further education",
        "abstract": "This paper looks at how the provision of information literacy training is supported in the further education sector. Using a case study, the results demonstrate how an online information literacy programme can be used as a staff development tool, in order that Learning Resources staff are better equipped to support students in their information literacy needs. The online programme was also used so that staff could become familiar with the institutional virtual learning environment (VLE), again in order that they are able to better support students.",
        "keywords": "e-learning; further education; information literacy; LolliPop; online learning; virtual learning environments",
        "released": 2010,
        "link": "https://doi.org/10.1177/0961000610368919"
    },
    {
        "title": "Re- using an existing wheel developing data architecture for cooperating autonomous and semi-autonomous, agent-based web services",
        "abstract": "This short position paper considers issues in developing Data Architecture for the Internet of Things (IoT) through the medium of an exemplar project, Domain Expertise Capture in Authoring and Development -Environments (DECADE). A brief discussion sets the background for IoT, and the development of the -distinction between things and computers. The paper makes a strong argument to avoid reinvention of the wheel, and to reuse approaches to distributed heterogeneous data architectures and the lessons learned from that work, and apply them to this situation. DECADE requires an autonomous recording system, -local data storage, semi-autonomous verification model, sign-off mechanism, qualitative and -quantitative -analysis -carried out when and where required through web-service architecture, based on ontology and analytic agents, with a self-maintaining ontology model. To develop this, we describe a web-service -architecture, -combining a distributed data warehouse, web services for analysis agents, ontology agents and a -verification engine, with a centrally verified outcome database maintained by certifying body for qualification/-professional status.",
        "keywords": "Distributed data warehouse; Domain expertise capture; Internet of things; Ontology agent; Self-maintaining ontology; Web-service architecture",
        "released": 2009,
        "link": "https://doi.org/10.4103/0256-4602.55277"
    },
    {
        "title": "Incorporating the ontology paradigm into software engineering: Enhancing domain-driven programming in clojure/java",
        "abstract": "There is a notable overlap of the challenges with which the semantic technologies and software engineering deal. They can also complement and mutually improve each other. Current efforts mostly focus on improving software tools around the resource description framework (RDF) and Web Ontology Language (OWL) Web-oriented ecosystem that helps ontology engineers but is alien to software engineers. This paper presents an opposite approach taken from the software developer’s viewpoint-an incorporation of the ontology paradigm into a general-purpose programming language, in a simple and agile way, on a small scale, and in an unpretentious manner. The objective is to help programmers write simple domain-driven code with richer semantics. The means to achieve this objective relies on metaprogramming to internalize the ontology modeling paradigm into a mainstream programming environment based on the Java ecosystem, in a lightweight manner suitable for small teams. An embedded meta domain-specific language (DSL), which is called Magic Potion, is implemented in Clojure and blends ontology, functional, object-oriented, and concurrent paradigms. An example from the technology enhanced learning (TEL) domain is used to illustrate Magic Potion in action.",
        "keywords": "Clojure; domain-specific languages; modeling spaces; multiparadigm programming; ontologies; programming languages; semantic technologies; software engineering",
        "released": 2012,
        "link": "https://doi.org/10.1109/TSMCC.2011.2140316"
    },
    {
        "title": "Discrete-event simulation on the internet and the web",
        "abstract": "Existing software development environments for discrete-event simulation have adopted either a language-based or a library-based approach. Although these approaches have advantages, they suffer from three Limitations: lack of portability, lack of interoperability, and difficulty in execution over the Internet/Web infrastructure. The paper addresses these Limitations by proposing to facilitate discrete-event simulation as a service on the Internet/Web. The simulation service is defined as a CORBA facility with a well-defined interface, and components of simulation applications access the facility to obtain execution support. The simulation facility enables portability and interoperability of simulation applications which may be developed in heterogeneous languages and environments and communicate over the Internet. Moreover, by integrating with Java, the approach also enables simulation applications to be developed in Java and executed on the Web. Application of the simulation facility is illustrated by using a bounded-buffer producer/consumer system. (C) 2000 Elsevier Science B.V. All rights reserved.",
        "keywords": "CORBA; discrete-event simulation; Internet; Java; Web",
        "released": 2000,
        "link": "https://doi.org/10.1016/S0167-739X(99)00113-2"
    },
    {
        "title": "Implementation and evaluation of various demons deformable image registration algorithms on a GPU",
        "abstract": "Online adaptive radiation therapy (ART) promises the ability to deliver an optimal treatment in response to daily patient anatomic variation. A major technical barrier for the clinical implementation of online ART is the requirement of rapid image segmentation. Deformable image registration (DIR) has been used as an automated segmentation method to transfer tumor/organ contours from the planning image to daily images. However, the current computational time of DIR is insufficient for online ART. In this work, this issue is addressed by using computer graphics processing units (GPUs). A gray-scale-based DIR algorithm called demons and five of its variants were implemented on GPUs using the compute unified device architecture (CUDA) programming environment. The spatial accuracy of these algorithms was evaluated over five sets of pulmonary 4D CT images with an average size of 256 x 256 x 100 and more than 1100 expert-determined landmark point pairs each. For all the testing scenarios presented in this paper, the GPU-based DIR computation required around 7 to 11 s to yield an average 3D error ranging from 1.5 to 1.8 mm. It is interesting to find out that the original passive force demons algorithms outperform subsequently proposed variants based on the combination of accuracy, efficiency and ease of implementation.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1088/0031-9155/55/1/012"
    },
    {
        "title": "CHOReVOLUTION: Service choreography in practice",
        "abstract": "This paper presents CHOReVOLUTION, a platform for the tool-assisted realization and execution of distributed applications. CHOReVOLUTION specifically targets service-oriented systems specified through service choreographies. It offers an Integrated Development and Runtime Environment (IDRE) organized into three layers, namely, front-end, back-end, and cloud. It comprises a wizard-aided development environment and a system monitoring console in the front-end layer, and a back-end for managing the deployment and execution of the choreographed system on the cloud. We describe the IDRE by using an industrial use case in the domain of Smart Mobility & Tourism, and finally we provide details on its experimental evaluation. (C) 2020 Elsevier B.V. All rights reserved.",
        "keywords": "Service choreographies; Distributed computing; Distributed coordination; Automated synthesis",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.scico.2020.102498"
    },
    {
        "title": "SPATIAL DATA INFRASTRUCTURES FOR WATER MANAGEMENT. THE CASE OF GUADALQUIVIR RIVER BASIN AUTHORITY",
        "abstract": "The Guadalquivir River Basin Authority have, among its responsibilities, the hydrological planning of water resources, which flow through the demarcation of the Guadalquivir, Ceuta and Melilla, the execution of works of general interest as well as the management of Hydraulic Public Domain. This requires an enormous amount of data related to water and territory, which are represented on digital cartography. The Guadalquivir River Basin Authority has worked in the construction of a Spatial Data Infrastructure that, in addition to bringing together cartographic and alphanumeric proven quality information, provides remote access to geospatial data. The final goal of the IDE-CHG is to facilitate the understanding of the complex geographic reality of the watershed and enhance relationships with users through a greater presence on the web, following the recommendations of the GIS guide in the Water Framework Directive, and responding to the requirements of the INSPIRE directive.",
        "keywords": "Guadalquivir River Basin Authority; Spatial Data Infrastructure; Geoportal; Water Framework Directive",
        "released": 2019,
        "link": "https://doi.org/10.21138/GF.548"
    },
    {
        "title": "TCIApathfinder: An r client for the cancer imaging archive REST API",
        "abstract": "The Cancer Imaging Archive (TCIA) hosts publicly available deidentified medical images of cancer from over 25 body sites and over 30,000 patients. Over 400 published studies have utilized freely available TCIA images. Images and metadata are available for download through a web interface or a REST API. Here, we present TCIApathfinder, an R client for the TCIA REST API. TCIApathfinder wraps API access in user-friendly R functions that can be called interactively within an R session or easily incorporated into scripts. Functions are provided to explore the contents of the large database and to download image files. TCIApathfinder provides easy access to TCIA resources in the highly popular R programming environment. TCIApathfinder is freely available under the MIT license as a package on CRAN (https://cran.r-project.org/web/packages/TCIApathfinder/index.html) and from https://github.com/pamelarussell/TCIApathfinder. Significance: These findings present a new tool, TCIApathfinder, the first client for The Cancer Imaging Archive (TCIA) for use in the highly popular R computing environment, that will dramatically lower the barrier of access to the valuable tools in TCIA. (C) 2018 AACR.",
        "keywords": "",
        "released": 2018,
        "link": "https://doi.org/10.1158/0008-5472.CAN-18-0678"
    },
    {
        "title": "Integration of computational thinking in compulsory education. Two pedagogical experiences of collaborative learning online",
        "abstract": "Countries in the world have included in their educational agendas the incorporation of Computational Thinking (CT) in Compulsory Education. Despite the efforts of the school authorities, the school systems have difficulties in developing this proposal in the traditional classrooms. In this article we present two experiences based on the use of educational technologies and methodologies that have helped overcome these difficulties by bringing the PC to the Classroom effectively for students, teachers and schools. Experiences have been developed in collaboration with educational institutions and ministries of education in two Latin American countries. The educational technologies used are virtual learning environments (VLE), synchronous collaboration systems, and Scratch / Snap online programming environments. The educational methodologies developed are based on collaborative work teachers-tutors, teachers-students, and students-students. Both educational technologies and methodologies have been used together to develop, through the PC, the ability of our students to solve problems related to daily life and the working world.",
        "keywords": "Computational Thinking; Online Collaborative Learning; Problem Solving",
        "released": 2020,
        "link": "https://doi.org/10.6018/red.409481"
    },
    {
        "title": "Distributed retrieval engine for the development of cloud-deployed biological databases",
        "abstract": "The integration of cloud resources with federated data retrieval has the potential of improving the maintenance, accessibility and performance of specialized databases in the biomedical field. However, such an integrative approach requires technical expertise in cloud computing, usage of a data retrieval engine and development of a unified data-model, which can encapsulate the heterogeneity of biological data. Here, a framework for the development of cloud-based biological specialized databases is proposed. It is powered by a distributed biodata retrieval system, able to interface with different data formats, as well as provides an integrated way for data exploration. The proposed framework was implemented using Java as the development environment, and MongoDB as the database manager. Syntactic analysis was based on BSON, jsoup, Apache Commons and w3c.dom open libraries. Framework is available in: http://nbel-lab.com and is distributed under the creative common agreement.",
        "keywords": "MongoDB; Specialized databases; Federated databases; Cloud-based databases",
        "released": 2018,
        "link": "https://doi.org/10.1186/s13040-018-0185-5"
    },
    {
        "title": "Reflections on iCODE: Using web technology and hands-on projects to engage urban youth in computer science and engineering",
        "abstract": "More than 200 middle school and high school students from underserved urban communities in Boston, Lowell, and Lawrence, Massachusetts, participated in after-school and summer enrichment programs over a three-year period, using hands-on learning materials and web resources to complete hands-on microcontroller-based projects. Program content was based on a suite of robotics and electronics kits developed by the University of Massachusetts Lowell and Machine Science Inc., together with on-line instructions, a web-based programming tool, and a shared electronic portfolio of student projects. Participating students worked with classroom teachers and undergraduate mentors to complete a series of projects, and took part each year in a non-competitive robotics exhibition and a competitive robot sumo tournament. Goodman Research Group assessed learning outcomes and attitudinal changes using a variety of measures, including observations of program sessions, group interviews with participating students, pre- and post-program student surveys, and educator feedback. The program was found to effectively engage participants, give them real engineering and programming skills, improve their attitudes toward science, technology, engineering, and mathematics (STEM) subjects, and increase their interest in STEM career pathways. These results are presented, along with lessons learned from the program implementation, technology development, and evaluation.",
        "keywords": "Robotics; Education; K-12; Informal; After school; Microcontroller; Programming; Logo; Sensors; Crafts; Evaluation; Competition; Career; Computer science; Engineering",
        "released": 2011,
        "link": "https://doi.org/10.1007/s10514-011-9218-3"
    },
    {
        "title": "A framework of electronic tendering for government procurement: A lesson learned in taiwan",
        "abstract": "To render government procurement efficient, transparent, nondiscriminating, and accountable, an electronic government procurement system is required. Accordingly, Taiwan Government Procurement Law (TGPL) states that suppliers may employ electronic devices to forward a tender. This investigation demonstrates how the electronic government procurement system functions and reengineers internal procurement processes, which in turn benefits both government bodies and venders. The system features explored herein include posting/receiving bids via the Internet, vender registration, certificate authorization, contract development tools, bid/Request For Proposal (RFP) development, online bidding, and online payment, all of which can be integrated easily within most existing information infrastructures. (C) 2002 Elsevier Science B.V. All rights reserved.",
        "keywords": "certification authority; payment gateway; government electronic procurement system; public key infrastructure",
        "released": 2002,
        "link": "https://doi.org/10.1016/S0926-5805(02)00013-4"
    },
    {
        "title": "ResponseNet: Revealing signaling and regulatory networks linking genetic and transcriptomic screening data",
        "abstract": "Cellular response to stimuli is typically complex and involves both regulatory and metabolic processes. Large-scale experimental efforts to identify components of these processes often comprise of genetic screening and transcriptomic profiling assays. We previously established that in yeast genetic screens tend to identify response regulators, while transcriptomic profiling assays tend to identify components of metabolic processes. ResponseNet is a network-optimization approach that integrates the results from these assays with data of known molecular interactions. Specifically, ResponseNet identifies a high-probability sub-network, composed of signaling and regulatory molecular interaction paths, through which putative response regulators may lead to the measured transcriptomic changes. Computationally, this is achieved by formulating a minimum-cost flow optimization problem and solving it efficiently using linear programming tools. The ResponseNet web server offers a simple interface for applying ResponseNet. Users can upload weighted lists of proteins and genes and obtain a sparse, weighted, molecular interaction sub-network connecting their data. The predicted sub-network and its gene ontology enrichment analysis are presented graphically or as text. Consequently, the ResponseNet web server enables researchers that were previously limited to separate analysis of their distinct, large-scale experiments, to meaningfully integrate their data and substantially expand their understanding of the underlying cellular response. ResponseNet is available at http://bioinfo.bgu.ac.il/respnet.",
        "keywords": "",
        "released": 2011,
        "link": "https://doi.org/10.1093/nar/gkr359"
    },
    {
        "title": "Database development based on deep learning and cloud computing",
        "abstract": "In this research, the author develops databases based on deep learning and cloud computing technology. On the basis of designing the overall architecture of the database with the distributed C/S mode as the core, use J2EE (Java 2 Platform, Enterprise Edition) as the development tool, apply Oracle server database, extract data features with in-depth learning technology, allocate data processing tasks based with cloud computing technology, so as to finally complete data fusion and compression. Finally, the overall development of the database is completed by designing the database backup scheme and external encryption. The test results show that the database developed by the above method has low performance loss, can quickly complete the processing of subdatabase and subtable, and can effectively support the distributed storage of data.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1155/2022/6208678"
    },
    {
        "title": "Interactive notebooks for achieving learning outcomes in a graduate course: A pedagogical approach",
        "abstract": "Social network analysis involves delicate and sophisticated mathematical concepts which are abstract and challenging to acquire by traditional methods. Many studies show that female students perform poorly in computer science-related courses compared to male students. To address these issues, this research investigates the impact of employing a web-based interactive programming tool, Jupyter notebooks, on supporting deeper conceptual understanding and, therefore, better attainment levels of the course learning outcomes in a female setting. The work also highlights the overall experience and enjoyment this tool brought to the classroom. Document analysis and questionnaire were used as data collection methods. A mixed approach was applied, mid-term exam documents were investigated qualitatively, and the questionnaire was analyzed quantitatively. Our results showed that most students correctly perceived the learning outcomes and knowledge introduced within the Jupyter environment. Moreover, the interactive nature of Jupyter enhanced engagement and brought enjoyment to the learning experience.",
        "keywords": "Qualitative research; Learning outcomes; Learning objectives; Interactive notebooks; Teaching strategies",
        "released": 2023,
        "link": "https://doi.org/10.1007/s10639-023-11854-x"
    },
    {
        "title": "AVAl: An extensible attribute-oriented programming validator for java",
        "abstract": "Attribute-oriented programming (@OP) permits programmers to extend the semantics of a base program by annotating it with attributes defined in an attribute domain-specific language (AttDSL). In this article, we propose AVal: a Java5 framework for the definition and checking of rules for @OP in Java. We define a set of meta-annotations to allow the validation of @OP programs, as well as the means to extend these meta-annotations by using a compile-time model of the program’s source code. AVal is fully integrated into the Eclipse IDE. We show the usefulness of the approach by using examples of its use applied to three AttDSLs: an @OP framework that helps programming Simple API for XML parsers, an @OP extension for the Fractal component model called Fraclet, and the JSR 181 for web services definition. Copyright (C) 2007 John Wiley & Sons, Ltd.",
        "keywords": "program checking; validation; attributes; annotations",
        "released": 2007,
        "link": "https://doi.org/10.1002/smr.349"
    },
    {
        "title": "Attitudes of active older internet users towards online social networking",
        "abstract": "Although information and communication technology (ICT) has a radical impact on people’s everyday lives, bringing new social experiences and new ways for people to interact, there is still a large difference in social behaviour in this area between people in different age groups. Online social networks such as Facebook, Twitter, LinkedIn etc. are becoming increasingly important social media, but people aged 50 years and over use these resources at lower rates than those in younger age groups. In this paper we aim to investigate the factors affecting the use of online social networks by active older Internet users in Slovenia. Additionally, we address how often, and to what extent, active older Internet users are engaged in using ICT. A research study is presented that was conducted among active older Internet users, on the basis of a non-standardised research questionnaire. Collected data were analysed with basic descriptive, univariate and multivariate statistical methods, followed by induction of a decision tree using the WEKA (Waikato Environment for Knowledge Analysis) 3.7 programming environment. Research findings revealed that female participants are more familiar with the term “online social network” and are also more frequent users, compared to male participants. Additionally, the results showed that age, gender and education seem to be the most important factors having a direct or indirect impact on the use of online social networks by active older Internet users. (C) 2015 Elsevier Ltd. All rights reserved.",
        "keywords": "Active older Internet users; Social media; Online social networks; Information communication technology (ICT); Social behaviour",
        "released": 2016,
        "link": "https://doi.org/10.1016/j.chb.2015.09.014"
    },
    {
        "title": "Development of an educational supply chain information system using object web technology",
        "abstract": "With the explosive growth of the Web, it becomes essential for developing a program to educate students to comprehend how to develop large-scale software systems for Internet applications. One of the major enabling technologies for constructing e-Commerce applications is the Object Web. Supply chains are typical large-scale applications of e-Commerce. Therefore, this work presents a collaborative and event-driven educational Supply Chain Information System (SCIS) by using Object Web technology. Unified Modeling Language (UML) is applied to analyze and design the SCIS framework that contains two major components: the Information Coordinator Component and the Agent Component. Demonstration results indicate that this SCIS framework provides a collaborative, event-driven, object-oriented, and agent-based infrastructure for the supply chain members to mutually exchange information efficiently. Educational aspects concerning development procedures, development tools, software reuse, and a system interface for distributed object-oriented software engineering courses are also emphasized in this paper.",
        "keywords": "e-Commerce; supply chain information system; object web technology; unified modeling language",
        "released": 2002,
        "link": "https://doi.org/10.1080/02533839.2002.9670747"
    },
    {
        "title": "Hybrid cloud and cluster computing paradigms for life science applications",
        "abstract": "Background: Clouds and MapReduce have shown themselves to be a broadly useful approach to scientific computing especially for parallel data intensive applications. However they have limited applicability to some areas such as data mining because MapReduce has poor performance on problems with an iterative structure present in the linear algebra that underlies much data analysis. Such problems can be run efficiently on clusters using MPI leading to a hybrid cloud and cluster environment. This motivates the design and implementation of an open source Iterative MapReduce system Twister. Results: Comparisons of Amazon, Azure, and traditional Linux and Windows environments on common applications have shown encouraging performance and usability comparisons in several important non iterative cases. These are linked to MPI applications for final stages of the data analysis. Further we have released the open source Twister Iterative MapReduce and benchmarked it against basic MapReduce (Hadoop) and MPI in information retrieval and life sciences applications. Conclusions: The hybrid cloud (MapReduce) and cluster (MPI) approach offers an attractive production environment while Twister promises a uniform programming environment for many Life Sciences applications. Methods: We used commercial clouds Amazon and Azure and the NSF resource FutureGrid to perform detailed comparisons and evaluations of different approaches to data intensive computing. Several applications were developed in MPI, MapReduce and Twister in these different environments.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1186/1471-2105-11-S12-S3"
    },
    {
        "title": "A service-oriented framework for the development of home robots",
        "abstract": "In recent years, researchers have been building home robots able to interact and work with people. Yet, because of the complicated and independent robot development environments, it is not always easy to share and reuse robot code created by different providers. In this work, we present an ontology-based framework that integrates service-oriented computing environments with the standard web interface to develop reusable robotic services. In addition to the service discovery, selection, and composition processes often performed by traditional web services, our work also includes an adaptive mechanism through which the user can iteratively modify composite robotic services to suit his or her needs. The proposed methodology has been implemented and evaluated, and the results show that our framework can be used to build robotic services successfully.",
        "keywords": "Service-Oriented Computing; AI Planning; Ontology; Service Composition; Robotic Service",
        "released": 2013,
        "link": "https://doi.org/10.5772/55055"
    },
    {
        "title": "WIVET-benchmarking coverage qualities of web crawlers",
        "abstract": "Web application vulnerability scanners (WAVS) include crawler components to extract all accessible links of tested web pages in order to identify attack entry points and parameters. After extracting links, they perform different types of attacks over each extracted link and try to find out existing vulnerabilities in the tested web application for reporting. A WAVS tool that has a low-quality crawler component would generate false-negative results, since failing to discover existing links would inhibit detection of possible vulnerabilities exposed through these links. Therefore, the coverage quality of its crawler plays a very important role in the success of a WAVS tool. In this paper, we propose a novel method for analyzing and comparing coverage qualities of WAVS crawlers. We developed WIVET (Web Input Vector Extractor Teaser) as a benchmarking tool for analyzing crawler components of WAVS. WIVET evaluates WAVS crawlers based on their extraction capability of 56 target links that are generated statically or dynamically by WIVET’s 21 test cases. We explain WIVET’s architecture, all WIVET test cases and target links with code examples, integration of WIVET into WAVS development environments and WAVS benchmarking results in detail.",
        "keywords": "web security; web application vulnerability scanner; black-box testing; web crawling; hidden web",
        "released": 2017,
        "link": "https://doi.org/10.1093/comjnl/bxw072"
    },
    {
        "title": "JPernLite: Extensible transaction services for the WWW",
        "abstract": "Concurrency control is one of the key problems in design and implementation of collaborative systems such as hypertext! hypermedia systems, CAD/CAM systems, and software development environments. Most existing systems store data in specialized databases with built-in concurrency control policies, usually implemented via locking. It is desirable to construct such collaborative systems on top of the World Wide Web, but most Web servers do not support even conventional transactions, let alone distributed (multi-Website) transactions or flexible concurrency control mechanisms oriented toward teamwork-such as event notification, shared locks, and fine granularity locks. We present a transaction server that operates independently of Web servers or the collaborative systems, to fill the concurrency control gap. By default, the transaction server enforces the conventional atomic transaction model, where sets of operations are performed in an all-or-nothing fashion and isolated from concurrent users. The server can be tailored dynamically to apply more sophisticated concurrency control policies appropriate for collaboration. The transaction server also supports applications employing information resources other than Web servers, such as legacy databases, CORBA objects, and other hypermedia systems. Our implementation permits a wide range of system architecture styles.",
        "keywords": "distributed transactions; extended transaction models; WWW; computer-supported collaborative work; middleware",
        "released": 1999,
        "link": "https://doi.org/10.1109/69.790823"
    },
    {
        "title": "Entecavir-based combination therapies for chronic hepatitis b a meta-analysis",
        "abstract": "Background: Currently. there is no consensus on the efficacy and safety of the entecavir (ETV) monotherapy versus the ETV based combination therapy for chronic hepatitis B. Methods: A comprehensive literature search was performed on the comparison of ETV-based combination therapy and monotherapy for chronical hepatitis B (CHB) patients in the PubMed, Embase, Web of Science, the Cochrane Libraries, and the Chinese BioMedical Literature Database. Both dichotomous and continuous variables were extracted. and pooled outcomes were expressed as odds ratio (OR) or mean difference (MD). Results: We included randomized clinical trials (RCTs) and cohorts involving Group A: nucleos(t)ide-naive patients (four RCTs, n=719 patients), Group B: nucleos(t)ide-resistant patients (four cohorts, n=196 patients), and Group C: entecavir-treated patients with undetectable hepatitis B virus DNA (two RCTs and two cohorts, n=297). Group A. ETV monotherapy was better for rates of undetectable HBV DNA, while the rates of the HBV DNA levels at the end of treatment, HBeAg Loss, ALT normalization were similar between the two groups [MD, -0.85 (95% CI, 0.173-0.03): OR, 0.92 (95% CI, 0.24-3.56): OR, 1.31 (95% CI, 0.17-9.82)]; Group B. ETV monotherapy was better for rates of undetectable HBV DNA, while the rates of the HBV DNA levels at the end of treatment, HBeAg Loss, ALT normalization were similar; Group C. The ETV-based combination therapy was better for the rate of HBV DNA relapse. Conclusion: Based on the current data, ETV-based combination therapy seemed to be no better than ETV monotherapy. Further studies are needed to verify this conclusion.",
        "keywords": "chronic hepatitis B; combination therapy; entecavir; monotherapy",
        "released": 2018,
        "link": "https://doi.org/10.1097/MD.0000000000013596"
    },
    {
        "title": "Deep integro-difference equation models for spatio-temporal forecasting",
        "abstract": "Integro-difference equation (IDE) models describe the conditional dependence between the spatial process at a future time point and the process at the present time point through an integral operator. Nonlinearity or temporal dependence in the dynamics is often captured by allowing the operator parameters to vary temporally, or by re-fitting a model with a temporally-invariant linear operator in a sliding window. Both procedures tend to be excellent for prediction purposes over small time horizons, but are generally time-consuming and, crucially, do not provide a global prior model for the temporally-varying dynamics that is realistic. Here, we tackle these two issues by using a deep convolution neural network (CNN) in a hierarchical statistical IDE framework, where the CNN is designed to extract process dynamics from the process’ most recent behaviour. Once the CNN is fitted, probabilistic forecasting can be done extremely quickly online using an ensemble Kalman filter with no requirement for repeated parameter estimation. We conduct an experiment where we train the model using 13 years of daily sea-surface temperature data in the North Atlantic Ocean. Forecasts are seen to be accurate and calibrated. A key advantage of our approach is that the CNN provides a global prior model for the dynamics that is realistic, interpretable, and computationally efficient. We show the versatility of the approach by successfully producing 10-minute nowcasts of weather radar reflectivities in Sydney using the same model that was trained on daily sea-surface temperature data in the North Atlantic Ocean. (C) 2020 Elsevier B.V. All rights reserved.",
        "keywords": "Convolution neural network; Deep learning; Dynamic model; Ensemble Kalman filter; Prediction; Spatio-temporal",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.spasta.2020.100408"
    },
    {
        "title": "2D-LC-MS approaches for the analysis of in-process samples and for the characterization of mAbs in a regulated environment",
        "abstract": "Biologics, and in particular monoclonal antibodies (mAbs), are an important class of therapeutics, and their market share keeps growing. The production of antibodies is a complex and lengthy process. In-process characterization of the mAb would help in optimizing the production steps. Efficiency in mAb characterization can be obtained by automating analysis and reducing hands-on time. Although mass spectrometry (MS) is an essential technique for detailed characterization of biomolecules, its use is limited to purified samples. However, the hyphenation of an MS system to two-dimensional liquid chromatography (2D-LC) allows for the analysis of more complex samples. The first dimension of a 2D-LC system can be used to purify the sample from its matrix or separate compounds using mobile phases that are not MS-compatible, whereas the second dimension coupled to MS can be used to desalt or separate the different variants or species obtained on the first dimension. A 2D-LC-MS system installed in a full good manufacturing practice (GMP)-compliant environment using validated software was used for the characterization of mAbs in complex mixtures at the intact and subunit levels using a Protein A affinity column with no sample preparation steps. In the second application, MS characterization of mAb subunits was made possible by digestion of the mAb online by an immobilized IdeS enzyme. The addition of a disulfide bridge reduction step online led to analyzing smaller molecules to access fine characterization.",
        "keywords": "",
        "released": 2023,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:001128774700004"
    },
    {
        "title": "“Just-in-case” answers: The twenty-first-century vertical file",
        "abstract": "This article discusses the use of OCLC’s QuestionPoint service for managing electronic publications and other items that fall outside the scope of OCLC Library’s OPAC and Web resources pages, yet need to be “put somewhere.” The local knowledge base serves as both a collection development tool and as a virtual vertical file, with records that are easy to enter, search, update,,or delete.",
        "keywords": "",
        "released": 2008,
        "link": "https://doi.org/10.6017/ital.v27i4.3239"
    },
    {
        "title": "WebDietAID: An interactive web-based nutritional counselor",
        "abstract": "WEBDIETAID is a Web-based system aimed at assisting individuals affected by high serum cholesterol. The system tries to reproduce the types of intervention performed by a nutritional counselor. It is structured as a set of advisors, each of which handles a different side of the counseling process. The tasks of the advisors range from monitoring weight, to teaching about healthy nutrition, to assessing motivation and psychological obstacles to behavior change. WEBDIETAID is based on a development environment for Web-based applications that includes a dynamical Web server, a knowledge base management system, and an interface to a relational database. We describe the architecture of the system, and several of the implemented advisors. Finally, we discuss how the architecture could be generalized to other counseling domains.",
        "keywords": "",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000170207500145"
    },
    {
        "title": "New 2D-LC-MS approaches for the analysis of in-process samples and for the characterization of mAbs in a regulated environment",
        "abstract": "Biologics, and in particular monoclonal antibodies (mAbs), are an important class of therapeutics, and their market share keeps growing. The production of antibodies is a complex and lengthy process. In-process characterization of the mAb would help in optimizing the production steps. Efficiency in mAb characterization can be obtained by automating analysis and reducing hands-on time. Although mass spectrometry (MS) is an essential technique for detailed characterization of biomolecules, its use is limited to purified samples. However, the hyphenation of an MS system to two-dimensional liquid chromatography (2D-LC) allows for the analysis of more complex samples. The first dimension of a 2D-LC system can be used to purify the sample from its matrix or separate compounds using mobile phases that are not MS-compatible, whereas the second dimension coupled to MS can be used to desalt or separate the different variants or species obtained on the first dimension. A 2D-LC-MS system installed in a full good manufacturing practice (GMP)-compliant environment using validated software was used for the characterization of mAbs in complex mixtures at the intact and subunit levels using a Protein A affinity column with no sample preparation steps. In the second application, MS characterization of mAb subunits was made possible by digestion of the mAb online by an immobilized IdeS enzyme. The addition of a disulfide bridge reduction step online led to analyzing smaller molecules to access fine characterization.",
        "keywords": "",
        "released": 2022,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000899448800005"
    },
    {
        "title": "ChemmineR: A compound mining framework for r",
        "abstract": "Motivation: Software applications for structural similarity searching and clustering of small molecules play an important role in drug discovery and chemical genomics. Here, we present the first open-source compound mining framework for the popularstatistical programming environment R. The integration with a powerful statistical environment maximizes the flexibility, expandability and programmability of the provided analysis functions. Results: We discuss the algorithms and compound mining utilities provided by the R package ChemmineR. It contains functions for structural similarity searching, clustering of compound libraries with a wide spectrum of classification algorithms and various utilities for managing complex compound data. It also offers a wide range of visualization functions for compound clusters and chemical structures. The package is well integrated with the online ChemMine environment and allows bidirectional communications between the two services.",
        "keywords": "",
        "released": 2008,
        "link": "https://doi.org/10.1093/bioinformatics/btn307"
    },
    {
        "title": "From discussion forum to discursive studio: Learning and creativity in design-oriented affinity spaces",
        "abstract": "In recent years, playful design environments and digital games have been offering increasingly accessible programming tools and integrated editors, significantly expanding opportunities for the creation and sharing of user-generated content. These practices have engendered the diffusion of participatory online environments in which users present, discuss, and critique their creations. This study analyzes one of these design-driven environments dedicated to game levels created with the popular series LittleBigPlanet. Findings suggest that participants interact guided by their desire to become skilled designers and be recognized as such by their peers. To do so, they enact situated discursive functions that entail a pervasive use of specialist language, the formation of shared design references, and the valorization of new forms of originality based on remixing and intertextuality. By engaging in multimodal practices in a competent community of peer designers, participants create a safe “discursive studio” that offers a multiplicity of trajectories for learning and creativity.",
        "keywords": "games and learning; discursive design; affinity spaces; discursive studio analysis; LittleBigPlanet",
        "released": 2015,
        "link": "https://doi.org/10.1177/1555412014557328"
    },
    {
        "title": "WikiPathways: Capturing the full diversity of pathway knowledge",
        "abstract": "WikiPathways (http://www.wikipathways.org) is an open, collaborative platform for capturing and disseminating models of biological pathways for data visualization and analysis. Since our last NAR update, 4 years ago, WikiPathways has experienced massive growth in content, which continues to be contributed by hundreds of individuals each year. New aspects of the diversity and depth of the collected pathways are described from the perspective of researchers interested in using pathway information in their studies. We provide updates on extensions and services to support pathway analysis and visualization via popular standalone tools, i.e. PathVisio and Cytoscape, web applications and common programming environments. We introduce the Quick Edit feature for pathway authors and curators, in addition to new means of publishing pathways and maintaining custom pathway collections to serve specific research topics and communities. In addition to the latest milestones in our pathway collection and curation effort, we also highlight the latest means to access the content as publishable figures, as standard data files, and as linked data, including bulk and programmatic access.",
        "keywords": "",
        "released": 2016,
        "link": "https://doi.org/10.1093/nar/gkv1024"
    },
    {
        "title": "A framework to help designing innovative massively multiplayer online games interactions",
        "abstract": "This paper presents FITGap, an object-oriented framework dedicated to designing innovative interactions occurring in a Virtual Environment such as a Massively Multiplayer Online Came (MMOG). This framework has been designed as the underlying software of a future development environment for online games prototyping. FITGap is an open framework based on an extensible basic-blocks library, and uses a deterministic scheduling model to ease the development and tuning of the final application. Its semantics is well-defined to allow future analysis tools.",
        "keywords": "",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000238089300059"
    },
    {
        "title": "RTSVC: Real-time system for visual control of robots",
        "abstract": "This article presents an image processing system that can work in hard real-time. Compared with systems that use the traditional multiprocessor architecture approach, this computer system takes advantage on recent technological advances and it is designed to work with a single processor PC under RTLinux. Its programming environment is similar to C programming language and it offers a friendly graphical user interface. The performance of the system is illustrated by means of experiments applied to visual guidance of mobile robots via velocity fields using a fixed high-speed camera. The experiments were carried out with a strict sampling frequency of 100 Hz. (C) 2008 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 18, 251 256, 2008; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/ima.20135",
        "keywords": "image processing; real-time; visual servoing; vision systems; robotics",
        "released": 2008,
        "link": "https://doi.org/10.1002/ima.20135"
    },
    {
        "title": "Native code generation as a service",
        "abstract": "With the widespread use of mobile applications in daily life, it has become crucial for enterprise software companies to quickly develop these applications for multiple platforms. Cross-platform mobile application development is one of the most adopted solutions for rapid development. Since most of these solutions do not generate native code for the underlying platform, the artefacts generally do not satisfy the requirements defined at the beginning of the project. This study designed and implemented a native code generation framework called Nativator built as a cloud service. The framework, which is capable of producing native code for iOS and Android platforms using web-based user interfaces, was implemented based on an open source compiler platform called “Roslyn”. Four case studies were performed to analyze the execution performance of the applications built with the proposed framework. The experimental results demonstrated that the execution performance of the applications built with Nativator is comparable with the applications generated via the state-of-the-art mobile application development framework called Xamarin. Because this framework was implemented as a cloud service, it has several advantages over traditional approaches such as access from anywhere, no installation and flexible and more resources from cloud infrastructure.",
        "keywords": "Cross-platform framework; cross-platform mobile application development tool; cloud computing; code generation; Rosyln",
        "released": 2019,
        "link": "https://doi.org/10.1142/S0218194019500128"
    },
    {
        "title": "A software reliability assessment method based on neural networks for distributed development environment",
        "abstract": "The software development environment has been changing into a new development paradigm in CSSs (client/server systems) by the use of network computing technologies. Therefore, it is important to assess the reliability of software system in a distributed development environment because of increasing demands on quality and productivity in social systems. However, it is difficult to assess software reliability in recent years because the complexity of software systems has been increasing as a result of distributed system development. In this paper, we propose a software reliability assessment method considering the interaction among software components in the system-testing phase of a distributed development environment. (C) 2003 Wiley Periodicals, Inc. Electron Comm Jpn Pt 3, 86(11): 13-20, 2003; Published online in Wiley InterScience (www.interscience.wiley. com). DOI 10.1002/ecjc.10092.",
        "keywords": "distributed development environment; software reliability; reliability assessment; software component; neural network",
        "released": 2003,
        "link": "https://doi.org/10.1002/ecjc.10092"
    },
    {
        "title": "Biogem: An effective tool-based approach for scaling up open source software development in bioinformatics",
        "abstract": "Biogem provides a software development environment for the Ruby programming language, which encourages community-based software development for bioinformatics while lowering the barrier to entry and encouraging best practices. Biogem, with its targeted modular and decentralized approach, software generator, tools and tight web integration, is an improved general model for scaling up collaborative open source software development in bioinformatics.",
        "keywords": "",
        "released": 2012,
        "link": "https://doi.org/10.1093/bioinformatics/bts080"
    },
    {
        "title": "Lexicon-based bot-aware public emotion mining and sentiment analysis of the nigerian 2019 presidential election on twitter",
        "abstract": "Online social networks have been widely engaged as rich potential platforms to predict election outcomes’ in several countries of the world. The vast amount of readily-available data on such platforms, coupled with the emerging power of natural language processing algorithms and tools, have made it possible to mine and generate foresight into the possible directions of elections’ outcome. In this paper, lexicon-based public emotion mining and sentiment analysis were conducted to predict win in the 2019 presidential election in Nigeria. 224,500 tweets, associated with the two most prominent political parties in Nigeria, People’s Democratic Party (PDP) and All Progressive Congress (APC), and the two most prominent presidential candidates that represented these parties in the 2019 elections, Atiku Abubakar and Muhammadu Buhari, were collected between 9th October 2018 and 17th December 2018 via the Twitter’s streaming API. tm and NRC libraries, defined in the “R” integrated development environment, were used for data cleaning and preprocessing purposes. Botometer was introduced to detect the presence of automated bots in the preprocessed data while NRC Word Emotion Association Lexicon (EmoLex) was used to generate distributions of subjective public sentiments and emotions that surround the Nigerian 2019 presidential election. Emotions were grouped into eight categories (sadness, trust, anger, fear, joy, anticipation, disgust, surprise) while sentiments were grouped into two (negative and positive) based on Plutchik’s emotion wheel. Results obtained indicate a higher positive and a lower negative sentiment for APC than was observed with PDP. Similarly, for the presidential aspirants, Atiku has a slightly higher positive and a slightly lower negative sentiment than was observed with Buhari. These results show that APC is the predicted winning party and Atiku as the most preferred winner of the 2019 presidential election. These predictions were corroborated by the actual election results as APC emerged as the winning party while Buhari and Atiku shared very close vote margin in the election. Hence, this research is an indication that twitter data can be appropriately used to predict election outcomes and other offline future events. Future research could investigate spatiotemporal dimensions of the prediction.",
        "keywords": "Nigeria; 2019 presidential_election; bots-awareness; EmoLex; lexicon_analysis; public_opinion; emotion_mining; sentiment_analysis; twitter; APC; PDP; win_prediction; muhammadu_buhari; atiku_abubaka",
        "released": 2019,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000499999700047"
    },
    {
        "title": "DARP: Java-based data analysis and rapid prototyping environment for distributed high performance computations",
        "abstract": "The integration of a compiled and interpreted HPF gives us an opportunity to design a powerful application development environment targeted for high performance parallel and distributed systems. This Web based system follows a three-tier model. The Java front-end holds proxy objects which can be manipulated with an interpreted Web client (a Java applet) interacting dynamically with compiled code through a tier-2 server. Although targeted for the HPF back-end, the system’s architecture is independent of the back-end language, and can be extended to support other high performance languages. (C) 1998 John Wiley & Sons, Ltd.",
        "keywords": "",
        "released": 1998,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000077644300027"
    },
    {
        "title": "Service matching in agent systems",
        "abstract": "The problem of service and resource matching is being actively discussed currently as a new challenging task for the next generation of semantic discovery approaches for Web services and Web agents. A significant advantage is expected when using an ontological approach to semantically describe and query services. A matchmaking problem arises when a service is being queried and it includes the distance measure between the required service description and the one from the service registry. We realized the need to analyze the applicability of different matchmaking methods to agent development tools when implemented according to agent technology specifications such as FIPA. We consider three main groups of cases: matchmaking between classes of service profiles in pure taxonomies, matchmaking between classes in faceted taxonomies, and matchmaking between instances of faceted taxonomies.",
        "keywords": "agent technology; ontology; service matching; similarity",
        "released": 2006,
        "link": "https://doi.org/10.1007/s10489-006-9655-4"
    },
    {
        "title": "A formal modeling tool for exploratory modeling in software development",
        "abstract": "The software development process is front-loaded when formal specification is deployed and as a consequence more problems are identified and solved at an earlier point of time. This places extra importance on the quality and efficiency of the different formal specification tasks. We use the term “exploratory modeling” to denote the modeling that is conducted during the early stages of software development before the requirements are clearly understood. We believe tools that support not only rigorous but also flexible construction of the specification at the same time are helpful in such exploratory modeling phases. This paper presents a web-based IDE named VDMPad to demonstrate the concept of exploratory modeling. VDMPad has been evaluated by experienced professional VDM engineers from industry. The positive evaluation resulting from such industrial users are presented. It is believed that flexible and rigorous tools for exploratory modeling will help to improve the productivity of the industrial software developments by making the formal specification phase more efficient.",
        "keywords": "lightweight formal methods; formal specification; VDM; integrated development environment",
        "released": 2017,
        "link": "https://doi.org/10.1587/transinf.2016FOP0003"
    },
    {
        "title": "<I>let’s HPC</i>: A web-based platform to aid parallel, distributed and high performance computing education",
        "abstract": "Let’s HPC (www.letshpc.org) is an evolving open-access web-based platform to supplement conventional classroom oriented High Performance Computing (HPC) and Parallel & Distributed Computing (PDC) education. This platform has been developed to allow users to learn, evaluate, teach and see the performance of parallel algorithms from a system’s viewpoint. The Let’s HPC platform’s motivation comes from the experiences of teaching HPC/PDC courses and it is designed to help streamline the process of analyzing parallel programs. At the heart of this platform is a database archiving the performance and execution environment related data of standard parallel algorithm implementations run on different computing architectures using different programming environments. The online plotting and analysis tools of our platform can be combined seamlessly with the database to aid self-learning, teaching, evaluation and discussion of different HPC related topics, with a particular focus on a holistic system’s perspective. The user can quantitatively compare and understand the importance of numerous deterministic as well as non deterministic factors of both the software and the hardware that impact the performance of parallel programs. Instructors of HPC/PDC related courses can use the platform’s tools to illustrate the importance of proper data collection and analysis in understanding factors impacting performance as well as to encourage peer learning among students. Scripts are provided for automatically collecting performance related data, which can then be analyzed using the platform’s tools. The platform also allows students to prepare a standard lab/project report aiding the instructor in uniform evaluation. The platform’s modular design enables easy inclusion of performance related data from contributors as well as addition of new features in the future. This paper summarizes the background and motivation behind the Let’s HPC project, the design philosophy of the platform, the present capabilities of the platform, as well as the plans for future developments. (C) 2018 Elsevier Inc. All rights reserved.",
        "keywords": "HPC education; Parallel & distributed programming; Performance analyzer; Multicore architecture; HPC database",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.jpdc.2018.03.001"
    },
    {
        "title": "Building voice applications from web content",
        "abstract": "Using voice to access on-line information from the web would be really useful, because of the proliferation of mobile devices which allow Internet access anytime and anywhere. However, vocal interface is sequential and not persistent, and thus, we have to restructure the information in order to achieve an efficient and natural way of interaction. Our proposal is based on converting original web contents into VoiceXML dialogues, using VoiceXML templates and extraction rules written in XSLT. Our system has two main components: a development tool to build voice applications and a transcoding server to access them. We have identified five typical HTML patterns and designed a way to browse them using voice.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000224026300074"
    },
    {
        "title": "The virtual kidney: An eScience interface and grid portal",
        "abstract": "The Virtual Kidney uses a web interface and distributed computing to provide experimental scientists and analysts with access to computational simulations and knowledge databases hosted in geographically separated laboratories. Users can explore a variety of complex models without requiring the specific programming environment in which applications have been developed. This initiative exploits high-bandwidth communication networks for collaborative research and for shared access to knowledge resources. The Virtual Kidney has been developed within a specialist community of renal scientists but is transferable to other areas of research requiring interaction between published literature and databases, theoretical models and simulations and the formulation of effective experimental designs. A web-based three-dimensional interface provides access to experimental data, a parameter database and mathematical models. A multi-scale kidney reconstruction includes blood vessels and serially sectioned nephrons. Selection of structures provides links to the database, returning parameter values and extracts from the literature. Models are run locally or remotely with a Grid resource broker managing scheduling, monitoring and visualization of simulation results and application, credential and resource allocation. Simulation results are viewed graphically or as scaled colour gradients on the Virtual Kidney structures, allowing visual and quantitative appreciation of the effects of simulated parameter changes.",
        "keywords": "kidney modelling; computational biology; three-dimensional anatomical visualization; Grid computing; Physiome; cyberinfrastructure",
        "released": 2009,
        "link": "https://doi.org/10.1098/rsta.2008.0291"
    },
    {
        "title": "A distance e-learning platform for signal analysis and measurement using FFT",
        "abstract": "The fast Fourier transform (FFT) and the power spectrum are still powerful tools for analyzing and measuring both stationary and transient signals in power systems. However, the misapplications of FFT can lead to incorrect results caused by some problems such as aliasing effect, spectral leakage, and picket-fence effect. Measurement errors can be efficiently reduced by understanding fundamentals of FFT as well as loading proper windows. This article develops a distance e-learning environment using a graphical programming tool to help electrical students and engineers for enhancing their signal analysis capability via the Internet Explorer (IE). Critical issues pertaining to engineering education, such as programming design of signal analysis. Internet connection, expected learning outcomes, and course evaluation, are discussed in detail. (C) 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 19: 71-80, 2011; View this article online at wileyonlinelibrary.com; DOI 10.1002/cae.20292",
        "keywords": "FFT; power system harmonics; VI; THD; IE",
        "released": 2011,
        "link": "https://doi.org/10.1002/cae.20292"
    },
    {
        "title": "Utilizing SIMULINK in modeling and simulation of generalized chaotic systems with multiple nonlinear functions",
        "abstract": "Some chaotic systems can be realized with different nonlinear functions. These systems consist of a fixed main system block and a changeable nonlinear function block. Studying with these generalized systems is very useful from the educational point of view including general modeling and design issues of chaotic systems. SIMULINK, a graphical programming tool, offers a very good environment for dynamic modeling of such generalized chaotic systems. This paper presents generalized two chaotic systems, which can be utilized in chaotic system modeling for engineering applications and the use of SIMULINK in dynamical modeling and simulation of these generalized systems which can be realized with multiple nonlinear functions. The proposed models have been integrated into undergraduate nonlinear circuits and systems course at Erciyes University, Kayseri, Turkey. (C) 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 18: 684-693, 2010; View this article online at wileyonlinelibrary.com; DOI 10.1002/cae.20273",
        "keywords": "chaotic systems; nonlinear function; education; SIMULINK; modeling and simulation",
        "released": 2010,
        "link": "https://doi.org/10.1002/cae.20273"
    },
    {
        "title": "The NASA astrophysics data system: The search engine and its user interface",
        "abstract": "The ADS Abstract and Article Services pros ide access to the astronomical literature through the World Wide Web (WWW). The forms based user interface provides access to sophisticated searching capabilities that allow our users to find references in the fields of Astronomy, Physics/Geophysics, and astronomical Instrumentation and Engineering. The returned information includes links to other an-line information sources, creating an extensive astronomical digital library. Otter interfaces to the ADS databases provide direct access to the ADS data to allow developers of other data systems to integrate our data into their system. The search engine is a custom-built software system that is specifically tailored to search astronomical references. It includes an extensive synonym list that contains discipline specific knowledge about search term equivalences. Search request logs show the usage pattern of the various search system capabilities. Access logs show the world-wide distribution of ADS users. The ADS can be accessed at: http://adswww.harvard.edu.",
        "keywords": "methods : data analysis; databases : misc; publications, bibliography",
        "released": 2000,
        "link": "https://doi.org/10.1051/aas:2000171"
    },
    {
        "title": "Neo-fuzzy integrated adaptive decayed brain emotional learning network for online",
        "abstract": "Adaptive decayed brain emotional learning (ADBEL) network is recently proposed for the online time series forecasting problems. As opposed to other popular learning networks, such as multilayer perceptron, adaptive neuro-fuzzy inference system, and locally linear neuro-fuzzy model, ADBEL network offers lower computational complexity and fast learning, which make it an ideal candidate for the time series prediction in an online fashion. In fact, these prominent features are inherited from the mechanism employed by the limbic system of the mammalian brain in processing the external stimuli, which also forms the basis of the ADBEL network. This paper aims at further enhancing the forecasting performance of the ADBEL network through its integration with a neo-fuzzy network. The selection of the neo-fuzzy network is made as it offers features required for online prediction in real time environments including simplicity, transparency, accuracy, and lower computational complexity. Furthermore, this integration is only considered in the orbitofrontal cortex section of the ADBEL network and only three membership functions are employed to realize the neo-fuzzy neuron. Thus, the resultant neo-fuzzy integrated ADBEL (NF-ADBEL) network is still simple and can be deployed in online prediction problems. Few chaotic time series namely the Mackey glass, Lorenz, Rossler, and the Disturbance storm time index as well as the Narendra dynamic plant identification problem are used to evaluate the performance of the proposed NF-ADBEL network in terms of the root mean squared error and correlation coefficient criterions using MATLAB (R) programming environment.",
        "keywords": "Brain emotional decayed learning; neo-fuzzy network; chaotic time series; dynamic plant identification; MATLAB.",
        "released": 2017,
        "link": "https://doi.org/10.1109/ACCESS.2016.2637381"
    },
    {
        "title": "Enhance composed image retrieval via multi-level collaborative localization and semantic activeness perception",
        "abstract": "Composed image retrieval (CIR) is an emerging and challenging research task that combines two modalities, a reference image, and a modification text, into one query to retrieve the target image. In online shopping scenarios, the user would use the modification text as feedback to describe the difference between the reference and the desired image. In order to handle the task, there must be two main problems needed to be addressed. One is the localization problem: how to precisely find those spatial areas of the image mentioned by the text. The other is the modification problem: how to effectively modify the image semantics based on the text. However, existing methods merely fuse information coarsely from the two-modality, while the accurate spatial and semantic correspondence between these two heterogeneous features tends to be neglected. Therefore, image details cannot be precisely located and modified. To this end, we consider integrating information from the two modalities more accurately from spatial and semantic aspects. Thus, we propose an end-to-end framework for the CIR task, which contains three key components, i.e., Multi-level Collaborative Localization module (MCL), Differential Semantics Discrimination module (DSD), and Image Difference Enhancement constraints (IDE). Specifically, to solve the localization problem, MCL precisely locates the text to the image areas by collaboratively using text positioning information on multiple image layers. For the modification problem, DSD builds a distribution to evaluate the modification possibility of each image semantic dimension, and IDE effectively learns the modification patterns of text against image embedding based on the distribution. Extensive experiments on three datasets show that the proposed method achieves outstanding performance against the SOTA methods.",
        "keywords": "Semantics; Location awareness; Task analysis; Image retrieval; Training; Collaboration; Transformers; Composed image retrieval; multi-modal fusion and embedding; multi-modal representation learning; multi-modal retrieval; image retrieval",
        "released": 2024,
        "link": "https://doi.org/10.1109/TMM.2023.3273466"
    },
    {
        "title": "An overview of web effort estimation",
        "abstract": "A cornerstone of Web project management is sound effort estimation, the process by which effort is predicted and used to determine costs and allocate resources effectively, thus enabling projects to be delivered on time and within budget. Effort estimation is a complex domain where the causal relationship among factors is nondeterministic with an inherently uncertain nature. For example, assuming there is a relationship between development effort and developers’ experience using the development environment, it is not necessarily true that higher experience will lead to decreased effort. However, as experience increases so does the probability of decreased effort. The objective of this chapter is to provide an introduction to the process of estimating effort, discuss existing techniques used for effort estimation, and explain how a Web company can take into account the uncertainty inherent to effort estimation when preparing a quote. Therefore, this chapter is aimed to provide Web companies, researchers, and students with an introduction to the topic of Web effort estimation.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1016/S0065-2458(10)78005-0"
    },
    {
        "title": "Human alterations of the global floodplains 1992-2019",
        "abstract": "Floodplains provide critical ecosystem services; however, loss of natural floodplain functions caused by human alterations increase flood risks and lead to massive loss of life and property. Despite recent calls for improved floodplain protection and management, a comprehensive, global-scale assessment quantifying human floodplain alterations does not exist. We developed the first publicly available global dataset that quantifies human alterations in 15 million km(2) floodplains along 520 major river basins during the recent 27 years (1992-2019) at 250-m resolution. To maximize the reuse of our dataset and advance the open science of human floodplain alteration, we developed three web-based programming tools supported with tutorials and step-by-step audiovisual instructions. Our data reveal a significant loss of natural floodplains worldwide with 460,000 km(2) of new agricultural and 140,000 km(2) of new developed areas between 1992 and 2019. This dataset offers critical new insights into how floodplains are being destroyed, which will help decision-makers to reinforce strategies to conserve and restore floodplain functions and habitat.",
        "keywords": "",
        "released": 2023,
        "link": "https://doi.org/10.1038/s41597-023-02382-x"
    },
    {
        "title": "Treebase: An r package for discovery, access and manipulation of online phylogenies",
        "abstract": "The TreeBASE portal is an important and rapidly growing repository of phylogenetic data. The R statistical environment has also become a primary tool for applied phylogenetic analyses across a range of questions, from comparative evolution to community ecology to conservation planning. We have developed treebase, an open-source software package (freely available from http://cran.r-project.org/web/packages/treebase) for the R programming environment, providing simplified, programmatic and interactive access to phylogenetic data in the TreeBASE repository. We illustrate how this package creates a bridge between the TreeBASE repository and the rapidly growing collection of R packages for phylogenetics that can reduce barriers to discovery and integration across phylogenetic research. We show how the treebase package can be used to facilitate replication of previous studies and testing of methods and hypotheses across a large sample of phylogenies, which may help make such important reproducibility practices more common.",
        "keywords": "application programming interface; database; programmatic; R; software; TreeBASE; workflow",
        "released": 2012,
        "link": "https://doi.org/10.1111/j.2041-210X.2012.00247.x"
    },
    {
        "title": "A robust early warning system for preventing flash floods in mountainous area in vietnam",
        "abstract": "The early-warning model for flash floods is based on a hydrological and geomorphological concept connected to the river basin, with the principle that flash floods will only occur where there is a high potential risk and when rainfall exceeds the threshold. In the model used to build flash-floods risk maps, the parameters of the basin are analyzed and evaluated and the weight is determined using Thomas Saaty’s analytic hierarchy process (AHP). The flash-floods early-warning software is built using open source programming tools. With the spatial module and online processing, a predicted precipitation of one to six days in advance for iMETOS (AgriMediaVietnam) automatic meteorological stations is interpolated and then processed with the potential risk maps (iMETOS is a weather-environment monitoring system comprising a wide range of equipment and an online platform and can be used in various fields such as agriculture, tourism and services). The results determine the locations of flash floods at several risk levels corresponding to the predicted rainfall values at the meteorological stations. The system was constructed and applied to flash floods disaster early warning for Thuan Chau in Son La province when the rainfall exceeded the 150 mm/d threshold. The system initially supported positive decision-making to prevent and minimize damage caused by flash floods.",
        "keywords": "early warning; flood; flash floods; analytic hierarchy process; threshold; disaster management",
        "released": 2019,
        "link": "https://doi.org/10.3390/ijgi8050228"
    },
    {
        "title": "RUTUT: Roman urdu to urdu translator based on character substitution rules and unicode mapping",
        "abstract": "Urdu language written in English alphabets for communication is known as Roman Urdu. In pronunciation, both are the same but different in spelling and have different shapes of the alphabet. A survey acknowledges that 300 million people are speaking Urdu and about 11 million speakers in Pakistan from which maximum users prefer Roman Urdu for the textual communication. Today most of the modern technologies like computers and mobile phones using English script, due to this local Urdu user has to use English letters to type Urdu script that is Roman Urdu. In this research, Roman Urdu to Urdu Translator (RUTUT) is proposed that consists of preprocessing methods, rule-based character substitution and Unicode based character mapping techniques. It can transliterate the messages or descriptions from the Roman Urdu script to Urdu script which may help the Urdu speaker to elaborate their message in efficient manners. The focus of this research is to analyze the issues related to the Roman Urdu script to Urdu script transliteration and develop a translator based on the concepts of transliteration. This research analyzed Roman Urdu data and identified different rules-based character substitution techniques that transform the Roman Urdu into Urdu script at fundamental levels. This research is carried out using a python programming language in programming tool Anaconda in Jupiter notebook and user-friendly Graphical User Interface (GUI) created by using Tkinter library. To evaluate the RUTUT, different translational tests are performed and compare those results with famous Google online translator and ijunoon online transliteration. The analyses of results show that the proposed RUTUT approach translates accurately than Google online translator and ijunoon online transliteration.",
        "keywords": "Information retrieval; Writing; Asia; Linguistics; Media; Data mining; Standards; Roman Urdu; transliteration; rule-based approach; character substitution; unicode mapping",
        "released": 2020,
        "link": "https://doi.org/10.1109/ACCESS.2020.3031393"
    },
    {
        "title": "Lower risk of hepatocellular carcinoma with tenofovir than entecavir treatment in subsets of chronic hepatitis b patients: An updated meta-analysis",
        "abstract": "Background and Aim Previous smaller meta-analyses comparing the incidence of hepatocellular carcinoma (HCC) in chronic hepatitis B (CHB) patients treated with tenofovir disoproxil fumarate (TDF) versus entecavir (ETV) provided controversial results. This updated meta-analysis aimed to reliably identify any difference in the HCC incidence between TDF-treated or ETV-treated CHB patients in general or in specific subgroups. Methods PubMed, EMBASE, Web of Science, and Cochrane Library were systematically searched for relevant studies with hazard ratios (HRs) for HCC between TDF-treated and ETV-treated CHB patients. Retrieved dates ranged from January 2009 to October 2021. HRs with or without adjustment were pooled with random-effects model. Results Twenty-four comparative studies involving 37 771 CHB patients treated with TDF and 72 094 treated with ETV were included. TDF was associated with lower risk of HCC compared with ETV, with pooled unadjusted HR of 0.76 (95% confidence interval [CI]: 0.67-0.86) (24 studies) and adjusted HR of 0.81 (95% CI: 0.72-0.91) (21 studies). In propensity score matching cohorts, the TDF superiority was confirmed for unadjusted HR 0.83 (95% CI: 0.71-0.97) (14 studies) and was close to significance for adjusted HR (0.78, 95% CI: 0.58-1.04) (8 studies). Subgroup analyses showed that TDF was associated with lower HCC risk than ETV treatment in CHB patients who were from Asia (adjusted HR: 0.76, 95% CI: 0.66-0.87; 15 studies) or nucleos(t)ide naive (adjusted HR:0.74, 95% CI: 0.65-0.84; 18 studies). Conclusion Current evidence from a sizable population suggests that TDF is associated with significantly lower HCC risk compared with ETV treatment in patients who are from Asia and/or nucleos(t)ide naive.",
        "keywords": "Chronic hepatitis B; Entecavir; Hepatocellular carcinoma; Tenofovir",
        "released": 2022,
        "link": "https://doi.org/10.1111/jgh.15783"
    },
    {
        "title": "Cloud data scraping for the assessment of outflows from dammed rivers in the EU. A case study in south eastern europe",
        "abstract": "The availability of environmentally related data is a crucial issue in simulating natural processes. The proposed research demonstrates that although currently there is a plethora of data published on the internet, by national and international official sources, their retrieval is sometimes hard to be achieved; hence smart programming tools/technologies could facilitate the automatic harvesting of these data in personal databases for the qualitative and quantitative assessment of hydrosystems that encompass hydropower schemes. The implemented methodology relies on the use of custom-made web scrapping tools to access dams’ hourly outflows in combination with water demand data in order to investigate the water balance, as modified by human intervention, in areas located downstream of dams. For this purpose, derivatives of the Electricity Market Directive and Water Framework Directive of the European Union (EU) have been regarded as web-based databases. The outcomes of the methodological approach demonstrate that the modelled net water discharges downstream-i.e., the river discharges after subtracting the water demands-are assessed with high simulation accuracy at a daily time scale. The reliability of the output estimates is further supported by the combination of measured data from gauge telemetry with the data derived by the proposed methodology. This research allows new insights, such as dams’ outflows and water demand retrieval and exploitation, into the sustainable management of water resources and contributes to the quantification of rivers’ outflows to the coastal zone.",
        "keywords": "Aliakmonas River; dams&#8217; water outflows; electricity market directive; river basin management plans; web scraping",
        "released": 2020,
        "link": "https://doi.org/10.3390/su12197926"
    },
    {
        "title": "iJADE web-miner: An intelligent agent framework for internet shopping",
        "abstract": "There is growing interest in using intelligent software agents for a variety of tasks, including navigating and retrieving information from the Internet and from databases, online shopping activities, user authentication, negotiation for resources, and decision making. This paper proposes an integrated framework for information retrieval and information filtering in the context of Internet shopping. The work focuses on applying agent technology, together with Web mining technology, to automate a series of product search and selection activities. It is based on a multiagent development platform, namely, iJADE (Intelligent Java Agent Development Environment), which supports various e-commerce applications. The framework comprises an automatic facial authentication utility and six other modules, namely, customer requirements definition, a requirement-fuzzification scheme, a fuzzy agents-negotiation scheme, a fuzzy product-selection scheme, a product-defuzzification scheme, and a product-evaluation scheme. A series of experiments were carried out and favorable results were produced in executing the framework. From an experimental point of view, we used a database of 1,020 facial,images that were obtained under various conditions of facial expression, viewing perspective and size. An overall correct recognition rate of over 85 percent was attained. For the product selection test of our fuzzy shopper system, an average matching rate of more than 81 percent was achieved.",
        "keywords": "iJADE Web miner; Web mining; visual data mining; e-commerce; intelligent agents",
        "released": 2004,
        "link": "https://doi.org/10.1109/TKDE.2004.1269670"
    },
    {
        "title": "Integrated GUI based software for data acquisition and analysis of electrochemical gas sensors",
        "abstract": "A integrated software for data acquisition and online analysis of electrochemical gas sensor using Visual Basic 4.0 as front tool under Win 95 environment is designed and implemented. The features of the software includes high performance windowing, deluxe tool kit for acquisition, analysis and graphics, menu driven monitoring with display and online integration of various events. It is user friendly. with capability to build and execute customised applications. The software supports good number of hardware which are useful for any research and development environment involved with the sensor related activities.",
        "keywords": "",
        "released": 1999,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000081448500030"
    },
    {
        "title": "A novel aspect-oriented BPEL framework for the dynamic enforcement of web services security",
        "abstract": "In this paper, we propose a new framework for the dynamic enforcement of composite Web services security, which is based on a synergy between Aspect-Oriented Programming (AOP) and BPEL (Business Process Execution Language). This synergy is achieved through the elaboration of a new language called AspectBPEL, which is used to specify security policies as separate components, referred to as aspects, to be weaved systematically in a BPEL process. The injected aspects activate the security policies at runtime on specific join points. Our approach enjoys several additional features such as (1) separating the business and security concerns of composite Web services (2) allowing the update of security mechanisms of composite Web services at run time, (3) providing modularity for modeling cross-cutting concerns between Web services, (4) centralising some security measurements at the BPEL side and (5) providing a framework fully compatible with any BPEL engine regardless of the adopted development environment.",
        "keywords": "web services; BPEL; security; AOP; RBAC",
        "released": 2012,
        "link": "https://doi.org/10.1504/IJWGS.2012.051526"
    },
    {
        "title": "Implementing and assessing computational modeling in introductory mechanics",
        "abstract": "Students taking introductory physics are rarely exposed to computational modeling. In a one-semester large lecture introductory calculus-based mechanics course at Georgia Tech, students learned to solve physics problems using the VPython programming environment. During the term, 1357 students in this course solved a suite of 14 computational modeling homework questions delivered using an online commercial course management system. Their proficiency with computational modeling was evaluated with a proctored assignment involving a novel central force problem. The majority of students (60.4%) successfully completed the evaluation. Analysis of erroneous student-submitted programs indicated that a small set of student errors explained why most programs failed. We discuss the design and implementation of the computational modeling homework and evaluation, the results from the evaluation, and the implications for computational instruction in introductory science, technology, engineering, and mathematics (STEM) courses.",
        "keywords": "",
        "released": 2012,
        "link": "https://doi.org/10.1103/PhysRevSTPER.8.020106"
    },
    {
        "title": "Repositioning HIV protease inhibitors and nucleos(t)ide RNA polymerase inhibitors for the treatment of SARS-CoV-2 infection and COVID-19",
        "abstract": "Aims SARS-CoV-2 is a single-stranded RNA virus which is part of the ss-coronavirus family (like SARS 2002 and MERS 2012). The high prevalence of hospitalization and mortality, in addition to the lack of vaccines and therapeutics, forces scientists and clinicians around the world to evaluate new therapeutic options. One strategy is the repositioning of already known drugs, which were approved drugs for other indications. Subject and method SARS-CoV-2 entry inhibitors, RNA polymerase inhibitors, and protease inhibitors seem to be valuable targets of research. At the beginning of the pandemic, the webpage listed n=479 clinical trials related to the antiviral treatment of SARS-CoV-2 (01.04.2020, “SARS-CoV-2,” “COVID-19,” “antivirals,” “therapy”), of which n=376 are still accessible online in January 2021 (10.01.2021). Taking into account further studies not listed in the CTG webpage, this narrative review appraises HIV protease inhibitors and nucleos(t)ide RNA polymerase inhibitors as promising candidates for the treatment of COVID-19. Results Lopinavir/ritonavir, darunavir/cobicistat, remdesivir, tenofovir-disoproxilfumarate, favipriravir, and sofosbuvir are evaluated in clinical studies worldwide. Study designs show a high variability and results often are contradictory. Remdesivir is the drug, which is deployed in nearly 70% of the reviewed clinical trials, followed by lopinavir/ritonavir, favipiravir, ribavirine, and sofosbuvir. Discussion This review discusses the pharmacological/clinical background and questions the rationale and study design of clinical trials with already approved HIV protease inhibitors and nucleos(t)ide RNA polymerase inhibitors which are repositioned during the SARS-CoV-2 pandemic worldwide. Proposals are made for future study design and drug repositioning of approved antiretroviral compounds.",
        "keywords": "Repositioning drugs; SARS-CoV-2; HIV protease inhibitors; RNA polymerase inhibitors",
        "released": 2021,
        "link": "https://doi.org/10.1007/s00228-021-03108-x"
    },
    {
        "title": "Device independent web applications - the author once - display everywhere approach",
        "abstract": "Building web applications for mobile and other non-desktop devices using established methods often requires a tremendous development effort. One of the major challenges is to find sound software engineering approaches enabling the cost efficient application development for multiple devices of varying technical characteristics. A new approach is to single author web content in a device independent markup language, which gets then adapted to meet the special characteristics of the accessing device. This paper describes our approach to single authoring, which was developed in the course a large European research project. The project has developed a device-independent language profile based on XHTML 2.0 and implemented a compliant rendering engine. We focus on layout and pagination capabilities of the RIML (Renderer Independent Markup Language) and show how authors can be assisted by development tools supporting device independent authoring.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000223024600031"
    },
    {
        "title": "A recursive method for estimating missing data in spatio-temporal applications",
        "abstract": "Missing data is a major data reliability problem in spatio-temporal (ST) applications. This article proposes an online method for estimating missing data in case of a network of n sensors. The true sensor value at a specific location is expressed using an integro-difference equation. The Karhunen-Loeve Expansion of the spatial process allows one to represent the ST field values at n locations in the form of a linear state-space model. The parameters of the model are identified using the maximum likelihood method. The parameters are updated in a rolling window approach. Whenever missing data are encountered, the algorithm predicts the missing observations based on the constrained solution of state evolution equation. The constrained solution is obtained by representing the optimal state as the orthogonal sum decomposition of a deterministic and a stochastic component. The utility of the algorithm is presented on two sensor network datasets.",
        "keywords": "Integro-difference equation (IDE); missing data; online estimation; sensor network; spatio-temporal (ST)",
        "released": 2022,
        "link": "https://doi.org/10.1109/TII.2021.3100501"
    },
    {
        "title": "Frequency analysis and synthesis using a gammatone filterbank",
        "abstract": "This paper describes an efficient implementation of the 4th-order linear Gammatone filter [1, 2] based on an impulse-invariant, all-pole design. A linear auditory filterbank is constructed from these filters, which has been used in several applications involving computational auditory peripheral filtering [3, 4, 5, 6, 7, 8, 9]. Additionally, a novel approach to the recombination of the Gammatone filterbank output is introduced that allows for the resynthesis of the signal with a total time delay of 4 ms. The signal reconstruction is nearly perfect, i.e., the difference between input and reconstructed output is barely audible. A detailed technical description of the analysis-synthesis system is given and an implementation using the Matlab programming environment is introduced, which is available online. A possible application of the analysis/synthesis system introduced here is speech processing for hearing aids.",
        "keywords": "",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000177715300015"
    },
    {
        "title": "Overview and architecture of the java integration framework, hybrid scheduler, and web-enabled LIMS",
        "abstract": "Most of the scheduling software and instrument integration frameworks are written in Visual Basic, C/C++, or the LabView programming environment. A lot of these frameworks are proprietary tools of instrument vendors and are used by these companies during system integration of their instruments. In addition to the closed architecture of these products, the scheduler choice is very limited. ReTiSoft Inc. has created a suite of software products that address these problems. In this article we would like to introduce ReTiSoft’s open-architecture framework for instrument integration, a hybrid scheduler (static and dynamic) and a Web-enabled interface to the automated system. In addition to ReTiSoft’s integration framework (Genera) and the hybrid-scheduling software (Supra), we recently developed a Web-enabled application that allows scientists to log onto the automated system remotely, set up and run assays, examine and analyze the data produced during the experiment. The software is called DataPilot and is comprised of a high-performance database engine and the Apache Web server. In unison with Genera’s and Supra’s open-architecture approach, DataPilot can be modified and customized by system integrators to suit their specific application needs. The application serves as a data repository and adheres to guidelines presented by the Code of Federal Regulations for electronic records and electronic signatures; the guidelines are known as 21 CFR Part II. This article provides an architectural overview of our software products and justifies its merits in comparison to other technologies commonly used in laboratories. We describe our Genera integration framework, give an overview of our scheduling algorithms, describe a Web-enabled data-tracking software and the enzyme-linked immunosorbent assay (ELISA assay), and describe different methods of automated system validation using our software. We also offer some conclusions. (JALA 2004;9:411-20)",
        "keywords": "scheduler; automation; robotics; integration; validation; LIMS; 21 CFR Part II; data-tracking; open architecture; Java; ELISA",
        "released": 2004,
        "link": "https://doi.org/10.1016/j.jala.2004.09.004"
    },
    {
        "title": "Simulation and experimental validation of the interplay between dielectrophoretic and electroosmotic behavior of conductive and insulator particles for nanofabrication and lab-on-chip applications",
        "abstract": "In this article, we investigated the different roles of dielectrophoresis (DEP) and AC electroosmosis (ACEO) phenomena in manipulating conductive and insulator particles as future fabrication and deposition technique. A computational model was implemented using the finite element method (FEM) to better understand the interplay and velocities caused by these two electrokinetic phenomena. The simulation results showed that Carbon Nanotubes (CNTs) experience positive DEP (pDEP) up to frequencies of 1 GHz, while polystyrene (PS) particles only showed negative DEP (nDEP) regardless of the frequency. The velocity due to the ACEO was significant at the electrodes’ level at low frequencies, especially between 100 Hz and 10 kHz. The simulation results were validated experimentally, where CNTs and PS were dispersed in a medium and subjected to non-uniform electric fields. The behavior and patterns the particles formed on planner interdigitated electrodes (IDEs) match with their trajectories from the simulation. At a frequency of 1 MHz, CNTs were attracted to regions with strong electric fields, while PS particles were repelled to regions with low electric fields indicating strong pDEP and nDEP, respectively. A cloud of particles was observed at 5 kHz, indicating strong ACEO at low frequencies. This study provides a simplified and reliable technique that can be applied to manipulate different types of particles for future multilayer fabrication, thin film deposition, and lap-on-chip applications.",
        "keywords": "Dielectrophoresis; Electroosmotic; Carbon nanotube; Polystyrene; Fabrication; Deposition",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.colsurfa.2023.131065"
    },
    {
        "title": "Evolution of increased volcanic activity in arjuno-welirang based on LST analysis of landsat 8 satellite imagery using GEE cloud computing",
        "abstract": "Typically, monitoring the volcanic activity of a volcano is carried out using volcanic seismic methods. However, this method is technically less flexible. Volcano seismic data is not freely available. Access to these data centers must be authorized by the data authority. Therefore, it is necessary to use other methods as an alternative. The alternative method used in this study is remote sensing using the Landsat 8 satellite sensors. Landsat 8 satellite imagery data can be freely accessed and easily downloaded. Landsat 8 image analysis is implemented with Google Earth Engine (GEE). GEE is a remote sensing image analysis programming tool with a cloud computing platform. The GEE programming implementation is open source. With GEE, the evolution of Arjuno-Welirang volcanic activity can be monitored accurately. The use of GEE with a cloud computing platform also makes it easier to process large remote sensing data because the downloaded file’s size is unlimited. GEE has successfully conducted an LST analysis on Landsat 8 satellite imagery of the Arjuno-Welirang complex area in the 2016-2021 range. The LST calculation is performed by adding the surface emissivity correction obtained based on the NDVI value. According to the results of the LST calculations that have been obtained, the surface temperature in the Arjuna Welirang Crater area experienced the highest increase in 2018, reaching 33.94 oC, with a larger contour size of thermal distribution image than the others. This increase in thermal based volcanic activity is in accordance with the increase in seismic activity monitored by the VSI (Volcanological Survey of Indonesia).",
        "keywords": "GEE; NDVI; LST; Arjuno-Welirang; Landsat 8; VSI",
        "released": 2023,
        "link": "https://doi.org/10.30880/ijie.2023.15.07.018"
    },
    {
        "title": "LITERATE PROGRAMMING AND THE LIPED ENVIRONMENT",
        "abstract": "Literate programming is a methodology that encourages the production of a program whose primary purpose is to explain to a human what it does, rather than to instruct a computer what to do. Each program element is clearly explained, and is presented in an order that is best for human understanding. The writer has the freedom to introduce parts of the program as they are needed-which is not necessarily the order required for compilation. The philosophy of literate programming was introduced by Donald Knuth while developing the documentation System TEX. His WEB system consists of two processes, WEAVE and TANGLE, that read a specially constructed literate program source file and produce as output a file containing compilable code and a file for input into TEX. WEB uses a batch approach that seems to hinder the development of new literate programs, and it has not been widely used outside its home base. The literate programming environment LIPED, described in this paper, aims to make the development of literate programs easier by being interactive (rather than batch as is WEB) and by providing instant access to a table of contents, a cross-reference table, and the extracted code. Language independence is achieved, and special facilities are made available to cater for modem programming paradigms. The system runs on minimal hardware and interfaces easily to existing compilers and text processors. This paper describes the background and progress of literate programming, compares the available literate programming systems, and provides an overview of LIPED.",
        "keywords": "LITERATE PROGRAMMING; PROGRAMMING ENVIRONMENTS",
        "released": 1992,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:A1992GU72600003"
    },
    {
        "title": "In-IDE code generation from natural language: Promise and challenges",
        "abstract": "A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code, especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries, but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. In this article, we perform the first comprehensive investigation of the promise and challenges of using such technology inside the PyCharm IDE, asking, “At the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?” To facilitate the study, we first develop a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality, and we orchestrate virtual environments to enable collection of many user events (e.g., web browsing, keystrokes, fine-grained code edits). We ask developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Further analysis identifies several pain points that could improve the effectiveness of future machine learning-based code generation/retrieval developer assistants and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies on this topic, as well as development of better code generation models.",
        "keywords": "Natural language programming assistant; code generation; code retrieval; empirical study",
        "released": 2022,
        "link": "https://doi.org/10.1145/3487569"
    },
    {
        "title": "Guidelines for benchmarking the performance of ontology management APIs",
        "abstract": "Ontology tools performance and scalability are critical to both the growth of the Semantic Web and the establishment of these tools in the industry. In this paper, we present briefly the benchmarking methodology used to improve the performance and the scalabilily of ontology development tools. We focus on the definition of the infrastructure for evaluating the performance of these tools’ ontology management APIs in terms of its execution efficiency. We also present the results of applying the methodology for evaluating the API of the WebODE ontology engineering workbench.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000233748600020"
    },
    {
        "title": "Deep-learning-incorporated augmented reality application for engineering lab training",
        "abstract": "Deep learning (DL) algorithms have achieved significantly high performance in object detection tasks. At the same time, augmented reality (AR) techniques are transforming the ways that we work and connect with people. With the increasing popularity of online and hybrid learning, we propose a new framework for improving students’ learning experiences with electrical engineering lab equipment by incorporating the abovementioned technologies. The DL powered automatic object detection component integrated into the AR application is designed to recognize equipment such as multimeter, oscilloscope, wave generator, and power supply. A deep neural network model, namely MobileNet-SSD v2, is implemented for equipment detection using TensorFlow’s object detection API. When a piece of equipment is detected, the corresponding AR-based tutorial will be displayed on the screen. The mean average precision (mAP) of the developed equipment detection model is 81.4%, while the average recall of the model is 85.3%. Furthermore, to demonstrate practical application of the proposed framework, we develop a multimeter tutorial where virtual models are superimposed on real multimeters. The tutorial includes images and web links as well to help users learn more effectively. The Unity3D game engine is used as the primary development tool for this tutorial to integrate DL and AR frameworks and create immersive scenarios. The proposed framework can be a useful foundation for AR and machine-learning-based frameworks for industrial and educational training.",
        "keywords": "artificial intelligence; augmented reality; machine learning; object detection; computer in education; lab equipment tutorial",
        "released": 2022,
        "link": "https://doi.org/10.3390/app12105159"
    },
    {
        "title": "IMP science gateway: From the portal to the hub of virtual experimental labs in e-science and multiscale courses in e-learning",
        "abstract": "Science gateway’ (SG) ideology means a user-friendly intuitive interface between scientists (or scientific communities) and different software components + various distributed computing infrastructures (DCIs), where researchers can focus on their scientific goals and less on the peculiarities of software/DCI. G.V.Kurdyumov Institute for Metal Physics IMP Science Gateway Portal’ () is presented for complex workflow management and integration of distributed computing resources (like clusters, service grids, desktop grids, and clouds). It is created on the basis of Web Service - Parallel Grid Run-time and Application Development Environment (WS-PGRADE) and gUSE (grid and cloud User Support Environment) technologies, where WS-PGRADE is designed for science workflow operation and gUSE for smooth integration of available resources for parallel and distributed computing in various heterogeneous DCIs. Some use cases (scientific workflows) are considered for molecular dynamics simulations of complex behavior of various nanostructures. The modular approach allows scientists to use SG portals as research hubs of various virtual experimental labs in the context of practical applications in material science, physics, and nanotechnologies. In addition, workflows and their components are proposed to be used as Lego-style construction units for learning modules of various scale by duration, complexity, targeted audience, and so on. These workflows can be used also in e-Learning infrastructures as constituent elements of learning hubs for the management of learning content, tools, resources, and users in the regular, vocational, lifelong, and informal learning. Copyright (c) 2015John Wiley & Sons, Ltd.",
        "keywords": "distributed computing; science gateway; e-Science; e-Learning; lifelong learning; ubiquitous learning; grid computing; cluster; service grid; desktop grid; physics; materials science; nanotechnologies",
        "released": 2015,
        "link": "https://doi.org/10.1002/cpe.3533"
    },
    {
        "title": "Quantum multi-user broadcast protocol for the “platform as a service” model",
        "abstract": "Quantum Cloud Computing is the technology which has the capability to shape the future of computing. In “Platform as a Service (PaaS)” type of cloud computing, the development environment is delivered as a service. In this paper, a multi-user broadcast protocol in network is developed with the mode of one master and N slaves together with a sequence of single photons. It can be applied to a multi-node network, in which a single photon sequence can be sent to all the slave nodes simultaneously. In broadcast communication networks, these single photons encode classical information directly through noisy quantum communication channels. The results show that this protocol can realize the secret key generation and sharing of multiple nodes. The protocol we propose is also proved to be unconditionally secure in theory, which indicates its feasibility in theoretical application.",
        "keywords": "Quantum Cloud Platform; phase-covariant cloning; Quantum Cloning Machine; multi-user broadcast; Platform as a Service",
        "released": 2019,
        "link": "https://doi.org/10.3390/s19235257"
    },
    {
        "title": "Developing front-end web 2.0 technologies to access services, content and things in the future internet",
        "abstract": "The future Internet is expected to be composed of a mesh of interoperable web services accessible from all over the web. This approach has not yet caught on since global user-service interaction is still an open issue. This paper states one vision with regard to next-generation front-end Web 2.0 technology that will enable integrated access to services, contents and things in the future Internet. In this paper, we illustrate how front-ends that wrap traditional services and resources can be tailored to the needs of end users, converting end users into prosumers (creators and consumers of service-based applications). To do this, we propose an architecture that end users without programming skills can use to create front-ends, consult catalogues of resources tailored to their needs, easily integrate and coordinate front-ends and create composite applications to orchestrate services in their back-end. The paper includes a case study illustrating that current user-centred web development tools are at a very early stage of evolution. We provide statistical data on how the proposed architecture improves these tools. This paper is based on research conducted by the Service Front End (SFE) Open Alliance initiative. (c) 2013 Elsevier B.V. All rights reserved.",
        "keywords": "Future Internet; Internet of services; Web 2.0; Service front-ends; User-service interaction; Context; SOA",
        "released": 2013,
        "link": "https://doi.org/10.1016/j.future.2013.01.006"
    },
    {
        "title": "Security issues with functions as a service",
        "abstract": "Functions as a Service (FaaS) is an ultimate expression of cloud computing. FaaS provides a significant business value proposition to users and offloads the security issues of the platform to the provider. The user is responsible for the security of their implementation. FaaS application implementations are typically subject to OWASP top 10 vulnerabilities and require corresponding security controls. For simple implementations this is straightforward. For large FaaS applications, strong process and operational controls combined with automation are necessary to provide reasonable assurance of application security during the development and deployment process. Without these controls, particularly in DevOps/continuous development environments, maintaining security can be problematic. The issue and control approaches are reviewed.",
        "keywords": "FAA; Cloud computing; Databases; Cryptography; Application security",
        "released": 2020,
        "link": "https://doi.org/10.1109/MITP.2019.2930049"
    },
    {
        "title": "Do programmers do change impact analysis in debugging?",
        "abstract": "“Change Impact Analysis” is the process of determining the consequences of a modification to software. In theory, change impact analysis should be done during software maintenance, to make sure changes do not introduce new bugs. Many approaches and techniques are proposed to help programmers do change impact analysis automatically. However, it is still an open question whether and how programmers do change impact analysis. In this paper, we conducted two studies, one in-depth study and one breadth study. For the in-depth study, we recorded videos of nine professional programmers repairing two bugs for two hours. For the breadth study, we surveyed 35 professional programmers using an online system. We found that the programmers in our studies did static change impact analysis before they made changes by using IDE navigational functionalities, and they did dynamic change impact analysis after they made changes by running the programs. We also found that they did not use any change impact analysis tools.",
        "keywords": "Change impact analysis; Program debugging; Empirical software engineering; Software maintenance; Programmer navigation",
        "released": 2017,
        "link": "https://doi.org/10.1007/s10664-016-9441-9"
    },
    {
        "title": "A blockchain-based model for cloud service quality monitoring",
        "abstract": "This paper introduces a novel blockchain-based decentralized federation model that embodies quality verification for cloud providers who lease computing resources from each other. The blockchain structure removes the barriers of a traditional centralized federation and offers a fully distributed and transparent administration by enforcing the involved agents to maintain consensus on the data. For a blockchain-based federation, it is vital to avoid blind-trust on the claimed SLA guarantees and monitor the quality of service which is highly desirable considering the multi-tenancy characteristic of cloud services. Due to the fact that the blockchain network is unable to access the outside world, it cannot handle, by its own, providers misbehavior in terms of SLA violations. Thus, we introduce oracle as a verifier agent to monitor the quality of the service and report to the smart contract agents deployed on the blockchain. Oracle is a trusted third-party agent who can communicate with the outside world of the blockchain network. The interaction between cloud service providers (either providing a service or requesting it from another provider) and the oracle through smart contracts comprises a system of autonomous and utility maximizer agents. Cloud requesters seek to receive high quality services with constant monitoring at cheap prices or even with no charge, while cloud providers aim to have a balanced work-load with less preserved capacity, and the oracle tends to charge higher for their monitoring services. Therefore, to model this conflicting situation, we formulate a dynamic Stackelberg differential game to optimize the cost of using the oracle and maximize the profit of the agents with the role of provider agent as a leader, and the requester and verifier agents as followers. Our designed Stackelberg differential game can seize the dynamicity of users’ demand and resource provisioning in a competitive cloud market. We implemented our proposed decentralized model using the Solidity language in the remix IDE on the Ethereum network. We further evaluated the optimal controls and agents’ profit with real-world data simulated for three concrete cloud providers. The results revealed that the requester agent initiates most of the quality verification requests at the beginning to the middle time of the contract. Thus, the provider agent could reserve less computing resources considering the fact that it could share the workload among other customers’ computing resources during the peak-time. Moreover, imposing a higher penalty on the provider agent increased the capacity and decreased the number of requests for quality verification at the equilibrium. The evaluation also disclosed that the impact of timing in the dynamic pricing strategy of the verifier agent is very minimal, and the provisioning capacity of the provider is strongly correlated with the monitoring price.",
        "keywords": "Cloud computing; Monitoring; Blockchain; Smart contracts; Games; Quality of service; Smart contracts; quality verification; stackleberg differential game; cloud computing; service provider; oracle",
        "released": 2020,
        "link": "https://doi.org/10.1109/TSC.2019.2948010"
    },
    {
        "title": "Comprehensive generation, visualization, and reporting of quality control metrics for single-cell RNA sequencing data",
        "abstract": "Quality control (QC) is a crucial step in single-cell RNA-seq data analysis. Here, the authors present the SCTK-QC pipeline which generates and visualizes a comprehensive set of QC metrics to streamline the process of detecting and removing poor quality cells and other artifacts. Single-cell RNA sequencing (scRNA-seq) can be used to gain insights into cellular heterogeneity within complex tissues. However, various technical artifacts can be present in scRNA-seq data and should be assessed before performing downstream analyses. While several tools have been developed to perform individual quality control (QC) tasks, they are scattered in different packages across several programming environments. Here, to streamline the process of generating and visualizing QC metrics for scRNA-seq data, we built the SCTK-QC pipeline within the singleCellTK R package. The SCTK-QC workflow can import data from several single-cell platforms and preprocessing tools and includes steps for empty droplet detection, generation of standard QC metrics, prediction of doublets, and estimation of ambient RNA. It can run on the command line, within the R console, on the cloud platform or with an interactive graphical user interface. Overall, the SCTK-QC pipeline streamlines and standardizes the process of performing QC for scRNA-seq data.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1038/s41467-022-29212-9"
    },
    {
        "title": "A shiny app for modeling the lifetime in primary breast cancer patients through phase-type distributions",
        "abstract": "Phase-type distributions (PHDs), which are defined as the distribution of the lifetime up to the absorption in an absorbent Markov chain, are an appropriate candidate to model the lifetime of any system, since any non-negative probability distribution can be approximated by a PHD with sufficient precision. Despite PHD potential, friendly statistical programs do not have a module implemented in their interfaces to handle PHD. Thus, researchers must consider others statistical software such as R, Matlab or Python that work with the compilation of code chunks and functions. This fact might be an important handicap for those researchers who do not have sufficient knowledge in programming environments. In this paper, a new interactive web application developed with shiny is introduced in order to adjust PHD to an experimental dataset. This open access app does not require any kind of knowledge about programming or major mathematical concepts. Users can easily compare the graphic fit of several PHDs while estimating their parameters and assess the goodness of fit with just several clicks. All these functionalities are exhibited by means of a numerical simulation and modeling the time to live since the diagnostic in primary breast cancer patients.",
        "keywords": "phase-type distributions; modelling; lifetime; shiny; interactive app; breast cancer",
        "released": 2024,
        "link": "https://doi.org/10.3934/mbe.2024065"
    },
    {
        "title": "Detecting spatial patterns of rivermouth processes using a geostatistical framework for near-real-time analysis",
        "abstract": "This paper proposes a geospatial analysis framework and software to interpret water-quality sampling data from towed undulating vehicles in near-real time. The framework includes data quality assurance and quality control processes, automated kriging interpolation along undulating paths, and local hotspot and cluster analyses. These methods are implemented in an interactive Web application developed using the Shiny package in the R programming environment to support near-real time analysis along with 2-and 3-D visualizations. The approach is demonstrated using historical sampling data from an undulating vehicle deployed at three rivermouth sites in Lake Michigan during 2011. The normalized root-mean-square error (NRMSE) of the interpolation averages approximately 10% in 3-fold cross validation. The results show that the framework can be used to track river plume dynamics and provide insights on mixing, which could be related to wind and seiche events. (C) 2017 Elsevier Ltd. All rights reserved.",
        "keywords": "Water quality; Rivermouth; Plumes; Dynamic; Decision support; Adaptive sampling",
        "released": 2017,
        "link": "https://doi.org/10.1016/j.envsoft.2017.06.049"
    },
    {
        "title": "Evaluating integration and performance of containerized climate applications on a hewlett packard enterprise cray system",
        "abstract": "Containers have taken over large swaths of cloud computing as the most convenient way of packaging and deploying applications. The features that containers offer for packaging and deploying applications translate to high performance computing (HPC) as well. At The National Oceanic and Atmospheric Administration, containers provide an easy way to build and distribute complex HPC applications, allowing faster collaboration, portability, and experiment computer environment reproducibility amongst the scientific community. The challenge arises when applications rely on message passing interface (MPI). This necessitates investigation into how to properly run these applications with their own unique requirements and produce performance on par with native runs. We investigate the MPI performance for benchmarks and containerized climate models for various containers covering selection of compiler and MPI library combinations from the Cray provided programming environments on the Cray XC supercomputer GAEA. Performance from the benchmarks and the climate models shows that for the most part containerized applications perform on par with the natively built applications when the system optimized Cray MPICH libraries are bound into the container, and the hybrid model containers have poor performance in comparison. We also describe several challenges and our solutions in running these containers, particularly challenges with heterogeneous jobs for the containerized model runs.",
        "keywords": "benchmark; climate; containers; cray",
        "released": 2024,
        "link": "https://doi.org/10.1002/cpe.7966"
    },
    {
        "title": "Evaluating legacy system migration technologies through empirical studies",
        "abstract": "We present two controlled experiments conducted with master students and practitioners and a case study conducted with practitioners to evaluate the use of MELIS (Migration Environment for Legacy Information Systems) for the migration of legacy COBOL programs to the web. MELIS has been developed as an Eclipse plug-in within a technology transfer project conducted with a small software company [16]. The partner company has developed and marketed in the last 30 years several COBOL systems that need to be migrated to the web, due to the increasing requests of the customers. The goal of the technology transfer project was to define a systematic migration strategy and the supporting tools to migrate these COBOL systems to the web and make the partner company an owner of the developed technology. The goal of the controlled experiments and case study was to evaluate the effectiveness of introducing MELIS in the partner company and compare it with traditional software development environments. The results of the overall experimentation show that the use of MELIS increases the productivity and reduces the gap between novice and expert software engineers. (C) 2008 Elsevier B.V. All rights reserved.",
        "keywords": "Software migration; Empirical studies; Controlled experiments; Case studies; Technology transfer",
        "released": 2009,
        "link": "https://doi.org/10.1016/j.infsof.2008.05.012"
    },
    {
        "title": "Intelligent web service -: From web services to .plug&play. Service integration",
        "abstract": "The service oriented architecture and its implementation by Web services have reached a considerable degree of maturity and also a wide adoption in different application domains. This is true for the R&D as well as for the industrial community. Standards for the description, activation, and combination of Web services have been established; UDDI registries are in place for the management of services, and development environments support the software engineer in the creation of Web services. However, the major benefit of service oriented architectures, the loose coupling of services, is still seldom explored in real world settings. The reason is the heterogeneity on different levels within the service oriented architecture. The heterogeneity problems reach from the semantics of service descriptions to compatibility problems between workflows, which have to be connected via service interfaces. In spite of compatible service signatures, workflows might, for example, not be compatible in their semantics. This talk discusses challenges and solutions for a real Plug&Play. service infrastructure, i.e. a Web service infrastructure, where integration of new Web services becomes as simple and straightforward as plugging a USB stick into your laptop. To achieve this goal various issues have to be addressed: Semantics of services as a foundation for intelligent service mediation and usage Effective, automatic, and intelligent service discovery taking into account application context Dynamic context-aware composition of services into processes The challenges and approaches for a “Plug&Play” service infrastructure are illustrated with a real world example.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000233741400002"
    },
    {
        "title": "Use of arduino in primary education: A systematic review",
        "abstract": "In the last two decades, technological advances have been spectacular, and their transcendence has touched all areas of society. Specifically, in the field of education, these advances have allowed projects and approaches such as computational thinking to be taken up more strongly through interdisciplinary visions such as the STEM subjects and technological devices such as Arduino. The main objective of this article is to analyse the uses of Arduino and the achievements it has attained at primary-education level. To this end, a systematic review was carried out in the SCOPUS and Web of Science databases. The methodology used was the PRISMA statement and the SALSA framework. In accordance with the exclusion criteria applied, nine scientific papers from the last seven years were obtained. The qualitative software ATLAS.ti was used to extract the results. These papers reveal that the most commonly used methodology for incorporating the Arduino board into teaching is problem based learning (PBL) in the context of STEM subjects. In addition, programming environments, such as Scratch, and other electronic components have been used, which have enabled the development of computational thinking and the acquisition of technological knowledge, among other achievements.",
        "keywords": "Arduino; elementary education; ATLAS; ti; computational thinking; robots; systematic review; primary education",
        "released": 2023,
        "link": "https://doi.org/10.3390/educsci13020134"
    },
    {
        "title": "Development and validation of an instrument (CSII - brazil) to assess users’ conceptual and procedural knowledge of continuous subcutaneous infusion systems",
        "abstract": "Objective: To develop, adapt and validate an instrument named “CSII - Brazil” to assess users’ conceptual and procedural knowledge of continuous subcutaneous insulin infusion systems. Materials and methods: Methodological and exploratory study developed in three stages: a) instrument development; b) content validation and cultural adaptation (evaluation by a committee of experts and pre-test with CSII users); c) psychometric validation through instrument application in a sample of 60 patients by means of the web tool e-Surv. Internal consistency and reproducibility analyses were performed within IBM SPSS Statistics 20 programming environment. Results:The 16 multiple-choice question instrument successfully attained a content validity index of 0.97, showing satisfactory internal consistency, with 0.61 Cronbach’s alpha [95% CI 0.462-0.746] and an intraclass correlation coefficient of 0.869 [95% CI: 0.789-0.919] between the test and retest scores. Conclusion: The CSII - Brazil instrument is considered adequate and validated to assess continuous subcutaneous infusion system users’ conceptual and procedural knowledge.",
        "keywords": "Diabetes mellitus; insulin infusion system; knowledge; cultural adaptation; validation studies",
        "released": 2021,
        "link": "https://doi.org/10.20945/2359-3997000000314"
    },
    {
        "title": "The open grid computing environments collaboration: Portlets and services for science gateways",
        "abstract": "We review the efforts of the Open Grid Computing Environments collaboration. By adopting a general three-tiered architecture based on common standards for portlets and Grid Web services, we can deliver numerous capabilities to science gateways from our diverse constituent efforts. In this paper, we discuss our support for standards-based Grid portlets using the Velocity development environment. Our Grid portlets are based on abstraction layers provided by the Java CoG kit, which hide the differences of different Grid toolkits. Sophisticated services are decoupled from the portal container using Web service strategies. We describe advance information, semantic data, collaboration, and science application services developed by our consortium. Copyright (c) 2006 John Wiley & Sons, Ltd.",
        "keywords": "portlets; collaboration; Grid services",
        "released": 2007,
        "link": "https://doi.org/10.1002/cpe.1078"
    },
    {
        "title": "Protection and interoperability for mobile agents: A secure and open programming environment",
        "abstract": "The Mobile Agent technology helps in the development of applications in open, distributed and heterogeneous environments such as the Internet and the Web, but it has to answer to the requirements of security and interoperability to achieve wide acceptance. The paper focuses on security and interoperability, and describes a Secure and Open Mobile Agent (SOMA) programming environment where both requirements are main design objectives. On the one hand, SOMA is based on a thorough security model and provides a wide range of mechanisms and tools to build and enforce flexible security policies. On the other hand, the SOMA framework permits to interoperate with different application components designed with different programming styles. SOMA grants interoperability by closely considering compliance with the OMG CORBA and MASIF standards. SOMA has already shown the feasibility and effectiveness of the approach for the development of flexible and adaptive applications in several areas, particularly in network and systems management.",
        "keywords": "mobile agents; security; interoperability; CORBA; network and systems management",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000087318300011"
    },
    {
        "title": "MakeCode and CODAL: Intuitive and efficient embedded systems programming for education",
        "abstract": "Historically, embedded systems development has been a specialist skill, requiring knowledge of low-level programming languages, complex compilation toolchains, and specialist hardware, firmware, device drivers and applications. However, it has now become commonplace for a broader range of non-specialists to engage in the making (design and development) of embedded systems - including educators to motivate and excite their students in the classroom. This diversity brings its own set of unique requirements, and the complexities of existing embedded systems development platforms introduce insurmountable barriers to entry. In this paper we present the motivation, requirements, implementation, and evaluation of a new programming platform that enables novice users to create effective and efficient software for embedded systems. The platform has two major components: (1) Microsoft MakeCode (www.makecode.com), a web app that encapsulates an accessible IDE for microcontrollers; and (2) CODAL, an efficient component-oriented C++ runtime for microcontrollers. We show how MakeCode and CODAL combine to provide an accessible, cross-platform, installation-free, high level programming experience for embedded devices without sacrificing performance and efficiency.",
        "keywords": "MakeCode; CODAL; BBC micro:bit; Embedded systems; Physical computing; Visual programming; Web-based programming",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.sysarc.2019.05.005"
    },
    {
        "title": "Thirty years of server technology - from transaction processing to web services",
        "abstract": "Server technology started with transaction-processing systems in the sixties. Database Management Systems (DBMS) soon adopted mechanism like multi-process and multi-threading. In distributed systems, the remote procedure call also needed process structures at the server side. The same is true for file servers, object servers (CORBA), Web servers, application servers, EJB containers, and Web Services. All these systems support a request-response behavior, sometimes enhanced with a session concept. They are facing thousands of requests per second and must manage thousands of session contexts at the same time. While programming the applications that run on the servers and actually process the requests should be as simple as possible, efficiency must still be very high. So a general programming environment should be defined that is easy to use and, on the other hand, allows for the efficient execution of thousands of program instances in parallel. This contribution will identify mechanisms that have been developed in the context of transaction processing and database management. It will then generalize them to server processing of any kind. This includes program structures, context management, multi-tasking and multi-threading, process structures, program management, naming, and transactions. The driving force behind the discussion is to avoid the re-invention of the wheel that far too often occurs in computer science, mostly in ignorance of older and presumably outdated systems.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000231115400004"
    },
    {
        "title": "Mastering object-oriented technology using a self-learning and self-assessment system",
        "abstract": "In this article, we describe a Web-based approach that defines training needs for object-oriented developers by identifying the strong and the weak areas of their knowledge. Our system is based on the use of two tools, Guide d’Auto-Apprentissage, Self-Learning Guide (GAA) and User Knowledge Assessment Technology (UKAT) developed at the Computer Research Institute of Montreal. UKAT uses a state-of-the-art user knowledge assessment method to create a user profile of the proficiency in a subject domain. GAA is a Web-based training system that uses the UKAT to personalize a training course to facilitate self-learning. As an exemplar, we are using a lava development environment. (C) 1999 John Wiley & Sons, Inc.",
        "keywords": "Internet technology-based training; intelligent training systems; object-oriented technology; knowledge assessment systems; self-learning; just in time training; intelligent learning resources",
        "released": 1999,
        "link": "https://doi.org/10.1002/(SICI)1099-0542(1999)7:3<162::AID-CAE4>3.0.CO;2-C"
    },
    {
        "title": "A model-based method for an online diagnostic knowledge-based system",
        "abstract": "Fault diagnosis is very important for modern production technology and has received increasing theoretical and practical attention during the last few years. This paper presents a model-based diagnostic method for industrial systems. An online, real-time, deep knowledge based fault detection system has been developed by combining different development environments and tools. The system diagnoses, predicts and compensates faults by coupling symbolic and numerical data in a new environment suitable for the interaction of different sources of knowledge and has been successfully implemented and tested on a real hydraulic system.",
        "keywords": "expert systems; model based fault diagnosis; fault prediction; real time process",
        "released": 2001,
        "link": "https://doi.org/10.1111/1468-0394.00167"
    },
    {
        "title": "Subunit mass analysis for monitoring antibody oxidation",
        "abstract": "Methionine oxidation is a common posttranslational modification (PTM) of monoclonal antibodies (mAbs). Oxidation can reduce the in-vivo half-life, efficacy and stability of the product. Peptide mapping is commonly used to monitor the levels of oxidation, but this is a relatively time-consuming method. A high-throughput, automated subunit mass analysis method was developed to monitor antibody methionine oxidation. In this method, samples were treated with IdeS, EndoS and dithiothreitol to generate three individual IgG subunits (light chain, Fd’ and single chain Fc). These subunits were analyzed by reversed phase-ultra performance liquid chromatography coupled with an online quadrupole time-of-flight mass spectrometer and the levels of oxidation on each subunit were quantitated based on the deconvoluted mass spectra using the UNIFI software. The oxidation results obtained by subunit mass analysis correlated well with the results obtained by peptide mapping. Method qualification demonstrated that this subunit method had excellent repeatability and intermediate precision. In addition, UNIFI software used in this application allows automated data acquisition and processing, which makes this method suitable for high-throughput process monitoring and product characterization. Finally, subunit mass analysis revealed the different patterns of Fc methionine oxidation induced by chemical and photo stress, which makes it attractive for investigating the root cause of oxidation.",
        "keywords": "Fc; LC-MS; mass spectrometry; methionine oxidation; monoclonal antibody; posttranslational modifications; process monitoring; product characterization; subunit mass analysis",
        "released": 2017,
        "link": "https://doi.org/10.1080/19420862.2017.1279773"
    },
    {
        "title": "Constraint-sensitive privacy management for personalized web-based systems",
        "abstract": "This research aims at reconciling web personalization with primacy constraints imposed by legal restrictions and by users’ privacy preferences. We propose a software product line architecture approach, where our privacy-enabling user modeling architecture can dynamically select personalization methods that satisfy current privacy constraints to provide personalization services. A feasibility study is being carried out with the support of an existing user modeling server and a software architecture based development environment.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000231387200076"
    },
    {
        "title": "Debugging MPI grid applications using net-dbx",
        "abstract": "Application-development in Grid environments is a challenging process, thus the need for grid enabled development tools is also one that has to be ftilfilled. In our work we describe the development of a Grid Interface for the Net-dbx parallel debugger, that can be used to debug MPI grid applications. Net-dbx is a web-based debugger enabling users to use it for debugging from anywhere in the Internet. The proposed debugging architecture is platform independent, because it uses Java, and it is accessible from anywhere, anytime because it is web based. Our architecture provides an abstraction layer between the debugger and the grid middleware and MPI implementation used. This makes the debugger easily adaptable to different middlewares. The grid-enabled architecture of our debugger carries the portability and usability advantages of Net-dbx on which we have based our design. A prototype has been developed and tested.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000224955400017"
    },
    {
        "title": "BiodivAR: A cartographic authoring tool for the visualization of geolocated media in augmented reality",
        "abstract": "Location-based augmented reality technology for real-world, outdoor experiences is rapidly gaining in popularity in a variety of fields such as engineering, education, and gaming. By anchoring medias to geographic coordinates, it is possible to design immersive experiences remotely, without necessitating an in-depth knowledge of the context. However, the creation of such experiences typically requires complex programming tools that are beyond the reach of mainstream users. We introduce BiodivAR, a web cartographic tool for the authoring of location-based AR experiences. Developed using a user-centered design methodology and open-source interoperable web technologies, it is the second iteration of an effort that started in 2016. It is designed to meet needs defined through use cases co-designed with end users and enables the creation of custom geolocated points of interest. This approach enabled substantial progress over the previous iteration. Its reliance on geolocation data to anchor augmented objects relative to the user’s position poses a set of challenges: On mobile devices, GNSS accuracy typically lies between 1 m and 30 m. Due to its impact on the anchoring, this lack of accuracy can have deleterious effects on usability. We conducted a comparative user test using the application in combination with two different geolocation data types (GNSS versus RTK). While the test’s results are undergoing analysis, we hereby present a methodology for the assessment of our system’s usability based on the use of eye-tracking devices, geolocated traces and events, and usability questionnaires.",
        "keywords": "location-based augmented reality; augmented reality authoring tool; cartographic authoring tool; user-centered design; user experience; usability; educational technology; nature exploration; geolocated media spatial visualization; cartographic symbols visualization; immersive cartography; open source web tools; biodiversity education",
        "released": 2023,
        "link": "https://doi.org/10.3390/ijgi12020061"
    },
    {
        "title": "An online expert system for fault diagnosis in hydraulic systems",
        "abstract": "Fault diagnosis has become increasingly important for industrial automation and many approaches have been investigated for the online diagnostic task. This work demonstrates a new online expert system for dynamic industrial automated processes. The emphasis of this diagnostic system lies in the functions provided for fault detection, prediction, compensation and diagnosis. The system uses experiential knowledge cooperatively with scientific knowledge in a new interactive formation. It is developed in two parts using the DASYLab software for the numerical computations and the KPWin development tool for the symbolic representations. The system was developed in collaboration with the company Automation Systems S.A., which specializes in hydraulic systems, and is used for fault detection in production machines.",
        "keywords": "expert systems; fault diagnosis; online process; hybrid systems; hydraulic systems",
        "released": 1999,
        "link": "https://doi.org/10.1111/1468-0394.00100"
    },
    {
        "title": "Interoperable information model for geovisualization and interaction in XR environments",
        "abstract": "Since the vision of a Digital Earth (DE) was introduced in 1998, geo-browsers seem to have nearly fulfilled this vision. Virtual reality (VR) for visualizing the DE provides an immersive user experience in a mirror world. Location-based augmented reality (AR) browsers have been introduced and provide content according to user and environmental contexts. However, the content models of traditional geo-browsers and AR browsers have very limited interoperability, because they are described in application-specific formats using their domain standards. Each application is vertically integrated from content to application. The Web is an interoperable and open platform, and hundreds of millions of users are already using it to create and share content. To envision DE browsers for cross-reality (XR) environments that concurrently support geovisualization as well as VR, AR, and mixed-reality environments, we propose a DE content model based on Web standards and architecture that provides full interoperability and openness for XR browsers as a first-class citizen of the Web. This is expected to improve the DE content development efficiency by fully using the Web content development environment.",
        "keywords": "augmented reality; virtual reality; cross-reality; information model; web standard",
        "released": 2020,
        "link": "https://doi.org/10.1080/13658816.2019.1706739"
    },
    {
        "title": "SeqHound: Biological sequence and structure database as a platform for bioinformatics research",
        "abstract": "Background: SeqHound has been developed as an integrated biological sequence, taxonomy, annotation and 3-D structure database system. It provides a high-performance server platform for bioinformatics research in a locally-hosted environment. Results: SeqHound is based on the National Center for Biotechnology Information data model and programming tools. It offers daily updated contents of all Entrez sequence databases in addition to 3-D structural data and information about sequence redundancies, sequence neighbours, taxonomy, complete genomes, functional annotation including Gene Ontology terms and literature links to PubMed. SeqHound is accessible via a web server through a Perl, C or C++ remote API or an optimized local API. It provides functionality necessary to retrieve specialized subsets of sequences, structures and structural domains. Sequences may be retrieved in FASTA, GenBank, ASN.I and XML formats. Structures are available in ASN.I, XML and PDB formats. Emphasis has been placed on complete genomes, taxonomy, domain and functional annotation as well as 3-D structural functionality in the API, while fielded text indexing functionality remains under development. SeqHound also offers a streamlined WWW interface for simple web-user queries. Conclusions: The system has proven useful in several published bioinformatics projects such as the BIND database and offers a cost-effective infrastructure for research. SeqHound will continue to develop and be provided as a service of the Blueprint Initiative at the Samuel Lunenfeld Research Institute. The source code and examples are available under the terms of the GNU public license at the Sourceforge site http://sourceforge.net/projects/slritools/ in the SLRI Toolkit.",
        "keywords": "",
        "released": 2002,
        "link": "https://doi.org/10.1186/1471-2105-3-32"
    },
    {
        "title": "On the easy use of scientific computing services for large scale linear algebra and parallel decision making with the p-grade portal",
        "abstract": "Scientific research is becoming increasingly dependent on the large-scale analysis of data using distributed computing infrastructures (Grid, cloud, GPU, etc.). Scientific computing (Petitet et al. 1999) aims at constructing mathematical models and numerical solution techniques for solving problems arising in science and engineering. In this paper, we describe the services of an integrated portal based on the P-Grade (Parallel Grid Run-time and Application Development Environment) portal (http://www.p-grade.hu) that enables the solution of large-scale linear systems of equations using direct solvers, makes easier the use of parallel block iterative algorithm and provides an interface for parallel decision making algorithms. The ultimate goal is to develop a single sign on integrated multi-service environment providing an easy access to different kind of mathematical calculations and algorithms to be performed on hybrid distributed computing infrastructures combining the benefits of large clusters, Grid or cloud, when needed.",
        "keywords": "Advanced trading; Sparse linear algebra solvers; P-grade portal; Parallel decision making",
        "released": 2013,
        "link": "https://doi.org/10.1007/s10723-013-9254-7"
    },
    {
        "title": "An efficient web service annotation for domain classification and information retrieval systems using HADLNN classifier",
        "abstract": "Aimed at exchanging the dissimilar data between distributed applications, web services (WSs) annotations have progressed as a versatile and cost-effectual solution. Information retrieval (IR)assists in establishing the user -essential related information’s searching impacts. However, rendering quick and efficient IR is a challenging problem. Also, the existent system yields slow accuracy, and as well the training time is high. Aimed at over-coming these problems, implemented an effective WS annotation aimed at domain classification and IR systems utilizing the Hybrid Artificial Deep Learning Neural Network (HADLNN). Firstly, the Semantic annotation (SA) stage is executed that comprises text preprocessing, repetitive data removal, feature extraction, and as well Ontology Construction. The text preprocessing offers the partitioning, stop word removal, and as well the stemming procedure aimed at the WSDL dataset. Next, continual WSs utilizing Hadoop Distributed File System (HDFS)is eliminated. After that, the CFC, confidence, support, and as well entropy attributes are taken out;next, the (Web Ontology Language) OWL files as of ontology construction are generated utilizing the prote ‘ ge ‘ tool. After producing the OWL, the owl file is visualized utilizing Eclipse IDE and extracted the values utilizing the reasoner in the protege tool. After WS annotations, the domain is categorized centered on the connecting of WSs utilizing HADLNN. Lastly, the IR procedure is executed on MK-means that groups identical services as of the categorized domain. Preliminary outcomes exhibit that the system proposed offers efficient performance anal-ogized to the existent techniques.",
        "keywords": "Web Service Description Language (WSDL); Hadoop Distributed File System (HDFS); Hybrid Artificial Deep Learning Neural&nbsp; Network (HADLNN); Cat swarm Optimization (CSO) and Modified K-means (MK-means)",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.advengsoft.2022.103292"
    },
    {
        "title": "Performance theory based outcome measurement in engineering education and training",
        "abstract": "An approach is presented to improve engineering education that is based on new concepts of systems performance and classic feedback theory. An important aspect is the use of General Systems Performance Theory (GSPT) to provide a performance model of the educational system and as a basis for the keg outcome metrics: the volumes of performance capacity envelopes of individual students. Feedback is aimed at achieving both better curriculum design and teaching methods. In addition to conceptual issues, a web-based implementation plan and experimental validation plan is described. The quantitative modeling approach taken, including choice of appropriate levels of abstraction, has provided better understanding of the system used to pro,ide engineering education and a basis for quantitatively linking components of the program to student performance in a causal manner. The educational system performance model is discussed in the context of competency models. It is believed that this approach holds promise for not only documenting a meaningful type of outcome but also for providing insight into the rationale for steps taken in attempts to improve an educational system.",
        "keywords": "assessment; education; outcome; performance; performance capacity envelope; performance theory",
        "released": 2000,
        "link": "https://doi.org/10.1109/13.848059"
    },
    {
        "title": "Advanced therapeutic strategies for HBV-related acute-on-chronic liver failure",
        "abstract": "BACKGROUND: Acute-on-chronic liver failure (ACLF) is increasingly recognized as a distinct clinical entity and is associated with a high short-term mortality. The most common cause of ACLF is chronic hepatitis B worldwide. Currently, there is no standardized approach for the management of ACLF and the efficacy and safety of therapeutic modalities are uncertain. DATA SOURCES: Pub Med and Web of Science were searched for English-language articles. The search criteria focused on clinical trials and observational studies on the treatment of patients with HBV-related ACLF. RESULTS: Therapeutic approaches for ACLF in patients with chronic hepatitis B included nudeos(t)ide analogues, artificial liver support systems, immune regulatory therapy, stem cell therapy and liver transplantation. All of these therapeutic approaches have shown the potential to improve liver function and increase patients’ survival rate, but most of the studies were not randomized or controlled. CONCLUSION: Substantial challenges for the treatment of HBV-related ACLF remain and further basic research and randomized controlled clinical trials are needed.",
        "keywords": "chronic hepatitis B; acute-on-chronic liver failure; therapy",
        "released": 2015,
        "link": "https://doi.org/10.1016/S1499-3872(15)60338-1"
    },
    {
        "title": "Technology-based management of environmental organizations using an environmental management information system (EMIS): Design and development",
        "abstract": "The adoption of Information and Communication Technologies (ICT) in environmental management has become a significant demand nowadays with the rapid growth of environmental information. This paper presents a prototype Environmental Management Information System (EMIS) that was developed to provide a systematic way of managing environmental data and human resources of an environmental organization. The system was designed using programming languages, a Database Management System (DBMS) and other technologies and programming tools and combines information from the relational database in order to achieve the principal goals of the environmental organization. The developed application can be used to store and elaborate information regarding: human resources data, environmental projects, observations, reports, data about the protected species, environmental measurements of pollutant factors or other kinds of analytical measurements and also the financial data of the organization. Furthermore, the system supports the visualization of spatial data structures by using geographic information systems (GIS) and web mapping technologies. This paper describes this prototype software application, its structure, its functions and how this system can be utilized to facilitate technology-based environmental management and decision-making process. (C) 2016 Elsevier B.V. All rights reserved.",
        "keywords": "Environmental information system; Environmental management; Protected areas; Relational database; Mapping; Web-GIS",
        "released": 2016,
        "link": "https://doi.org/10.1016/j.eti.2016.01.006"
    },
    {
        "title": "ONLINE e-INVOICING PLATFORM FOR ABC-INNOVATECH. IN AMBATO: A MODERN AND EFFICIENT TECHNOLOGICAL SOLUTION",
        "abstract": "ABC-INNOVATECH, a company specialized in the commercialization of bazaar and stationery items, faces operational challenges due to its dependence on manual processes. This inconvenience has generated difficulties with the company’s external population in Ambato. The improvement proposal addresses inventory management and electronic invoicing through cloud-based software. This development, guided by the XP (Extreme Programming) methodology, uses a relational database managed with SQL Server and is implemented in the Microsoft Visual Studio development environment. The resulting automation seeks to mitigate operational problems, strengthening the efficiency of ABC-INNOVATECH.",
        "keywords": "Technological innovation; Management; Inventory; Online invoicing; Database",
        "released": 2024,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:001276959400006"
    },
    {
        "title": "The security enhancement of symmetric key crypto mechanism based on double stage secret model",
        "abstract": "All smart services in cloud computing platforms have a data-sharing process to execute the task. Moreover, several crypto techniques were implemented to provide efficient and secure data transmission channel. However, it has been suffered with a number of issues because of several harmful attacks and unauthenticated key retrieval. Thus in the wireless medium, transferring the data amid two users are challengeable task. To overcome this issue, a novel Double-Stage Secret (DSS) cryptosystem is proposed to prevent the attacks in the communication channel, here the encryption and decryption function is processed based on some specific condition. If the DSS condition is satisfied then the client or user is requested to decrypt the message. To validate the secure and success rate of the proposed model, an efficient novel back sniff is developed and launched in the data transmission channel and its secure range is evaluated. In addition, the simulation of this research is done by Java, running on net beans IDE 8.2 in windows 10 platform and the results illustrated that the proposed model has attained high performance by gaining a high secure rate and less processing time of encryption and decryption than the existing algorithms.",
        "keywords": "Cloud computing; encryption; decryption; cryptography; attacks in channel; transmission channel",
        "released": 2021,
        "link": "https://doi.org/10.1080/19393555.2020.1842945"
    },
    {
        "title": "Building database-backed web applications: Process and issues",
        "abstract": "In this article, the process of implementing database-backed Web applications from initial planning to final implementation and beyond will be explored. Both technical and “people” issues will be addressed, including choosing hardware and database platforms; selection of development tools and approaches; making decisions on the allocation of time, personnel, and equipment; and assigning data and system administration tasks.’ It is necessary to consider these points, even if a project seems simple, to ensure that no issues have been overlooked. Carefully thinking through all aspects beforehand can minimize the number of problems one will encounter later.",
        "keywords": "",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000176193400004"
    },
    {
        "title": "Improving e-book access via a library-developed full-text search tool",
        "abstract": "Purpose: This paper reports on the development of a tool for searching the contents of licensed full-text electronic book (e-book) collections. Setting: The Health Sciences Library System (HSLS) provides services to the University of Pittsburgh’s medical programs and large academic health system. Brief Description: The HSLS has developed an innovative tool for federated searching of its e-book collections. Built using the XML-based Vivisimo development environment, the tool enables a user to perform a full-text search of over 2,500 titles from the library’s seven most highly used e-book collections. From a single “Google-style” query, results are returned as an integrated set of links pointing directly to relevant sections of the full text. Results are also grouped into categories that enable more precise retrieval without reformulation of the search. Results/Evaluation: A heuristic evaluation demonstrated the usability of the tool and a web server log analysis indicated an acceptable level of usage. Based on its success, there are plans to increase the number of online book collections searched. Conclusion: This library’s first foray into federated searching has produced an effective tool for searching across large collections of full-text e-books and has provided a good foundation for the development of other library-based federated searching products.",
        "keywords": "",
        "released": 2007,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000243733500007"
    },
    {
        "title": "English course e-learning system based on relative item difficulty using web component composition",
        "abstract": "Many researches about e-learning system have been applied item difficulty to increase learning effectiveness. And development environment was changed the internet based learning media contents into the more various technology such as component, web 2.0, service oriented development and so on. Especially, service-oriented development is one of new trend in web based system and has become mainstream in software development. In the development, web components aims at providing support to service-oriented technique by enabling automatic discovery, composition, invocation and interoperation of the services. In this paper, we aimed the implementation of English e-learning system including the item guessing parameter and considering the relative correction of item difficulty. In the system, a learner was given to choose the learning step by the relative difficulty. In order to process and combine, all the learning contents are based on Sharable Content Object Reference Model (SCORM) with Learning Management System (LMS). Also, each learning contents are belong to Sharable Content Objects (SCOs).",
        "keywords": "E-learning system; SCORM; Item analysis; Web component; Relative item difficulty",
        "released": 2012,
        "link": "https://doi.org/10.1007/s11042-010-0708-7"
    },
    {
        "title": "TRAITRECORDJ: A programming language with traits and records",
        "abstract": "Traits have been designed as units for fine-grained reuse of behavior in the object-oriented paradigm. Records have been devised to complement traits for fine-grained reuse of state. In this paper, we present the language TRAITRECORDJ, a JAVA dialect with records and traits. Records and traits can be composed by explicit linguistic operations, allowing code manipulations to achieve fine-grained code reuse. Classes are assembled from (composite) records and traits and instantiated to generate objects. We introduce the language through examples and illustrate the prototypical implementation of TRAITRECORDJ using XTEXT, an Eclipse framework for the development of programming languages as well as other domain-specific languages. Our implementation comprises an Eclipse-based editor for TRAITRECORDJ with typical IDE functionalities, and a stand-alone compiler, which translates TRAITRECORDJ programs into standard JAVA programs. As a case study, we present the TRAITRECORDJ implementation of a part of the software used in a web-based information system previously implemented in JAVA. (C) 2011 Elsevier B.V. All rights reserved.",
        "keywords": "Java; Trait; Type system; Implementation; Eclipse",
        "released": 2013,
        "link": "https://doi.org/10.1016/j.scico.2011.06.007"
    },
    {
        "title": "Sketching interactive systems with sketchify",
        "abstract": "Recent discussions in the interaction design community have called attention to sketching as an omnipresent element of any disciplined activity of design, and have pointed out that sketching should be extended beyond the simple creation of a pencil trace on paper. More specifically, the need to deal with all attributes of a user experience, especially the timing, phrasing, and feel of the interaction, has been identified. In this article, we propose extending the concept of sketching with a pencil on paper to the more generic concept of fluent exploration of interactive materials. We define interactive materials as any piece of software or hardware that represents or simulates a part of the interactive user experience, such as input from sensors, output in the form of sound, video, or image, or interaction with Web services or specialized programs. We have implemented the proposed concept within Sketchify, a tool for sketching user interfaces. Sketchify gives designers the freedom to manipulate interactive materials by combining elements of traditional freehand sketching with functional extensions and end-user programming tools, such as spreadsheets and scripting. We have evaluated Sketchify in the education of interaction designers, identifying both successful aspects and aspects that need further improvements.",
        "keywords": "Sketching; interaction design; user interface software tools; design process; rapid prototyping",
        "released": 2011,
        "link": "https://doi.org/10.1145/1959022.1959026"
    },
    {
        "title": "Application of AFFDE-RBF-PID-PI cascade control in heating furnace temperature control system",
        "abstract": "Given the large hysteresis in the material outlet temperature of the heating furnace, a differential evolution algorithm with adaptive adjustment factors (AAFDE)-radial basis function (RBF)-proportional integral derivative (PID)-PI cascade is proposed. First, we introduce an adaptive mutation factor into the differential evolution (DE) algorithm and define the individual merit coefficient to incorporate a self-adaptive crossover probability factor. Second, the initial parameters of RBF are optimized by the AFFDE algorithm, and RBF neural network model is established. Then, we obtain gradient information by RBF online identification. Finally, we perform online adjustments to the three parameters of PID based on the gradient information. The three parameters are applied to the adjustment of the main controller, while the sub-controller employs PI control. Experimental results show that the anti-disturbance performance of the AFFDE-RBF-PID-PI cascade control outperforms the improved differential evolution (IDE)-RBF-PID-PI cascade control, generalized opposition-based differential evolution (GODE)-RBF-PID-PI cascade control, and MCOBDE-RBF-PID-PI cascade control respectively by 16%, 11%, and 7%, with the response speed improving by 18%, 12%, and 9%, and the stability improving by 19%, 13%, and 8%. These results show that AFFDE-RBF-PID-PI cascade control exhibits enhanced anti-disturbance performance, faster response speed, improved stability, and superior control effect.",
        "keywords": "adaptive adjustment factor; differential evolution algorithm; radial basis function neural network; cascade control; heating furnace system",
        "released": 2024,
        "link": "https://doi.org/10.6180/jase.202407\\_27(7).0008"
    },
    {
        "title": "Deep learning for clothing style recognition using YOLOv5",
        "abstract": "With the rapid development of artificial intelligence, much more attention has been paid to deep learning. However, as the complexity of learning algorithms increases, the needs of computation power of hardware facilities become more crucial. Instead of the focus being on computing devices like GPU computers, a lightweight learning algorithm could be the answer for this problem. Cross-domain applications of deep learning have attracted great interest amongst researchers in academia and industries. For beginners who do not have enough support with software and hardware, an open-source development environment is very helpful. In this paper, a relatively lightweight algorithm YOLOv5s is addressed, and the Google Colab is used for model training and testing. Based on the developed environment, many state-of-art learning algorithms can be studied for performance comparisons. To highlight the benefits of one-stage object detection algorithms, the recognition of clothing styles is investigated. The image samples are selected from datasets of fashion clothes and the web crawling of online stores. The image data are categorized into five groups: plaid; plain; block; horizontal; and vertical. Average precison, mean average precison, recall, F1-score, model size, and frame per second are the metrics used for performance validations. From the experimental outcomes, it shows that YOLOv5s is better than other learning algorithms in the recognition accuracy and detection speed.",
        "keywords": "clothing style recognition; deep learning; one-stage detection; YOLO",
        "released": 2022,
        "link": "https://doi.org/10.3390/mi13101678"
    },
    {
        "title": "Numerical guidance methods for decision support in aviation meteorological forecasting",
        "abstract": "Numerical guidance methods for decision making support of aviation meteorological forecasters a-re presented. The methods have been developed to enhance the usefulness of numerical weather prediction (NWP) model data and local and upstream observations in the production of terminal aerodrome forecasts (TAFs) and trend-type forecasts (TRENDs) for airports. In this paper two newly developed methods are described and it is shown how they are used to derive numerical guidance products for aviation. The first is a combination of statistical and physical postprocessing of NWP model data and in situ observations. This method is used to derive forecasts for all aviation-related meteorological parameters at the airport. The second is a high-resolution wind transformation method. a technique used to derive local wind at airports from grid-box-averaged NWP model winds. For operational use of the numerical guidance products encoding software is provided for automatic production of an alphanumeric TA-F and TREND code. A graphical user interface,with an integrated code editor enables the forecaster to modify the suggested automatic codes. For aviation, the most important parameters in the numerical guidance are visibility and cloud-base height. Both have been subjected to a,statistical verification analysis. together with their automatically produced codes. The results in terms of skill score are compared to the skill of the forecasters’ TAT and TREND code. The statistical measures suggest that the guidance has the best skill at lead times of +4 h and more. For the short term, mainly trend-type forecasts, the persistence forecast based on recent observations is difficult to beat. Verification has also shown that the wind transformation method. which has been applied to generate 10-m winds at Amsterdam Airport Schiphol, reduces the mean error in the (grid box averaged.) NWP model wind significantly. Among the potential benefits of these numerical guidance methods is increasing forecast accuracy. As a result. the related numerical guidance products and encoding software have been integrated in the operational environment for the production of TA.Fs and TRENDs.",
        "keywords": "",
        "released": 2005,
        "link": "https://doi.org/10.1175/WAF-827.1"
    },
    {
        "title": "A semantic web environment for components",
        "abstract": "Component-based development (CBD) has become an important emerging topic in the software engineering field. It promises long-sought-after benefits such as increased software reuse, reduced development time to market and, hence, reduced software production cost. Despite the huge potential, the lack of reasoning support and development environment of component modeling and verification may hinder its development. Methods and tools that can support component model analysis are highly appreciated by industry. Such a tool support should be fully automated as well as efficient. At the same time, the reasoning tool should scale LIP well as it may need to handle hundreds or even thousands of components that a modern software system may have. Furthermore, a distributed environment that can effectively manage and compose components is also desirable. In this paper, we present an approach to the modeling and verification of a newly proposed component model using Semantic Web languages and their reasoning tools. We use the Web Ontology Language and the Semantic Web Rule Language to precisely capture the inter-relationships and constraints among the entities in a component model. Semantic Web reasoning tools are deployed to perform automated analysis support of the component models. Moreover, we also proposed a service-oriented architecture (SOA)-based semantic web environment for CBD. The adoption of Semantic Web services and SOA make our component environment more reusable, scalable, dynamic and adaptive.",
        "keywords": "",
        "released": 2009,
        "link": "https://doi.org/10.1017/S0269888909000137"
    },
    {
        "title": "Specification of personalization in web application design",
        "abstract": "Personalization of websites has become an important issue in Web modeling methods due to their big and heterogeneous audience. However, due to the existence of too many notations to represent the same design concepts in different methodologies, personalization specifications cannot be used out of the scope of a single tool or method. Moreover, in some cases, personalization is not defined as a separate aspect, being difficult to maintain and update. This paper tackles the aforementioned problems presenting a generic modeling technique to facilitate the specification of the personalization. Personalization specifications can be reused across different websites and different development environments. (C) 2010 Elsevier B.V. All rights reserved.",
        "keywords": "Personalization; Web engineering; Reusability; Rule language",
        "released": 2010,
        "link": "https://doi.org/10.1016/j.infsof.2010.04.001"
    },
    {
        "title": "Definition and evaluation of a COSMIC measurement procedure for sizing web applications in a model-driven development environment",
        "abstract": "Context: Model-driven development approaches facilitate the production of Web applications. Among them, the Object-Oriented Hypermedia method (OO-H) has been successfully used for the development of industrial Web applications. Similarly to other development approaches, it is important also in this context to put measures in place to support project managers in resource allocation, cost and schedule control, and productivity monitoring. Objective. This motivated us to define a measurement procedure, named OO-HCFP, specifically conceived for OO-H Web applications based on COSMIC, a second-generation functional size measurement method. Method. We present mapping and measurement rules devised to automatically derive size measures from OO-H models. We also carry out an empirical study to evaluate whether our proposed measurement procedure, OO-HCFP, is useful for estimating the effort needed to realise industrial Web applications developed with OO-H. Results. The estimates obtained by using OO-HCFP are more accurate than those obtained by using other measurement approaches based on Function Points and design measures. Conclusions. The proposed approach can be profitably exploited to size Web applications developed with OO-H. Based on our experience, we also provide some guidelines to support the formulation of COSMIC measurement procedures for other model-driven approaches.",
        "keywords": "Web applications; Model-driven development; Functional size measurement; COSMIC; OO-H method",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.infsof.2018.07.012"
    },
    {
        "title": "Web-based simulation of fruit fly to support biosecurity decision-making",
        "abstract": "This paper explores the engineering contributions of a Web-based simulator of fruit fly, particularly with regards to software and system infrastructure. This simulation system is the result of a multidisciplinary effort in the areas of ecological simulation modelling and software/systems engineering, in order to provide tools to support decision making in biosecurity. We report on the following interesting software and system engineering aspects: catering for a variety of users, scalability issues, response time requirements, a three-tiered development environment, Web-suitable architecture, a flexible data interchange format, a range of cooperating underlying temporal-spatial models, and the need for evaluation in for non-functional aspects such as scalability in execution time and memory requirements. (C) 2012 Elsevier B.V. All rights reserved.",
        "keywords": "Simulation modelling; Web-based simulation; Fruit fly; Software engineering",
        "released": 2012,
        "link": "https://doi.org/10.1016/j.ecoinf.2012.02.002"
    },
    {
        "title": "The role of HCI models in service front-end development",
        "abstract": "This article discusses how human-computer interaction (HCI) models can support the development of interactive applications based on Web services. It also introduces a specific method exploiting such models for this purpose and the associated tool support. An example application of the method for an educational scenario is presented. The results of an early test of the development environment are reported as well. Lastly, some conclusions are drawn along with indications for future work.",
        "keywords": "HCI models; Web services; service front-ends",
        "released": 2012,
        "link": "https://doi.org/10.1080/0144929X.2011.563795"
    },
    {
        "title": ".NET framework and web services: A profit combination to implement and enhance the IEEE 1451.1 standard",
        "abstract": "In 1999, the 1451.1 Standard, which defines a common object model and interface specification to develop open multivendor distributed measurement and control systems, was published. However, despite the well-known advantages of the model, there have been a few initiatives to implement a commercial version of the standard. In this paper, we describe the implementation of a Network Capable Application Processor (NCAP) in a well-known and well-proven infrastructure, i.e., the Microsoft .NET Framework. The use of a commercial framework increases productivity because it handles low-level programming details and provides good development tools, which allows us to produce a reusable and robust code. In addition, a solution to enhance the 1451.1 Standard is presented by proposing a new format for inter-NCAP communication based on eXtended Markup Language (XML) Web Services. This approach promotes interoperability because any device with a compatible Web Service middleware can locate and communicate with our prototype.",
        "keywords": "IEEE 1451.1; Network Capable Application Processor (NCAP); smart sensors; Web Services; .NET Framework; .NET Remoting",
        "released": 2007,
        "link": "https://doi.org/10.1109/TIM.2007.908136"
    },
    {
        "title": "An online integrated programming platform to acquire students’ behavior data for immediate feedback teaching",
        "abstract": "Programming-related courses prefer to put importance on practice rather than theoretical knowledge. However, the students’ programming behavior data is difficult to be recorded and fed back to teachers anytime, allowing teachers to adjust their teaching in real-time. Aiming at this problem, this study develops an online integrated programming platform that provides a unified programming environment for teachers and students and logs students’ procedural programming behavior data automatically to assist teachers in providing real-time feedback teaching. To verify the effectiveness of the data-driven immediate feedback teaching method, an exploratory case was conducted in the 2020/2021 academic year autumn semester. As the control group, cloud201 adopted the traditional hybrid online/offline mode, while cloud202, as the experimental group, adopted the hybrid online/offline mode and immediate feedback teaching mode. The effect is demonstrated through the questionnaire, the programming behavior data analysis, and the performance analysis. The results of the questionnaire show that there are significant differences in the students’ satisfaction with interaction, supervision, feedback, and evaluation, of which feedback is the biggest part. In addition, the two analysis results indicate that students’ individual learning development and the learning status of the class are objectively characterized, the overall grade distribution of cloud202 is more concentrated in medium grades and the difference on average scores between cloud201 and cloud202 is gradually increased. Since the students’ immediate and comprehensive programming behavior data is perceived by teachers, the teaching adjustment is so flexible that the improvement of teaching efficiency and teaching effect is promoted accordingly.",
        "keywords": "behavior data; feedback teaching; learning analytics; online programming platform; real time",
        "released": 2023,
        "link": "https://doi.org/10.1002/cae.22596"
    },
    {
        "title": "A NEW END-USER COMPOSITION MODEL TO EMPOWER KNOWLEDGE WORKERS TO DEVELOP RICH INTERNET APPLICATIONS",
        "abstract": "Enabling real end-user programming development is the next logical stage in the evolution of Internet-wide service-based applications. Even so, the vision of end users programming their own web-based solutions has not yet materialized. This will continue to be so unless both industry and the research community rise to the ambitious challenge of devising an end-to-end compositional model for developing a new age of end-user web application development tools. This paper describes a new composition model designed to empower programming-illiterate end users to create and share their own off-the-shelf rich Internet applications in a fully visual fashion. This paper presents the main insights and outcomes of our research and development efforts as part of a number of successful European Union research projects. A framework implementing this model was developed as part of the European Seventh Framework Programme FAST Project and the Spanish EzWeb Project and allowed us to validate the rationale behind our approach.",
        "keywords": "end-user programming; user-centred service-oriented architectures; rich internet application",
        "released": 2011,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000295605600002"
    },
    {
        "title": "ChEAP: ChIP-exo analysis pipeline and the investigation of <i>escherichia coli</i> RpoN protein-DNA interactions",
        "abstract": "Genome-scale studies of the bacterial regulatory network have been leveraged by declining sequencing cost and advances in ChIP (chromatin immunoprecipitation) methods. Of which, ChIP-exo has proven competent with its near-single base-pair resolution. While several algorithms and programs have been developed for different analytical steps in ChIP-exo data processing, there is a lack of effort in incorporat-ing them into a convenient bioinformatics pipeline that is intuitive and publicly available. In this paper, we developed ChIP-exo Analysis Pipeline (ChEAP) that executes the one-step process, starting from trim-ming and aligning raw sequencing reads to visualization of ChIP-exo results. The pipeline was imple-mented on the interactive web-based Python development environment - Jupyter Notebook, which is compatible with the Google Colab cloud platform to facilitate the sharing of codes and collaboration among researchers. Additionally, users could exploit the free GPU and CPU resources allocated by Colab to carry out computing tasks regardless of the performance of their local machines. The utility of ChEAP was demonstrated with the ChIP-exo datasets of RpoN sigma factor in E. coli K-12 MG1655. To analyze two raw data files, ChEAP runtime was 2 min and 25 s. Subsequent analyses identified 113 RpoN binding sites showing a conserved RpoN binding pattern in the motif search. ChEAP application in ChIP-exo data analysis is extensive and flexible for the parallel processing of data from various organisms. (c) 2022 The Authors. Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.",
        "keywords": "ChIP-exo; Analysis pipeline; RpoN; Sigmulon; Sigma factor; Escherichia coli",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.csbj.2022.11.053"
    },
    {
        "title": "Expanded neo-fuzzy adaptive decayed brain emotional learning network for online time series predication",
        "abstract": "The Neo-Fuzzy integrated Adaptive Decayed Brain Emotional Learning (NF-ADBEL) network has recently been proposed for online time series predicting problems. The NF-ADBEL network is suitable for online time series prediction with shorter update intervals and offers features such as fast learning, accuracy, simplicity, and lower computational complexity. However, the neo-fuzzy neuron network in NF-ADBEL was integrated only in the orbitofrontal cortex (OFC) part of the ADBEL network. This paper aims to further improve the performance of the NF-ADBEL network by integrating the neo-fuzzy neuron network into the amygdala (AMY) section as well, inspired by a fully integrated version of a neo-fuzzy-based pattern recognizer. As is known, the AMY has two outputs: one response is based on imprecise information received from the thalamus, and the second response is based on information received from the sensory cortex. In this study, the imprecise response generation is operated as previously, while the other AMY process is treated by neo-fuzzy neurons. The resultant network is called Expanded Neo-Fuzzy integrated Adaptive Decayed Brain Emotional Learning (ENF-ADBEL). The modified network is still simple and meets the requirement for online prediction problems. A few chaotic and stochastic nonlinear systems, namely the Mackey-Glass, Lorenz, Rossler, disturbance storm time index, Narendra dynamic plant identification, wind speed and wind power series, are used to evaluate the performance of the proposed network in terms of the root mean squared error (RMSE) and correlation coefficient (COR) criteria in a MATLAB programming environment.",
        "keywords": "Neurons; Time series analysis; Adaptive systems; Indexes; Brain modeling; Wind power generation; Training; Amygdala; brain emotional decayed learning; chaotic time series; disturbance storm time index; dynamic plant identification; forecasting; MATLAB; neo-fuzzy networks; wind speed; wind power series",
        "released": 2021,
        "link": "https://doi.org/10.1109/ACCESS.2021.3076668"
    },
    {
        "title": "Behavior of the digital communities on twitter during the mexico elections 2018",
        "abstract": "In order to assess the importance of political communication in the Mexican political culture, this paper probes into the behavior of digital communities in Twitter during the 2018 Mexico’s presidential election. Research followed a two staged design. First, a large sample of tweets were collected on a daily basis using the package “rtweet” in the R programming environment. Following that, the collected tweets were filtered, cleaned, and a codebook was designed to analyze their contents. The results show that, out of a sample of 4664 publications, candidate Jose Antonio Meade had the largest number of mentions (47%), followed by Andres Manuel Lopez Obrador (29%) and Ricardo Anaya (24%). “Common citizens” issued 31% of the tweets analyzed, out of which 20% show links to online news outlets, while journalists and media publications accounted for 40% of the sampled tweets. The authors consider that these results characterize the observed digital communication practices as either transformations or extensions of the offline public sphere.",
        "keywords": "big data; content analysis; Twitter; political culture; Mexico",
        "released": 2020,
        "link": "https://doi.org/10.26441/RC19.1-2020-A2"
    },
    {
        "title": "Task-specific information retrieval systems for software engineers",
        "abstract": "This paper discusses the development of task-specific information retrieval systems for software engineers. We discuss how software engineers interact with information and information retrieval systems and investigate to what extent a domain-specific search and recommendation system can be developed in order to support their work related activities. We have conducted a user study which is based on the “Cognitive Research Framework” to identify the relation between the information objects used during the code development (code snippets and search queries), the tasks users engage in and the associated use of search interfaces. Based on our user studies, a questionnaire and an automated observation of user interactions with the browser and software development environment, we identify that software engineers engage in a finite number of work related tasks and they also develop a finite number of “work practices”/”archetypes of behaviour”. Secondly we identify a group of domain specific behaviours that can successfully be used as a source of strong implicit relevance feedback. Based on our results, we design a snippet recommendation interface, and a code related recommendation interface which are embedded within the standard search engine. (C) 2011 Elsevier Inc. All rights reserved.",
        "keywords": "Domain-specific information retrieval; Implicit feedback; Collaborative filtering; Contextualization of information retrieval",
        "released": 2012,
        "link": "https://doi.org/10.1016/j.jcss.2011.10.009"
    },
    {
        "title": "Translation, cross-cultural adaptation and validation of the diabetes empowerment scale - short form",
        "abstract": "OBJECTIVE: To translate, cross-culturally adapt and validate the Diabetes Empowerment Scale - Short Form for assessment of psychosocial self-efficacy in diabetes care within the Brazilian cultural context. METHODS: Assessment of the instrument’s conceptual equivalence, as well as its translation and cross-cultural adaptation were performed following international standards. The Expert Committee’s assessment of the translated version was conducted through a web questionnaire developed and applied via the web tool e-Surv. The cross-culturally adapted version was used for the pre-test, which was carried out via phone call in a group of eleven health care service users diagnosed with type 2 diabetes mellitus. The pre-test results were examined by a group of experts, composed by health care consultants, applied linguists and statisticians, aiming at an adequate version of the instrument, which was subsequently used for test and retest in a sample of 100 users diagnosed with type 2 diabetes mellitus via phone call, their answers being recorded by the web tool e-Surv. Internal consistency and reproducibility of analysis were carried out within the statistical programming environment R. RESULTS: Face and content validity were attained and the Brazilian Portuguese version, entitled Escala de Autoeficacia em Diabetes - Versao Curta, was established. The scale had acceptable internal consistency with Cronbach’s alpha of 0.634 (95% CI 0.494-0.737), while the correlation of the total score in the two periods was considered moderate (0.47). The intraclass correlation coefficient was 0.50. CONCLUSIONS: The translated and cross-culturally adapted version of the instrument to spoken Brazilian Portuguese was considered valid and reliable to be used for assessment within the Brazilian population diagnosed with type 2 diabetes mellitus. The use of a web tool (e-Surv) for recording the Expert Committee responses as well as the responses in the validation tests proved to be a reliable, safe and innovative method.",
        "keywords": "Diabetes Mellitus; Cost of Illness; Surveys and Questionnaires; Translations; Reproducibility of Results; Validation Studies",
        "released": 2017,
        "link": "https://doi.org/10.1590/S1518-8787.2017051006336"
    },
    {
        "title": "Modeling- and simulation-driven methodology for the deployment of an inland water monitoring system",
        "abstract": "In response to the challenges introduced by global warming and increased eutrophication, this paper presents an innovative modeling and simulation (M&S)-driven model for developing an automated inland water monitoring system. This system is grounded in a layered Internet of Things (IoT) architecture and seamlessly integrates cloud, fog, and edge computing to enable sophisticated, real-time environmental surveillance and prediction of harmful algal and cyanobacterial blooms (HACBs). Utilizing autonomous boats as mobile data collection units within the edge layer, the system efficiently tracks algae and cyanobacteria proliferation and relays critical data upward through the architecture. These data feed into advanced inference models within the cloud layer, which inform predictive algorithms in the fog layer, orchestrating subsequent data-gathering missions. This paper also details a complete development environment that facilitates the system lifecycle from concept to deployment. The modular design is powered by Discrete Event System Specification (DEVS) and offers unparalleled adaptability, allowing developers to simulate, validate, and deploy modules incrementally and cutting across traditional developmental phases.",
        "keywords": "Internet of Things; early warning system; harmful algal and cyanobacterial bloom; model-based system engineering; Discrete Event System Specification",
        "released": 2024,
        "link": "https://doi.org/10.3390/info15050267"
    },
    {
        "title": "Performance analysis of multi-threading on mobile game contents in smart virtual machine",
        "abstract": "As a virtual machine is a software emulation to provide a logical environment without hardware dependency, it is a key technology for cloud, mobile and IoT computing and one of major technologies to offer better IT service in the information and knowledge-based society. Smart Virtual Machine (SVM) is a core module of a smart cross platform that provides users with language-platform independent functions for a higher level of contents reusability. This is a software processor executable with the medium code named SIL as the stack basis. This study introduces a way of multi-thread embodiment in which SVM can execute multiple tasks. The multi thread for SVM was designed based on a thread model of C++ and Java Language. Since it supports C/C++ and Java programming environments, SVM processes various tasks simultaneously with a higher level of program concurrency. This study focuses on a thread for Smart Virtual Machine. As a virtual machine-based compiler generates a program created by means of C/C++ or Java and the semantically equivalent intermediate code, SVM receives and executes them. The thread for SVM was designed based on a thread model of C++/Java, thread scheduling is possible for multithreading in programming. A programmer may use the thread regardless of the developing language, and it is executable with no limitation due to certain hardware types. Besides, the thread enhances program concurrency.",
        "keywords": "multi-threading; Smart Virtual Machine (SVM); Smart cross platform; cloud computing; mobile computing; IoT computing; program concurrency",
        "released": 2015,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000362047800026"
    },
    {
        "title": "Online parental mediation strategies in family contexts of spain",
        "abstract": "This article explores online parental mediation strategies in Spain and their association with sociodemographic and family context Factors. The results of a survey conducted at the end of 2018 are presented herein, based on a sample of 2,900 Spanish minors between 9 and 17 years of age who use Internet. The impact of the diverse parental mediation strategies applied to Internet use has been calculated by taking into account the sociodemographic Factor’s ofthe participating minors (age and gender). Association analysis was performed using the SPSS statistical analysis programme. In this case, an extra analysis was carried out with regard to the relationship of influence between different strategies and the rules of behaviour and family support in the household context as perceived by the minor Findings suggest that enabling and restrictive mediation strategies are very common in Spanish families, while technical mediation strategies have a very limited presence. It is noteworthy that restrictions and security strategies generally apply more to girls than to boys. Household Ides related to the behaviour of minors have a positive correlation with an increase of influence of nearly all strategies. However; there is no relevant association between family support perceived by children and restrictive strategies and techniques applied by parents.",
        "keywords": "Internet; mediation; strategies; behavior; family; children; parents; intervention",
        "released": 2020,
        "link": "https://doi.org/10.3916/C65-2020-06"
    },
    {
        "title": "Analyzing and visual programming internet of things and autonomous decentralized systems",
        "abstract": "The development of Internet of Things, fueled by cloud computing and big data processing from upper level, and by ubiquitous sensory and actuator devices from the lower level, has taken a sharp turn towards integrating the entire information, computing, communication, and control systems. This special issue selected seven papers from the 2015 IEEE twelfth International Symposium on Autonomous Decentralized Systems (ISADS). These papers cover the latest research on IoT and ADS based system science and system engineering methods; the wearable sensor network development and applications; and data analysis for security and reliability in IoT and ADS applications. As an addition to these selected topics, this guest editorial paper also adds IoT education and dissemination aspects to this special issue. As the IoT research and applications expand explosively into all the domains, schools and universities must prepare students to understand and to be able to program the IoT devices. This paper presents a visual programming environment that allows students without programming background to learn the key concepts of computing and IoT devices, and to program IoT devices into different application systems. (C) 2015 Published by Elsevier B.V.",
        "keywords": "Internet of Things; autonomous decentralized system; Visual programming; IoT education",
        "released": 2016,
        "link": "https://doi.org/10.1016/j.simpat.2016.05.002"
    },
    {
        "title": "Towards web-based computing",
        "abstract": "In a problem solving environment for geometric computing, a graphical user interface, or GUI, for visualization has become an essential component for geometric software development. In this paper we describe a visualization system, called GeoJAVA, which consists of a GUI and a geometric visualization library that enables the user or algorithm designer to (1) execute and visualize an existing algorithm in the library or (2) develop new code over the Internet. The library consists of geometric code written in C/C++. The GUI is written using the Java programming language. Taking advantage of the socket classes and system-independent application programming interfaces (API’s) provided with the Java language, GeoJAVA offers a platform independent environment for distributed geometric computing that combines Java and C/C++ Users may remotely join a ‘channel” or discussion group in a location transparent manner to do collaborative research. The visualization of an algorithm, a C/C++ program located locally or remotely and controlled by a “floor manager,” can be viewed by all the members in the channel through a visualization sheet called GeoJAVASheet. A chat box is also provided to enable dialogue among the members. Furthermore, this system not only allows visualization of pre-compiled geometric code, but also serves as a web-based programming environment where the user may submit a geometric code, compile it with the libraries provided by the system, and visualize it directly over the web sharing it with other users immediately.",
        "keywords": "geometric computation; visualization tool; distributed system; Java programming language",
        "released": 2001,
        "link": "https://doi.org/10.1142/S0218195901000407"
    },
    {
        "title": "T-PCCE: Twitter personality based communicative communities extraction system for big data",
        "abstract": "The identification of social media communities has recently been of major concern, since users participating in such communities can contribute to viral marketing campaigns. In this work, we focus on users’ communication considering personality as a key characteristic for identifying communicative networks i.e., networks with high information flows. We describe the Twitter Personality based Communicative Communities Extraction (T-PCCE) system that identifies the most communicative communities in a Twitter network graph considering users’ personality. We then expand existing approaches in users’ personality extraction by aggregating data that represent several aspects of user behavior using machine learning techniques. We use an existing modularity based community detection algorithm and we extend it by inserting a post-processing step that eliminates graph edges based on users’ personality. The effectiveness of our approach is demonstrated by sampling the Twitter graph and comparing the communication strength of the extracted communities with and without considering the personality factor. We define several metrics to count the strength of communication within each community. Our algorithmic framework and the subsequent implementation employ the cloud infrastructure and use the MapReduce Programming Environment. Our results show that the T-PCCE system creates the most communicative communities.",
        "keywords": "Twitter; Data mining; Big Data; Detection algorithms; Feature extraction; Facebook; Classification; cloud computing; communicative community detection; MapReduce; personality mining; social media analytics",
        "released": 2020,
        "link": "https://doi.org/10.1109/TKDE.2019.2906197"
    },
    {
        "title": "Optimizing salmon farm cage net management using integer programming",
        "abstract": "Salmon farming in Chile constitutes one of the nation’s principal food exporting sectors. In the seawater stage, one of the most important in the farm production chain, salmon are cultivated in floating cages fitted with nets that hold the fish during the entire grow-out process. The maintenance of the cage nets is carried out at land-based facilities. This article reports on the creation of an integer programming tool for grow-out centres that optimizes resource use, improves planning and generates economic evaluations for supporting analysis and decision-making relating to the maintenance, repair and periodic changing of cage nets. The tool prototype was tested in a single operating area of one of Chile’s largest salmon farmers. The results demonstrated a reduction in net maintenance costs of almost 18%, plus a series of important qualitative benefits. Implementation of the tool by farm operators awaits the end of the current crisis in the industry. Journal of the Operational Research Society (2013) 64, 735-747. doi:10.1057/jors.2012.74 Published online 22 August 2012",
        "keywords": "integer programming; planning; sea transport; maritime logistics",
        "released": 2013,
        "link": "https://doi.org/10.1057/jors.2012.74"
    },
    {
        "title": "An approach to develop digital twins in industry",
        "abstract": "The industry is currently undergoing a digital revolution driven by the integration of several enabling technologies. These include automation, robotics, cloud computing, industrial cybersecurity, systems integration, digital twins, etc. Of particular note is the increasing use of digital twins, which offer significant added value by providing realistic and fully functional process simulations. This paper proposes an approach for developing digital twins in industrial environments. The novelty lies in not only focusing on obtaining the model of the industrial system and integrating virtual reality and/or augmented reality but also in emphasizing the importance of incorporating other enabled technologies of Industry 4.0, such as system integration, connectivity with standard and specific industrial protocols, cloud services, or new industrial automation systems, to enhance the capabilities of the digital twin. Furthermore, a proposal of the software tools that can be used to achieve this incorporation is made. Unity is chosen as the real-time 3D development tool for its cross-platform capability and streamlined industrial system modeling. The integration of augmented reality is facilitated by the Vuforia SDK. Node-RED is selected as the system integration option, and communications are carried out with MQTT protocol. Finally, cloud-based services are recommended for effective data storage and processing. Furthermore, this approach has been used to develop a digital twin of a robotic electro-pneumatic cell.",
        "keywords": "digital twin; Industry 4.0; augmented reality; virtual reality; robotic electro-pneumatic cell",
        "released": 2024,
        "link": "https://doi.org/10.3390/s24030998"
    },
    {
        "title": "Logic programming representation of the compound term composition algebra",
        "abstract": "The Compound Term Composition Algebra (CTCA) is an algebra with four algebraic operators, which can be used to generate the valid (meaningful) compound terms of a given faceted taxonomy, in an efficient and flexible manner. The positive operations allow the derivation of valid compound terms through the declaration of a small set of valid compound terms. The negative operations allow the derivation of valid compound terms through the declaration of a small set of invalid compound terms. In this paper, we show how CTCA can be represented in logic programming with negation-as-failure, according to both Clark’s and well-founded semantics. Indeed, the SLDNF-resolution can be used for checking compound term validity and well-formedness of an algebraic expression in polynomial time w.r.t. the size of the expression and the number of terms in the taxonomy. This result makes our logic programming representation a competitive alternative to imperative algorithms. Embedding of our logic programming representation to the programming environment of a web portal for a computer sales company is demonstrated.",
        "keywords": "faceted taxonomies; logic programming; computational complexity; applications",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000242241900001"
    },
    {
        "title": "Size-selective detection of nanoparticles in solution and air by imprinting",
        "abstract": "Monitoring of nanoparticles (NPs) in air and aquatic environments is an unmet challenge accentuated by the rising exposure to anthropogenic or engineered NPs. The inherent heterogeneity in size, shape, and the stabilizing shell of NPs makes their selective recognition a daunting task. Thus far, only a few technologies have shown promise in detecting NPs; however, they are cumbersome, costly, and insensitive to the NPs morphology or composition. Herein, we apply an approach termed nanoparticle-imprinted matrices (NAIM), which is based on creating voids in a thin layer by imprinting NPs followed by their removal. The NAIM was formed on an interdigitated electrode (IDE) and used for the size-selective detection of silica NPs. Three- and 5-fold increases in capacitance were observed for the reuptake of NPs with similar diameter, compared to smaller or larger NPs, in air and liquid phase, respectively. En masse, the proposed approach lays the foundation for the emergence of field-effective, inexpensive, real-life applicable sensors that will allow online monitoring of NPs in air and liquids.",
        "keywords": "Nanoparticles detection; interdigitated electrode; imprinting nanoparticle-imprinted matrices; aerosol; capacitive sensing",
        "released": 2022,
        "link": "https://doi.org/10.1021/acssensors.1c02324"
    },
    {
        "title": "Efficacy and resistance in de novo combination lamivudine and adefovir dipivoxil therapy versus entecavir monotherapy for the treatment-naive patients with chronic hepatitis b: A meta-analysis",
        "abstract": "Background: Currently, there is no consensus on the efficacy and resistance of de novo combination therapy versus monotherapy for treatment naive patients of chronic hepatitis B (CHB). Objectives: The aim of this study was to evaluate the effectiveness and resistance of de novo combination of lamivudine (LAM) and adefovir dipivoxil (ADV) compared with entecavir (ETV) monotherapy for nucleos(t)ide-naive patients with CHB. Study design: Publications on the effectiveness and resistance of LAM plus ADV versus ETV monotherapy for nucleos(t) ide-naive patients with CHB were identified by a search of PubMed, Embase, the Cochrane Library, Web of science, OVID, and CBM (Chinese Biological Medical Literature) until May 1, 2013. Biochemical response, hepatitis B e antigen seroconversion, and viroligic response were extracted and combined to obtain an integrated result. Viral resistance and safety were reviewed. Results: Five eligible studies (328 patients in total) were included in the analysis. LAM plus ADV combination therapy produced more rapid HBV DNA reduction rate at 12 weeks than that of ETV monotherapy. At 48 weeks, the combination group had superior viroligic response rates compared with ETV group (90.0% vs. 78.9%, P=0.01). The difference in the ALT normalization and HBeAg seroconversion rates was not found. At week 96, LAM + ADV was more effective than ETV in ALT normalization [RR = 1. 11, 95% CI (1.02, 1.21), P = 0.01] and HBeAg seroconversion [RR = 2.00, 95% CI (1.26, 3.18, P=0.003)], and no significant difference was found in the virologic response (P = 0.23). No viral resistance occurred in combination therapy and six patients in ETV group were experienced with viral breakthrough. Both groups were well tolerated. Conclusion: The de novo LAM plus ADV combination therapy for treatment-naive patients with CHB was greater than ETV monotherapy in both biochemical response and HBeAg seroconversion rate up to 96 weeks. The rate of emergence of viral resistance in the combination group was less than that in the ETV monotherapy.",
        "keywords": "Adefovir dipivoxil; Combination; De novo; Entecavir; Lamivudine; Naive",
        "released": 2014,
        "link": "https://doi.org/10.1186/1743-422X-11-59"
    },
    {
        "title": "Implementation on visualization of some complex physics problems embodied in difficulty with interactive materials for undergraduates",
        "abstract": "Physical phenomena and the nature of materials require both qualitative and quantitative research to be understood thoroughly. This process often necessitates the use of complicated and expensive equipments. Also in electrical-electronics discipline, especially the nature of semiconductor materials and the behavior of them have a special place. In engineering education, it is often more important for undergraduates to understand the behavior of material and the results of that at the basic level. For this purpose it is sufficient to analyze semiconductor materials with interactive software unlike physics scholars who apply mostly physically to analyze. Understanding the physical properties of applied materials and digitalizing the characteristics of these materials by coding in programming environment maintain its importance. In this context, designing of interactive programs to analyze physical phenomena in electrical-electronics engineering without the need for licensed software has been presented. For this purpose, some semiconductor phenomena involved with electronics engineering have been selected as a pilot and web based 3D Java graphic AWT applications have been designed.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.12693/APhysPolA.131.74"
    },
    {
        "title": "Towards a knowledge driven framework for bridging the gap between software and data engineering",
        "abstract": "In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain. (C) 2018 Published by Elsevier Inc.",
        "keywords": "Ontologies; Data engineering; Software engineering; Alignment; Integration",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.jss.2018.12.017"
    },
    {
        "title": "Investigation of examples of e-education environment for scientific collaboration and distance graduate studies. Part 2",
        "abstract": "The aim is to investigate two emerging information technologies in graduate studies and scientific cooperation. Internet is the first technology. The open source is the second. They help each other in many ways. The joint influence of both is regarded in this paper. Results of complexity theory show the limitations of exact analysis. That explains popularity of heuristic algorithms. It is well known that efficiency of heuristics depends on the parameters. Therefore automatic procedures for tuning the heuristics help to compare results of different heuristics and enhance their efficiency. The theory and some applications of Bayesian Approach were discussed in (Mockus, 2006a). In this paper examples of Bayesian Approach to automated tuning of heuristics are investigated. This is the Bayesian Heuristic Approach, in short. The examples of traditional methods of optimization, including applications of linear and dynamic programming, will be investigated in the next paper. These three papers represents three parts of the same work. However each part can be read independently. All the algorithms are implemented as platform independent Java applets or servlets. Readers can easily verify and apply the results for studies and for real life optimization problems. The theoretical result is application of unified Bayesian Heuristic Approach for different discrete optimization models. The practical result is adaptation of these models for graduate distance studies and scientific collaboration by a common java global optimization framework. The software is regularly updated and corrected responding to new programming tools and users reports. However the general structure of web sites remains. The information is on the web site: http: //pilis. if. ktu. lt/(similar to)mockus and four mirror sites.",
        "keywords": "Bayesian approach; tuning heuristics; global optimization; distance studies",
        "released": 2008,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000254272100004"
    },
    {
        "title": "Impact of reflection in auto-graders: An empirical study of novice coders",
        "abstract": "Background and ContextAuto-graders are praised by novice students learning to program, as they provide them with automatic feedback about their problem-solving process. However, some students often make random changes when they have errors in their code, without engaging in deliberate thinking about the cause of the error.ObjectiveTo investigate whether requiring students using an auto-grading system to reflect on the errors in their code would reduce trial and error behavior often seen in such systems.MethodThe paper analyzes the impact of reflection per student and per problem using paired t-tests.FindingsStudents took fewer steps to solve the problem in reflective sessions than in Usual Debugging Sessions (4.33 vs 8.04) and they made half as many syntax errors, logic errors, and runtime errors. However, they took more time between runs.ImplicationsThis paper provides evidence that requiring reflection in autograding systems can improve student debugging skills.",
        "keywords": "Reflective debugging; introductory programming classes; online IDE; actionable learning analytics; auto graders; novice programming",
        "released": 2023,
        "link": "https://doi.org/10.1080/08993408.2023.2262877"
    },
    {
        "title": "Introducing SourceXplorer, an open-source statistical tool for guided lithic sourcing",
        "abstract": "Archaeologists’ access to analytical infrastructure has grown exponentially over the last two decades. This is especially the case for benchtop X-ray fluorescence (XRF) and portable XRF (pXRF) instruments, which are now practically commonplace in archaeological laboratories and provide users with a non-destructive and rapid means to analyze the elemental compositions of archaeological specimens. As XRF has become more accessible, the volume of analytical measurements available in archaeological datasets as well as the number and diversity of researchers participating in data collection have inherently increased. Those researchers, who have various levels of experience with the nuances of lithic sourcing procedures, are also often the ones attempting to interpret the elemental data they produce. While standardized analytical procedures have enabled inexperienced analysts to take accurate and reproducible XRF measurements, interpreting the resulting data is more difficult to convert and standardize with the same degree of user-friendliness. To address this challenge, we have bundled a series of statistical approaches and data exploration tools into an intuitive open-source graphical user interface designed to facilitate reproducible and robust outcomes during lithic sourcing studies. Our application, SourceXplorer, permits easy access to and exploration of numeric baseline data using a map interface while facilitating a guided interpretation of source affiliations for archaeological specimens (e.g., lithics) within any natural context using multivariate statistical analyses. We demonstrate SourceXplorer’s functionality in relation to a complex archaeological challenge by examining evidence for the procurement and use of lithic material from previously undocumented toolstone source locations in southwestern British Columbia, Canada. We also provide open access to SourceXplorer, including both a deployed version of the application that can be used with any Internet browser and the packaged script, which can be run locally in the open-source R statistical programming environment.",
        "keywords": "R Shiny; Open science; Lithic provenance; Stone tools; Trace elements; X-ray fluorescence; Sts ?ailes coast Salish",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.jas.2022.105626"
    },
    {
        "title": "FirebrowseR: An r client to the broad institute’s firehose pipeline",
        "abstract": "With its Firebrowse service (http://firebrowse.org/) the Broad Institute is making large-scale multi-platform omics data analysis results publicly available through a Representational State Transfer (REST) Application Programmable Interface (API). Querying this database through an API client from an arbitrary programming environment is an essential task, allowing other developers and researchers to focus on their analysis and avoid data wrangling. Hence, as a first result, we developed a workflow to automatically generate, test and deploy such clients for rapid response to API changes. Its underlying infrastructure, a combination of free and publicly available web services, facilitates the development of API clients. It decouples changes in server software from the client software by reacting to changes in the RESTful service and removing direct dependencies on a specific implementation of an API. As a second result, FirebrowseR, an R client to the Broad Institute’s RESTful Firehose Pipeline, is provided as a working example, which is built by the means of the presented workflow. The package’s features are demonstrated by an example analysis of cancer gene expression data.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1093/database/baw160"
    },
    {
        "title": "THE EMPLOYMENT OF MACHINE LEARNING ALGORITHMS FOR PREDICTION IN LEARNING ANALYTICS AND EDUCATIONAL DATA MINING WITHIN THE CONTEXT OF HIGHER EDUCATION",
        "abstract": "This paper presents a review of the literature from the last five years on predictive methods of Learning Analytics and Educational Data Mining based on Machine Learning algorithms. The primary selection criterion for the papers analyzed was to identify those that use Machine Learning algorithms to predict outcomes in the areas of Learning Analytics and Educational Data Mining in the context of higher education. It is important to highlight that there are no universal guidelines or protocols for predicting outcomes in education, including higher education. The methodology used for such predictions depends primarily on the target variable and the type of input data used. Twenty-five papers from the Web of Science CC and Scopus citation databases were included in the detailed analysis. Six research questions were used to examine what is being predicted in higher education, what input data were used, how many Machine Learning algorithms were used in the research, and which were most effective. In addition, the research looked at what other predictive modeling techniques were mentioned and whether the programming environment used for prediction was mentioned.",
        "keywords": "Learning Analytics; Educational Data Mining; prediction; Machine Learning",
        "released": 2024,
        "link": "https://doi.org/10.31784/zvr.12.1.1"
    },
    {
        "title": "Developing a web-based flood forecasting system for reservoirs with J2EE",
        "abstract": "A flood forecasting system is a crucial component in flood mitigation. For certain important large-scale reservoirs, cooperation and communication among federal, state, and local stakeholders are required when heavy flood events are encountered. The Web-based environment is emerging as a very important development and delivery platform for real-time flood forecasting systems. In this paper, the findings of a case study are presented of the development of a Web-based flood forecasting system for reservoirs using Java 2 platform Enterprise Edition (J2EE). J2EE of Sun Microsystems is chosen as the development solution for the Web-based flood forecasting system, Weblogic 6.0 of BEA as the container provider, and JBuilder 7.0 of Borland as the development tool. One of the key objectives in this project is to establish a collaborative platform for flood forecasting via Web technology in order to render hydrological models and data available to stakeholders and experts involved and thus offer an efficient medium for transferring and sharing information, knowledge and experiences among them. Compared with general Webbased query systems and traditional flood forecasting systems, the Web-based flood forecasting system is more focused on the on-line analysis of model-based forecasting of floods and provides opportunities for improving the transfer of information and knowledge from the hydrological scientists and managers to decision makers. Finally, a prototype system is used to demonstrate the system application.",
        "keywords": "collaborative problem solving; EJB; flood forecasting; J2EE; World Wide Web",
        "released": 2004,
        "link": "https://doi.org/10.1623/hysj.49.6.973.55723"
    },
    {
        "title": "Social value of high bandwidth networks: Creative performance and education",
        "abstract": "This paper considers limitations of existing network technologies for distributed theatrical performance in the creative arts and for symmetrical real-time interaction in online learning environments. It examines the experience of a multidisciplinary research consortium that aimed to introduce a solution to latency and other network problems experienced by users in these sectors. The solution builds on the Multicast protocol, Access Grid, an environment supported by very high bandwidth networks. The solution is intended to offer high-quality image and sound, interaction with other network platforms, maximum user control of multipoint transmissions, and open programming tools that are flexible and modifiable for specific uses. A case study is presented drawing upon an extended period of participant observation by the authors. This provides a basis for an examination of the challenges of promoting technological innovation in a multidisciplinary project. We highlight the kinds of technical advances and cultural and organizational changes that would be required to meet demanding quality standards, the way a research consortium planned to engage in experimentation and learning, and factors making it difficult to achieve an open platform that is responsive to the needs of users in the creative arts and education sectors.",
        "keywords": "very high bandwidth; multicast; creative industry; artistic performance; Massive Open Online Courses; interactivity",
        "released": 2016,
        "link": "https://doi.org/10.1098/rsta.2015.0124"
    },
    {
        "title": "Development of a supportive parenting app to improve parent and infant outcomes in the perinatal period: Development study",
        "abstract": "Background: The transition to parenthood can be challenging, and parents are vulnerable to psychological disorders during the perinatal period. This may have adverse long-term consequences on a child’s development. Given the rise in technology and parents’ preferences for mobile health apps, a supportive mobile health intervention is optimal However, there is a lack of a theoretical framework and technology-based perinatal educational intervention for couples with healthy infants. Objective: The aim of this study is to describe the Supportive Parenting App (SPA) development procedure and highlight the challenges and lessons learned. Methods: The SPA development procedure was guided by the information systems research framework, which emphasizes a nonlinear, iterative, and user-centered process involving 3 research cycles-the relevance cycle, design cycle, and rigor cycle. Treatment fidelity was ensured, and team cohesiveness was maintained using strategies from the Tuckman model of team development. Results: In the relevance cycle, end-user requirements were identified through focus groups and interviews. In the rigor cycle, the user engagement pyramid and well-established theories (social cognitive theory proposed by Bandura and attachment theory proposed by Bowlby) were used to inform and justify the features of the artifact. In the design cycle, the admin portal was developed using Microsoft Visual Studio 2017, whereas the SPA, which ran on both iOS and Android, was developed using hybrid development tools. The SPA featured knowledge-based content, informational videos and audio clips, a discussion forum, chat groups, and a frequently asked questions and expert advice section. The intervention underwent iterative testing by a small group of new parents and research team members. Qualitative feedback was obtained for further app enhancements before official implementation. Testing revealed user and technological issues, such as web browser and app incompatibility, a lack of notifications for both administrators and users, and limited search engine capability. Conclusions: The information systems research framework documented the technical details of the SPA but did not take into consideration the interpersonal and real-life challenges. Ineffective communication between the health care research team and the app developers, limited resources, and the COVID-19 pandemic were the main challenges faced during content development. Quick adaptability, team cohesion, and hindsight budgeting are crucial for intervention development. Although the effectiveness of the SPA in improving parental and infant outcomes is currently unknown, this detailed intervention development study highlights the key aspects that need to be considered for future app development.",
        "keywords": "depression; development; education; parent; perinatal; support; telehealth; mobile phone",
        "released": 2021,
        "link": "https://doi.org/10.2196/27033"
    },
    {
        "title": "A tool for introducing computer science with automatic formative assessment",
        "abstract": "In this paper we present a software platform called Chatbot designed to introduce high school students to Computer Science (CS) concepts in an innovative way: by programming chatbots. A chatbot is a bot that can be programmed to have a conversation with a human or robotic partner in some natural language such as English or Spanish. While programming their chatbots, students use fundamental CS constructs such as variables, conditionals, and finite state automata, among others. Chatbot uses pattern matching, state of the art lemmatization techniques, and finite state automata in order to provide automatic formative assessment to the students. When an error is found, the formative feedback generated is immediate and task-level. We evaluated Chatbot in two observational studies. An online nation-wide competition where more than 10,000 students participated. And, a mandatory in-class 15-lesson pilot course in three high schools. We measured indicators of student engagement (task completion, participation, self reported interest, etc.) and found that girls’ engagement with Chatbot was higher than boys’ for most indicators. Also, in the online competition, the task completion rate for the students that decided to use Chatbot was five times higher than for the students that chose to use the renowned animation and game programming tool Alice. Our results suggest that the availability of automatic formative assessment may have an impact on task completion and other engagement indicators among high school students.",
        "keywords": "Interactive learning environments; K-12 education; computer science education; automatic formative assessment",
        "released": 2018,
        "link": "https://doi.org/10.1109/TLT.2017.2682084"
    },
    {
        "title": "Python tools for ESA’s swarm mission: VirES for swarm and surrounding ecosystem",
        "abstract": "ESA’s Swarm mission is a constellation probing both Earth’s interior and geospace, delivering magnetic and plasma measurements which are used to generate many derived data products. From empirical magnetic field models of the core, crust, ionosphere, and magnetosphere, to multi-point estimates of ionospheric currents and in-situ plasma properties, these are challenging to navigate, process, and visualize. The VirES for Swarm platform () has been built to tackle this problem, providing tools to increase usability of Swarm data products. The VirES (Virtual environments for Earth Scientists) platform provides both a graphical web interface and an API to access and visualise Swarm data and models. This is extended with a cloud-hosted development environment powered by JupyterHub (the “Virtual Research Environment/VRE “). VirES provides two API’s: the full VirES API for which a dedicated Python client is provided, viresclient, and the more interoperable Heliophysics API (HAPI). The VRE is furnished with a bespoke Python environment containing thematic libraries supporting science with Swarm. This service aims to ease the pathway for scientists writing computer code to analyze Swarm data products, increase opportunities for collaboration, and leverage cloud technologies. Beyond simply providing data and model access to Python users, it is extremely helpful to provide higher-level analysis and visualization tools, and ready-to-use code recipes that people can explore and extend. Critically for space physics, this involves crossover with many other datasets and so it is highly valuable to embed such tools within the wider data and software ecosystems. Through Swarm DISC (Data, Innovation, and Science Cluster), we are tackling this through cookbooks and Python libraries. Cookbooks are built and presented using Jupyter technologies, and tested to work within the VRE. A new library we are building is SwarmPAL, which includes tools for time-frequency analysis and inversion of magnetic field measurements for electric current systems, among others, while relying on the VirES server to provide data portability and other utilities. This paper reviews the current state of these tools and services for Swarm, particularly in the context of the Python in Heliophysics Community, and the wider heliophysics and geospace data environment.",
        "keywords": "swarm; python; space weather; geospace; geomagnetism",
        "released": 2022,
        "link": "https://doi.org/10.3389/fspas.2022.1002697"
    },
    {
        "title": "A hybrid approach to selective-disassembly sequence planning for de-manufacturing and its implementation on the internet",
        "abstract": "De-manufacturing (DM) is a process to separate a product into components and materials that will be maintained, replaced, reused, or recycled. Disassembling a selected set of parts in a product, defined as selective-disassembly, is an essential need in product DM. Although it is necessary to have an efficient and optimized sequence planning for selective-disassembly to reduce DM-related cost, it is more important to consider de-manufacturability for product life cycle cost at the early stage of a product development. However, the product analysis related to DM is generally regarded as a post-process in product development. Current product development environments require all industry in a supply chain to concurrently develop their specialized components corresponding to the end item requirement within a short time frame. Therefore, it is an emerging issue to add global concurrent de-manufacturability analysis into product development environments. An efficient sequence planning approach and a supporting tool are highly demanded. This paper presents a hybrid approach to selective-disassembly sequence planning for DM, which is based on both topological disassemblability and tool accessibility. In addition, a Web-based application on a three-tier Internet environment is implemented for the global concurrent de-manufacturability analysis.",
        "keywords": "de-manufacturing; global concurrent de-manufacturability analysis; hybrid sequence planning; selective-disassembly; tool accessibility",
        "released": 2006,
        "link": "https://doi.org/10.1007/s00170-005-0038-5"
    },
    {
        "title": "Gene-centric analysis of serum cotinine levels in african and european american populations",
        "abstract": "To date, most genetic association studies of tobacco use have been conducted in European American subjects using the phenotype of smoking quantity (cigarettes per day). However, smoking quantity is a very imprecise measure of exposure to tobacco smoke constituents. Analyses of alternate phenotypes and populations may improve our understanding of tobacco addiction genetics. Cotinine is the major metabolite of nicotine, and measuring serum cotinine levels in smokers provides a more objective measure of nicotine dose than smoking quantity. Previous genetic association studies of serum cotinine have focused on individual genes. We conducted a genetic association study of the biomarker in African American (N = 365) and European American (N 315) subjects from the Coronary Artery Risk Development in Young Adults study using a chip containing densely-spaced tag SNPs in similar to 2100 genes. We found that rs11187065, located in the non-coding region (intron I) of insulin-degrading enzyme (IDE), was the most strongly associated SNP (p = 8.91 x 10(-6)) in the African American cohort, whereas rs11763963, located on chromosome 7 outside of a gene transcript, was the most strongly associated SNP in European Americans (p = 1.53 x 10(-6)). We then evaluated how the top variant association in each population performed in the other group. We found that the association of rs11187065 in IDE was also associated with the phenotype in European Americans (p = 0.044). Our top SNP association in European Americans, rs11763963 was non-polymorphic in our African American sample. It has been previously shown that psychostimulant self-administration is reduced in animals with lower insulin because of interference with dopamine transmission in the brain reward centers. Our finding provides a platform for further investigation of this, or additional mechanisms, involving the relationship between insulin and self-administered nicotine dose. Neuropsychopharmacology (2012) 37, 968-974; doi: 10.1038/npp. 2011.280; published online 16 November 2011",
        "keywords": "cotinine; nicotine; IDE; CARDIA; MORF4L1; IMAT-Broad-CARe",
        "released": 2012,
        "link": "https://doi.org/10.1038/npp.2011.280"
    },
    {
        "title": "Toward a knowledge-based personalised recommender system for mobile app development",
        "abstract": "Over the last few years, the arena of mobile application development has expanded considerably beyond the demand of the world’s software markets. With the growing number of mobile software companies and the increasing sophistication of smartphone technology, developers have been establishing several categories of applications on dissimilar platforms. However, developers confront several challenges when undertaking mobile application projects. In particular, there is a lack of consolidated systems that can competently, promptly and efficiently provide developers with personalised services. Hence, it is essential to develop tailored systems that can recommend appropriate tools, IDEs, platforms, software components and other correlated artifacts to mobile application developers. This paper proposes a new recommender system framework comprising a robust set of techniques that are designed to provide mobile app developers with a specific platform where they can browse and search for personalised artifacts. In particular, the new recommender system framework comprises the following functions: (i) domain knowledge inference module: including various semantic web technologies and lightweight ontologies; (ii) profiling and preferencing: a new proposed time aware multidimensional user modelling; (iii) query expansion: to improve and enhance the retrieved results by semantically augmenting users’ query; and (iv) recommendation and information filtration: to make use of the aforementioned components to provide personalised services to the designated users and to answer a user’s query with the minimum mismatches.",
        "keywords": "Semantic Analytics; User Profiling; Machine Learning; Mobile App Development; &nbsp; Software Engineering; Recommender Systems",
        "released": 2021,
        "link": "https://doi.org/10.3897/jucs.65096"
    },
    {
        "title": "Teaching mobile and wireless information systems development in engineering courses",
        "abstract": "MADEE (Mobile Application Development and Execution Environment) is a platform that supports the development of small and middle size mobile and wireless information systems for handheld devices. MADEE allows a student to develop applications faster and easier than using conventional development tools. This study presents the results and experience obtained using MADEE to support the introduction of mobile and wireless information systems development concepts in the context of computer engineering courses. (C) 2009 Wiley Periodicals, Inc. Comput Appl Eng Hue 18: 261-268, 2010; Published online in Wiley Inter Science (www.interscience.wiley.com); DOI 10.1002/cae.20251",
        "keywords": "mobile computing; wireless computing",
        "released": 2010,
        "link": "https://doi.org/10.1002/cae.20251"
    },
    {
        "title": "Java classes for nonprocedural variogram modeling",
        "abstract": "A set of Java(TM) classes was written for variogram modeling to support research for US EPA’s Regional Vulnerability Assessment Program (ReVA). The modeling objectives of this research program are to use conceptual programming tools for numerical analysis for regional risk assessment. The classes presented use of object-oriented design elements, and their use is described for the benefit of programmers. To help facilitate their use, class diagrams and standard JavaDoc commenting were employed, Java’s support for polymorphism and inheritance is used and these are described as ways to promote extension of these classes for other geostatistical applications. Among the advantages is the case of programming, code reuse, and conceptual, rather than procedural implementation. A graphical application for variogram modeling that uses the classes is also provided and described. It can also be used by non-programmers. This application uses a generalized least-squares fitting algorithm for robust parametric variogram model fitting through the variogram cloud. This feature makes this program unique from other freely available variogram modeling programs, though the classes are presented primarily so they may be extended for use in other Java programs. More traditional variogram plotting and fitting utilities are also provided. This application is graphical and platform-neutral. It uses classes of the recently proposed Java API for linear algebra, called the JAMA package. Published by Elsevier Science Ltd.",
        "keywords": "object-oriented modeling; ReVA; iteratively reweighted least squares; multi-disciplinary modeling; JAMA; variogram cloud",
        "released": 2002,
        "link": "https://doi.org/10.1016/S0098-3004(01)00049-8"
    },
    {
        "title": "OntoBroker",
        "abstract": "OntoBroker provides a comprehensive, scalable and high-performance Semantic Web middleware. It supports all of the W3C Semantic Web recommendations for ontology languages and query languages. It is an ontology repository that includes a high performance deductive reasoning engine. Especially reasoning with rules is a major unique selling point for ontoprise. OntoBroker integrates a connector framework which makes it easy to connect a multitude of data sources like databases, web services etc. Thus it combines structured and unstructured data in one framework, OntoBroker is easy to extend and to integrate into existing IT landscapes and applications as it offers a variety of open interfaces. OntoBroker is also closely connected to ontoprise’s ontology modeling environment OntoStudio which is the development environment for handling ontologies, mappings to information sources, rules, generating queries, creating business intelligence reports etc. At many customers OntoBroker serves as a common semantic layer which is accessed by various applications and integrates different information sources. OntoBroker is the run-time environment for industrial solutions like SemanticGuide, SemanticXpress, and SemanticIntegrator. As part of those meanwhile thousands of installations are in productive use.",
        "keywords": "Ontobroker; semantic middleware; F-Logic; ObjectLogic; rule; semantic information integration; triple store",
        "released": 2014,
        "link": "https://doi.org/10.3233/SW-2012-0067"
    },
    {
        "title": "Web-based monitoring and control for BAS using multi-protocol converter with embedded linux",
        "abstract": "In this paper, a Web-based management system for the building network is described. We developed a multi-protocol converter based on SoC and embedded Linux. It requires an appropriate operating system for handling. protocols and an advanced development environment very similar to embedded linux. The multi-protocol converter integrates control networks of RS-485 and LonWorks devices to BAS through TCP/IP protocol or a client with Java applet. The system consists of three-tier architecture, such as BAS or clients, a multi-protocol converter, and control devices. In order to compare the feasibility of system architecture, it was applied to a small BAS system. By using UML, we modeled a Web-based control system with a unified TCP/IP socket communication and the system architecture. The developed system includes the inverter motor control System with modbus protocol for the RS485 network. The experiment results show that the multi-protocol converter using embedded Linux is a flexible and effective way to build a Web-based monitoring and control system.",
        "keywords": "remote monitoring and control; Web-based control; embedded system; LonWorks network; BAS",
        "released": 2005,
        "link": "https://doi.org/10.1093/ietele/e88-c.3.450"
    },
    {
        "title": "Threshold multiparty multi-randomness secure partially homomorphic encryption for data security in cloud",
        "abstract": "Threshold cryptography is used to secure a cryptographic secret by dividing it into N shares and storing each claim on a diverse server. Any subgroup of t servers can use the secret without re-making it. However, an adversary cannot recover the secret by compromising t - 1 servers. Homomorphic encryption operates directly on the ciphertext. In this study, a Threshold multiparty multi-randomness secure partially homomorphic encryption (TM-MR-SPHE) algorithm is proposed to secure the outsourced data and perform multiplication and division operations on the ciphertext. The model is implemented in Eclipse IDE and AWS Toolkit for Eclipse and deployed in Amazon Elastic Beanstalk (EB) environment. This model is mainly used to secure the patient e-health details and perform computation on outsourced data. The patient details are encrypted by the algorithm MR-SPHE and uploaded in AWS (Amazon Web Service) S3 bucket. AWS identity and access management (IAM) creates the users’ service and secret shares among multiple users in the EB environment. The proposed model performance is studied by comparing with other partially homomorphic Elgamal, Pailler, and Benaloh. This model achieves data integrity and confidentiality using the threshold multiparty with multi-randomness secure partially homomorphic encryption.",
        "keywords": "cloud security; multiparty; partially homomorphic encryption; threshold cryptography",
        "released": 2023,
        "link": "https://doi.org/10.1111/exsy.13043"
    },
    {
        "title": "Developing a remote gamma-ray spectra collection system (RGSCS) by coupling a high purity germanium (HPGe) detector with a cosmicguard background reduction device",
        "abstract": "Despite being widely used for high-resolution spectral analysis and quantifying low activity in natural samples, the operations and data analysis of High Purity Germanium (HPGe) gamma-ray detectors are seldom fully automated due to the excessive costs associated with commercially available automatic sample changing systems. This paper introduces the design and implementation of a cost-effective, customized remote gamma-ray spectra collection system centered around the HPGe detector coupled to a cosmic-ray veto background reduction device. The HPGe detector system, equipped with a Lynx DSA, is seamlessly integrated with an economically viable automatic sample changer. This sample vial changer is controlled by a high-torque NEMA 34 stepper servo motor from Vention. Web control of the rotary actuator is facilitated through a CAD-based programming tool. The remote-controlled sample pick-and-place procedure is executed using a robotic arm (Trossen Robotics, Viper X 250). The DYNAMIXEL servomotors of the robotic arm are programmed using Python software supported by the Robotic Operating System. Beyond its technical construction, this system is uniquely fashioned for academic research, providing invaluable hands-on experience in gamma spectrometry to both junior researchers and students.",
        "keywords": "Automatic sampler; Gamma-ray detector; Remote Gamma Spectrum Collection System; CosmicGuard System",
        "released": 2024,
        "link": "https://doi.org/10.1016/j.ohx.2024.e00513"
    },
    {
        "title": "A development and evaluation of nursing KMS using QFD in outpatient departments",
        "abstract": "Purpose: This study was done to develop and implement the Nursing KMS (knowledge management system) in order to improve knowledge sharing and creation among clinical nurses in outpatient departments. Methods; This study was a methodological research using the “System Development Life Cycle”: consisting of planning, analyzing, design, implementation, and evaluation. Quality Function Deployment (QFD) was applied to establish nurse requirements and to identify important design requirements. Participants were 32 nurses and for evaluation data were collected pre and post intervention at K Hospital in Seoul, a tertiary hospital with over 1,000 beds. Results: The Nursing KMS was built using a Linux-based operating system, Oracle DBMS, and Java 1.6 web programming tools. The system was implemented as a sub-system of the hospital information system. There was statistically significant differences in the sharing of knowledge but creating of knowledge was no statistically meaningful difference observed. In terms of satisfaction with the system, system efficiency ranked first followed by system convenience, information suitability and information usefulness. Conclusion: The results indicate that the use of Nursing KMS increases nurses’ knowledge sharing and can contribute to increased quality of nursing knowledge and provide more opportunities for nurses to gain expertise from knowledge shared among nurses.",
        "keywords": "Nurses; Knowledge management; Outpatients",
        "released": 2014,
        "link": "https://doi.org/10.4040/jkan.2014.44.1.64"
    },
    {
        "title": "Comparison of tenofovir <i>versus</i> entecavir on reducing incidence of hepatocellular carcinoma in chronic hepatitis b patients: A systematic review and meta-analysis",
        "abstract": "Background and Aim Studies had shown that tenofovir (TDF) and entecavir (ETV) are widely used as the first-line therapy to inhibit hepatitis B virus replication, which can reduce the risk of hepatocellular carcinoma (HCC) in chronic hepatitis B (CHB) patients, but it was unclear which nucleos(t)ide analogue was most effective. Therefore, we performed a meta-analysis and a systematic review to compare the incidence of HCC in CHB patients who are either on TDF or ETV. Methods For this study, the following databases were searched for clinical trials published from its inception until November 2019: PubMed, Web of Science, MEDLINE, Embase, and Cochrane Library. Results A total of 11 eligible studies were selected, including 70 864 patients. The meta-analysis showed that TDF was superior to ETV with regard to the incidence of HCC, the incidence of death or transplantation, and virologic response. There were no significant differences in terms of biochemical response and loss of seroconversion response among the entire cohort. Conclusions The conclusion was that CHB patients treated with TDF had a reduced incidence of HCC compared with patients treated with ETV.",
        "keywords": "Chronic hepatitis B; Entecavir; Hepatocellular carcinoma; Meta-analysis; Tenofovir",
        "released": 2020,
        "link": "https://doi.org/10.1111/jgh.15036"
    },
    {
        "title": "Density-aware cellular coverage control: Interference-based density estimation",
        "abstract": "As demand for mobile communications increases, cells have to become smaller to efficiently use the scarce spectrum and to increase capacity, and small-cell networks will hereby emerge. They may be large in scale and highly dynamic resembling ad hoc networks due to the moving base stations. The variations in the density of the small cell networks impact the quality of service and introduce many novel challenges such as coverage control. We propose two novel base station density estimators, the interference-based density estimator (IDE) and the multi-access edge cloud-based density estimator (CDE) in a three-dimensional field. The estimators employ received signal strength measurements. We validate these two density estimators by using Monte-Carlo simulations. Furthermore, we analyze the impact of density on network outage in cellular networks and propose a density-aware cell zooming technique. According to the observations, base station (BS) density affects network coverage significantly. Received signal strength (RSS)-based density estimators can easily be implemented and applied in the network communication stack although they are more prone to shadowing and fading. Under favour of the density-aware cell zooming method, the network outage can be managed dynamically by adapting the transmit power, which provides a self-configurable and -organized network. (C) 2019 Elsevier B.V. All rights reserved.",
        "keywords": "Cellular networks; Moving base stations; Base station density estimation; Density-adaptive networking; Network coverage",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.comnet.2019.106922"
    },
    {
        "title": "Complex structures made simple - continuous flow production of core cross-linked polymeric micelles for paclitaxel pro-drug-delivery",
        "abstract": "Translating innovative nanomaterials to medical products requires efficient manufacturing techniques that enable large-scale high-throughput synthesis with high reproducibility. Drug carriers in medicine embrace a complex subset of tasks calling for multifunctionality. Here, the synthesisof pro-drug-loaded core cross-linked polymeric micelles (CCPMs) in a continuous flow processis reported, which combines the commonly separated steps of micelle formation, core cross-linking, functionalization, and purification into a single process. Redox-responsive CCPMs are formed from thiol-reactive polypept(o)ides of polysarcosine-block-poly(S-ethylsulfonyl-l-cysteine) and functional cross-linkers based on dihydrolipoic acid hydrazide for pH-dependent release of paclitaxel. The precisely controlled microfluidic process allows the production of spherical micelles (D-h = 35 nm) with low polydispersity values (PDI < 0.1) while avoiding toxic organic solvents and additives with unfavorable safety profiles. Self-assembly and cross-linking via slit interdigital micromixers produces 350-700 mg of CCPMs/h per single system, while purification by online tangential flow filtration successfully removes impurities (unimer <= 0.5%). The formed paclitaxel-loaded CCPMs possess the desired pH-responsive release profile, display stable drug encapsulation, an improved toxicity profile compared to Abraxane (a trademark of Bristol-Myers Squibb), and therapeutic efficiency in the B16F1-xenotransplanted zebrafish model. The combination of reactive polymers, functional cross-linkers, and microfluidics enables the continuous-flow synthesis of therapeutically active CCPMs in a single process.",
        "keywords": "cross-linking; microfluidics; nanomedicine; polymeric micelles; polypept(o)ides",
        "released": 2023,
        "link": "https://doi.org/10.1002/adma.202210704"
    },
    {
        "title": "An architecture for supporting disconnected operation in workflow: An XML-based approach",
        "abstract": "Nowadays, there exist few options to support workflow and cooperation among mobile users using non-traditional computing platforms such as Personal Digital Assistants (PDAs). Considering the growing need for mobility and the more frequent use of mobile devices, it is necessary to provide support for the integration of these to the work environments. In this work, we present the development of the SysCoor Workflow Management System (WFMS) architecture, which provides support for the semi-automatic generation of Web-based workflow systems and disconnected workflow through the use of models described in the eXtensible Markup Language (XML) and also considers the use of image, video and multimedia presentations as part of the workflow information entities. We present a scenario that illustrates a disconnected process and how it could benefit by this approach. We establish the requirements for supporting disconnected workflow and discuss the architecture of the system that is based in the use of XML for the specification of the workflow systems and disconnected operations. The workflow systems’ specification is generated in XML to enable interoperability with other WFMS engines. The development of the disconnection mechanism is presented in terms of a lightweight architecture for PDA devices developed using the SuperWaba programming language, however, the architecture described can be implemented using other programming tools.",
        "keywords": "",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000188006200024"
    },
    {
        "title": "Accelerometer based structural health monitoring system on the go: Developing monitoring systems with NI LabVIEW",
        "abstract": "Structural Health Monitoring (SHM) is a very crucial part of maintenance and management of buildings and structures. The use of SHM in recent years has been increasing due to the advancement in technology and the availability of nanodevices and nanosensors which can detect damaged part or crack in a structure. In this paper, PSpice simulation was carried out to show the response of the integrated electronic piezoelectric (IEPE) with a VPWL-source. Then, practical experiment was done using Arduino Mega with the ADXL335 accelerometer in a laboratory setup. LabVIEW software was used along with Arduino IDE software to make graphical visualization of accelerometer reading to be captured. Furthermore, a web service was deployed which enabled LabVIEW data transmission to a smartphone running Data Dashboard application for real-time monitoring anywhere. Therefore, making the system an ecosystem of Internet of Things enabling the user to access monitoring system while on the move. The result of the vibration test on the accelerometer showed that the accelerometer response to small changes in the x, y and z axis of the accelerometer which can be used to detect micro-movements in a structure.",
        "keywords": "Structural Health Monitoring (SHM); Accelerometer; ADXL335; LabVIEW; LabView Interface for Arduino (LIFA); Virtual Instruments (VI); Internet of Things (IoT)",
        "released": 2019,
        "link": "https://doi.org/10.3991/ijoe.v15i07.10427"
    },
    {
        "title": "A collaborated computing system by web services based P2P architecture",
        "abstract": "The peer-to-peer (P2P) model which shares the content or resources over the network gradually replaces the traditional client/server architecture. A new computing architecture: Computing Power Services (CPS) which utilizes Web Services and Business Process Execution Language (BPEL) to overcome the problems of P2P about flexibility, compatibility and workflow management is proposed in this study. CPS is a lightweight Web Services based P2P power sharing environment, and suitable for enterprise computing works which are able to run in batch format in a trusty network. The architecture relies on BPEL which provides a visualized development environment and workflow control management. In this paper, the collaborated computing system has been applied to analyze the robustness of digital watermark by filter bank selection. As the result of this case, the performance can be improved in the aspect of speedup, efficiency and process time.",
        "keywords": "",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000237080800020"
    },
    {
        "title": "Using simulation for understanding and reproducing distributed software development processes in the cloud",
        "abstract": "Context Organizations increasingly develop software in a distributed manner. The Cloud provides an environment to create and maintain software-based products and services. Currently, it is unknown which software processes are suited for Cloud-based development and what their effects in specific contexts are. Objective: We aim at better understanding the software process applied to distributed software development using the Cloud as development environment. We further aim at providing an instrument, which helps project managers comparing different solution approaches and to adapt team processes to improve future project activities and outcomes. Method: We provide a simulation model, which helps analyzing different project parameters and their impact on projects performed in the Cloud. To evaluate the simulation model, we conduct different analyses using a Scrumban process and data from a project executed in Finland and Spain. An extra adaptation of the simulation model for Scrum and Kanban was used to evaluate the suitability of the simulation model to cover further process models. Results: A comparison of the real project data with the results obtained from the different simulation runs shows the simulation producing results close to the real data, and we could successfully replicate a distributed software project. Furthermore, we could show that the simulation model is suitable to address further process models. Conclusion: The simulator helps reproducing activities, developers, and events in the project, and it helps analyzing potential tradeoffs, e.g., regarding throughput, total time, project size, team size and work-in-progress limits. Furthermore, the simulation model supports project managers selecting the most suitable planning alternative thus supporting decision-making processes.",
        "keywords": "Scrum; Kanban; Process simulation; Comparison",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.infsof.2018.07.004"
    },
    {
        "title": "Ontology design for identifying instructional strategies for an intelligent tutoring system",
        "abstract": "This paper provides an agent based on an ontology that helps instructors to guide the process of selecting teaching and learning strategies. We present an approach of ontology representation based on a novel architecture of agent-based simulation of teaching and learning process and propose a conceptual architecture of such system. This paper also is focused on proposing a framework of integrating domain knowledge, content knowledge, adaptation rules, and student models in a Web-based ITS. We illustrate how these techniques can be put into practice using the modern ontology development tool Protege, and indicate future possibilities.",
        "keywords": "Intelligent agents; Ontologies; Intelligent tutoring systems; Knowledge-based system",
        "released": 2011,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000293730200002"
    },
    {
        "title": "HPG: The hera presentation generator",
        "abstract": "This paper presents a model-based design and development environment for Web Information Systems (WISs) using Semantic Web technologies. This environment called the Hera Presentation Generator (HPG) integrates a number of software tools created for the presentation generation phase of the Hera methodology. These tools are the HPG model builders that help constructing the WIS input specifications and the HPG engine that implements the data transformations involved in a WIS. There are two versions of the HPG engine: HPG-XSLT and HPG-Java. HPG-XSLT is characterized by the use of XSLT stylesheets for the data transformations and by the full generation of a Web presentation. HPG-Java uses Java code for the data transformations and thus exploits more than HPG-XSLT the RDF(S) semantics captured in the Hera models and builds one-page-at-a-time. Generating one-page-at-a-time is motivated by recent extensions to the Hera methodology in order to better sustain the building of WISs with richer user interaction support (e.g., form-based). Nevertheless, HPG-Java lost the declarativity, simplicity, and reuse capabilities of the XSLT transformation templates. HPG thus fills the existing gap for tool support for the design of WIS using Semantic Web technologies.",
        "keywords": "WIS; semantic web; RDF(S); design environment",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000251057100005"
    },
    {
        "title": "The ACOUCOU platform: Online acoustic education developed by an interdisciplinary team",
        "abstract": "The ACOUCOU platform is a web-based, interactive, acoustics training platform that includes a set of free educational materials in various technical fields of acoustics. Educational materials are designed to serve as a modern self-development tool for students and engineers, as well as a comprehensive solution for professional education in the work environment. On the other hand, the provided materials of the platform can be a useful tool, supporting teachers, company researchers, and academic lecturers in the process of teaching acoustics. The ACOUCOU platform is a part of a strategic plan for expanding and strengthening acoustic knowledge web-based tools and supporting the development of innovative teaching methods based on attractive and effective delivery of digital content, and best practices at national and international levels. It addresses the challenge of a lack of experts in the acoustics field and the growing needs of the market. (C) 2022 Acoustical Society of America.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1121/10.0014170"
    },
    {
        "title": "Web-based decision support system tools: The soil and water assessment tool online visualization and analyses (SWATOnline) and NASA earth observation data downloading and reformatting tool (NASAaccess)",
        "abstract": "The current influx of climate related information required scientists to communicate their findings to decision makers in governments, disaster preparedness organizations, and the general public. The Soil and Water Assessment Tool (SWAT) is a powerful modelling tool that allows scientists to simulate many of the physical processes involved in the water cycle. This article presents the design, methods and development efforts to overcome some of the limitations of the previously developed SWAT visualization software programs by creating a set of modular web applications that can be duplicated, customized, and run. Moreover, this article features a web application development tool for climate data retrieval. The NASAaccess fetches, extracts and reformats climate data from the National Aeronautics and Space Administration servers and outputs data compatible with hydrological models. This work has the potential to increase the SWAT’s model impact on non-technically trained stakeholders and decision makers charged with water and climate management.",
        "keywords": "Tethys; SWAT; Remote sensing; Climate; Water management; NASA",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.envsoft.2019.104499"
    },
    {
        "title": "An open framework for distributed biometric authentication in a web environment",
        "abstract": "Large-scale deployment of biometric systems for web-based services has to tackle technological issues related to security, interoperability and accuracy, and social issues related to privacy protection and biometric acquisition process acceptance. The variety of biometric traits, capturing devices, targeted populations and working scenarios makes the development of a universal solution for all-purpose deployment quite a difficult task. This paper describes the design, implementation and applicability of an open framework for distributed biometric authentication oriented to access-control in web environment. The open principle makes this framework a novel and practical development tool for testing and integrating biometric algorithms and devices from third parties. Special attention has been paid to security and interoperability standards to ease concurrent integration and testing of biometric trait matchers developed by different laboratories or companies. Finally, in order to demonstrate the versatility and usability of the framework, we describe the construction process for a distributed multibiometric database acquisition tool based on this framework.",
        "keywords": "biometrics; authentication; distributed processing; World Wide Web; interoperability; open system",
        "released": 2007,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000245366300009"
    },
    {
        "title": "Web based software for the study of USDA soil taxonomy and classification of newly found soil",
        "abstract": "United States Department of Agriculture (USDA) Soil Taxonomy is based on soil properties that can be objectively observed and measured in the natural conditions as they exist today. There are many soil classification systems but USDA Soil Taxonomy is most accepted worldwide. Ontologies are the new form of knowledge representation that acts in synergy with agents and Semantic Web Architecture. Soil ontology developed for USDA soil taxonomy has been used to develop a query interface that will help in detailed study of soil taxonomy, classification of new soil as well as exchange knowledge between software agents and systems. This is a web based application having N-tier architecture. Application development environment is NetBeans 6.9 editor and Prot g. Web development technology is Java Server Pages (JSP). Programming languages JAVA and SPARQL are used for querying. Client interface is developed with Hyper Text Markup Language (HTML), Cascading Style Sheet (CSS) and JavaScript. Third tier of software consist of database which is in MS-SQL server 2005. Other two layers are Web Ontology Language (OWL) Ontology layer and Semantic Web Framework layer. OWL layer contains soil taxonomy information in the form of Ontology. Semantic Web Framework layer is implemented using JENA. In the search panel user can search anything related to USDA Soil Taxonomy, which comprises of twelve orders. However, this software contains information about seven soil orders reported in India. Domain experts can see and edit the knowledge base (i.e. Soil Ontology) or can suggest anything related to the creation of Soil Taxonomy Ontology through WebProtege.",
        "keywords": "Ontology; OWL; USDA Soil Taxonomy; WebProtege",
        "released": 2014,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000342116900012"
    },
    {
        "title": "Advancing ocean science through open science and software on the cloud",
        "abstract": "This Ocean-Shot will seek to advance community awareness and practice around open science, based on the activities and outcomes generated by a growing cadre of U.S.-based and international developers working with open software libraries and in cloud computing environments. This practice will require additional software and tooling and required a shift in thinking toward a philosophy of trust and transparency. Key to this endeavor will be to take advantage of innovative development tools and practices that will better enable science by decreasing barriers to collaborations, reproducibility, and interdisciplinary research. For data, this will mark a shift from a central repository model to central service model, enabling data-proximate computing. Most commonly today, data are downloaded from central locations for analysis on local environments. We hope to advance dataproximate computing that facilitates the accessing of data in central locations and running analysis there. This will mark a move from current reliance on locally deployed packages and servers to making products and services readily available on the cloud. The project will contribute to a step change of how datasets are accessed and used across the community. Throughout the decade it will build on the training and capacity development currently supported by NASA, and upon the professional development activities of other stakeholders.",
        "keywords": "",
        "released": 2021,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000707639700053"
    },
    {
        "title": "Using 3D printing to visualize 2D chromatograms and NMR spectra for the classroom",
        "abstract": "The use of three-dimensional printing in chemistry education has expanded greatly in the past 10 years. The technique has been used to demonstrate a range of concepts including molecular structure, orbitals, and point groups; to produce chemical equipment such as cuvettes and columns; and even to print out mathematical shapes and functions. Here, 3D printing is used to create physical models of two-dimensional NMR spectra and HPLC chromatograms to facilitate student understanding of these challenging concepts. The target audience are undergraduate and postgraduate students as well as university teachers and researchers. Methods to create and print tangible models of the data are presented using the Mathematica and MATLAB programming environments. The models can then be used as useful teaching tools to assist with developing representational competence. The underlying data for the models may be created in-house or obtained from online databases. Instructions to convert the raw data to 3D printable files are provided, and the options for optimizing the resulting files are discussed. These innovative physical models allow students, particularly those who are visual and/or tactile learners, to better understand the complex information presented in multidimensional spectra and chromatograms and enhance student understanding of these forms of data.",
        "keywords": "General Public; Analytical Chemistry; Hands-On Learning/Manipulatives; NMR Spectroscopy; HPLC",
        "released": 2021,
        "link": "https://doi.org/10.1021/acs.jchemed.0c01130"
    },
    {
        "title": "An XML-based quality of service enabling language for the web",
        "abstract": "In this paper, we introduce an XML-based hierarchical QoS markup language, called HQML, to enhance distributed multimedia applications on the World Wide Web (WWW) with quality of service (QoS) capability. The design of HQML is based on two observations: (1) the absence of a systematic QoS specification language, that can be used by distributed multimedia applications on the WWW to utilize the state-of-the-art QoS management technology and (2) the power and popularity of XML to deliver richly structured contents over the Web. HQML allows distributed multimedia applications to specify all kinds of application-specific QoS policies and requirements. During runtime, the HQML Executor translates the HQML file into desired data structures and cooperates with the QoS proxies that assist applications in end-to-end QoS negotiation, setup and enforcement. In order to make QoS services tailored toward user preferences and meet the challenges of uncertainty in the distributed heterogeneous environments, the design of HQML is featured as interactive and flexible. In order to allow application developers to create HQML specifications correctly and easily, we have designed and developed a unified visual QoS programming environment, called QoSTalk. In QoSTalk, we adopt a grammatical approach to perform consistency check on the visual QoS specifications and generate HQML files automatically. Finally, we introduce the distributed QoS compiler, which performs the automatic mappings between application- and resource-level QoS parameters to relieve the application developer of the burden of dealing with low-level QoS specifications. (C) 2002 Elsevier Science Ltd. All rights reserved.",
        "keywords": "quality of service; XML; distributed multimedia applications; visual programming environment",
        "released": 2002,
        "link": "https://doi.org/10.1006/jvlc.2001.0227"
    },
    {
        "title": "Smart fitting: An augmented reality mobile application for virtual try-on",
        "abstract": "While online shopping has become popular in recent years, certain problems related to customer expectations about the appearance and fitting of clothing items still need to be solved. Online customers looking for clothing products may be unsatisfied as they do not have the possibility to try on clothes and view how they would look on them. Additionally, vendors are burdened with the cost of items returned by unsatisfied customers. Augmented Reality (AR) can help vendors to attract more customers and enhance their shopping experience. Customers are more likely to be satisfied with their purchases when they can try the clothes on before purchasing them, which ultimately reduces the return costs for vendors. This paper presents an AR mobile application called Smart Fitting which was developed in order to allow online customers to try on clothing items by using 3D models virtually. The most recent AR development tools for iOS operating systems, including XCode and Unity, are utilized for creating the proposed mobile application. After an integration testing for the mobile application functions, seven target female consumers participated in the usability evaluation for this mobile application which was meant to prove its ease of use and evaluate the customers’ satisfaction. The obtained results showed that Smart Fitting can improve the users shopping experience by providing a more realistic clothes visualization than the traditional online shopping interfaces.",
        "keywords": "Augmented Reality; Body Tracking; Unity; Superimposition; Virtual Try-on",
        "released": 2023,
        "link": "https://doi.org/10.33436/v33i2y202308"
    },
    {
        "title": "Upgrading a TCABR data analysis and acquisition system for remote participation using java, XML, RCP and modern client/server communication/authentication",
        "abstract": "The TCABR data analysis and acquisition system has been upgraded to support a joint research programme using remote participation technologies. The architecture of the new system uses Java language as programming environment. Since application parameters and hardware in a joint experiment are complex with a large variability of components, requirements and specification solutions need to be flexible and modular, independent from operating system and computer architecture. To describe and organize the information on all the components and the connections among them, systems are developed using the extensible Markup Language (XML) technology. The communication between clients and servers uses remote procedure call (RPC) based on the XML (RPC-XML technology). The integration among Java language, XML and RPC-XML technologies allows to develop easily a standard data and communication access layer between users and laboratories using common software libraries and Web application. The libraries allow data retrieval using the same methods for all user laboratories in the joint collaboration, and the Web application allows a simple graphical user interface (GUI) access. The TCABR tokamak team in collaboration with the IPFN (Instituto de Plasmas e Fusao Nuclear, Instituto Superior Tecnico, Universidade Tecnica de Lisboa) is implementing this remote participation technologies. The first version was tested at the Joint Experiment on TCABR (TCABRJE), a Host Laboratory Experiment, organized in cooperation with the IAEA (International Atomic Energy Agency) in the framework of the IAEA Coordinated Research Project (CRP) on “Joint Research Using Small Tokamaks”. (C) 2010 Elsevier B.V. All rights reserved.",
        "keywords": "Remote participation; Data acquisition; Control; Java; XML-RPC",
        "released": 2010,
        "link": "https://doi.org/10.1016/j.fusengdes.2010.04.067"
    },
    {
        "title": "A near real-time hydrological information system for the upper danube basin",
        "abstract": "The multi-national catchment of the Upper Danube covers an area of more than 100,000 km(2) and is of great ecological and economic value. Its hydrological states (e.g., runoff conditions, snow cover states or groundwater levels) affect fresh-water supply, agriculture, hydropower, transport and many other sectors. The timely knowledge of the current status is therefore of importance to decision makers from administration or practice but also the interested public. Therefore, a web-based, near real-time hydrological information system was conceptualized and developed for the Upper Danube upstream of Vienna (Upper Danube HIS), utilizing ERA5 reanalysis data (ERA5) and hydrological simulations provided by the semi-distributed hydrological model COSERO. The ERA5 reanalysis data led to comparatively high simulation performance for a total of 65 subbasins with a median NSE and KGE of 0.69 and 0.81 in the parameter calibration and 0.63 and 0.75 in the validation period. The Upper Danube HIS was implemented within the R programming environment as a web application based on the Shiny framework. This enables an intuitive, interactive access to the system. It offers various capabilities for a hydrometeorological analysis of the 65 subbasins of the Upper Danube basin, inter alia, a method for the identification of hydrometeorological droughts. This proof of concept and system underlines how valuable information can be obtained from freely accessible data and by the means of open source software and is made available to the hydrological community, water managers and the public.",
        "keywords": "Upper Danube basin; hydrological information system; hydrological modelling; COSERO model; hydrometeorological deficit",
        "released": 2021,
        "link": "https://doi.org/10.3390/hydrology8040144"
    },
    {
        "title": "CodLncScape provides a self-enriching framework for the systematic collection and exploration of coding LncRNAs",
        "abstract": "Recent studies have revealed that numerous lncRNAs can translate proteins under specific conditions, performing diverse biological functions, thus termed coding lncRNAs. Their comprehensive landscape, however, remains elusive due to this field’s preliminary and dispersed nature. This study introduces codLncScape, a framework for coding lncRNA exploration consisting of codLncDB, codLncFlow, codLncWeb, and codLncNLP. Specifically, it contains a manually compiled knowledge base, codLncDB, encompassing 353 coding lncRNA entries validated by experiments. Building upon codLncDB, codLncFlow investigates the expression characteristics of these lncRNAs and their diagnostic potential in the pan-cancer context, alongside their association with spermatogenesis. Furthermore, codLncWeb emerges as a platform for storing, browsing, and accessing knowledge concerning coding lncRNAs within various programming environments. Finally, codLncNLP serves as a knowledge-mining tool to enhance the timely content inclusion and updates within codLncDB. In summary, this study offers a well-functioning, content-rich ecosystem for coding lncRNA research, aiming to accelerate systematic studies in this field. codLncScape consists of a set of interconnected tools that work together to create a self-sustaining loop of knowledge expansion in coding lncRNA research. The cycle begins with codLncDB, moves on to codLncFlow, and then to codLncWeb, which leads to codLncNLP, before returning to codLncDB. Specifically, it includes a knowledge base, analysis workflow, online platform, and NLP model. image",
        "keywords": "coding lncRNAs; computational biology; computational precision health; data collection; machine learning",
        "released": 2024,
        "link": "https://doi.org/10.1002/advs.202400009"
    },
    {
        "title": "Dynamic digital map of the springerville volcanic field and the DDM-template: An example of an open-source tool to distribute maps, data, articles, and multi-media materials",
        "abstract": "Dynamic Digital Maps (DDMs) are computer programs that manage the display and distribution of high-quality color maps, digital images, movies, analytical data, and explanatory text, including field guides. They do this in a cross-platform format that opens associated files of maps, images, and movies either from a local device (e. g., a hard drive) or from a web source (server). DDMs are intuitive to use, can be easily and quickly searched for sample and image sites and analytical data, and require no additional software such as web browsers or readers to operate. DDMs fill a niche between the extremes in the digital mapping world that range from a simple digital copy of a paper map to the highly linked geographic information system (GIS) product. A DDM enables one to create an integrated study that confines its focus on a specific map, unlike other interfaces. They offer an ideal way to present, for example, premeeting or postmeeting field trips, so they can be pre-run or revisited, enriching the experience. All DDM maps and images can be saved to disk for printing, and data saved to tab-delimited files. DDMs are made using the open-source DDM-Template, written in the cross-platform programming environment Runtime Revolution, as assisted by videos, tutorials, and the DDM-Cookbook. The DDM of the Springerville volcanic field, the example used here to demonstrate these capabilities, was made from this template. The template is highly extensible, and ongoing modifications and updates are available, as are more than 20 other examples of DDMs.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1130/GES00531.1"
    },
    {
        "title": "The eight learning events model: A pedagogic conceptual tool supporting diversification of learning methods",
        "abstract": "This paper presents the eight Learning Events Model ( 8LEM), a pedagogical reference framework which was used, in more than 100 online courses, as a starting point for instructional planning. Besides supporting teachers in early stages of the learning design continuum, the paper shows how this learning/teaching model, as a professional development tool, prompts them to diversify the learning methods experienced by students in their courses. A three-pronged rationale about the importance of this diversification with respect to “mathetic” competence development, epistemology and personalization is also submitted to discussion.",
        "keywords": "",
        "released": 2007,
        "link": "https://doi.org/10.1080/10494820701343694"
    },
    {
        "title": "Evaluating datalog tools for meta-reasoning over OWL 2 QL",
        "abstract": "Metamodeling is a general approach to expressing knowledge about classes and properties in an ontology. It is a desirable modeling feature in multiple applications that simplifies the extension and reuse of ontologies. Nevertheless, allowing metamodeling without restrictions is problematic for several reasons, mainly due to undecidability issues. Practical languages, therefore, forbid classes to occur as instances of other classes or treat such occurrences as semantically different objects. Specifically, meta-querying in SPARQL under the Direct Semantic Entailment Regime uses the latter approach, thereby effectively not supporting meta-queries. However, several extensions enabling different metamodeling features have been proposed over the last decade. This paper deals with the Metamodeling Semantics (MS) over OWL 2 QL and the Metamodeling Semantic Entailment Regime (MSER), as proposed in Lenzerini et al. (2015, Description Logics) and Lenzerini et al. (2020, Information Systems 88, 101294), Cima et al. (2017, Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics, 1-6). A reduction from OWL 2 QL to Datalog for meta-querying was proposed in Cima et al. (2017, Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics, 1-6). In this paper, we experiment with various logic programming tools that support Datalog querying to determine their suitability as back-ends to MSER query answering. These tools stem from different logic programming paradigms (Prolog, pure Datalog, Answer Set Programming, Hybrid Knowledge Bases). Our work shows that the Datalog approach to MSER querying is practical also for sizeable ontologies with limited resources (time and memory). This paper significantly extends Qureshi and Faber (2021, International Joint Conference on Rules and Reasoning, Springer, 218-233.) by a more detailed experimental analysis and more background.",
        "keywords": "ontology; Datalog; metamodeling; rules",
        "released": 2024,
        "link": "https://doi.org/10.1017/S1471068424000073"
    },
    {
        "title": "Ganesh: Grid application management and enhanced scheduling",
        "abstract": "Grid computing is emerging as the next generation computing environment. Various meta-computing and distributed computing environments are becoming grid compliant with the aim of achieving interoperability. We have developed a web-based development environment for distributed computing that supports heterogeneous resources and provides ease of use. We present herewith GANESH that achieves grid compliance using Globus and also extends features for added value. This paper describes GANESH architecture and its services, and explains its grid compliance and extended features. We also explain in this paper how and where it is interoperable with other environments.",
        "keywords": "",
        "released": 2007,
        "link": "https://doi.org/10.1177/1094342007083777"
    },
    {
        "title": "A comprehensive framework and tool for supporting progressive learning of software development in an academic learning environment",
        "abstract": "The use of technology-enabled environments to facilitate online education has increasingly gained importance to promote learning. Tools like learning management systems, virtual laboratory platforms, online monitoring, and mentoring tools, all form a concrete technology bed for augmenting online education. However, successful use of these segregated tools is possible only through the implementation of a structured process or framework of usage of these tools for learning. In this paper, we propose a comprehensive framework for supporting progressive learning of software development by students. This framework provides guidelines for structuring individual and collaborative programming activities of students at various levels and is realized through the implementation of a tool named eGuru. This tool is used for managing, monitoring, mentoring, and facilitating collaborative programming activities of students and also provides a detailed user interface for facilitating the academic activities of teachers and lab instructors. Peers, alumni, and industry experts also play important roles in providing online mentoring using this tool. The execution structure proposed in the framework is effective not only for promoting online programming education but also creates a virtual environment analogous to the collaborative development environment existing in the software industry, and thus helps in preparing students for the future.",
        "keywords": "collaborative pair programming; collaborative quadruple programming; collaborative software development; progressive learning; team-based learning",
        "released": 2022,
        "link": "https://doi.org/10.1002/cae.22460"
    },
    {
        "title": "Steps towards the automatic production of performance models of web applications",
        "abstract": "The automatic production of performance models of software products can, encourage software designers to include performance validation in their best practices. The incorporation of methods for automatic production can also be of interest of CASE tool vendors to improve the capabilities of their commercial software development environments. This paper deals with a method that introduces a systematic approach towards the automatic production of performance models of web applications (i.e. software applications run on web platforms). The method takes in input two sets of data, the description of the platform architecture (a general view of the system platform and a detailed view of the packet flow in the platform itself) and a set of data that describes the. workload imposed on the platform by the application. The produced model is an extended queueing network ready to be used by conventional evaluation tools to derive predictions on the performance of the software applications. An example is given of the method application, in which predictions of the performance of the application area obtained versus various combinations of the processing powers of the interacting hosts. (C) 2002 Elsevier Science B.V. All rights reserved.",
        "keywords": "web performance; web configuration; performance prediction; model production",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000180401200003"
    },
    {
        "title": "A high-level semantic approach to end-user development in the internet of things",
        "abstract": "Various programming environments for End-User Development (EUD) allow the composition of Internet of Things (IoT) applications, i.e., connections between IoT objects to personalize their joint behavior. These environments, however, only support a one-to-one mapping between pairs of object instances, and adopt a low level of abstraction that forces users to be aware of every single technology they may encounter in their applications. As a consequence, numerous open questions remain: would a “higher level” of abstraction help users creating their IoT applications more effectively and efficiently compared with the contemporary low-level representation? Which representation would users prefer? How high-level IoT applications could be actually executed? To answer these questions, we introduce EUPont, a high-level semantic model for EUD in the IoT. EUPont allows the creation of high-level IoT applications, able to adapt to different contextual situations. By integrating the ontology in the architecture of an EUD platform, we demonstrate how the semantic capabilities of the model allow the execution of high-level IoT applications. Furthermore, we evaluate the approach in a user study with 30 participants, by comparing a Web interface for composing IoT applications powered by EUPont with the one employed by a widely used EUD platform. Results show that the high-level approach is understandable, and it allows users to create IoT applications more correctly and quickly than contemporary solutions.",
        "keywords": "End-User Development; Internet of Things; Trigger-action programming; Semantic Web; Abstraction",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.ijhcs.2018.12.008"
    },
    {
        "title": "The data sensor hub (DaSH): A physical computing system to support middle school inquiry science instruction",
        "abstract": "This article describes a sensor-based physical computing system, called the Data Sensor Hub (DaSH), which enables students to process, analyze, and display data streams collected using a variety of sensors. The system is built around the portable and affordable BBC micro:bit microcontroller (expanded with the gator:bit), which students program using a visual, cloud-based programming environment intended for novices. Students connect a variety of sensors (measuring temperature, humidity, carbon dioxide, sound, acceleration, magnetism, etc.) and write programs to analyze and visualize the collected sensor data streams. The article also describes two instructional units intended for middle grade science classes that use this sensor-based system. These inquiry-oriented units engage students in designing the system to collect data from the world around them to investigate scientific phenomena of interest. The units are designed to help students develop the ability to meaningfully integrate computing as they engage in place-based learning activities while using tools that more closely approximate the practices of contemporary scientists as well as other STEM workers. Finally, the article articulates how the DaSH and units have elicited different kinds of teacher practices using student drawn modeling activities, facilitating debugging practices, and developing place-based science practices.",
        "keywords": "sensor use in education; programmable sensors in education; inquiry science education",
        "released": 2021,
        "link": "https://doi.org/10.3390/s21186243"
    },
    {
        "title": "Designing electronic reference documentation for software component libraries",
        "abstract": "Contemporary software development is based on global sharing of software component libraries. As a result, programmers spend much time reading reference documentation rather than writing code, making library reference documentation a central programming tool. Traditionally, reference documentation is designed for textbooks even though it may be distributed online. However, the computer provides new dimensions of change, evolution, and adaptation that can be utilized to support efficiency and quality in software development. What is difficult to determine is how the electronic text dimensions best can be utilized in library reference documentation. This article presents a study of the design of electronic reference documentation for software component libraries. Results are drawn from a study in an industrial environment based on the use of an experimental electronic reference documentation (called Dynamic Javadoc or DJavadoc) used in a real-work situation for 4 months. The results from interviews with programmers indicate that the electronic library reference documentation does not require adaptation or evolution on an individual level. More importantly, reference documentation should facilitate the transfer of code from documentation to source files and also support the integration of multiple documentation sources. (C) 2002 Elsevier Inc. All rights reserved.",
        "keywords": "electronic documentation; programming; reference documentation",
        "released": 2003,
        "link": "https://doi.org/10.1016/S0164-1212(02)00136-X"
    },
    {
        "title": "Towards gestured-based technologies for human-centred smart factories",
        "abstract": "Despite the increasing degree of automation in industry, manual or semi-automated are commonly and inevitable for complex assembly tasks. The transformation to smart processes in manufacturing leads to a higher deployment of data-driven approaches to support the worker. Upcoming technologies in this context are oftentimes based on the gesture-recognition, - monitoring or - control. This contribution systematically reviews gesture or motion capturing technologies and the utilization of gesture data in the ergonomic assessment, gesture-based robot control strategies as well as the identification of COVID-19 symptoms. Subsequently, two applications are presented in detail. First, a holistic human-centric optimization method for line-balancing using a novel indicator - ErgoTakt - derived by motion capturing. ErgoTakt improves the legacy takt-time and helps to find an optimum between the ergonomic evaluation of an assembly station and the takt-time balancing. An optimization algorithm is developed to find the best-fitting solution by minimizing a function of the ergonomic RULA-score and the cycle time of each assembly workstation with respect to the workers’ ability. The second application is gesture-based robot-control. A cloud-based approach utilizing a generally accessible hand-tracking model embedded in a low-code IoT programming environment is shown.",
        "keywords": "Gesture-Based monitoring; gesture-based control; assembly; manufacturing",
        "released": 2023,
        "link": "https://doi.org/10.1080/0951192X.2022.2121424"
    },
    {
        "title": "Extract, transform, load framework for the conversion of health databases to OMOP",
        "abstract": "Common data models standardize the structures and semantics of health datasets, enabling reproducibility and large-scale studies that leverage the data from multiple locations and settings. The Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) is one of the leading common data models. While there is a strong incentive to convert datasets to OMOP, the conversion is time and resource-intensive, leaving the research community in need of tools for mapping data to OMOP. We propose an extract, transform, load (ETL) framework that is metadata-driven and generic across source datasets. The ETL framework uses a new data manipulation language (DML) that organizes SQL snippets in YAML. Our framework includes a compiler that converts YAML files with mapping logic into an ETL script. Access to the ETL framework is available via a web application, allowing users to upload and edit YAML files via web editor and obtain an ETL SQL script for use in development environments. The structure of the DML maximizes readability, refactoring, and maintainability, while minimizing technical debt and standardizing the writing of ETL operations for mapping to OMOP. Our framework also supports transparency of the mapping process and reuse by different institutions.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1371/journal.pone.0266911"
    },
    {
        "title": "Product channeling in an O2O supply chain management as power transmission in electric power distribution systems",
        "abstract": "With the aim of delivering goods and services to customers, optimal delivery channel selection is a significant part of supply chain management. Several heuristics have been developed to solve the variants of distribution center allocation and vehicle routing problems. In reality, small-scale suppliers cannot afford research and development departments to optimize their distribution networks. In this context, this research work develops a model for an online to offline (O2O) supply chain management network of a small-scale household electric components manufacturer for delivering goods to its distribution centers and retailers. Retailers are acquired by the company through investment in the O2O channel of e-commerce. Electric power transmission and distribution is considered as representative of the product distribution network. A model is developed using a combination of the supply chain management technique and power transmission terminologies. The constrained linear programming model is solved through the linear programming tool of the LINGO optimization software and the global optimum results for the proposed quantity allocation problem are achieved. A numerical experiment is provided to illustrate the practical applicability of the model and the optimal results are analyzed for model robustness.",
        "keywords": "supply chain management; e-commerce; O2O channel; customer acquisition cost; transshipments; electric power distribution",
        "released": 2019,
        "link": "https://doi.org/10.3390/math7010004"
    },
    {
        "title": "Microdroplet ultrafast reactions speed antibody characterization",
        "abstract": "Recently, microdroplet reactions have aroused much interest because the microdroplet provides a unique medium where organic reactions could be accelerated by a factor of 10(3) or more. However, microdroplet reactions of proteins have been rarely studied. We report the occurrence of multiple-step reactions of a large protein, specifically, the digestion, reduction, and deglycosylation of an intact antibody, which can take place in microseconds with high reaction yields in aqueous microdroplets at room temperature. As a result, fast structural characterization of a monoclonal antibody, essential for assessing its quality as a therapeutic drug, can be enabled. We found that the IgG1 antibody can be digested completely by the IdeS protease in aqueous microdroplets in 250 microseconds, a 7.5 million-fold improvement in speed in comparison to traditional digestion in bulk solution (>30 min). Strikingly, inclusion of the reductant tris(2-carboxyethyl)phosphine in the spray solution caused simultaneous antibody digestion and disulfide bond reduction. Digested and reduced antibody fragments were either collected or analyzed online by mass spectrometry. Further addition of PNGase F glycosylase into the spray solution led to antibody deglycosylation, thereby producing reduced and deglycosylated fragments of analytical importance. In addition, glycated fragments of IgG1 derived from glucose modification were identified rapidly with this ultrafast digestion/reduction technique. We suggest that microdroplets can serve as powerful microreactors for both exploring large-molecule reactions and speeding their structural analyses.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1021/acs.analchem.0c04974"
    },
    {
        "title": "A survey on visual programming languages in internet of things",
        "abstract": "Visual programming has transformed the art of programming in recent years. Several organizations are in race to develop novel ideas to run visual programming in multiple domains with Internet of Things. IoT, being the most emerging area of computing, needs substantial contribution from the visual programming paradigm for its technological propagation. This paper surveys visual programming languages being served for application development, especially in Internet of Things field. 13 such languages are visited from several popular research-electronic databases (e. g., IEEE Xplore, Science Direct, Springer Link, Google Scholar, Web of Science, and Postscapes) and compared under four key attributes such as programming environment, license, project repository, and platform supports. Grouped into two segments, open source and proprietary platform, these visual languages pertain few crucial challenges that have been elaborated in this literature. The main goal of this paper is to present existing VPLs per their parametric proforma to enable na “ ive developers and researchers in the field of IoT to choose appropriate variant of VPL for particular type of application. It is also worth validating the usability and adaptability of VPLs that is essential for selection of beneficiary in terms of IoT.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1155/2017/1231430"
    },
    {
        "title": "FPGA-based real-time implementation of a digital reactivity-meter",
        "abstract": "The aim of this paper is to present a Hardware/Software design approach for the real-time implementation of a digital reactivity meter on a Zynq SoC platform (FPGA: Field-Programmable Gate Array). As a primary design tool, the Xilinx System Generator (XSG) programming tool has been used. The elementary hardware digital processing units, used to solve the inverse kinetics equations, have been implemented inside the Programming Logic (PL) part of the Zynq platform. Other secondary tasks, such as communication with the host PC, data storage and display have been devoted to the software application running on the Processing System (PS) part. A dedicated Graphical User Interface (GUI) has been also developed on a remote host computer. This application will be essentially used to handle the communication with the embedded reactivity measurement system and to display the reactivity evolution curves through the time. The design has been validated by comparing the obtained results with those proposed in the literature. Real plant data, acquired from Es-salam research reactor, has been also used to confirm the online efficiency of the design. Experimental results demonstrate that the system works well for both negative and positive reactivity insertions.",
        "keywords": "Digital reactivity-meter; Reactor point kinetics equations; Neutronic power; FPGA; Hardware implementation",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.pnucene.2022.104313"
    },
    {
        "title": "Automatic acquisition of annotated training corpora for test-code generation",
        "abstract": "Open software repositories make large amounts of source code publicly available. Potentially, this source code could be used as training data to develop new, machine learning-based programming tools. For many applications, however, raw code scraped from online repositories does not constitute an adequate training dataset. Building on the recent and rapid improvements in machine translation (MT), one possibly very interesting application is code generation from natural language descriptions. One of the bottlenecks in developing these MT-inspired systems is the acquisition of parallel text-code corpora required for training code-generative models. This paper addresses the problem of automatically synthetizing parallel text-code corpora in the software testing domain. Our approach is based on the observation that self-documentation through descriptive method names is widely adopted in test automation, in particular for unit testing. Therefore, we propose synthesizing parallel corpora comprised of parsed test function names serving as code descriptions, aligned with the corresponding function bodies. We present the results of applying one of the state-of-the-art MT methods on such a generated dataset. Our experiments show that a neural MT model trained on our dataset can generate syntactically correct and semantically relevant short Java functions from quasi-natural language descriptions of functionality.",
        "keywords": "test automation; code generation; neural machine translation; naturalness of software; statistical semantics",
        "released": 2019,
        "link": "https://doi.org/10.3390/info10020066"
    },
    {
        "title": "Components and workflow based grid programming environment for integrated image-processing applications",
        "abstract": "With the development of high-performance computing technologies, Grid computing has emerged as a new infrastructure for applications with resource integration. However, some obstacles for the prevalence of Grid and Grid application developments are still to be overcome, which is largely due to the immature Grid-enabled computing environment and its geographically distributed and non-centralized administrative nature. In order to improve the situation and support Grid applications, a powerful Grid system is proposed as an efficient solution, which enables components and workflow support for loosely coupled and geographically distributed components on a Grid. An image-processing Grid is a project aiming at providing an efficient programming environment. In this paper, we discuss the infrastructure of the integrated environment, a flexible and useful workflow mechanism, a dynamic resource-scheduling strategy to achieve multiple image-processing operations, and an easy-to-use Web portal. In the system, components and workflow techniques provide the benefits of flexibility, reusability and scalability, so that even the unskilled user can easily specify the processing of complex, cooperative applications without knowledge of underlying Grid platform configuration details. Copyright (c) 2006 John Wiley & Sons, Ltd.",
        "keywords": "high-performance computing; Grid computing; image processing; Grid workflow; Grid application; components and workflow",
        "released": 2006,
        "link": "https://doi.org/10.1002/cpe.1034"
    },
    {
        "title": "WebGRMS: Prototype software for web-based mapping of biological collections",
        "abstract": "Biological collections are gaining recognition as priceless sources of information about the historic distribution and diversity of life. The Internet is emerging as the major venue for sharing biodiversity information since it supports globalization and broad-scale interoperability. This research demonstrates how a Web-based mapping application for biological collections was developed using WebGD, an open-source software development tool, and illustrates how simple spatial analysis help collection users describe the range of ecogeographic variation in collections and customize the selection of accessions based on georeferenced variables. Our prototype can be viewed at http://www.yukon.een.orst.edu/ greene/ms_apps/home/index.htm. The demonstration site has three functional areas: (i) Query, (ii) Analyze Collections, and (iii) Add Data. The application was developed relatively quickly and at a low cost, since the complex workings for delivering GIS functions over the Web were an internal part of the WebGD framework. Because it was based on open-source code, costs were greatly decreased compared to commercially available software. In its current form, the prototype WebGRMS application provides users interested in Medicago and Trifolium germplasm with an innovative method to better understand the germplasm collections. More importantly, we hope the prototype provides a glimpse into the future of Web-based spatial analysis of biological collections.",
        "keywords": "biodiversity informatics; geographic information system (GIS); Germplasm collection; Medicago; plant genetic resources; Trifolium",
        "released": 2007,
        "link": "https://doi.org/10.1007/s10531-006-9067-0"
    },
    {
        "title": "Degrees of tenant isolation for cloud-hosted software services: A cross-case analysis",
        "abstract": "A challenge, when implementing multi-tenancy in a cloud-hosted software service, is how to ensure that the performance and resource consumption of one tenant does not adversely affect other tenants. Software designers and architects must achieve an optimal degree of tenant isolation for their chosen application requirements. The objective of this research is to reveal the trade-offs, commonalities, and differences to be considered when implementing the required degree of tenant isolation. This research uses a cross-case analysis of selected open source cloud-hosted software engineering tools to empirically evaluate varying degrees of isolation between tenants. Our research reveals five commonalities across the case studies: disk space reduction, use of locking, low cloud resource consumption, customization and use of plug-in architecture, and choice of multi-tenancy pattern. Two of these common factors compromise tenant isolation. The degree of isolation is reduced when there is no strategy to reduce disk space and customization and plug-in architecture is not adopted. In contrast, the degree of isolation improves when careful consideration is given to how to handle a high workload, locking of data and processes is used to prevent clashes between multiple tenants and selection of appropriate multi-tenancy pattern. The research also revealed five case study differences: size of generated data, cloud resource consumption, sensitivity to workload changes, the effect of the software process, client latency and bandwidth, and type of software process. The degree of isolation is impaired, in our results, by the large size of generated data, high resource consumption by certain software processes, high or fluctuating workload, low client latency, and bandwidth when transferring multiple files between repositories. Additionally, this research provides a novel explanatory framework for (i) mapping tenant isolation to different software development processes, cloud resources and layers of the cloud stack; and (ii) explaining the different trade-offs to consider affecting tenant isolation (i.e. resource sharing, the number of users/requests, customizability, the size of generated data, the scope of control of the cloud application stack and business constraints) when implementing multi-tenant cloud-hosted software services. This research suggests that software architects have to pay attention to the trade-offs, commonalities, and differences we identify to achieve their degree of tenant isolation requirements.",
        "keywords": "Multitenancy; Degree of isolation; Cloud patterns; Global software development; Software development tools; Cloud-hosted software services; Application component; Case study research; Cross-case analysis",
        "released": 2018,
        "link": "https://doi.org/10.1186/s13677-018-0121-8"
    },
    {
        "title": "Improved algorithm for parallel mining collaborative frequent itemsets in multiple data streams",
        "abstract": "With the rapid development of the World Wide Web technology, complex and diverse data present explosive growth, so frequent itemset mining plays an essential role. In view of the mining frequent itemsets in multiple data streams by limited computing power of a single processor, an improved algorithm of Parallel Mining Collaborative frequent itemsets in multiple data streams (PMCMD-Stream) was proposed. Firstly, the algorithm compresses the potential and frequent itemsets into CP-Tree only by one-scan and applies increment method to inserting or deleting related branch on CP-Tree, we do not need to repeatedly scanning the databases to generate many candidate frequent itemsets and save the running time. Secondly, this parallelized algorithm can be run in the MapReduce programming environment. Finally, the valuable frequent itemsets, namely global collaborative frequent itemsets, were obtained. Because each candidate frequent itemset is independent, and different candidate frequent itemsets can be processed by multiple computing machines concurrently. The experimental results show that PMCMD-Stream algorithm not only can improve the mining efficiency but also have much better scalability than the existing algorithms, so as to discover the collaborative frequent itemsets from large-scale data streams.",
        "keywords": "Stream data mining; Multiple data streams; Parallel algorithm; Sliding window; Frequent itemsets; Collaborative frequent itemsets",
        "released": 2019,
        "link": "https://doi.org/10.1007/s10586-018-1859-y"
    },
    {
        "title": "Data management in life cycle assessment: A case study of wastewater treatment",
        "abstract": "To assess the environmental impact of wastewater treatment, life cycle assessment (LCA) is a frequently applied instrument. However, these studies often require large amounts of data. The complexity and heterogeneity of these data result in the need for a systematic data management approach. Especially the generation of the life cycle inventory (LCI) holds the potential to be facilitated by automation. A case study in the wastewater sector was used to demonstrate the implementation of data management. A database structure was developed to store the raw data of the wastewater plants (WWTPs) and make it accessible through code. The code interacted with the database, implemented calculations, and automatically created the inventory based on the processed data. The database provides a consistent structure for the raw data and can also be used for backup purposes. Because it is machine-readable it can be accessed through the code that enables the automated generation of the LCI. As a proof of concept, a sequence of the code is provided with a user interface and can be tested online. We found that for most use cases, basic programming tools were sufficient for systematic data management, and, therefore, the approach is considered accessible for LCA practitioners.",
        "keywords": "automation; database; data management; LCA; Python; wastewater",
        "released": 2023,
        "link": "https://doi.org/10.2166/wst.2023.200"
    },
    {
        "title": "Water data explorer: An open-source web application and python library for water resources data discovery",
        "abstract": "We present the design and development of an open-source web application called Water Data Explorer (WDE), designed to retrieve water resources observation and model data from data catalogs that follow the WaterOneFlow and WaterML Service-Oriented Architecture standards. WDE is a fully customizable web application built using the Tethys Platform development environment. As it is open source, it can be deployed on the web servers of international government agencies, non-governmental organizations, research teams, and others. Water Data Explorer provides uniform access to international data catalogs, such as the Consortium of Universities for the Advancement of Hydrologic Science (CUAHSI) Hydrologic Information System (HIS) and the World Meteorological Organization (WMO) Hydrological Observing System (WHOS), as well as to local data catalogs that support the WaterOneFlow and WaterML standards. WDE supports data discovery, visualization, downloading, and basic data interpolation. It can be customized for different regions by modifying the user interface (i.e., localization), as well as by including pre-defined data catalogs and data sources. Access to WDE functionality is provided by a new open-source Python package called “Pywaterml” which provides programmable access to WDE methods to discover, visualize, download, and interpolate data. We present two case studies that access the CUAHSI HIS and WHOS catalogs and demonstrate regional customization, data discovery from WaterOneFlow web services, data visualization of time series observations, and data downloading.",
        "keywords": "observation networks; WHOS; CUAHSI; Tethys; HydroShare; HydroServer",
        "released": 2021,
        "link": "https://doi.org/10.3390/w13131850"
    },
    {
        "title": "Building ontologies for different natural languages",
        "abstract": "Ontology construction of a certain domain is an important step in applying the Semantic web. A number of software tools adapted for building domain ontologies of most wide spread natural languages are available, but accomplishing that for any given natural language presents a challenge. Here we propose a semi-automatic procedure to create ontologies for different natural languages. Our approach utilizes various software tools available on the Internet most notably DODDLE-OWL - a domain ontology development tool implemented for English and Japanese languages. By using this tool, Word Net, Prot g and XSLT transformations, we propose a general procedure to construct domain ontology for any natural language.",
        "keywords": "Semantic Web; Ontology; Natural language; DODDLE-OWL",
        "released": 2014,
        "link": "https://doi.org/10.2298/CSIS130429023A"
    },
    {
        "title": "On effectiveness and efficiency of gamified exploratory GUI testing",
        "abstract": "Context: Gamification appears to improve enjoyment and quality of execution of software engineering activities, including software testing. Though commonly employed in industry, manual exploratory testing of web application GUIs was proven to be mundane and expensive. Gamification applied to that kind of testing activity has the potential to overcome its limitations, though no empirical research has explored this area yet. Goal: Collect preliminary insights on how gamification, when performed by novice testers, affects the effectiveness, efficiency, test case realism, and user experience in exploratory testing of web applications. Method: Common gamification features augment an existing exploratory testing tool: Final Score with Leaderboard, Injected Bugs, Progress Bar, and Exploration Highlights. The original tool and the gamified version are then compared in an experiment involving 144 participants. User experience is elicited using the Technology Acceptance Model (TAM) questionnaire instrument. Results: Statistical analysis identified several significant differences for metrics that represent the effectiveness and efficiency of tests showing an improvement in coverage when they were developed with gamification. Additionally, user experience is improved with gamification. Conclusions: Gamification of exploratory testing has a tangible effect on how testers create test cases for web applications. While the results are mixed, the effects are most beneficial and interesting and warrant more research in the future. Further research shall be aimed at confirming the presented results in the context of state-of-the-art testing tools and real-world development environments.",
        "keywords": "Software testing; web application testing; gamification",
        "released": 2024,
        "link": "https://doi.org/10.1109/TSE.2023.3348036"
    },
    {
        "title": "Impact of online learning on the satisfaction of agroindustrial engineering university students",
        "abstract": "The objective of the study was to analyze the impact of online learning on the satisfaction of agroindustrial engineering students. The study was of an applied type, quantitative approach, quasi-experimental design and explanatory level. The population was 235 students of the seventh cycle of Engineering from a private university in Lima. The sample was 70 Agroindustrial Engineering students, from the Supply Chain Management subject, divided into two groups, the control group and the experimental group. The questionnaire had 30 items to measure student satisfaction and its dimensions: teacher competencies, quality of the virtual course, technological tools; virtual classroom design and virtual course development environment. To measure online learning, six online sessions were implemented through Google Meet. A pre-and post-test was performed on each group. In the pre-test, both groups obtained a medium level of perception, while, in the post-test, the control group reached a medium level, and the experimental group reached a high level. In the pre-test, no significant differences were identified between the groups; however, in the post-test it was confirmed that online learning significantly impacts the satisfaction of the students in the experimental group.",
        "keywords": "Online learning; satisfaction; technology; teacher competencies; quality",
        "released": 2024,
        "link": "https://doi.org/10.46925//rdluz.42.31"
    },
    {
        "title": "The package blueprint: Visually analyzing and quantifying packages dependencies",
        "abstract": "Large object-oriented applications are structured over many packages. Packages are important but complex structural entities that are difficult to understand since they act as containers of classes, which can have many dependencies with other classes spread over multiple packages. However to be able to take decisions (e.g. refactoring and/or assessment decisions), maintainers face the challenges of managing (sorting, grouping) the massive amount of dependencies between classes spread over multiple packages. To help maintainers, there is a need for at the same time understanding, and quantifying, dependencies between classes as well as understanding how packages as containers of such classes depend on each other. In this paper, we present a visualization, named Package Blueprint, that reveals in detail package internal structure, as well as the dependencies between an observed package and its neighbors, at both package and class levels. Package blueprint aims at assisting maintainers in understanding package structure and dependencies, in particular when they focus on few packages and want to take refactoring decisions and/or to assess the structure of those packages. A package blueprint is a space filling matrix-based visualization, using two placement strategies that are enclosure and adjacency. Package blueprint is structured around the notion of surfaces that group classes and their dependencies by their packages (i.e., enclosure placement); whilst surfaces are placed next to their parent node which is the package under-analysis (i.e., adjacency placement). We present two views: one stressing how an observed package depends upon the rest of the system and another stressing how the system depends upon that package. To evaluate the contribution of package blueprint for understanding packages we performed an exploratory user study comparing package blueprint with an advanced IDE. The results show that users of package blueprint are faster in analyzing and assessing package structure. The results are proved statically significant and they show that package blueprint considerably improves the experience of standard browser users. (C) 2014 Elsevier B.V. All rights reserved.",
        "keywords": "Software engineering; Software comprehension; Software maintenance; Software visualization",
        "released": 2014,
        "link": "https://doi.org/10.1016/j.scico.2014.02.016"
    },
    {
        "title": "Best practices for population genetic analyses",
        "abstract": "Population genetic analysis is a powerful tool to understand how pathogens emerge and adapt. However, determining the genetic structure of populations requires complex knowledge on a range of subtle skills that are often not explicitly stated in book chapters or review articles on population genetics. What is a good sampling strategy? How many isolates should I sample? How do I include positive and negative controls in my molecular assays? What marker system should I use? This review will attempt to address many of these practical questions that are often not readily answered from reading books or reviews on the topic, but emerge from discussions with colleagues and from practical experience. A further complication for microbial or pathogen populations is the frequent observation of clonality or partial clonality. Clonality invariably makes analyses of population data difficult because many assumptions underlying the theory from which analysis methods were derived are often violated. This review provides practical guidance on how to navigate through the complex web of data analyses of pathogens that may violate typical population genetics assumptions. We also provide resources and examples for analysis in the R programming environment.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1094/PHYTO-12-16-0425-RVW"
    },
    {
        "title": "Using collaborative course development to achieve online course quality standards",
        "abstract": "The issue of quality is becoming front and centre as online and distance education moves into the mainstream of higher education. Many believe collaborative course development is the best way to design quality online courses. This research uses a case study approach to probe into the collaborative course development process and the implementation of quality standards at a Canadian university. Four cases are presented to discuss the effects of the faculty member/instructional designer relationship on course quality, as well as the issues surrounding the use of quality standards as a development tool. Findings from the study indicate that the extent of collaboration depends on the degree of course development and revision required, the nature of the established relationship between the faculty member and designer, and the level of experience of the faculty member. Recommendations for the effective use of quality standards using collaborative development processes are provided.",
        "keywords": "Course development; course development team; online course quality; quality standards; instructional design standards; distance education; online learning; online education",
        "released": 2010,
        "link": "https://doi.org/10.19173/irrodl.v11i3.912"
    },
    {
        "title": "A vertical-horizontal-intersections feature based method for identification of bharatanatyam double hand mudra images",
        "abstract": "Bharatanatyam is an Indian classical dance, which has to be studied under an expert. In villages, semi-urban areas and foreign countries the experts are scarce. In order to promote, popularize and make it self-pursuable, this Indian art requires technological leveraging. With this motivation, the goal of this work is to automate identification of mudras through Image processing. This paper presents a three stage methodology for identification of 24 double hand mudra images of Bharatanatyam dance. In the first stage, acquired images of Bharatanatyam mudras are preprocessed to obtain contours of mudras using canny edge detector. In the second stage, cell features are extracted that include number of vertical and horizontal intersections of grid lines with the contours of the mudras. In the third stage, a rule based classifier is developed to classify the given image into 24 classes of mudras. The proposed method is implemented using OpenCV with Microsoft visual C++ IDE. The proposed method finds many applications such as e-learning of mudras and proper postures leading to self-learning of Bharatanatyam dance, online commentary during concerts, and adoption to many other forms of dances prevailing in India and outside.",
        "keywords": "Samyukta mudras; Contour of mudras; Cell features; Intersections; Rule based classifier",
        "released": 2018,
        "link": "https://doi.org/10.1007/s11042-018-6223-y"
    },
    {
        "title": "Metamodel search: Using XPath to search domain-specific models",
        "abstract": "A common task that is often needed in many software development tools is the capability to search the artifact that is being created in a flexible and efficient manner. However, this capability is typically absent in meta-programmable modeling tools, which can be a serious disadvantage as the size of a model increases. As a remedy, we introduce a method to search domain models using XPath - a World Wide Web Consortium (W3C) standard that uses logical predicates to search an XML document. In this paper an XPath search engine is described that traverses the internal representation of a modeling tool (rather than an XML document) and returns those model entities that match the XPath predicate expression. A set of search queries are demonstrated on a case study.",
        "keywords": "XPath; domain-specific modeling; metamodeling; model search",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000241421400005"
    },
    {
        "title": "Analysis of outcomes in radiation oncology: An integrated computational platform",
        "abstract": "Radiotherapy research and outcome analyses are essential for evaluating new methods of radiation delivery and for assessing the benefits of a given technology on locoregional control and overall survival. In this article, a computational platform is presented to facilitate radiotherapy research and outcome studies in radiation oncology. This computational platform consists of (1) an infrastructural database that stores patient diagnosis, IMRT treatment details, and follow-up information, (2) an interface tool that is used to import and export IMRT plans in DICOM RT and AAPM/RTOG formats from a wide range of planning systems to facilitate reproducible research, (3) a graphical data analysis and programming tool that visualizes all aspects of an IMRT plan including dose, contour, and image data to aid the analysis of treatment plans, and (4) a software package that calculates radiobiological models to evaluate IMRT treatment plans. Given the limited number of general-purpose computational environments for radiotherapy research and outcome studies, this computational platform represents a powerful and convenient tool that is well suited for analyzing dose distributions biologically and correlating them with the delivered radiation dose distributions and other patient-related clinical factors. In addition the database is web-based and accessible by multiple users, facilitating its convenient application and use.",
        "keywords": "dosimetry; medical administrative data processing; medical diagnostic computing; patient diagnosis; radiation therapy",
        "released": 2009,
        "link": "https://doi.org/10.1118/1.3114022"
    },
    {
        "title": "Leadership, support and organisation for academics’ participation in engineering education change for sustainable development",
        "abstract": "This work spotlights the experiences from ten years of implementing sustainable development in all educational programs at a technical university. With a focus on the critical issue of involving more academics in the work, experiences are shared through an ethnographic account including focus group interviews. “Sustainable development” has been perceived as both superficial and overwhelming; unclear yet somehow predetermined; it has been perceived to demand non-existent space in the curriculum; and it has challenged the academics regardless of the subjects’ relatedness to sustainability. It is concluded that the evolution of a web of interconnected people, key academics, activities, norms and tools has contributed to an increased participation. The work for authenticity, reliability and feasibility, along with institution-wide and long-term academic development tools is presented.",
        "keywords": "Academic leadership; academic development; activity theory; social practice theory; participatory practices",
        "released": 2023,
        "link": "https://doi.org/10.1080/03043797.2022.2106824"
    },
    {
        "title": "A systematic review of tools, languages, and methodologies for mashup development",
        "abstract": "Web 2.0 has become a powerful means of transmitting information in a number of fields, such as communication, e-commerce, and entertainment. Nowadays, companies and organizations transmit specific information through different mechanisms, such as Web feeds and Web services. These data sources enable third parties to incorporate data from service providers into their own applications. On the basis of this understanding, mashups have emerged as a new approach to develop applications and which combine data and resources from heterogeneous sourcessuch as internal data sources, Web feeds, screen scraping, and Web serviceswith the aim of solving specific needs. Mashup development involves activities such as accessing heterogeneous sources, combining data from different data sources, and building graphical interfaces. These activities restrict the development of these kinds of applications only to experienced computer users. Today, a number of tools and programming languages are used to help carry out some of the aforementioned activities. These tools and programming languages have features enabling the integration of different technologies in order to solve problems such as data management from different data sources and content publication. If this is taken into account, there is a growing need to learn about the features, advantages, and disadvantages of these tools and programming languages in order to select the tool or language that best fits a specific need and a specific level of knowledge and experience in terms of software development. This paper presents a systematic review and analysis of the tools, programming languages, and software development methodologies involved in mashup development in order to learn more about the features and services provided by mashups. Furthermore, this research also explains the qualitative and quantitative evaluation used for the mashup development tools. The evaluation was performed in order to measure not only the usability of these tools but also the support that they provide for standardized features of Web development that they provide. Finally, new trends in the development of mashups are discussed. Copyright (c) 2013 John Wiley & Sons, Ltd.",
        "keywords": "mashup language; mashup tool; software development methodology",
        "released": 2015,
        "link": "https://doi.org/10.1002/spe.2233"
    },
    {
        "title": "Explainable argumentation as a service",
        "abstract": "Gorgias Cloud offers an integrated application development environment that facilitates the development of argumentation-based systems over the internet. Argumentation is offered as a service in a way that this allows application systems to remotely access the argumentation service and utilize the results of the argumentative computation. Moreover, the service results include the explanation of the decision in both human and machine-readable formats. The first is useful for allowing the application validation to be done by experts, while the second is useful for development. It appears that this is the first case where argumentation is offered to developers in such an open and distributed way. (c) 2023 Elsevier B.V. All rights reserved.",
        "keywords": "Argumentation; SaaS; Explainable AI",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.websem.2023.100772"
    },
    {
        "title": "The time has come: A paradigm shift in diagnostic radiology education via simulation training",
        "abstract": "Current radiology training for medical students and residents predominantly consists of reviewing teaching files, attending lectures, reading textbooks and online sources, as well as one-on-one teaching at the workstation. In the case of medical schools, radiology training is quite passive. In addition, the variety of important and high-yield cases that trainees are exposed to may be limited in scope. We utilized an open-source dcm4chee-based Picture Archiving and Communication System (PACS) named “Weasis” in order to simulate a radiologist’s practice in the real world, using anonymized report-free complete cases that could easily be uploaded live during read-outs for training purposes. MySQL was used for database management and JBOSS as application server. In addition, we integrated Weasis into a web-based reporting system through Java programming language using the MyEclipse development environment. A freeware, platform-independent, image database was established to simulate a real-world PACS. The sever was implemented on a dedicated non-workstation PC connected to the hospital secure network. As the client access is through a webpage, the cases can be viewed from any computer connected to the hospital network. The reporting system allows for evaluation purposes and providing feedback to the trainees. Brief survey results are available. Implementation of such a low-cost, versatile, and customizable tool provides a new opportunity for training programs in offering medical students with an active and more realistic radiology experience, junior radiology residents with potentially better preparation for independent call, and senior resident and fellows with the ability to fine-tune high-level specialty-level knowledge.",
        "keywords": "PACS; Radiology education; Simulation training; Medical education; Weasis; Curriculum",
        "released": 2021,
        "link": "https://doi.org/10.1007/s10278-020-00405-2"
    },
    {
        "title": "GCAC: Galaxy workflow system for predictive model building for virtual screening",
        "abstract": "BackgroundTraditional drug discovery approaches are time-consuming, tedious and expensive. Identifying a potential drug-like molecule using high throughput screening (HTS) with high confidence is always a challenging task in drug discovery and cheminformatics. A small percentage of molecules that pass the clinical trial phases receives FDA approval. This whole process takes 10-12years and millions of dollar of investment. The inconsistency in HTS is also a challenge for reproducible results. Reproducible research in computational research is highly desirable as a measure to evaluate scientific claims and published findings. This paper describes the development and availability of a knowledge based predictive model building system using the R Statistical Computing Environment and its ensured reproducibility using Galaxy workflow system.ResultsWe describe a web-enabled data mining analysis pipeline which employs reproducible research approaches to confront the issue of availability of tools in high throughput virtual screening. The pipeline, named as Galaxy for Compound Activity Classification (GCAC) includes descriptor calculation, feature selection, model building, and screening to extract potent candidates, by leveraging the combined capabilities of R statistical packages and literate programming tools contained within a workflow system environment with automated configuration.ConclusionGCAC can serve as a standard for screening drug candidates using predictive model building under galaxy environment, allowing for easy installation and reproducibility. A demo site of the tool is available at http://ccbb.jnu.ac.in/gcac",
        "keywords": "Predictive model building; Reproducible results; Galaxy workflow system; High throughput screening; Drug discovery; R statistical package; Cheminformatics",
        "released": 2019,
        "link": "https://doi.org/10.1186/s12859-018-2492-8"
    },
    {
        "title": "A service-oriented process to develop web applications",
        "abstract": "Web applications are widely disseminated, but, traditional development methods for this type of application still require a substantial amount of new modeling and programming. Current methods do not take significant advantage of reuse techniques, such as software product lines (PL). This paper presents the WIDE-PL environment focusing on its application generation process, called Application DEvelopment based on Services - ADESE. This environment is an evolution of WIDE - Waterloo Informatics Development Environment. The WIDE-PL environment supports the generation of Web applications based on the Service-oriented Architecture (SOA) and the product line approach. Our solution encompasses a general software architecture, an application generation process, and a set of mandatory and optional services. Examples of applications from two different domains were developed using ADESE to evaluate its feasibility. The results show that the process offers several advantages including an increase in reuse and an explicit separation between the services and the business process connecting those services.",
        "keywords": "web applications; web-based services; product line; business process",
        "released": 2008,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000257026500009"
    },
    {
        "title": "A web-based XML information sharing system for collaborative product development",
        "abstract": "A successful product development project must tightly integrate multi-disciplinary experts through intensive information sharing. Although many computer integrated design systems have tried to fulfil this need, information sharing in a heterogeneous, distributed, and collaborative product development environment is still problematic. This research proposes a web-based information sharing system that uses XML (eXtensible Markup Language), ontology, and access control technologies to deal with product design collaboration problems. In this study, the critical product development information is transformed into XML documents so that data can be exchanged using existing Internet protocols. Ontology analysis is conducted on the XML documents allowing semantic differences in cross-domains to be resolved. Furthermore, a role-based access control mechanism is developed to allow secure and fine-grained access control for each element in the XML documents. This web-based XML information sharing system was implemented in a new product development project. The proposed method allows designers to be more productive and innovative through seamless information sharing, exchanging, and searching.",
        "keywords": "collaborative product development; XML (eXtensible Markup Language); ontology analysis; role-based access control",
        "released": 2006,
        "link": "https://doi.org/10.1080/00207540600579490"
    },
    {
        "title": "IoT-EMS: An internet of things based environment monitoring system in volunteer computing environment",
        "abstract": "Environment monitoring is an important area apart from environmental safety and pollution control. Such monitoring performed by the physical models of the atmosphere is unstable and inaccurate. Machine Learning (ML) techniques on the other hand are more robust in capturing the dynamics in the environment. In this paper, a novel approach is proposed to build a cost-effective standardized environment monitoring system (IoT-EMS) in volunteer computing environment. In volunteer computing, the volunteers (people) share their resources for distributed computing to perform a task (environment monitoring). The system is based on the Internet of Things and is controlled and accessed remotely through the Arduino platform (volunteer resource). In this system, the volunteers record the environment information from the surrounding through different sensors. Then the sensor readings are uploaded directly to a web server database, from where they can be viewed anytime and anywhere through a website. Analytics on the gathered time-series data is achieved through ML data modeling using R Language and RStudio IDE. Experimental results show that the system is able to accurately predict the trends in temperature, humidity, carbon monoxide level, and carbon dioxide. The prediction accuracy of different ML techniques such as MLP, k-NN, multiple regression, and SVM are also compared in different scenarios.",
        "keywords": "Environment monitoring; IoT-EMS; machine learning; internet of things; arduino",
        "released": 2022,
        "link": "https://doi.org/10.32604/iasc.2022.022833"
    },
    {
        "title": "Mission-oriented service development using capability-based semantic recommendation for the internet of things",
        "abstract": "This paper presents a mission-oriented service development environment with web-based modeling tool that enable users create workflow-based service composition. This approach utilizes the ontology-based mission service model composed of mission, task, service and resource, and task/service recommendation. During developing the mission-oriented service, capability-based semantic matching and hierarchical relationship-based filtering are used for three types of recommendation. Also, we develop a modeling environment to monitor and execute the mission service application. In experiments, we have conducted on test beds in two domains such as military environment and smart building.",
        "keywords": "Mission service; Capability-based semantic matching; Recommendation; Ontology; IoT",
        "released": 2019,
        "link": "https://doi.org/10.1007/s11042-017-4889-1"
    },
    {
        "title": "Assessing therapeutic potential of molecules: Molecular property diagnostic suite for tuberculosis",
        "abstract": "Molecular Property Diagnostic Suite () is a web tool ( http://mpds.osdd.net) designed to assist the in silico drug discovery attempts towards Mycobacterium tuberculosis (Mtb). tool has nine modules which are classified into data library (1-3), data processing (4-5) and data analysis (6-9). Module 1 is a repository of literature and related information available on the Mtb. Module 2 deals with the protein target analysis of the chosen disease area. Module 3 is the compound library consisting of 110.31 million unique molecules generated from public domain databases and custom designed search tools. Module 4 contains tools for chemical file format conversions and 2D to 3D coordinate conversions. Module 5 helps in calculating the molecular descriptors. Module 6 specifically handles QSAR model development tools using descriptors generated in the Module 5. Module 7 integrates the AutoDock Vina algorithm for docking, while module 8 provides screening filters. Module 9 provides the necessary visualization tools for both small and large molecules. The workflow-based open source web portal, 1.0.1 can be a potential enabler for scientists engaged in drug discovery in general and in anti-TB research in particular. SYNOPSIS: A web-based Galaxy tool is developed for assessing therapeutic potential of molecules. is categorized into Data Library, Data Processing and Data Analysis. It can be a potential enabler for scientists engaged in drug discovery in general and in anti-TB research in particular.",
        "keywords": "Tuberculosis; chemoinformatics; open science; neglected diseases; drug discovery portal; web-based technology",
        "released": 2017,
        "link": "https://doi.org/10.1007/s12039-017-1268-4"
    },
    {
        "title": "Faster ARMA maximum likelihood estimation",
        "abstract": "A new likelihood based AR approximation is given for ARMA models. The usual algorithms for the computation of the likelihood of an ARMA model require O(n) flops per function evaluation. Using our new approximation, an algorithm is developed which requires only O(1) flops in repeated likelihood evaluations. In most cases, the new algorithm gives results identical to or very close to the exact maximum likelihood estimate (MLE). This algorithm is easily implemented in high level quantitative programming environments (QPEs) such as Mathemtica, MatLab and R. In order to obtain reasonable speed, previous ARMA maximum likelihood algorithms are usually implemented in C or some other machine efficient language. With our algorithm it is easy to do maximum likelihood estimation for long time series directly in the QPE of your choice. The new algorithm is extended to obtain the MLE for the mean parameter. Simulation experiments which illustrate the effectiveness of the new algorithm are discussed. Mathematica and R packages which implement the algorithm discussed in this paper. are available [McLeod, A.I., Zhang, Y, 2007. Online supplements to “Faster ARMA Maximum Likelihood Estimation”, (http://www.stats.uwo.ca/faculty/aim/2007/faster/)]. Based on these package implementations, it is expected that the interested researcher would be able to implement this algorithm in other QPEs. (c) 2007 Elsevier B.V. All rights reserved.",
        "keywords": "autoregressive approximation; efficiency of the sample mean; maximum likelihood estimator; high-order autoregression; long time series and massive data sets; quantitative programming environments",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.csda.2007.07.020"
    },
    {
        "title": "The evidence synthesis and meta-analysis in r conference (ESMARConf): Levelling the playing field of conference accessibility and equitability",
        "abstract": "Rigorous evidence is vital in all disciplines to ensure efficient, appropriate, and fit-for-purpose decision-making with minimised risk of unintended harm. To date, however, disciplines have been slow to share evidence synthesis frameworks, best practices, and tools amongst one another. Recent progress in collaborative digital and programmatic frameworks, such as the free and Open Source software R, have significantly expanded the opportunities for development of free-to-use, incrementally improvable, community driven tools to support evidence synthesis (e.g. EviAtlas, robvis, PRISMA2020 flow diagrams and metadat). Despite this, evidence synthesis (and meta-analysis) practitioners and methodologists who make use of R remain relatively disconnected from one another. Here, we report on a new virtual conference for evidence synthesis and meta-analysis in the R programming environment (ESMARConf) that aims to connect these communities. By designing an entirely free and online conference from scratch, we have been able to focus efforts on maximising accessibility and equity-making these core missions for our new community of practice. As a community of practice, ESMARConf builds on the success and groundwork of the broader R community and systematic review coordinating bodies (e.g. Cochrane), but fills an important niche. ESMARConf aims to maximise accessibility and equity of participants across regions, contexts, and social backgrounds, forging a level playing field in a digital, connected, and online future of evidence synthesis. We believe that everyone should have the same access to participation and involvement, and we believe ESMARConf provides a vital opportunity to push for equitability across disciplines, regions, and personal situations.",
        "keywords": "Online conference; Equity; Pay-it-forwards; Volunteer; Systematic review; Synthesis; Community of Practice; Evidence synthesis technology",
        "released": 2022,
        "link": "https://doi.org/10.1186/s13643-022-01985-6"
    },
    {
        "title": "Improving resilience to timing errors by exposing variability effects to software in tightly-coupled processor clusters",
        "abstract": "Manufacturing and environmental variations cause timing errors in microelectronic processors that are typically avoided by ultra-conservative multi-corner design margins or corrected by error detection and recovery mechanisms at the circuit-level. In contrast, we present here runtime software support for cost-effective countermeasures against hardware timing failures during system operation. We propose a variability-aware OpenMP (VOMP) programming environment, suitable for tightly-coupled shared memory processor clusters, that relies upon modeling across the hardware/software interface. VOMP is implemented as an extension to the OpenMP v3.0 programming model that covers various parallel constructs, including,, and. Using the notion of work-unit vulnerability (WUV) proposed here, we capture timing errors caused by circuit-level variability as high-level software knowledge. WUV consists of descriptive metadata to characterize the impact of variability on different work-unit types running on various cores. As such, WUV provides a useful abstraction of hardware variability to efficiently allocate a given work-unit to a suitable core for execution. VOMP enables hardware/software collaboration with online variability monitors in hardware and runtime scheduling in software. The hardware provides online per-core characterization of WUV metadata. This metadata is made available by carefully placing key data structures in a shared L1 memory and is used by VOMP schedulerss. Our results show that VOMP greatly reduces the cost of timing error recovery compared to the baseline schedulers of OpenMP, yielding speedup of 3%-36% for tasks, and 26%-49% for sections. Further, VOMP reaches energy saving of 2%-46% and 15%-50% for tasks, and sections, respectively.",
        "keywords": "Cross-layer variability management; OpenMP; processor clusters; recovery; robust system design; scheduling; timing errors; variations",
        "released": 2014,
        "link": "https://doi.org/10.1109/JETCAS.2014.2315883"
    },
    {
        "title": "A virtualized test automation framework: A DellEMC case study of test automation practice",
        "abstract": "As software development life-cycles continue to shorten, the need for reliable, maintainable and scalable test automation solutions becomes more and more important. This importance arises as software, test and automation activities occur earlier in the development life cycle with the aim of providing accelerated feedback on the code and software produced. In this environment, infrastructure and development environments are made available earlier to test teams, the demand of which can create environment bottlenecks as software and automation teams also require the same resources at the same time. To help maximize the use of the underlying resources virtualization and cloud based solutions have been proposed. However, at present there is no guidance on how to design and implement a test automation solution that leverages an underlying virtualized infrastructure. In this paper a virtualized test automation framework designed and implemented by DellEMC Software Quality team in Cork, Ireland is presented along with a macro benchmark analysis of its performance over different virtualization environments.",
        "keywords": "Selenium Grid; software engineering; software practice; test automation; TestNG; virtualized; test environments",
        "released": 2019,
        "link": "https://doi.org/10.1002/spe.2658"
    },
    {
        "title": "Extending smart phone based techniques to provide AI flavored interaction with DIY robots, over wi-fi and LoRa interfaces",
        "abstract": "Inspired by the mobile phone market boost, several low cost credit card-sized computers have made the scene, able to support educational applications with artificial intelligence features, intended for students of various levels. This paper describes the learning experience and highlights the technologies used to improve the function of DIY robots. The paper also reports on the students’ perceptions of this experience. The students participating in this problem based learning activity, despite having a weak programming background and a confined time schedule, tried to find efficient ways to improve the DIY robotic vehicle construction and better interact with it. Scenario cases under investigation, mainly via smart phones or tablets, involved from touch button to gesture and voice recognition methods exploiting modern AI techniques. The robotic platform used generic hardware, namely arduino and raspberry pi units, and incorporated basic automatic control functionality. Several programming environments, from MIT app inventor to C and python, were used. Apart from cloud based methods to tackle the voice recognition issues, locally running software alternatives were assessed to provide better autonomy. Typically, scenarios were performed through Wi-Fi interfaces, while the whole functionality was extended by using LoRa interfaces, to improve the robot’s controlling distance. Through experimentation, students were able to apply cutting edge technologies, to construct, integrate, evaluate and improve interaction with custom robotic vehicle solutions. The whole activity involved technologies similar to the ones making the scene in the modern agriculture era that students need to be familiar with, as future professionals.",
        "keywords": "smart control; artificial intelligence; educational robotics; DIY; agricultural vehicles; android app; arduino; raspberry pi; LoRa",
        "released": 2019,
        "link": "https://doi.org/10.3390/educsci9030224"
    },
    {
        "title": "Analyzing the temporal behavior of noisy intermediate-scale quantum nodes and algorithm fidelity",
        "abstract": "In the past decade, quantum computing has undergone rapid evolution, capturing the increasing interest of the scientific community, industry, and governments. This enthusiasm has resulted in ambitious growth plans which stimulate the development of more efficient quantum computing devices and programming environments. The easy accessibility of quantum platforms in the cloud has attracted individuals to explore quantum computing, prompting a comprehensive analysis and assessment of quantum device’s behavior. The extensive benchmarking presented in this study involved all free available quantum computing devices within the IBM Quantum Platform. These devices are employed to execute tens of thousands of quantum program executions, with the objective of evaluating quantum computer behavior and performance over time and under different optimization options. Special emphasis has been placed on analyzing the transpile operation and the depth of generated quantum circuits. The machine analysis tests are conducted using Quantum Computing Run Assistant (QCRA), a versatile software tool specifically designed to streamline the effortless distribution of quantum programs across a range of quantum computing platforms. This software not only streamlines the optimization of benchmarking processes but also simplifies the assessment of different configurations and result quality through the collection of advanced job metadata. This study provides an extensive benchmark of Noisy Intermediate-Scale Quantum (NISQ) devices, assessing behavior and performance with thousands of runs. Emphasizing transpile operations and circuit depth, it explores the correlation between final result fidelity and quantum circuit depth, comparing results over time for specific quantum machines. Quantum Computing Run Assistant (QCRA) optimizes benchmark processes across various configurations. image",
        "keywords": "job management; quantum computer benchmark; quantum computing; quantum resource allocation",
        "released": 2024,
        "link": "https://doi.org/10.1002/qute.202300451"
    },
    {
        "title": "Enabling smart agriculture: An IoT-based framework for real-time monitoring and analysis of agricultural data",
        "abstract": "With the progress in sensor and cloud technologies in contemporary times, a range of intelligent agriculture applications has gained considerable prominence. It is predicted that these developments can continue to pique the interest of researchers in the future. On the other hand, it is seen that IoT (Internet of Things)-based models are used in various fields. Herein, the primary objectives of this study are to enable farmers to remotely monitor and manage field conditions through sensor technology and IoT integration. In addition, these technological advancements make it possible to take the required measurements. Farmers can optimize their agricultural practices based on the analysis of the data obtained for this application. Thus, the aim is to manage the agricultural process more effectively and efficiently. In this study, an IoT-based framework is proposed for agricultural data monitoring. Light, temperature-pressure, smoke, humidity, and soil dryness values can be measured from GY-30, BME280, MQ-2, DHT11, and YL-69, respectively. An ESP-32S development board is used to collect data from sensors, and this board is coded using Arduino IDE. Subsequently, using ESP-32S, it is sent to the ThingSpeak cloud service provided by MATLAB via a Wi-Fi connection. Thus, these data can be easily transferred to MATLAB. We create a user-friendly Graphical User Interface application so that the data can be monitored and analyzed in MATLAB as well as ThingSpeak. This application allows users to monitor the data flow in real time and can easily provide the requested values such as maximum, minimum, mean, standard deviation, and current with the help of a button. In addition, the proposed system sends an e-mail to the user when soil dryness and smoke values exceed a certain threshold value. The results obtained in the study indicate that the proposed model can save time and labor in addition to providing reliable and fast data flow.",
        "keywords": "ESP-32S; IoT; MATLAB; Smart agriculture; Remote monitoring; ThingSpeak",
        "released": 2024,
        "link": "https://doi.org/10.1007/s40003-024-00705-x"
    },
    {
        "title": "Emotion recognition from MIDI musical file using enhanced residual gated recurrent unit architecture",
        "abstract": "The complex synthesis of emotions seen in music is meticulously composed using a wide range of aural components. Given the expanding soundscape and abundance of online music resources, creating a music recommendation system is significant. The area of music file emotion recognition is particularly fascinating. The RGRU (Enhanced Residual Gated Recurrent Unit), a complex architecture, is used in our study to look at MIDI (Musical Instrument and Digital Interface) compositions for detecting emotions. This involves extracting diverse features from the MIDI dataset, encompassing harmony, rhythm, dynamics, and statistical attributes. These extracted features subsequently serve as input to our emotion recognition model for emotion detection. We use an improved RGRU version to identify emotions and the Adaptive Red Fox Algorithm (ARFA) to optimize the RGRU hyperparameters. Our suggested model offers a sophisticated classification framework that effectively divides emotional content into four separate quadrants: positive-high, positive-low, negative-high, and negative-low. The Python programming environment is used to implement our suggested approach. We use the EMOPIA dataset to compare its performance to the traditional approach and assess its effectiveness experimentally. The trial results show better performance compared to traditional methods, with higher accuracy, recall, F-measure, and precision.",
        "keywords": "emotion recognition; Musical Instrument Digital Interface; Enhanced Residual Gated Recurrent Unit; adaptive Red Fox algorithm; EMOPIA",
        "released": 2023,
        "link": "https://doi.org/10.3389/fcomp.2023.1305413"
    },
    {
        "title": "The community of multimedia agents",
        "abstract": "Multimedia data mining requires the ability to automatically analyze and understand the content. The Community of Multimedia Agents project is devoted to creating a community of researchers and students who are interested in developing multimedia annotation algorithms. It provides an open environment for developing, testing, learning and prototyping multimedia content analysis and annotation methods. It serves as a medium for researchers to contribute and share their achievements while protecting their proprietary techniques. Each method is represented as an agent that can communicate with the other agents registered in the environment using templates that are based on the descriptors and description schemes in the MPEG-7 standard. Using the standard allows agents that are developed by different organizations to operate and communicate with each other seamlessly regardless of their programming languages and internal architecture. A development environment is provided to facilitate the construction of media analysis methods. The tool contains a workbench, which allows the user integrating agents to build more sophisticated systems, and a blackboard browser, which visualizes the processing results. It enables researchers to compare the performance of different agents and combine them to build a rapid prototype of more powerful and robust system. The Community can also serve as a learning environment for researchers and students to acquire and exchange of cutting edge multimedia analysis algorithms.",
        "keywords": "",
        "released": 2003,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000189123000010"
    },
    {
        "title": "Design and implementation of an adaptive predictive controller for combustor NO<sub>x</sub> emissions",
        "abstract": "This paper is concerned with the design and implementation of an adaptive predictive controller for oxides of nitrogen (NOx) emissions from gas turbine combustors. Predictive control techniques with both fixed and adaptive parameters are introduced. An online parameter estimation algorithm is used to model the nonlinear characteristics of the combustor NOx process. The predictive control strategies are implemented using the MATLAB/dSPACE, controller development environment. Their performance is evaluated on an atmospheric test rig fitted with a commercial combustor and also compared with a PID controller. (C) 1999 Elsevier Science Ltd. All rights reserved.",
        "keywords": "combustor; industrial process; predictive control",
        "released": 1999,
        "link": "https://doi.org/10.1016/S0959-1524(99)00016-5"
    },
    {
        "title": "Towards a type system for analyzing JavaScript programs",
        "abstract": "JavaScript is a popular language for client-side web scripting. It has a dubious reputation among programmers for two reasons. First, many JavaScript programs are written against a rapidly evolving API whose implementations are sometimes contradictory and idiosyncratic. Second, the language is only weakly typed and comes virtually without development tools. The present work is a first attempt to address the second point. It does so by defining a type system that tracks the possible traits of an object and flags suspicious type conversions. Because JavaScript is a classless, object-based language with first-class functions, the type system must include singleton types, subtyping, and first class record labels. The type system covers a representative subset of the language and there is a type soundness proof with respect to an operational semantics.",
        "keywords": "dynamic type systems; program analysis; object; functions",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000229101800028"
    },
    {
        "title": "Rotary machine vibration monitoring and smart balance correction",
        "abstract": "During the rotary machine operation process, seemingly small amounts of abnormal vibration can often cause serious damage to the machinery over time and even increase the risk of accidents. Although professional vibration engineers can determine the current health status of a machine by interpreting the vibration spectrum information and predicting which components will fail, if even ordinary operators can send feedback regarding the vibration signals reaching the human-machine interface through a system when an abnormality is detected in the machine, the abnormality can be made known and processed in time. This can prevent the magnified impact of rotary inertia, thereby lowering the risk of major damage and the failure of machinery and equipment, as well as effectively saving on equipment maintenance costs. This study mainly adopted LabVIEW and Arduino IDE to develop a control program and human-machine monitoring interface. As the initial experiment on rotary machine vibration monitoring and smart balance correction, the measurement system setup in this study was applied to determine vibration abnormality as well as to carry out continuous online automatic balance correction. Experimental verification was carried out using active correction and smart correction. In terms of active online balance correction, the amplitude correction rate was 96%, the double-frequency correction rate was 102.9%, and the correction process was performed in 5 min. In terms of smart balance correction, the amplitude correction rate was 103.8%, the double-frequency correction rate was 103.3%, and the correction process was performed in 3 min. Through feedback signaling, the operator can effectively learn the current health status of the mechanical equipment from the human-machine interface.",
        "keywords": "LabVIEW; Arduino IDE; human-machine monitoring interface; vibration monitoring; smart balance correction",
        "released": 2020,
        "link": "https://doi.org/10.1177/1687814020936032"
    },
    {
        "title": "Developing usable web interfaces with the aid of automatic verification of their formal specification",
        "abstract": "The development of interactive visual applications is a complex work, usually performed with the help of advanced visual programming environments. Although a number of tools are available to support designers and developers in the specification of a GUI’s layout and behavior, and in the generation of the corresponding code that implements the interface, theoretical guidelines and/or semi-automatic mechanisms rely upon the knowledge of the designer to manage usability and accessibility issues. Indeed, the evaluation of the visual environments is traditionally performed by means of expert-based evaluations or by testing with end users. In this work we describe a methodology to specify and evaluate interactive visual environments, in particular web interfaces, based on the SR-Action Grammars formalism and we present a bottom - up approach to aid the designer to develop graphical applications that automatically respect a significant number of usability rules before the software is released and tested by standard methods. We show how it is possible to assess the usability metrics of consistency, completeness and user control by means of checks performed at a high level of abstraction. VALUTA (Automatic Tool for the Usability Verification at Abstract Level) is the implemented tool that allows developers to generate the formal specification of an interactive visual application in automatic manner, so to perform the related usability controls at a very early stage. Thanks to usability controls automatically performed at formal level, the designer can use the evaluation results to perform feedback analysis of the visual environment. The tool is applicable to an already existing interface, allowing the designer to evaluate its usability in the development of a more usable version. We have analyzed the home page of three web sites (www.lycos.it, www.google.it, http://it.mail.yahoo.com/) as a case study and we show the related evaluation report generated by the described approach. (C) 2011 Published by Elsevier Ltd.",
        "keywords": "Graphical user interface; Usability evalution; Grammars formalism",
        "released": 2011,
        "link": "https://doi.org/10.1016/j.jvlc.2010.12.001"
    },
    {
        "title": "Medical, health and wellness tourism research-a review of the literature (1970-2020) and research agenda",
        "abstract": "Medical, health and wellness tourism and travel represent a dynamic and rapidly growing multi-disciplinary economic activity and field of knowledge. This research responds to earlier calls to integrate research on travel medicine and tourism. It critically reviews the literature published on these topics over a 50-year period (1970 to 2020) using CiteSpace software. Some 802 articles were gathered and analyzed from major databases including the Web of Science and Scopus. Markets (demand and behavior), destinations (development and promotion), and development environments (policies and impacts) emerged as the main three research themes in medical-health-wellness tourism. Medical-health-wellness tourism will integrate with other care sectors and become more embedded in policy-making related to sustainable development, especially with regards to quality of life initiatives. A future research agenda for medical-health-tourism is discussed.",
        "keywords": "medical-health-wellness tourism; bibliometric analysis; thematic analysis; research agenda",
        "released": 2021,
        "link": "https://doi.org/10.3390/ijerph182010875"
    },
    {
        "title": "Intelligent human-machine interface: An agile operation and decision support for an ANAMMOX SBR system at a pilot-scale wastewater treatment plant",
        "abstract": "Eco-efficient anaerobic ammonium oxidation (ANAMMOX) can eliminate toxic nutrients from wastewater and has been used in several nutrient removal technologies. However, its implementation for robust operation remains challenging because of process nonlinearity and time-variant characteristic, higher energy consumption, excess sludge produced, and biomass loss during sludge pumping. Also, sensor failure, process startup, and shut down present additional difficulties. In this article, an intelligent human-machine interface using an advanced numerical solution for a knowledge-based system (called ANKSys) was developed by integrating the fully optimized-functionality (soft sensing, decision making, and simulating model) data driven by supervisory control. This control consists of advanced algorithms (artificial neural network, Kalman filter, principal component analysis, least-square technique/renowned root-mean-squared error) using commercial software (MATLAB R2018a, Microsoft Visual Studio IDE 2016, Microsoft SQL Server 2014, OPC Automation with XGT series programmable logic controller). The developed ANKSys can help in online monitoring and optimal process operation by assessing risk and failure occurrences, acquiring data for data analysis, and managing operating expenditure. In real-time implementation, ANKSys enhanced the energy efficiency, i.e., 16% of a pilot-scale “LEAOX” wastewater treatment plant located at Daegu, Republic of Korea. Using this strategy, an optimal and sustainable operation for the removal of biological nitrogen was achieved.",
        "keywords": "Wastewater; Nitrogen; Mathematical models; Inductors; Kinetic theory; Microorganisms; Soft sensors; Advanced numerical solution for a knowledge-based system human-machine interface (ANKSys-HMI); anaerobic ammonium oxidation (ANAMMOX); decision support; soft sensor (SS); supervisory control and data acquisition (SCADA)",
        "released": 2022,
        "link": "https://doi.org/10.1109/TII.2022.3153468"
    },
    {
        "title": "Platform supporting intelligent human-machine interface (HMI) applications for smart machine tools",
        "abstract": "As the Internet of Things, artificial intelligence, and the fourth industrial revolution advance, smart factories and machines increasingly gain intelligent features that enable the integration of more sophisticated functionalities. Approaches to achieving this intelligence involve both internal systems, such as human-machine interface (HMI), and external systems, such as big data platforms and cloud services. Although current research leans toward studying external systems, accomplishing intelligent functions through such means poses more challenges in achieving real-time responses during machining processes than using internal systems. When intellectualizing machine tools through internal HMI systems, three critical issues must be addressed. First, HMI functions are structured to depend on the HMI itself, leading to a ripple effect where a problem occurring in one HMI function impacts the entire system. Second, owing to differences in development tools and programming languages, the interconnectivity between functions developed by multiple stakeholders to be loaded onto the HMI may suffer, leading to potential inefficiencies and increased maintenance costs. Third, although various types of computer numerical control (CNC) machines need to communicate with the HMI function, the diverse communication methods and development tools used by each CNC manufacturer result in identical intelligent functions being developed separately for each CNC type. To address these challenges, this study proposes an innovative HMI platform capable of executing and developing various intelligent functions. The HMI platform and its major components are designed and implemented through component-based development (CBD). Subsequently, the performance and effectiveness of the platform are validated using quality attribute scenarios.",
        "keywords": "Component-based development; Multi-vendors’ computer numerical control; Intelligent HMI; HMI application; Smart machine tools",
        "released": 2024,
        "link": "https://doi.org/10.1007/s12541-024-00960-6"
    },
    {
        "title": "Perspectives for effective integration of e-learning tools in university mathematics instruction for developing countries",
        "abstract": "This paper analyses student views on an e-learning intervention that incorporated a content development tool and computer algebra systems, aimed at improving performance and applicability of mathematics knowledge. The study deliberately relied on open source tools, with high usability both online and offline, that can be customized to address the peculiarities of mathematics instruction in developing countries. Repeated ANOVA and logistic regression were among the statistical methods used to analyze the data. Key findings showed that usability and detailed feedback were the qualities of computer algebra system that were most desired by students. Content quality, problem solving abilities and internet availability were key factors for mathematical e-learning satisfaction. The research showed that the use of an interactive content development tool and computer algebra systems can help the teachers to be more innovative and adopt project-based examination formats that encourage knowledge applicability. The e-learning tools helped the students to self-regulate and discover their own knowledge, which increased their chances of handling application type problems. Among others, the study recommended set up of mathematical e-laboratories which can be accessed by students for at least 3 days per week.",
        "keywords": "Assessment and feedback; Dynamic and interactive content; Content development tool; Computer algebra systems; Knowledge application; Logistic regression",
        "released": 2020,
        "link": "https://doi.org/10.1007/s10639-019-09995-z"
    },
    {
        "title": "A multi-agent-based simulator for a transmission control protocol/internet protocol network",
        "abstract": "The main goal of this paper is building a novel transmission control protocol/internet protocol (TCP/IP) network simulator engine for simulation of distributed applications for which capturing both higher and lower layer network parameters are important. There are not many comprehensive simulators available in industry and academia to simulate distributed applications while reporting the parameters of all the layers of the TCP/IP network active in such simulations. The major problem in building a comprehensive simulation scenario for applications residing on the higher layers of a network by using currently available simulators is that a core simulator for lower layers of the network should be used together with add-ons or other programs simulating higher network layers to be able to simulate the whole TCP/IP network. This paper presents a novel idea for network simulation that has not been implemented before, which is using agents to simulate all layers of the network. In this simulator, each TCP/IP layer is simulated separately by using a separate agent and its behavior. It is an integrated environment based on agent systems capable of simulating all layers of a TCP/IP network, including application and lower layers. The final goal is other agent systems simulating a complex higher level web-based distributed application being easily used together with these agents, which are simulating the core TCP/IP network. For evaluation and testing purposes, a simple distributed application consisting of several remote procedure calls is simulated. For the validation of the conducted simulations, the achieved results are compared with the results of two non-agent-based simulators. For the verification of each individual agent function, a report is generated that shows the information flow between agents. The communication routes between agents are checked manually to make sure the route selection is based on the expected behavior of each agent. The scalability of the proposed multi-agent-based simulator is tested for the given distributed application.",
        "keywords": "Multi-agent system based simulation; network simulators; remote procedure calls; agent development environment; web application simulation",
        "released": 2014,
        "link": "https://doi.org/10.1177/0037549714528268"
    },
    {
        "title": "Application of artificial intelligence technology in product design",
        "abstract": "Artificial intelligence (Al) technology applied to product design in Monozukuri (Japanese way of manufacturing) aims to provide computerized support to various tasks in developing products that currently rely on human experience. As the conventional approach, in which knowledge and rules are explicitly given, has its limit, new technologies based on machine learning have been recognized as important in research and development of Al. Applying machine learning that predicts data to be acquired in the future with a certain accuracy, we can obtain efficient and less -variable judgment and eliminate conventional work depended on personal knowledge or experience. In order to apply Al technology to a product development environment, MONOZUKURI Al framework was developed on the cloud as a system to facilitate efficient collection of product development data, and, at the same time, for managing and leveraging learning models extracted from such data. By connecting this framework with Flexible Technical Computing Platform (FTCP), Fujitsu’s integrated development platform, this new design development environment will provide various design tools on a platform with new, enhanced design-assisting features. This paper describes various cases in which machine learning is applied to designing, and presents our plan to introduce it into an integrated development platform.",
        "keywords": "",
        "released": 2017,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000405766700007"
    },
    {
        "title": "Adaptive assessment of student’s knowledge in programming courses",
        "abstract": "This paper presents Programming Adaptive Testing (PAT), a Web-based adaptive testing system for assessing students’ programming knowledge. PAT was used in two high school programming classes by 73 students. The question bank of PAT is composed of 443 questions. A question is classified in one out of three difficulty levels. In PAT, the levels of difficulties are adapted to Bloom’s taxonomy lower levels, and students are examined in their cognitive domain. This means that PAT has been designed according to pedagogical theories in order to be appropriate for the needs of the course “Application Development in a Programming Environment”. If a student answers a question correctly, a harder question is presented, otherwise an easier one. Easy questions examine the student’s knowledge, while difficult questions examine the student’s skills to apply prior knowledge to new problems. A student answers a personalized test composed of 30 questions. PAT classifies a student in one out of three programming skills’ levels. It can predict the corresponding classification of students in Greek National Exams. Furthermore, it can be helpful to both students and teachers. A student could discover his or her programming shortcomings. Similarly, a teacher could objectively assess his or her students as well as discover the subjects that need to be repeated.",
        "keywords": "assessment of programming; computerized adaptive assessment; computerized adaptive testing; grading; personalized test; programming assessment",
        "released": 2010,
        "link": "https://doi.org/10.1111/j.1365-2729.2010.00363.x"
    },
    {
        "title": "Emerging tools in the changing landscape of chronic hepatitis b management",
        "abstract": "Introduction: The availability of a preventative vaccine, interferon, and nucleos(t)ide analogs have provided progress in the control of chronic hepatitis B (CHB). Despite this, it remains a major contributor to global morbidity and mortality. Developments in our understanding of the pathogenesis of CHB and the emergence of new therapies are paving the way, as we move toward HBV cure. Areas covered: We performed bibliographical searches of online databases to review the literature regarding conventional disease phases of CHB. We provide the latest evidence challenging the perception of the natural history of CHB, noting that previously considered quiescent disease phases may not represent benign disease states devoid of progression. We explore the use of potential novel immunological and viral tools which should enhance disease stratification and management decisions in the coming years. Finally, we discuss the timing of treatment and how this could be initiated earlier to improve treatment outcomes, preventing sequelae of chronic infection. Expert opinion: The treatment paradigm in CHB is set to change with multiple novel agents in early phase clinical trials with the aim of a functional cure. An improved understanding of disease pathogenesis and the timing of treatment will be critical to the success of new therapies.",
        "keywords": "Hepatitis B virus; immune response; viral factors; treatment; disease progression",
        "released": 2019,
        "link": "https://doi.org/10.1080/14787210.2019.1694906"
    },
    {
        "title": "Uses, improvements, and extensions of prolog+CG: Case studies",
        "abstract": "Prolog+CG is a CG-Based logic programming language which integrates Prolog, the manipulation of conceptual graphs (CGs), Java and object-oriented constructs. It provides a powerful development environment for the creation of knowledge-based applications and their integration on the web. Java provides object-oriented capabilities allowing the development of multi-platform applications. Object-oriented Prolog provides the full power of an object-oriented logic programming language and CGs provide the expressive power of an advanced knowledge representation language. This paper presents the recent extensions that have been added to Prolog+CG and illustrates some typical uses of the environment for the development of various applications.",
        "keywords": "",
        "released": 2001,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000175011100025"
    },
    {
        "title": "A method for computing hourly, historical, terrain-corrected microclimate anywhere on earth",
        "abstract": "Microclimates are the thermal and hydric environments organisms actually experience, and estimates of them are increasingly needed in environmental research. The availability of global weather and terrain datasets, together with increasingly sophisticated microclimate modelling tools, makes the prospect of a global, web-based microclimate estimation procedure feasible. We have developed such an approach for the r programming environment which integrates existing r packages for obtaining terrain and sub-daily atmospheric forcing data (elevatr and rncep), and two complementary microclimate modelling packages (NicheMapR and microclima). The procedure can be used to generate NicheMapR’s hourly time-series outputs of above- and below-ground conditions, including convective and radiative environments, soil temperature, soil moisture and snow cover, for a single point, using microclima to account for local topographic and vegetation effects. Alternatively, it can use microclima to produce high-resolution grids of near-surface temperatures, using NicheMapR to derive calibration coefficients normally obtained from experimental data. We validate this integrated approach against a series of microclimate observations used previously in the tests of the respective models and show equivalent performance. It is thus now feasible to produce realistic estimates of microclimate at fine (<30 m) spatial and temporal scales anywhere on earth, from 1957 to present.",
        "keywords": "biophysical ecology; GIS; microclimate; modelling; soil moisture; soil temperature; topograhy",
        "released": 2020,
        "link": "https://doi.org/10.1111/2041-210X.13330"
    },
    {
        "title": "Real-time vital signs monitoring and data management using a low-cost IoT-based health monitoring system",
        "abstract": "This study describes the creation and evaluation of a low-cost internet of things (IoT)-based health monitoring system for the continuous monitoring of vital signs such as temperature, pulse rate, oxygen saturation (SpO2) and blood pressure (BP) (both systolic and diastolic). Along with an organic light-emitting diode (OLED) display and an ESP8266 microcontroller, the system includes BP, non-contact temperature, SpO2 and electrocardiogram (ECG) sensors. Using the visual programming tool, Node-RED, the data from these sensors are gathered, processed and transmitted to the Google Cloud platform for archival and visualisation. The process involved mounting the sensors and microcontrollers on a special printed circuit board and designing the circuit with EasyEDA. The device measures systolic, diastolic and pulse rates from the BP sensor, as well as temperature, ECG and SpO2 values. The system works by using three push switches to read and display these values on demand. The gathered data are simultaneously shown on the OLED and sent to the Node-RED dashboard, where it is then sent to a Google Spreadsheet for archiving and analysis. This research article gives a thorough overview of the health monitoring system, the way it was implemented, and how it was successfully validated in a real-time setting. This study examines certain vital signs but additional health measures, such as respiration rate or glucose monitoring, could be included. Machine learning algorithms could also be used for predictive analytics. This would uncover data anomalies and trends early, improving healthcare management.",
        "keywords": "Organic light-emitting diode; blood pressure sensor; non-contact temperature sensor; oxygen saturation; electrocardiogram sensor; Node-RED Android app",
        "released": 2024,
        "link": "https://doi.org/10.1177/09720634241246926"
    },
    {
        "title": "An app visualization design based on IoT self-diagnosis micro control unit for car accident prevention",
        "abstract": "This paper proposes an App Visualization (AppV) based on IoT Self-diagnosis Micro Control Unit (ISMCU) for accident prevention. It collects a current status of a vehicle through a sensor, visualizes it on a smart phone and prevents vehicles from accident. The AppV consists of 5 components. First, a Sensor Layer (SL) judges noxious gas from a current vehicle and a driver’s driving habit by collecting data from various sensors such as an Accelerator Position Sensor, an O2 sensor, an Oil Pressure Sensor, etc. and computing the concentration of the CO collected by a semiconductor gas sensor. Second, a Wireless Sensor Communication Layer (WSCL) supports Zigbee, Wi-Fi, and Bluetooth protocol so that it may transfer the sensor data collected in the SL to ISMCU and the data in the ISMCU to a Mobile. Third, an ISMCU integrates the transferred sensor information and transfers the integrated result to a Mobile. Fourth, a Mobile App Block Programming Tool (MABPT) is an independent App generation tool that changes to visual data just the vehicle information which drivers want from a smart phone. Fifth, an Embedded Module (EM) records the data collected through a Smart Phone real time in a Cloud Server. Therefore, because the AppV checks a vehicle’ fault and bad driving habits that are not known from sensors and performs self-diagnosis through a mobile, it can reduce time and cost spending on accidents caused by a vehicle’s fault and noxious gas emitted to the outside.",
        "keywords": "Self-diagnosis; sensor modeling; APP design; Eco-drive; sensor network",
        "released": 2017,
        "link": "https://doi.org/10.3837/tiis.2017.02.020"
    },
    {
        "title": "Microservices’ libraries enabling server-side business logic visual programming for digital twins",
        "abstract": "Digital twins are evermore adopted for planning activities in smart city and industrial contexts, thus requiring platforms able to handle their complexity, continuously adapting and improving data flow business logic behind to the user needs. To respond to such needs microservice architecture paradigm and business logic scripting solutions can be exploited. They should include facilities for data ingestion, transformation, visualization & event driven user interaction, formal definition of functional aspects, exploitation and management of data analytics and simulation, interoperability with external services of any kind, etc. To provide an easy and quick development tools, a large number of microservice has been formalized in a suite of new Nodes for the Node-RED framework and distributed in terms of Libraries via JS Foundation. The proposed suites of nodes (e.g., Snap4City libraries on Node-RED) are widely adopted by academic and industrial groups and fully integrated into Snap4City, an open-source platform for digital twins realization which can be used on cloud and on premise.",
        "keywords": "Microservices; Digital twin; Server-side business logic; Node-RED Business Intelligence",
        "released": 2024,
        "link": "https://doi.org/10.1016/j.softx.2024.101805"
    },
    {
        "title": "Resource-aware aggregate planning for the distributed manufacturing enterprise",
        "abstract": "The realization of “intelligent and resource aware” distributed enterprises requires substantial development of the underpinning modelling, information management and knowledge representation technologies. This paper deals with the “resource-aware, aggregate planning” of manufacturing operations at early design stages. The term “resource aware” indicates the creation of a dynamic inter-relationship between the planning entities and the enterprise resources, humans and machines. The technologies employed for implementing the pilot methods include; a web-centric co-development environment, unique methods for enriching planning entities with knowledge, and a flexible engine supporting planning scenarios by using evolutionary computing for optimisation and capability analysis techniques for feedback evaluation.",
        "keywords": "computer automated process planning (CAPP); distributed design manufacturing integration",
        "released": 2002,
        "link": "https://doi.org/10.1016/S0007-8506(07)61537-6"
    },
    {
        "title": "Interpolating hourly temperatures for computing agroclimatic metrics",
        "abstract": "Calculating many agroclimatic metrics, e.g., chill or heat accumulation in orchards, requires continuous records of hourly temperature. Such records are often unavailable, with farm managers and researchers relying on daily data or hourly records with gaps. While procedures for generating idealized temperature curves exist, interpolating hourly records has long been a challenge. The SolveHours procedure combines measured hourly temperatures, idealized daily temperature curves and proxy data to fill gaps in such records. It first determines daily temperature extremes by solving systems of linear equations that express the typical relationships between hourly temperatures and daily temperature extremes for every hour. After filling gaps in this record with bias-corrected data from proxy stations or by linear interpolation, SolveHours uses these data to generate an idealized temperature curve. Deviations of recorded hourly temperatures from this curve are calculated, linearly interpolated, and added to the idealized curve to obtain a gapless record. The procedure was compared to alternative gap-filling algorithms using an 8-month dataset from an orchard near Winters, CA, in which half the records were replaced by 500 gaps of random length. The SolveHours procedure achieved ratio of performance to interquartile distance (RPIQ) values of 6.7 (when using temperature extremes from a proxy station) and 8.2 (with temperature extremes measured on site), with root mean square errors of 1.6 and 1.3 A degrees C, respectively. It outperformed all other algorithms in reproducing recorded accumulation of Chill Portions and Growing Degree Hours. The SolveHours procedure is implemented in the chillR package for the R programming environment (<ExternalRef><RefSource>https://cran.r-project.org/web/packages/chillR/ vignettes/hourly_temperatures.html</RefSource><RefTarget Address=”https://cran.r-project.org/web/packages/chillR/vignettes/hou rly_temperatures.html” TargetType=”URL” /></ExternalRef>).",
        "keywords": "Agroclimatic metrics; chillR; Hourly temperature data; Interpolation; SolveHours",
        "released": 2018,
        "link": "https://doi.org/10.1007/s00484-018-1582-7"
    },
    {
        "title": "Environmental controls on the tropical island diurnal cycle in the context of intraseasonal variability",
        "abstract": "The mechanisms regulating the relationship between the tropical island diurnal cycle and large-scale modes of tropical variability such as the boreal summer intraseasonal oscillation (BSISO) are explored in observations and an ide-alized model. Specifically, the local environmental conditions associated with diurnal cycle variability are explored. Using Luzon Island in the northern Philippines as an observational test case, a novel probabilistic framework is applied to im-prove the understanding of diurnal cycle variability. High-amplitude diurnal cycle days tend to occur with weak to moder-ate offshore low-level wind and near to above average column moisture in the local environment. The transition from the BSISO suppressed phase to the active phase is most likely to produce the wind and moisture conditions supportive of a substantial diurnal cycle over western Luzon and the South China Sea (SCS). Thus, the impact of the BSISO on the local diurnal cycle can be understood in terms of the change in the probability of favorable environmental conditions. Idealized high-resolution 3D Cloud Model 1 (CM1) simulations driven by base states derived from BSISO composite profiles are able to reproduce several important features of the observed diurnal cycle variability with BSISO phase, including the strong, land-based diurnal cycle and offshore propagation in the transition phases. Background wind appears to be the pri-mary variable controlling the diurnal cycle response, but ambient moisture distinctly reduces precipitation strength in the suppressed BSISO phase and enhances it in the active phase.",
        "keywords": "Maritime Continent; Madden-Julian oscillation; Monsoons; Statistics; Cloud resolving models; Diurnal effects",
        "released": 2023,
        "link": "https://doi.org/10.1175/JCLI-D-22-0824.1"
    },
    {
        "title": "Developmental gene expression in amphioxus: New insights into the evolutionary origin of vertebrate brain regions, neural crest, and rostrocaudal segmentation",
        "abstract": "Amphioxus is widely held to be the closest invertebrate relative of the vertebrates and the best available stand-in for the proximate ancestor of the vertebrates. The spatiotemporal expression patterns of developmental genes can help suggest body part homologies between vertebrates and amphioxus, This approach is illustrated using five homeobox genes (AmphiHox1, AmphiHox2, AmphiOtx, AmphiDll, and AmphiEn) to pro,ide insights into the evolutionary origins of three important vertebrate features: the major brain regions, the neural crest, and rostrocaudal segmentation. During amphioxus development, the neural expression patterns of these genes are consistent with the presence of a forebrain (detailed neuroanatomy indicates that the forebrain is all diencephalon without any telencephalon) and an extensive hindbrain; the possible presence of a midbrain requires additional study. Further, during neurulation, the expression pattern of AmphiDll as web as migratory cell behavior suggest that the epidermal cells bordering the neural plate may represent a phylogenetic precursor of the vertebrate neural crest. Finally, when the paraxial mesoderm begins to segment, the earliest expression of AmphiEn is detected in the posterior part of each nascent and newly formed somite, This pattern recalls the expression of the segment-polarity gene engrailed during establishment of the segments of metameric protostomes. Thus, during animal evolution, the role of engrailed in establishing and maintaining metameric body plans may have arisen in a common segmented ancestor of both the protostomes and deuterostomes.",
        "keywords": "",
        "released": 1998,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000076265300006"
    },
    {
        "title": "Web application commercial design for financial entities based on business intelligence",
        "abstract": "Multiple customer data management has become a focus of attention in big organizations. Although much information is available, it does not translate into significant profitable value-added services. We present a design of a commercial web application based on business intelligence that generates information on social and financial behavior of clients in an organization; with the purpose of obtain additional information that allows to get more profits. This app will provide a broader perspective for making strategic decisions to increase profits and reduce internal investment costs. A case in point is the financial sector, a group of financial entities were used to make measurements and test them. A design to build a web application aimed at achieving a large and ambitious goal by means of defined tools reflecting clients’ business needs is proposed. In this research, different techniques and technologies are explored, such as diagrams, frameworks, design, architecture, model entity-relationship, tables, equations, mental maps and development tools. Through the Personal Software Process methodology and with the help of information extraction, consolidation, and visualization, the implementation can be carried out. This article provides the importance of implementing business intelligence in an organization and expands on the steps needed for the implementation of this valuable technology.",
        "keywords": "Business intelligence; banking application; web application; trend analysis; decision making",
        "released": 2021,
        "link": "https://doi.org/10.32604/cmc.2021.014738"
    },
    {
        "title": "An interactive bayesian model for prediction of lymph node ratio and survival in pancreatic cancer patients",
        "abstract": "Background Regional lymph node status has long been used as a dichotomous predictor of clinical outcomes in cancer patients. More recently, interest has turned to the prognostic utility of lymph node ratio (LNR), quantified as the proportion of positive nodes examined. However, statistical tools for the joint modeling of LNR and its effect on cancer survival are lacking. Methods Data were obtained from the NCI SEER cancer registry on 6400 patients diagnosed with pancreatic ductal adenocarcinoma from 2004 to 2010 and who underwent radical oncologic resection. A novel Bayesian statistical approach was developed and applied to model simultaneously patients’ true, but unobservable, LNR statuses and overall survival. New web development tools were then employed to create an interactive web application for individualized patient prediction. Results Histologic grade and T and M stages were important predictors of LNR status. Significant predictors of survival included age, gender, marital status, grade, histology, T and M stages, tumor size, and radiation therapy. LNR was found to have a highly significant, non-linear effect on survival. Furthermore, predictive performance of the survival model compared favorably to those from studies with more homogeneous patients and individualized predictors. Conclusions We provide a new approach and tool set for the prediction of LNR and survival that are generally applicable to a host of cancer types, including breast, colon, melanoma, and stomach. Our methods are illustrated with the development of a validated model and web applications for the prediction of survival in a large set of pancreatic cancer patients.",
        "keywords": "",
        "released": 2014,
        "link": "https://doi.org/10.1136/amiajnl-2013-002171"
    },
    {
        "title": "Semantic framework for mapping object-oriented model to semantic web languages",
        "abstract": "The article deals with and discusses two main approaches in building semantic structures for electrophysiological metadata. It is the use of conventional data structures, repositories, and programming languages on one hand and the use of formal representations of ontologies, known from knowledge representation, such as description logics or semantic web languages on the other hand. Although knowledge engineering offers languages supporting richer semantic means of expression and technological advanced approaches, conventional data structures and repositories are still popular among developers, administrators and users because of their simplicity, overall intelligibility, and lower demands on technical equipment. The choice of conventional data resources and repositories, however, raises the question of how and where to add semantics that cannot be naturally expressed using them. As one of the possible solutions, this semantics can be added into the structures of the programming language that accesses and processes the underlying data. To support this idea we introduced a software prototype that enables its users to add semantically richer expressions into a Java object-oriented code. This approach does not burden users with additional demands on programming environment since reflective Java annotations were used as an entry for these expressions. Moreover, additional semantics need not to be written by the programmer directly to the code, but it can be collected from non-programmers using a graphic user interface. The mapping that allows the transformation of the semantically enriched Java code into the Semantic Web language OWL was proposed and implemented in a library named the Semantic Framework. This approach was validated by the integration of the Semantic Framework in the EEG/ERP Portal and by the subsequent registration of the EEG/ERP Portal in the Neuroscience Information Framework.",
        "keywords": "EEG/ERP portal; electrophysiology; object-oriented code; ontology; semantic framework; semantic web",
        "released": 2015,
        "link": "https://doi.org/10.3389/fninf.2015.00003"
    },
    {
        "title": "Middle-down analysis of monoclonal antibodies with electron transfer dissociation orbitrap fourier transform mass spectrometry",
        "abstract": "The rapid growth of approved biotherapeutics, e.g., monoclonal antibodies or immunoglobulins G (IgGs), demands improved techniques for their quality control. Traditionally, proteolysis-based bottom-up mass spectrometry (MS) has been employed. However, the long, multistep sample preparation protocols required for bottom-up MS are known to potentially introduce artifacts in the original sample. For this reason, a top-down MS approach would be preferable. The current performance of top-down MS of intact monoclonal Ig Gs, though, enables reaching only up to similar to 30% sequence coverage, with incomplete sequencing of the complementarity determining regions which are fundamental for IgG’s antigen binding. Here, we describe a middle-down MS protocol based on the use of immunoglobulin G-degrading enzyme of Streptococcus pyogenes (IdeS), which is capable of digesting IgGs in only 30 min. After chemical reduction, the obtained similar to 25 kDa proteolytic fragments were analyzed by reversed phase liquid chromatography (LC) coupled online with an electron transfer dissociation (ETD)-enabled hybrid Orbitrap Fourier transform mass spectrometer (Orbitrap Elite FTMS). Upon optimization of ETD and product ion transfer parameters, results show that up to similar to 50% sequence coverage for selected IgG fragments is reached in a single LC run and up to similar to 70% when data obtained by distinct LC MS runs are averaged. Importantly, we demonstrate the potential of this middle-down approach in the identification of oxidized methionine residues. The described approach shows a particular potential for the analysis of IgG mixtures.",
        "keywords": "",
        "released": 2014,
        "link": "https://doi.org/10.1021/ac4036857"
    },
    {
        "title": "Development of web content for music education using AR human facial recognition technology",
        "abstract": "As the media market changes rapidly, market demand is increasing for content that can be consumed on web platforms. It’s required to produce differentiated web content that can attract viewers’ interest. In order to increase the productivity and efficiency of content creation, cases of content production using AR engines are increasing. This study has a development environment in which parametrics and muscle-based model techniques are mixed. The faces of famous Western classical musicians, such as Mozart, Beethoven, Chopin and List are created as 3D characters and augmented on human’s face based on facial recognition technology in this study. It analyzes and traces the changed of facial expression of each person, then apply to 3D character’s facial expression in real-time. Each person who augmented musicians’ faces can become those who lived in different times, deliver information and communicate with viewers of the present era based on the music educational scripts. This study presents a new direction for video production required in the media market.",
        "keywords": "Augmented reality (AR); unity engine; face recognition; facial; expression; real-time tracking; YouTube; web content.",
        "released": 2023,
        "link": "https://doi.org/10.13052/jwe1540-9589.2252"
    },
    {
        "title": "A data sharing method in the open web environment: Data sharing in hydrology",
        "abstract": "Data sharing plays a fundamental role in providing data resources for geographic modeling and simulation. Although there are many successful cases of data sharing through the web, current practices for sharing data mostly focus on data publication using metadata at the file level, which requires identifying, restructuring and synthesizing raw data files for further usage. In hydrology, because the same hydrological information is often stored in data files with different formats, modelers should identify the required information from multisource data sets and then customize data requirements for their applications. However, these data customization tasks are difficult to repeat, which leads to repetitive labor. This paper presents a data sharing method that provides a solution for data manipulation based on a structured data description model rather than raw data files. With the structured data description model, multisource hydrological data can be accessed and processed in a unified way and published as data services using a designed data server. This study also proposes a data configuration manager to customize data requirements through an interactive programming tool, which can help in using the data services. In addition, a component-based data viewer is developed for the visualization of multisource data in a sharable visualization scheme. A case study that involves sharing and applying hydrological data is designed to examine the applicability and feasibility of the proposed data sharing method.",
        "keywords": "Data sharing; Data configuration; Hydrological data; Hydrological modeling and simulation",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.jhydrol.2020.124973"
    },
    {
        "title": "Implementing dementia care mapping as a practice development tool in dementia care services: A systematic review",
        "abstract": "Dementia Care Mapping (DCM) is an observational tool set within a practice development process. Following training in the method, DCM is implemented via a cyclic process of briefing staff, conducting mapping observations, data analysis and report preparation, feedback to staff and action planning. Recent controlled studies of DCM’s efficacy have found heterogeneous results, and variability in DCM implementation has been indicated as a potential contributing factor. This review aimed to examine the primary research evidence on the processes and the barriers and facilitators to implementing DCM as a practice development method within formal dementia care settings. PUBMED, PsycINFO, CINAHL, The Cochrane Library-Cochrane reviews, HMIC (Ovid), Web of Science and Social Care Online were searched using the term “Dementia Care Mapping”. Inclusion criterion was primary research studies in any formal dementia care settings where DCM was used as a practice development tool and which included discussion/critique of the implementation processes. Assessment of study quality was conducted using the Mixed Methods Appraisal Tool. Twelve papers were included in the review, representing nine research studies. The papers included discussion of various components of the DCM process, including mapper selection and preparation; mapping observations; data analysis, report writing and feedback; and action planning. However, robust evidence on requirements for successful implementation of these components was limited. Barriers and facilitators to mapping were also discussed. The review found some consensus that DCM is more likely to be successfully implemented if the right people are selected to be trained as mappers, with appropriate mapper preparation and ongoing support and with effective leadership for DCM within the implementing organization/unit and in organizations that already have a person-centered culture or ethos. Future development of the DCM tool should consider ways to save on time taken to conduct DCM cycles. More research to understand the ingredients for effective DCM implementation is needed.",
        "keywords": "dementia; practice development; staff training; person-centered care",
        "released": 2018,
        "link": "https://doi.org/10.2147/CIA.S138836"
    },
    {
        "title": "A cloud-based cognitive computing solution with interoperable applications to counteract illegal dumping in smart cities",
        "abstract": "The study presented in this paper is the outcome of the activity carried on within the program “Party Cloud Challenge per Genova”, promoted by IBM in collaboration with the city municipality of Genoa, Italy. This challenge aimed to show how using cognitive computing solutions in an integrated cloud-based development environment enables the rapid deployment of advanced services with interoperable applications. Specifically, we investigated a solution to cope with the problem of illegal dumping prevention in a smart city. In this respect, we will describe the study of the prototype of an automated visual recognition and alerting system. The presented solution relies on the use of cognitive computing technologies to analyze videos provided by cameras installed in urban areas, to identify trash, especially bulky waste, where it should not be, and trigger an alarm to the municipality. In particular, we want to take advantage of the pictures, frames and videos continuously recorded by cameras installed for traffic monitoring, for surveillance, etc. in smart cities where the waste management system is supposed to be integrated with other municipality services for environment control and management. Besides, an organization plan is also proposed for intelligent waste collection as well as some organizational ideas for scalability.",
        "keywords": "Smart city; Cloud; Cognitive computing; Image recognition; Waste management; Environment",
        "released": 2022,
        "link": "https://doi.org/10.1007/s11042-021-11238-8"
    },
    {
        "title": "Integrating BIM into sensor-based facilities management operations",
        "abstract": "Purpose To mitigate the problems in sensor-based facility management (FM) such as lack of detailed visual information about a built facility and the maintenance of large scale sensor deployments, an integrated data source for the facility’s life cycle should be used. Building information modeling (BIM) provides a useful visual model and database that can be used as a repository for all data captured or made during the facility’s life cycle. It can be used for modeling the sensing-based system for data collection, serving as a source of all information for smart objects such as the sensors used for that purpose. Although few studies have been conducted in integrating BIM with sensor-based monitoring system, providing an integrated platform using BIM for improving the communication between FMs and Internet of Things (IoT) companies in cases encountered failed sensors has received the least attention in the technical literature. Therefore, the purpose of this paper is to conceptualize and develop a BIM-based system architecture for fault detection and alert generation for malfunctioning FM sensors in smart IoT environments during the operational phase of a building to ensure minimal disruption to monitoring services. Design/methodology/approach This paper describes an attempt to examine the applicability of BIM for an efficient sensor failure management system in smart IoT environments during the operational phase of a building. For this purpose, a seven-story office building with four typical types of FM-related sensors with all associated parameters was modeled in a commercial BIM platform. An integrated workflow was developed in Dynamo, a visual programming tool, to integrate the associated sensors maintenance-related information to a cloud-based tool to provide a fast and efficient communication platform between the building facility manager and IoT companies for intelligent sensor management. Findings The information within BIM allows better and more effective decision-making for building facility managers. Integrating building and sensors information within BIM to a cloud-based system can facilitate better communication between the building facility manager and IoT company for an effective IoT system maintenance. Using a developed integrated workflow (including three specifically designed modules) in Dynamo, a visual programming tool, the system was able to automatically extract and send all essential information such as the type of failed sensors as well as their model and location to IoT companies in the event of sensor failure using a cloud database that is effective for the timely maintenance and replacement of sensors. The system developed in this study was implemented, and its capabilities were illustrated through a case study. The use of the developed system can help facility managers in taking timely actions in the event of any sensor failure and/or malfunction to ensure minimal disruption to monitoring services. Research limitations/implications However, there are some limitations in this work which are as follows: while the present study demonstrates the feasibility of using BIM in the maintenance planning of monitoring systems in the building, the developed workflow can be expanded by integrating some type of sensors like an occupancy sensor to the developed workflow to automatically record and identify the number of occupants (visitors) to prioritize the maintenance work; and the developed workflow can be integrated with the sensors’ data and some machine learning techniques to automatically identify the sensors’ malfunction and update the BIM model accordingly. Practical implications Transferring the related information such as the room location, occupancy status, number of occupants, type and model of the sensor, sensor ID and required action from the BIM model to the cloud would be extremely helpful to the IoT companies to actually visualize workspaces in advance, and to plan for timely and effective decision-making without any physical inspection, and to support maintenance planning decisions, such as prioritizing maintenance works by considering different factors such as the importance of spaces and number of occupancies. The developed framework is also beneficial for preventive maintenance works. The system can be set up according to the maintenance and time-based expiration schedules, automatically sharing alerts with FMs and IoT maintenance contractors in advance about the IoT parts replacement. For effective predictive maintenance planning, machine learning techniques can be integrated into the developed workflow to efficiently predict the future condition of individual IoT components such as data loggers and sensors, etc. as well as MEP components. Originality/value Lack of detailed visual information about a built facility can be a reason behind the inefficient management of a facility. Detecting and repairing failed sensors at the earliest possible time is critical to ensure the functional continuity of the monitoring systems. On the other hand, the maintenance of large-scale sensor deployments becomes a significant challenge. Despite its importance, few studies have been conducted in integrating BIM with a sensor-based monitoring system, providing an integrated platform using BIM for improving the communication between facility managers and IoT companies in cases encountered failed sensors. In this paper, a cloud-based BIM platform was developed for the maintenance and timely replacement of sensors which are critical to ensure minimal disruption to monitoring services in sensor-based FM.",
        "keywords": "Building information modeling; Operational phase; Sensor-based facility management; Fault detection; Smart IoT environments; Sensor management",
        "released": 2022,
        "link": "https://doi.org/10.1108/JFM-08-2020-0055"
    },
    {
        "title": "From bytecode to JavaScript: The js_of_ocaml compiler",
        "abstract": "We present the design and implementation of a compiler from OCaml bytecode to JavaScript. The compiler first translates the bytecode into a static single-assignment intermediate representation on which optimizations are performed, before generating JavaScript. We believe that taking bytecode as an input instead of a high-level language is a sensible choice. Virtual machines provide a very stable API. Such a compiler is thus easy to maintain. It is also convenient to use, and it can just be added to an existing installation of the development tools. Already-compiled libraries can be used directly, with no need to reinstall anything. Finally, some virtual machines are the target of several languages. A bytecode to JavaScript compiler would make it possible to retarget all these languages to Web browsers at once. Copyright (C) 2013 John Wiley & Sons, Ltd.",
        "keywords": "compiler; OCaml; JavaScript; bytecode",
        "released": 2014,
        "link": "https://doi.org/10.1002/spe.2187"
    },
    {
        "title": "The application environment",
        "abstract": "There is consensus within the communications industry that the design pattern for the intelligence architecture will be based on a componentisation model where the richness of the underlying network and services will be available as capabilities. This approach is aimed at maximising reuse of capabilities for next generation applications and supporting the diverse range of business models. To make this model work, an application environment is proposed that exposes the capabilities to the service provider (and their application developers) and supports them through the complete application life cycle from engagement, development, deployment, execution and withdrawal. The architecture for the application environment consists of four major components - capability exposure function, application hosting, the service provider framework and the service subscription function. This architecture can be implemented using technologies such as Web Services (including UDDI), open standard interfaces such as Parlay and SIP servlets, and development environments such as J2EE and .NET.",
        "keywords": "",
        "released": 2005,
        "link": "https://doi.org/10.1007/s10550-005-0109-z"
    },
    {
        "title": "Clinical procedures for the prevention of preeclampsia in pregnant women: A systematic review",
        "abstract": "Objective To identify the most effective procedures recommended for the prevention of preeclampsia. Data Sources A systematic review was performed in the following databases: Pubmed/MEDLINE, CINAHL, Web of Science, Cochrane and LILACS via the Virtual Health Library (VHL). A manual search was also performed to find additional references. The risk of bias, the quality of the evidence, and the classification of the strength of the recommendations were evaluated using the Grading of Recommendations, Assessment, Development and Evaluations (GRADE) approach. Selection of Studies In the initial search in the databases, the total number of articles retrieved was 351, and 2 were retrieved through the manual search; after duplicate articles were removed, 333 citations remained. After a thorough review of the titles and abstracts, 315 references were excluded. Accordingly, 18 articles weremaintained for selection of the complete text (phase 2). This process led to the exclusion of 6 studies. In total, 12 articles were selected for data extraction and qualitative synthesis. Data Collection The articles selected for the study were analyzed, and we inserted the synthesis of the evidence in the online software GRADEpro Guideline Development Tool (GDT) (McMaster University and Evidence Prime Inc. All right reserved. McMaster University, Hamilton, Ontario, Canada); thus, it was possible to develop a table of evidence, with the quality of the evidence and the classification of the strength of the recommendations. Data Synthesis In total, seven studies recommended the individual use of aspirin, or aspirin combined with calcium, heparin or dipyridamole. The use of calcium alone or in combination with phytonutrients was also highlighted. All of the studies were with women at a high risk of developing preeclampsia. Conclusion According to the studies evaluated, the administration of aspirin is still the best procedure to be used in the clinical practice to prevent preeclampsia.",
        "keywords": "pregnant women; preeclampsia; prevention and control; maternal health; pregnancy complications",
        "released": 2020,
        "link": "https://doi.org/10.1055/s-0040-1714135"
    },
    {
        "title": "Barriers to physiotherapists’ use of professional development tools for chronic pain: A knowledge translation study",
        "abstract": "Purpose: The Pain Science Division (PSD) is a special interest group of the Canadian Physiotherapy Association that serves physiotherapists who have an interest in better understanding and managing patients’ pain. The PSD developed evidence-based resources for its members with the goal of improving patient care by supporting professional development. However, online metrics tracking access to these resources indicated that access was low. The purpose of this study was to identify the barriers PSD members encountered to the use of PSD resources and to recommend interventions to address these barriers guided by the Theory and Techniques Tool (TTT). Method: We distributed an online survey to PSD members across Canada. We used the TTT, a knowledge translation tool, to guide the design of the questionnaire and identify actionable findings. Results: Response rates from 621 non-student members and 1,470 student members were 26.9% and 1.4%, respectively. Based on the frequency of practicing physiotherapists’ (N = 167) agreement with items in the TTT, the primary barriers to use of the PSD resources were forgetting that the resources were available and forgetting to use them. Conclusions: The TTT can be used to identify barriers to use of professional development tools.",
        "keywords": "continuing education; evidence-based practice; knowledge translation; pain management; physical therapists",
        "released": 2022,
        "link": "https://doi.org/10.3138/ptc-2020-0148"
    },
    {
        "title": "Platform for hands-on remote labs based on the ESP32 and NOD-red",
        "abstract": "Not only in Morocco, throughout the walks of the world covid 19 pandemics has seriously questioned policymakers from different sectors. Think-tank in the educational sector notably higher education addressed by such a wide range of challenges brought about by covid 19. The characteristic concern that educationalists in Moroccan universities have to reconsider in this pandemic period should not be beyond rethinking new pedagogical alternatives including approaches, methods, techniques and didactic materials which can successfully assist practioners of the teaching and learning process to keep up with the current alterations. Practical work (PW) is an indispensable type of teaching in scientific and technical training and meets a real complementary need through real, remote or virtual laboratories. Students can consolidate what they have learnt and develop analytical skills by comparing experimental results with those obtained during the manipulation. In this context, the Laboratory of Engineering Sciences and Energy Management (LASIME) at the Superior School of Technology of Agadir has developed a low-cost platform called LABERSIME installed in the cloud (LMS, IDE) and equipped with an embedded system to drive real laboratory equipment and perform experiments qualitatively more efficient than those in face-to-face mode. The ultimate goal is to stimulate self-learning motivation in students through a creative approach.(c) 2022 The Author(s). Published by Elsevier B.V. on behalf of African Institute of This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ )",
        "keywords": "Remote laboratory; e-learning; Remote control; Remote monitoring; Open-source hardware; IoT",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.sciaf.2022.e01502"
    },
    {
        "title": "Digital teaching management system based on deep learning of internet of things",
        "abstract": "In order to solve a series of problems similar to the repetitive construction of resources and low degree of resource sharing in cruciform teaching, this paper studies the digital disarming management. Based on the in-depth analysis of the actual needs of the digital teaching resource service system and the key problems in the system, taking the resources and business process as the starting point, based on Java Web related technology, combined with the related processes of resources and business processing, this paper designs the digital teaching resource system of colleges and universities. The system has the functions of resource digitization, process management, and interaction with other platforms. The system database platform and operation platform are built; the development environment is configured; and the user login service function, data conversion service function, process management service function, and system management service function are completed. The function is tested by unit test, and the performance is tested and analyzed according to user requirements and design objectives. The system adopts Java web development technology, uses S2SH framework as the development basis, and takes Oracle database as the data storage platform through Tomcat6.0 web server for program publishing. Through system testing and analysis, the software can operate normally and achieve the expected functions and can be put into use.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1155/2022/3414935"
    },
    {
        "title": "Conceptual design support system in a collaborative environment for injection moulding",
        "abstract": "The shortened time-to-market of new products requires experts from multiple disciplines to cooperate in product development. In this collaborative product development environment, an Internet-based conceptual design support system for Injection moulding is developed. It can help designers to put forward feasible design solutions quickly and provide strong support for collaborative design. The architecture, implementation and evaluation of this system are discussed in this paper. The proposed system makes fully use of component-based web technology and has a flexible distributed architecture. A hybrid neural network and genetic algorithm approach is also adopted for more accurate and efficient retrieval. Experiment results indicate it is superior to conventional systems in accuracy and time.",
        "keywords": "collaborative product development; CBR; injection mould; Internet",
        "released": 2004,
        "link": "https://doi.org/10.1007/s00170-002-1505-x"
    },
    {
        "title": "Vaa3D-x for cross-platform teravoxel-scale immersive exploration of multidimensional image data",
        "abstract": "Vaa3D is a software package that has been widely used to visualize and analyze multidimensional microscopic images in a number of cutting edge bioimage informatics applications. However, due to many recent updates of both software development environments and operating systems, it was highly requested to maintain Vaa3D and disseminate it on latest operating systems. In addition, there has never been a showcase about how to use Vaa3D’s cross-platform visualization and immersive exploration functions for multidimensional and teravoxel-scale images. Here, we introduce a newly developed version of the software, called Vaa3D-x, to address all the above issues. Availability and implementation: Vaa3D-x is released in both binary and Open-Source available at vaa3d.org and GitHub (https://github.com/Vaa3D). Contact: 230218257@seu.edu.cn Supplementary information: Supplementary data are available at Bioinformatics online.",
        "keywords": "",
        "released": 2023,
        "link": "https://doi.org/10.1093/bioinformatics/btac794"
    },
    {
        "title": "Organizational knowledge encapsulation and re-use in collaborative product development",
        "abstract": "This paper discusses the theoretical aspects and applications of a novel methodology for exploiting a knowledge management editor tool to structure organizational knowledge and integrate it with product development activities. An organizational knowledge framework for capturing and representing manufacturing know-how has been developed using an ontological approach. The captured knowledge is converted into the industry-standard eXtensible mark-up language (XML) and then shared within a web-centric product data management (PDM) system to support a collaborative and distributed product development environment. The key business benefit of adopting such an approach arises from the closer integration between the key technical and business activities taking place during early design. In particular the effectiveness of decision making is increased.",
        "keywords": "ontology; organizational knowledge-based system; manufacturing know-how; product data management; collaborative product development",
        "released": 2006,
        "link": "https://doi.org/10.1080/09511920500504479"
    },
    {
        "title": "Stabilization of gas-lift oil wells by a nonlinear model predictive control scheme based on adaptive neural network models",
        "abstract": "Producing oil from gas-lift wells are often faced with severe producing oscillatory flow regimes. A major source of the oscillations is recognized as casing-heading instability which is caused by dynamic interaction between injection gas and multiphase fluid. This phenomenon poses strict production-related challenges in terms of lower average production and strain on downstream equipment. In this paper, an effective solution is proposed based on integration of an online interpretation dynamic model and a nonlinear model predictive control (NMPC) scheme. The paper uses adaptive growing and pruning radial basis function (GAP-RBF) neural networks (NNs) to recursively capture the essential dynamics of casing-heading instability in a nonlinear model structure. Extended Kalman filter (EKF) and unscented Kalman filter (UKF) are comparatively investigated to adaptively train modified GAP-RBF NNs. NMPC methodology is developed on the basis of the identified nonlinear NN model for real-time stabilization of casing-heading instability in an oil reservoir equipped with a gas-lift production well. A set of test studies has been conducted to explore the superior performance of the proposed adaptive NMPC controller under different scenarios for an oil reservoir simulated in ECLIPSE and linked to a complementary gas-lifted oil well simulated in programming environment. (C) 2013 Elsevier Ltd. All rights reserved.",
        "keywords": "Gas-lifted well; Online identification; GAP-RBF NN; Reservoir simulation; Genetic algorithm; NMPC",
        "released": 2013,
        "link": "https://doi.org/10.1016/j.engappai.2013.03.007"
    },
    {
        "title": "Easy and efficient agent-based simulations with the OpenABL language and compiler",
        "abstract": "Agent-based simulations represent an effective scientific tool, with numerous applications from social sciences to biology, which aims to emulate or predict complex phenomena through a set of simple rules performed by multiple agents. To simulate a large number of agents with complex models, practitioners have developed high-performance parallel implementations, often specialized for particular scenarios and target hardware. It is, however, difficult to obtain portable simulations, which achieve high performance and at the same time are easy to write and to reproduce on different hardware. This article gives a complete presentation of OpenABL, a domain-specific language and a compiler for agent-based simulations that enable users to achieve high-performance parallel and distributed agent simulations with a simple and portable programming environment. OpenABL is comprised of (1) an easy-to-program language, which relies on domain abstractions and explicitly exposes agent parallelism, synchronization and locality, (2) a source-to-source compiler, and (3) a set of pluggable compiler backends, which generate target code for multi-core CPUs, GPUs, and cloud-based systems. We evaluate OpenABL on simulations from different fields. In particular, our analysis includes predator-prey and keratinocyte, two complex simulations with multiple step functions, heterogeneous agent types, and dynamic creation and removal of agents. The results show that OpenABL-generated codes are portable to different platforms, perform similarly to manual target-specific implementations, and require significantly fewer lines of codes. (C) 2020 Elsevier B.V. All rights reserved.",
        "keywords": "Agent-based simulation; Domain specific language; GPU; Parallel and distributed computing; Compilers",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.future.2020.10.014"
    },
    {
        "title": "Google earth engine for geo-big data applications: A meta-analysis and systematic review",
        "abstract": "Google Earth Engine (GEE) is a cloud-based geospatial processing platform for large-scale environmental monitoring and analysis. The free-to-use GEE platform provides access to (1) petabytes of publicly available remote sensing imagery and other ready-to-use products with an explorer web app; (2) high-speed parallel processing and machine learning algorithms using Google’s computational infrastructure; and (3) a library of Application Programming Interfaces (APIs) with development environments that support popular coding languages, such as JavaScript and Python. Together these core features enable users to discover, analyze and visualize geospatial big data in powerful ways without needing access to supercomputers or specialized coding expertise. The development of GEE has created much enthusiasm and engagement in the remote sensing and geospatial data science fields. Yet after a decade since GEE was launched, its impact on remote sensing and geospatial science has not been carefully explored. Thus, a systematic review of GEE that can provide readers with the “big picture” of the current status and general trends in GEE is needed. To this end, the decision was taken to perform a meta-analysis investigation of recent peer-reviewed GEE articles focusing on several features, including data, sensor type, study area, spatial resolution, application, strategy, and analytical methods. A total of 349 peer-reviewed articles published in 146 different journals between 2010 and October 2019 were reviewed. Publications and geographical distribution trends showed a broad spectrum of applications in environmental analyses at both regional and global scales. Remote sensing datasets were used in 90% of studies while 10% of the articles utilized ready-to-use products for analyses. Optical satellite imagery with medium spatial resolution, particularly Landsat data with an archive exceeding 40 years, has been used extensively. Linear regression and random forest were the most frequently used algorithms for satellite imagery processing. Among ready-to-use products, the normalized difference vegetation index (NDVI) was used in 27% of studies for vegetation, crop, land cover mapping and drought monitoring. The results of this study confirm that GEE has and continues to make substantive progress on global challenges involving process of geo-big data.",
        "keywords": "Google Earth Engine; Geo-big data; Cloud-based platform; Remote sensing; Planetary-scale; Geospatial; Machine learning; Environmental monitoring",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.isprsjprs.2020.04.001"
    },
    {
        "title": "Gaming in second life via Scratch4SL: Engaging high school students in programming courses",
        "abstract": "While pedagogical and technological affordances of three-dimensional (3D) multiuser virtual worlds in various educational disciplines are largely well-known, a study about their effect on high school students’ engagement in introductory programming courses is still lacking. This case study presents students’ opinions about their participation in a 3D multiuser game-like environment, by harnessing Second Life in combination with the two-dimensional (2D) programming environment of Scratch4SL. Following a blended instructional format (face-to-face in a computer laboratory and supplementary online courses), 56 students utilizing Scratch4SL participated in this study, with a view to reduce the steep learning curve created during their first-time entrance into Second Life. This study identifies Papert’s theory of Constructionism as a potentially appropriate theoretical foundation for the development of an instructional framework, in order to assist students to coordinate and manage learning materials with other teammates, using their computational thinking skills in collaborative problem-based programming tasks. The study findings based on a mixed-method research (a close-ended questionnaire and an open-ended interview) indicated the effectiveness of this constructionist-oriented instructional process for students’ engagement to acquire or empower social, cognitive, higher-order, and computational thinking skills. Educational implications and recommendations for future research are also discussed.",
        "keywords": "3D environment; computational thinking skills; constructionism; programming; student engagement; second life; scratch4SL",
        "released": 2016,
        "link": "https://doi.org/10.1177/0735633115612785"
    },
    {
        "title": "Identifying cloud internet of things requirements in healthcare: A delphi-based study",
        "abstract": "The Internet of Things (IoT) and cloud computing are emerging technologies whose use has been proven in various fields of medicine and healthcare. The positive impact of these technologies on improving the performance of medical centers is also evident. Therefore, the purpose of the present study is to investigate the technological requirements of the Cloud Internet of Things (CIoT) in the healthcare industry. This was a quantitative study conducted in two rounds using the Delphi method. A non-random, purposeful, and criterion-based method was used for sampling. A minimum of one paper or research project in the IoTs, cloud computing, or CIoT fields should be completed by participants. In addition to having a master’s degree or specialized doctorate and at least 5 years of work experience, they should be faculty members. Fifteen participants in the first round of Delphi and 12 participants in the second round who completed all of the questionnaire’s questions made up the research sample. A questionnaire was used to gather information. This questionnaire was designed in four main parts with 14 axes and 48 questions. After collecting the questionnaires in the first round of Delphi, each axis was scored on a five-point Likert scale. Finally, the requirements for the implementation of CIoT in healthcare were determined by the collective agreement of experts. Functional requirements of the network layer and cloud computing, platform layer requirements, and components of quality requirements were extracted. The architectures of the application layer, cloud layer, server layer, network layer, and user layer are explored thoroughly. Furthermore, communication technologies, equipment types, analysis and management, wireless networks and sensors, secure data exchange and management, deployment details, and approaches for security management and control are the main infrastructural keys that are extracted during the Delphi rounds. The findings showed that, with a very high level of importance, the use of wireless local networks was approved more than other network layer components. Compliance with security principles emerged as the most crucial component of the platform layer. We also found the use of hybrid clouds and the necessity of software as a service to be important. All the experts emphasized the need to use application development tools, cloud services, development, and deployment on the platform as a service, data analysis, and application programming interface related to the requirements of the platform layer. Scientifically speaking, they found that the components of secure information exchange, establishing secure protocol communication, key exchange mechanisms, and maintaining information confidentiality are crucial in the field of secure data exchange. Based on the findings, it can be concluded that the implementation of this technology can improve the quality of clinical services and increase the speed of responses and patient satisfaction. It is expected that by identifying the technological requirements of CIoT, a suitable model can be designed for healthcare.",
        "keywords": "Internet of Things; Cloud computing; Healthcare; Functional requirements; Quality requirements",
        "released": 2024,
        "link": "https://doi.org/10.1007/s11227-024-06253-z"
    },
    {
        "title": "AN ONLINE ADVISORY CONTROL-SYSTEM FOR THE LACTIC-ACID FERMENTATION PROCESS",
        "abstract": "A fuzzy expert system was developed for online diagnosing and controlling of bioprocesses. The system was constructed in object-oriented Smalltalk/V for diagnosing and controlling of bioprocesses. Lactic acid fermentation with an industrial strain of Lactobacillus casei was chosen as the model system. The performance of the fuzzy expert system and the knowledge base utilizing experts’ knowledge and several facts obtained from the experiments were successfully validated with on-line fermentations. The fuzzy expert system could diagnose a fault on-line and give reasonable advice to the process operator. In order to achieve the diagnosing faculty, a database, a knowledge base, and both backward and forward chaining procedures were implemented employing the object-oriented programming environment. A defuzzifier was implemented in the system to achieve on-line control. In order to realize a decision-making system with a human operator and a fuzzy expert system, a new control strategy named Advice was also introduced. Several cultivations were carried out in order to collect knowledge on the effects concerned with inoculum properties to the process and to construct a database including standard time-course profiles. The performance of the fuzzy expert control system was successfully tested with on-line experiments.",
        "keywords": "",
        "released": 1994,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:A1994PW48300004"
    },
    {
        "title": "An evaluation of cross-platform frameworks for multimedia mobile applications development",
        "abstract": "Cross-Platform Mobile Development Tools allows developers to write a unique base code and create the same application for multiple mobile platforms (e.g., Android and iOS). A variety of frameworks have been proposed to support cross-platform mobile application development. PhoneGap, Sencha Touch, and Titanium are well-known frameworks designed for this purpose. The main goal of this research is to analyze their current state of maturity. In that direction, an application was developed in PhoneGap, Titanium, and Sencha, as well as native versions for Android and iOS were implemented. Such mobile application obtain information from sensors, uses camera to take pictures, applies image filters, and establishes connection to a remote Web Service. We analyzed the feasibility to build this kind of mobile application, the performance of each version, and the development effort. With our results, we intend to provide an overview of the qualities and limitations of these frameworks.",
        "keywords": "Cross-Platform; Sensibilidade ao Contexto; iOS; Android; PhoneGap; Sencha Touch; Titanium",
        "released": 2018,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000433485100029"
    },
    {
        "title": "Migration from PLC to IEC 61499 using semantic web technologies",
        "abstract": "This paper proposes a new methodology of migration from IEC 61131-3 PLCs to IEC 61499 function blocks. The aim of this migration process is to recreate IEC 61131-3 applications in IEC 61499 implementations with equivalent execution behavior. The formal model of the IEC 61131-3 standard for migration and cyclical execution model is defined. This method also creates a foundation for correct-by-design development tools and automatic migration between the IEC 61131-3 and IEC 61499 standard. Formal migration rules based on ontology mappings, restoring execution model including tasks and programs scheduling and variables mapping with different access levels, are provided. A transformation engine for importing PLC code, mapping from PLC ontology model to function block model and code generation is implemented based on the ontological knowledge base and semantic query-enhanced web rule language. The migration approach is demonstrated on a simple airport baggage handling system.",
        "keywords": "Code generation and IEC 61131-3 formal execution model; execution semantics; IEC 61131-3 PLC; IEC 61499 function blocks; migration; ontology (OWL); ontology mapping",
        "released": 2014,
        "link": "https://doi.org/10.1109/TSMCC.2013.2264671"
    },
    {
        "title": "Web-based simulation tools for the allocation of cross-border transmission rights and implementation aspects in south-east europe",
        "abstract": "In South-East Europe (SEE), system operators (SO) started with the gradual implementation of market-based methods for the transmission capacity allocation. in this paper, the special web-based simulating software, developed for the daily explicit auction and allocation of tie-lines transmission rights on the borders of TSO-EPCG (Transmission System Operator of Montenegro), is presented as well as the monthly results achieved with its implementation. Details on the aforementioned simulator are given. After that, a small, coordinated flow-based internet simulator is created for the testing purposes, using PHP/MySQL and MATLAB as the development tools. The main advantages of the coordinated flow-based auction over the explicit NTC-based method are clearly underlined. Therefore, the proposed method can be a successful substitution for the currently applied market-based methods, and a step towards the open electricity market in SEE. (C) 2008 Elsevier B.V. All rights reserved.",
        "keywords": "congestion management; capacity allocation; explicit auction; coordinated auction; NTC; PTDF; flow-based; web simulator",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.epsr.2008.02.013"
    },
    {
        "title": "Characterization and management of noise in HDX-MS data modeling",
        "abstract": "Quantification of hydrogen deuterium exchange (HDX) kinetics can provide information on the stability of individual amino acids in proteins by finding the degree to which the local backbone environment corresponds to that of a random coil. When characterized by mass spectrometry, extraction of HDX kinetics is not possible because different residue exchange rates become merged depending on the peptides that are formed during proteolytic digestion. We have recently developed an advanced programming tool called HDXmodeller, which enables the exchange rates of individual amino acids to be understood by optimization of low-resolution HDX-mass spectrometry (MS) data. HDXmodeller is also uniquely able to appraise each optimization and quantify the accuracy of modeled exchange rates ab initio using a novel autovalidation method based on a covariance matrix. Here, we address the noise-handling capabilities of HDXmodeller and demonstrate the effectiveness of the algorithm on self-inconsistent datasets. Reference intervals for experimental HDX-MS data are also derived, and this information is presented in an updated online workflow for HDXmodeller, allowing users to evaluate the consistency of their data. The development of a modified version of HDXmodeller is also discussed with enhanced noise-handling capability brought about through loss function optimization. Changes in optimizer accuracy with different loss functions are also demonstrated along with the effectiveness of HDXmodeller to select the most effective optimizer for different data using currently embedded autovalidation criteria.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1021/acs.analchem.1c00894"
    },
    {
        "title": "Online shop for integrated software, hardware, and human services",
        "abstract": "Sustainable social and economic programs for aging citizens are needed to address the increase in life expectancy in developed countries. Ambient Assisted Living (AAL) services, also known as Active and Health Aging (AHA) services, is a European initiative that promotes a new era of integrated solutions that include hardware, software, and human services. One of the impediments to the adoption of these services is the lack of an economically viable marketplace. The electronic commerce (eCommerce) industry, in which buyers and sellers meet over electronic systems, can facilitate this emerging market of integrated services. We present the concept of an online store for integrated services, which enables users to connect the services available in the store to their development environment and to the infrastructure in which they are deployed, supporting the services full life cycle. This novel market-oriented approach, driven by technology and not by a specific vendor or tailored to a proprietary platform, is expected to radically improve and promote the adoption of these services. The general design approach and architecture of the online store and its capabilities are analyzed, and a specific implementation of a one-stop online shop for AAL services, named uStore, is described in depth.",
        "keywords": "",
        "released": 2014,
        "link": "https://doi.org/10.1147/JRD.2014.2352473"
    },
    {
        "title": "Polaris: An undergraduate online portfolio system that encourages personal reflection and career planning",
        "abstract": "Portfolios and other assessments of student achievement are proving to be important topics of concern in engineering education. While portfolios have a long history in other disciplines, their use in engineering is fairly new This paper provides a case study on the development and implementation of electronic portfolios in engineering education through our Polaris system built specifically for undergraduate engineering students. The end goal of Polaris is to provide students with a presentation of their academic accomplishments in a variety of multimedia formats on a professional looking website. While there are many web-development tools for creating a portfolio, the distinguishing characteristic of Polaris is that it specifically engages engineering students in developmental exercises to help them understand their budding professional skills. This case study provides background history and reveals issues that are germane to creating a developmentally appropriate resource to enhance engineering students’ scholastic experiences.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000233032100019"
    },
    {
        "title": "Enabling end-users in designing and executing of complex, collaborative robotic processes",
        "abstract": "Over the last years, capabilities of robotic systems have quantitatively and qualitatively improved. But going beyond isolated robotic systems, the integration and interoperability of robotic capabilities in complex work processes remains a major challenge. This lack of tools to integrate robots needs to be addressed on technical, semantic and organizational level. In the ROBxTASK research project, we developed an approach to support cooperation between different types of users in order to enable domain experts, with no robotic know-how, to work with robot-assisted workflows. By engineering robotic skills at a useful and usable level of abstraction for experts in different domains, we aim to increase re-usability of these skills on two different levels, (robotic) device level, and on level of application specific workflows. The researched prototype consists of a web platform, which allows (a) engineers to register (robotic) devices and the implemented skills of the devices, (b) domain experts to use a graphical task design environment to create workflows across multiple robotic devices and lastly (c) robot co-workers to download and execute the workflow code in a local environment with digital twins or real robots. Additionally skills and workflows can be shared across organisations. Initial user studies have shown that the visual programming environment is accessible and the defined skill-set is easy to understand even for domain experts that are inexperienced in the field of robotics.",
        "keywords": "visual programming environment; skill-based programming; digital twin; robotic devices; end-user programming; healthcare robotics",
        "released": 2023,
        "link": "https://doi.org/10.3390/asi6030056"
    },
    {
        "title": "Browser-based simulation for novice-friendly classroom robotics",
        "abstract": "Robots are a popular and engaging educational tool for teaching computational thinking, but they often have significant costs and limitations for classroom use. Switching to a simulated environment can eliminate many of these difficulties. By also providing students with a block-based programming environment, the barrier to entry can be further reduced. This paper presents a networked virtual robotics platform designed to create an environment which is highly accessible for novice students and their teachers alike, along with components of a curriculum designed to teach computational thinking skills through robotics programming challenges, including autonomous challenges and in-class competitions. Students access this platform through an extension of the same web interface used for programming their robots, which allows students to collaborate on code and view a shared simulated virtual space. Previously, this virtual robotics platform was used only to facilitate distance education. This paper demonstrates its use in an in-person class during the Spring 2022 semester, illustrating the affordances of a virtual robotics environment for face-to-face learning contexts as well. Students’ computational thinking skills were evaluated with assessments both before and after the class, along with surveys and interviews given to determine their opinions and outlooks regarding computer science. The results show that students had a significant improvement in both attitudes and aptitudes.",
        "keywords": "educational robotics; robotics simulation; computational thinking; STEM education; distance education",
        "released": 2023,
        "link": "https://doi.org/10.3389/fcomp.2022.1031572"
    },
    {
        "title": "Integration of computational thinking in k-12 mathematics education: A systematic review on CT-based mathematics instruction and student learning",
        "abstract": "There has been substantial research undertaken on the integration of computational thinking (CT) in K-12 mathematics education in recent years, particularly since 2018 when relevant systematic reviews were conducted on the topic. Many empirical studies in this area have yet to elaborate clearly and explicitly on how CT may support mathematics learning, or otherwise, in CT-based mathematics activities. Addressing this research gap, we conducted a systematic review on the integration of CT in K-12 mathematics education with a focus on CT-based mathematics instruction and students learning under such instruction. The Web of Science database was searched for in terms of studies published from 2006 to 2021, from which 24 articles were selected to provide illustrations of CT-based mathematics instruction and related student learning, and they were further analyzed according to education levels and contexts, programming tools, learning outcomes in CT and mathematics, and the mutual relationship between CT and mathematics learning. Among the results, this review found that geometrized programming and student-centered instructional approaches were facilitators of productive learning in CT and mathematics. Moreover, CT-based mathematics learning entails an interactive and cyclical process of reasoning mathematically and reasoning computationally, which can occur when: (1) applying mathematics to construct CT artefacts; (2) applying mathematics to anticipate and interpret CT outputs; and (3) generating new mathematical knowledge in parallel with the development of CT. The findings contribute to an in-depth understanding of what, and how, CT-based mathematics instruction impacts student learning in K-12 contexts.",
        "keywords": "Computational thinking; Mathematics education; K-12; Systematic review",
        "released": 2023,
        "link": "https://doi.org/10.1186/s40594-023-00396-w"
    },
    {
        "title": "Semantic node-RED for rapid development of interoperable industrial IoT applications",
        "abstract": "The evolution of IoT has revolutionized industrial automation. Industrial devices at every level such as field devices, control devices, enterprise level devices etc., are connected to the Internet, where they can be accessed easily. It has significantly changed the way applications are developed on the industrial automation systems. It led to the paradigm shift where novel IoT application development tools such as Node-RED can be used to develop complex industrial applications as IoT orchestrations. However, in the current state, these applications are bound strictly to devices from specific vendors and ecosystems. They cannot be re-used with devices from other vendors and platforms, since the applications are not semantically interoperable. For this purpose, it is desirable to use platform-independent, vendor-neutral application templates for common automation tasks. However, in the current state in Node-RED such reusable and interoperable application templates cannot be developed. The interoperability problem at the data level can be addressed in IoT, using Semantic Web (SW) technologies. However, for an industrial engineer or an IoT application developer, SW technologies are not very easy to use. In order to enable efficient use of SW technologies to create interoperable IoT applications, novel IoT tools are required. For this purpose, in this paper we propose a novel semantic extension to the widely used Node-RED tool by introducing semantic definitions such as iot.schema.org semantic models into Node-RED. The tool guides a non-expert in semantic technologies such as a device vendor, a machine builder to configure the semantics of a device consistently. Moreover, it also enables an engineer, IoT application developer to design and develop semantically interoperable IoT applications with minimal effort. Our approach accelerates the application development process by introducing novel semantic application templates called Recipes in Node-RED. Using Recipes, complex application development tasks such as skill matching between Recipes and existing things can be automated. We will present the approach to perform automated skill matching on the Cloud or on the Edge of an automation system. We performed quantitative and qualitative evaluation of our approach to test the feasibility and scalability of the approach in real world scenarios. The results of the evaluation are presented and discussed in the paper.",
        "keywords": "Industrial Internet of Things; Web of Things; Node-RED; iot.schema.org; semantic interoperability; edge computing; skill matching",
        "released": 2020,
        "link": "https://doi.org/10.3233/SW-200405"
    },
    {
        "title": "Developing a web-based tourism demand forecasting system",
        "abstract": "Tourism demand is the foundation on which all tourism-related business decisions ultimately rest and so accurate forecasts of tourism demand are crucial for tourism industry practitioners. From the functional point of view, a tourism demand forecasting system (TDFS) is a forecasting support system capable of providing quantitative tourism demand forecasts and allowing users to make their own “what-if” scenario forecasts. From the technical point of view, a TDFS is an information system consisting of a set of computer-based modules or components that support tourism demand forecasting and scenario analysis. This paper establishes a widely accessible Web-based TDFS which not only takes advantage of advanced econometric tourism demand forecasting techniques but also incorporates the real-time judgemental contribution of experts in the field. Furthermore, scenario forecasts are permitted within the system. Built on Web-based technology, the system provides advanced information sharing and communication and brings considerable convenience to various stakeholders engaged in tourism demand forecasting at different locations. In attempting to generate more accurate tourism demand forecasts, the system is designed to incorporate a two-stage forecasting methodology, which integrates judgemental adjustments with statistically based forecasts. The software architecture, detailed components and development environment of the Web-based TDFS are described in detail. A three-tiered client-server architecture is employed, which offers great flexibility, reusability and reliability. The prototype system has been developed and screen shots of interaction with the system are presented using Hong Kong tourism as an example.",
        "keywords": "tourism demand forecasts; econometric models; forecasting systems; judgemental forecasts; Web/Internet; Hong Kong",
        "released": 2008,
        "link": "https://doi.org/10.5367/000000008785633578"
    },
    {
        "title": "iMPACT3: Internet-based development and administration of utility elicitation protocols",
        "abstract": "iMPACT3 (Internet Multimedia Preference Assessment Instrument Construction Tool, version 3) is a software development environment that helps researchers build Internet-capable multimedia utility elicitation software programs. The program is a free, openly accessible Web site (http://preferences.ucsd.edu/impact3/asp). To develop a utility elicitation software program using iMPACT3, a researcher selects modular protocol components from a library and custom tailors the components to the details of his or her research protocol. iMPACT3 builds a Web site implementing the protocol and downloads it to the researcher’s computer. In a study of 75 HIV-infected patients, an iMPACT3-generated protocol showed substantial evidence of construct validity and good internal consistency (logic error rates of 4% to 10% and procedural invariance error rates of 10% to 26%, depending on the elicitation method) but only fair 3- to 6-week test-re-test reliability(intraclass correlation coefficient = 0.42 to 0.55). Further work may be needed on specific utility assessment procedures, but this study’s results confirm iMPACT3’s feasibility in facilitating the collection of health state utility data. Key words: computers; quality of life; cost-effectiveness analysis; decision support systems; utilities.",
        "keywords": "computers; quality of life; cost-effectiveness analysis; decision support systems; utilities",
        "released": 2002,
        "link": "https://doi.org/10.1177/0272989X02238296"
    },
    {
        "title": "Design for six sigma integrated product development reference model through systematic review",
        "abstract": "Purpose This paper aims to present a systematic review of design for six sigma (DFSS) methods applicable to the product development process (PDP) of durables goods and identify a research opportunity on the subject proposing integration of DFSS and a reference model for the PDP. In this way, through the analysis of the theoretical references identified in the scientific databases, it was possible to propose a conceptual model for the PDP oriented to the DFSS. Design/methodology/approach This paper is based on the theoretical framework presented in peer-reviewed scientific research papers during the period 2000 to 2018 on the theme DFSS applied in the PDP, as well as such as the product development tools/techniques and statistics addressed. By means of key words defined by the acronyms of DFSS methods (DMADOV, ICOV, DMEDI, IDOV, DDOV, PIDOV, DMADIC, DCCDI, DMADV, IDDOV, CDOV and DCOV), DFSS and the acronym DFSS. Applying Boolean expression during the conduction of the searches through the scientific evidence at the Brazilian scientific database platform (Capes database). This database platform is maintained by coordination for the improvement of higher education personnel, which including Emerald Insight (Emerald), Scopus (Elsevier), Science Direct, SpringerLink, Taylor Francis, Scielo (Web of Science), Wiley Online Library, Web of Science (Clarivate Analytics), etc. It was obtained, by means of the searches, 269 papers related to subject DFSS, of which 18 papers had been critically selected for the composition of a conceptual model for the process of development of product guided to the DFSS. Findings This study presents a review of the literature (systematic review and content analysis) on DFSS and its effectiveness for the PDP. The DFSS methodology is disseminated in the scientific literature through a variety of methods that are often mistaken for the six sigma methodology - DMAIC, which is directed toward process improvement. The PDP integrated with the DFSS concepts contributes to eliminating possible failures during the design of a new product, directing to reduce costs and improve the quality of the product and process. Originality/value Through the systematic review and content analysis, it was possible to observe that the DFSS methods applied to product development are not related to the PDP reference models available in the literature. In this way, the fusion of the concepts of the DFSS methods and PDP reference models for the construction and proposition of a preliminary conceptual model DFSS oriented to the process of product development intends to contribute in the development of new products with the reduction of time, reduction of the cost, competitive price and consumer satisfaction.",
        "keywords": "Design for six sigma; PDP; Reference models for product development; Design for six sigma methods; Product development techniques; Product development and manufacturing process; DFSS",
        "released": 2020,
        "link": "https://doi.org/10.1108/IJLSS-05-2019-0052"
    },
    {
        "title": "Increased TIMP/MMP ratio in varicose veins: A possible explanation for extracellular matrix accumulation",
        "abstract": "Primary varicose veins are functionally characterized by venous back-how and blood stagnation in the upright position, Dilatation and tortuosity pro,ide evidence for progressive venous wall remodelling, with disturbance of smooth muscle cell/extracellular matrix organization. Affected areas are not uniformly distributed, some areas being hypertrophic, whereas others are atrophic or unaffected. In 12 varicose veins and ten control veins, the proteolytic enzyme/inhibitor balance which may participate in the remodelling of the venous wall was investigated. For this purpose, the presence and enzymatic activity of matrix metalloproteinases (MMP-2,MMP-9), tissue inhibitors of MMPs (TIMP-1, TIMP-2), urokinase-type (uPA) and tissue-type (tPA) plasminogen activators (PAs), and plasminogen activator inhibitor-1 (PAI-1) were quantified by western blot and gelatin or plasminogen-casein zymography. In addition, MMP-2, TIMP-1, TIMP-2, and PAI-l levels were measured by ELISA. A high TIMP-1 level and a low MMP-2 level/activity were found in varicose veins (p < 0.005), resulting in a three-fold increase in the TIMP-1/MMP-2 ratio in varicose versus control veins. Levels of PAs (uPA and tPA) as web as PAI-L were both lower in varicose veins (p<0.005), with minimal change in the PAI/PA ratio. These results These results demonstrate that varicose veins are characterized by a higher than normal TIMP/MMP ratio, which may facilitate extracellular matrix accumulation in the diseased venous wall. Copyright (C) 2000 John Wiley & Sons, Ltd.",
        "keywords": "MMP; TIMP; human; varicose veins",
        "released": 2000,
        "link": "https://doi.org/10.1002/1096-9896(2000)9999:9999<::AID-PATH670>3.0.CO;2-1"
    },
    {
        "title": "Secure data computation using deep learning and homomorphic encryption: A survey",
        "abstract": "Deep learning and its variant techniques have surpassed classical machine algorithms due to their high performance gaining remarkable results and are used in a broad range of applications. However, adopting deep learning models over the cloud introduces privacy and security issues for data owners and model owners, including computational inefficiency, expansion in ciphertext, error accumulation, security and usability trade-offs, and deep learning model attacks. With homomorphic encryption, computations on encrypted data can be performed without disclosing its content. This research examines the basic concepts of homomorphic encryption limitations, benefits, weaknesses, possible applications, and development tools concentrating on neural networks. Additionally, we looked at systems that integrate neural networks with homomorphic encryption in order to maintain privacy. Furthermore, we classify modifications made on neural network models and architectures that make them computable via homomorphic encryption and the effect of these changes on performance. This paper introduces a thorough review focusing on the privacy of homomorphic cryptosystems targeting neural network models and identifies existing solutions, analyzes potential weaknesses, and makes recommendations for further research.",
        "keywords": "homomorphic encryption deep learning privacy-preserving convolutional neural networks; privacy-preserving deep learning bootstrapping",
        "released": 2023,
        "link": "https://doi.org/10.3991/ijoe.v19i11.40267"
    },
    {
        "title": "Design, validation and initial testing of the electronic lung™ device",
        "abstract": "The Electronic Lung(TM) Device (ELD) is an inhalation simulator designed to provide an in vitro assessment of dry powder inhalers (DPIs), and is used as a research and development tool. The ELD “inhales” through an inhalation device, following a preprogrammed theoretical or patient inhalation profile, and the resultant aerosol cloud is analysed for dose and particle size. This paper describes the design and validation of the ELD. The validation work shows that respirable particle losses are low in the ELD and the particle size and dose results are comparable with other in vitro testing methods for DPIs. The performance of Serevent Diskus inhaler is evaluated using computer-generated inhalation profiles. Comparison with results using a cascade impactor alone, show that the ELD itself does not affect the aerosol quality, either in terms of emitted dose or fine particle dose and consistent dosing performance is achieved over a range of inhalation parameters. (C) 1998 Elsevier Science Ltd. All rights reserved.",
        "keywords": "",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0021-8502(97)10039-8"
    },
    {
        "title": "Characterizing visual programming approaches for end-user developers: A systematic review",
        "abstract": "Recently many researches have explored the potential of visual programming in robotics, the Internet of Things (IoT), and education. However, there is a lack of studies that analyze the recent evidence-based visual programming approaches that are applied in several domains. This study presents a systematic review to understand, compare, and reflect on recent visual programming approaches using twelve dimensions: visual programming classification, interaction style, target users, domain, platform, empirical evaluation type, test participants’ type, number of test participants, test participants’ programming skills, evaluation methods, evaluation measures, and accessibility of visual programming tools. The results show that most of the selected articles discussed tools that target IoT and education, while other fields such as data science, robotics are emerging. Further, most tools use abstractions to hide implementation details and use similar interaction styles. The predominant platforms for the tools are web and mobile, while desktop-based tools are on the decline. Only a few tools were evaluated with a formal experiment, whilst the remaining ones were evaluated with evaluation studies or informal feedback. Most tools were evaluated with students with little to no programming skills. There is a lack of emphasis on usability principles in the design stage of the tools. Additionally, only one of the tools was evaluated for expressiveness. Other areas for exploration include supporting end users throughout the life cycle of applications created with the tools, studying the impact of tutorials on improving learnability, and exploring the potential of machine learning to improve debugging solutions developed with visual programming.",
        "keywords": "Visual programming; end-user development; human-computer interaction; systematic literature review",
        "released": 2021,
        "link": "https://doi.org/10.1109/ACCESS.2021.3051043"
    },
    {
        "title": "Evaluating distributed IoT databases for edge/cloud platforms using the analytic hierarchy process",
        "abstract": "Decision-making is not a trivial process. It involves studying and analyzing different alternatives. In addition, it requires defining criteria for evaluating the alternatives. A problem arises when evaluating criteria that conflict or when dealing with qualitative criteria. The analytic hierarchy process (AHP) is a multi-criteria decision tool that simplifies the decision-making process. It can evaluate both qualitative and quantitative criteria. Moreover, AHP justifies the final decision by providing the mathematical reasoning behind the judgment. The aim of this research is to evaluate available Internet of Things (IoT) databases in an edge/cloud platform by applying AHP and to suggest a suitable approach for developing a database application. In this study, four alternative database development tools are evaluated: DaDaBIK, DataFlex, Oracle Application Express, and FileMaker. We define our criteria, explain why they are selected, and assign each a weight based on its importance. We then evaluate the candidates using the weighted criteria. FileMaker is found to be the best choice because it offers the best usability, portability, and supportability for IoT scenarios. (C) 2018 Elsevier Inc. All rights reserved.",
        "keywords": "Distributed IoT database; Edge/cloud; Evaluation criteria; Analytical hierarchy process",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.jpdc.2018.10.008"
    },
    {
        "title": "Big data-driven cognitive computing system for optimization of social media analytics",
        "abstract": "The integration of big data analytics and cognitive computing results in a new model that can provide the utilization of the most complicated advances in industry and its relevant decision-making processes as well as resolving failures faced during big data analytics. In E-projects portfolio selection (EPPS) problem, big data-driven decision-making has a great importance in web development environments. EPPS problem deals with choosing a set of the best investment projects on social media such that maximum return with minimum risk is achieved. To optimize the EPPS problem on social media, this study aims to develop a hybrid fuzzy multi-objective optimization algorithm, named as NSGA-III-MOIWO encompassing the non-dominated sorting genetic algorithm III (NSGA-III) and multi-objective invasive weed optimization (MOIWO) algorithms. The objectives are to simultaneously minimize variance, skewness and kurtosis as the risk measures and maximize the total expected return. To evaluate the performance of the proposed hybrid algorithm, the data derived from 125 active E-projects in an Iranian web development company are analyzed and employed over the period 2014-2018. Finally, the obtained experimental results provide the optimal policy based on the main limitations of the system and it is demonstrated that the NSGA-III-MOIWO outperforms the NSGA-III and MOIWO in finding efficient investment boundaries in EPPS problems. Finally, an efficient statistical-comparative analysis is performed to test the performance of NSGA-III-MOIWO against some well-known multi-objective algorithms.",
        "keywords": "Big data-driven cognitive computing system; social media; E-projects portfolio selection problem; fuzzy system",
        "released": 2020,
        "link": "https://doi.org/10.1109/ACCESS.2020.2991394"
    },
    {
        "title": "Developing an artificial intelligence framework for online destination image photos identification",
        "abstract": "With the development of advanced technologies in computer science, such as deep learning and transfer learning, the tourism field is facing a more intelligent and automated future development environment. In this study, an artificial intelligence (AI) framework is developed to identify tourism photos without human interaction. Adopting online destination photos of Australia as a data source, the results show that the model combining a deep convolutional neural network and mixed transfer learning achieved the best image identification performance. This study identified 25 image classification categories covering all the tourism scenes to serve as a foundation for future tourism computer vision research. The results indicate that the AI photo identification framework is of great benefit for the understanding of projected destination images and enhancing tourism experiences. This study contributes to the existing literature by introducing an intelligent automation framework to big data research in the tourism field, as well as by advancing innovative methodologies of online destination image analysis. Practically, the proposed framework contributes to the marketing and management of smart destinations by offering a state-of-the-art data mining method.",
        "keywords": "Artificial intelligence; Destination management; Photo identification; Deep learning; Smart tourism",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.jdmm.2020.100512"
    },
    {
        "title": "Web-based technologies for teaching and using molecular simulation",
        "abstract": "Computing power and molecular simulation methodology have improved dramatically over the past decade, and for some systems molecular models are reaching a point where they can provide information that approaches or surpasses the quality of real experimental data. Molecular simulation can also be a potent tool for developing qualitative understanding, and this feature suggests widespread use of molecular simulation as a tool for teaching. Broader use of molecular simulation for these purposes requires tools that promote understanding and application of simulation software. We present an “object-oriented” view of molecular simulation that can help conceptualize and organize its elements, and discuss software development tools that we have constructed that enable non-experts in simulation or graphical programming to create instructive molecular simulations. We finish with an example of one such simulation applet, which we have used in the classroom. Tools described here are available on the web at www.ccr.buffalo.edu/etomica. (C) 2002 Elsevier Science B.V. All rights reserved.",
        "keywords": "molecular simulation; application programming interface; Java; education; software",
        "released": 2002,
        "link": "https://doi.org/10.1016/S0378-3812(01)00685-9"
    },
    {
        "title": "Software product line modeling proposal for mashup applications",
        "abstract": "A Mashup is a composite application that integrates two or more types of components available on the Web, creating a new value from the components or artifacts that compose it, allowing their reuse and providing functionality that did not exist before. The current tools and approaches to develope these applications do not have any model to integrate similar components. Software Product Lines (SPL) is a software development approach whose main objective is reusability, allowing the creation of a family of products where each product has common characteristics, and differs from another in a set of functionalities. This project work proposed to model a Mashup application from a variability approach, which will later allow to implement a SPL. For this, a development tool based on Pipes is referenced, and it is specified from a characteristic model, finally various examples are generated.",
        "keywords": "Mashup; Software Product Line; Feature Oriented Software Development; Features Model; Pipes",
        "released": 2021,
        "link": "https://doi.org/10.22305/ict-unpa.v13.n2.814"
    },
    {
        "title": "Graphics support for a world-wide-web based architectural design service",
        "abstract": "This paper describes the design and implementation of a Web-based graphical editor and visualization tool for 3D architectural forms. This Design Development Tool enables end-users to Select, Customize and Visualize house designs drawn from a large library. Having used this tool to select a base house design, the user can customize their selected design using a constraint-based graphical editor, and then view and walk through a 3D model of the resulting house. The work described is part of the larger LaHave House Project which explores the creation of an automated architectural design service based on an industrial design approach to architecture in which Architects design families of similarly structured objects, rather than individual ones. The houses in the library have been generated in terms of a modular kit of over 1400 3D parts. (C) 1997 Elsevier Science B.V.",
        "keywords": "automated architectural design service; knowledge-based computer aided architectural design; VRML; Java",
        "released": 1997,
        "link": "https://doi.org/10.1016/S0169-7552(97)00076-7"
    },
    {
        "title": "Diverse mentoring connections across institutional boundaries in the biomedical sciences: Innovative graph database analysis",
        "abstract": "Background: With an overarching goal of increasing diversity and inclusion in biomedical sciences, the National Research Mentoring Network (NRMN) developed a web -based national mentoring platform (MyNRMN) that seeks to connect mentors and mentees to support the persistence of underrepresented minorities in the biomedical sciences. As of May 15, 2024, the MyNRMN platform, which provides mentoring, networking, and professional development tools, has facilitated more than 12,100 unique mentoring connections between faculty, students, and researchers in the biomedical domain. Objective: This study aimed to examine the large-scale mentoring connections facilitated by our web -based platform between students (mentees) and faculty (mentors) across institutional and geographic boundaries. Using an innovative graph database, we analyzed diverse mentoring connections between mentors and mentees across demographic characteristics in the biomedical sciences. Methods: Through the MyNRMN platform, we observed profile data and analyzed mentoring connections made between students and faculty across institutional boundaries by race, ethnicity, gender, institution type, and educational attainment between July 1, 2016, and May 31, 2021. Results: In total, there were 15,024 connections with 2222 mentees and 1652 mentors across 1625 institutions contributing data. Female mentees participated in the highest number of connections (3996/6108, 65%), whereas female mentors participated in 58% (5206/8916) of the connections. Black mentees made up 38% (2297/6108) of the connections, whereas White mentors participated in 56% (5036/8916) of the connections. Mentees were predominately from institutions classified as Research 1 (R1; doctoral universities-very high research activity) and historically Black colleges and universities (556/2222, 25% and 307/2222, 14%, respectively), whereas 31% (504/1652) of mentors were from R1 institutions. Conclusions: To date, the utility of mentoring connections across institutions throughout the United States and how mentors and mentees are connected is unknown. This study examined these connections and the diversity of these connections using an extensive web -based mentoring network.",
        "keywords": "online platform; mentorship; diversity; network analysis; graph database; online communities",
        "released": 2024,
        "link": "https://doi.org/10.2196/47560"
    },
    {
        "title": "Endoscopic hemostatic devices",
        "abstract": "The American Society for Gastrointestinal Endoscopy (ASGE) Technology Committee prot,ides reviews of existing. new, or emerging endoscopic technologies, that have an impact on the practice of GI endoscopy. Evidence-based methodology is used, with a MEDLINE literature search to identify pertinent clinical studies On The topic and a MAUDE (US Food and Drug Administration Center for Devices and Radiological Health) database search to identify the reported complications of a given technology. Both are supplemented by accessing the “related articles” feature of PubMed and by scrutinizing, pertinent references cited by the identified studies. Controlled clinical trials are emphasized, but, in many cases, data from randomized, controlled trials are lacking. In such cases, large case series, preliminary clinical studies, and expert opinions are used. Technical data are traditional and Web-based publications, proprietary publications, and informed communications with pertinent vendors. Technology Status Evaluation Reports are drafted by 1 or 2 members of the ASGE Technology Committee, reviewed and edited by the by the committee as a whole, and approved by The Governing Board of the ASGE. When financial guidance is indicated, the most recent coding data and list prices at the time of publication are provided. For this review, The MEDLINE database was searched through September 2008 for articles related to endoscopic hemostatic devices by using the keywords “multipolar electrocautery,” “bipolar electrocautery,” “heater probe, “ “hemostatic grasper,” “argon plasms coagulator,” “injection needle,” “endoloop,” “clip,” paired with “complication.” “perforation,” “peptic ulcer disease. “ “gastric antral vascular ectasia, “ “Dieulafoy lesion,” “Mallory-Weiss tear,” “radiation induced angioectasias,” “diverticular bleeding,” “angiodysplasia,” and “postpolypectomy bleeding.” Technology Status Evaluation, Reports are scientific reviews provided solely for educational and informational purposes. Technology Status Evaluation, Reports are not rules and should not be construed as establishing a legal standard of care or as encouraging, advocating requiring or discouraging any particular treatment or payment for such treatment.",
        "keywords": "",
        "released": 2009,
        "link": "https://doi.org/10.1016/j.gie.2008.12.251"
    },
    {
        "title": "Automatic property-based testing and path validation of XQuery programs",
        "abstract": "Property-based testing has gained popularity in recent years in many areas of software development. The specification of assertions/properties helps to understand the semantics of pieces of code, and in modern programming environments, it can serve to test the program behavior. In this paper an XQuery property-based testing tool is presented, which enables to automatically test XQuery programs. The tool is able to systematically generate XML instances (i.e., test cases) from a given XML schema, and to filter XML instances with input properties specified by the programmer. Additionally, the tool automatically checks output (respectively, input-output) properties in each output instance (respectively, each pair of input-output instances). The tool is able to report whether the XQuery program passes the test, that is, if all the test cases satisfy the (input-)output property, as well as the number of test cases used for testing. In addition, if the XQuery program fails the test, the tool shows counterexamples found in the test cases. Properties are specified with XQuery Boolean functions, and the testing tool has been implemented in XQuery. Additionally, an XQuery path validation tool is presented. This tool is able to detect wrong paths in XQuery expressions. The path validation tool takes as input an XML schema, and it reports those paths on the XQuery program that do not match the XML schema. The path validation tool is a complement to the testing tool rejecting XQuery programs that do not conform to the XML schema. The path validation tool has been also implemented in XQuery. Finally, a web tool has been developed enabling to test and validate XQuery programs.",
        "keywords": "testing; XML; XQuery",
        "released": 2017,
        "link": "https://doi.org/10.1002/stvr.1625"
    },
    {
        "title": "AUTOMATIC DETECTION OF VIDEOS’ SCENES WITH AGGRESSION UTILIZING MOVIES’ TRANSCRIPTS BY USING TEXT MINING TECHNIQUES",
        "abstract": "The world is witnessing revolutionary evolution of internet and with the advent of social media; users are empowered to easily post contents on the web at any time and from any place in the form of opinions, comments, and feelings. Manual approaches of detecting and analyzing such huge amount of posts are not feasible and there is a need for automated methods and techniques to discover the knowledge and patterns of the text content without human involvement. Text mining refers to the process of extracting interesting and significant patterns or knowledge from text documents. YouTube is known for its free provision of video sharing service. The content of YouTube videos may sometimes comprise of images or sequence(s) of images with unwanted material, such as aggression, which is the reason of emergence of many social problems, particularly among children such as demonstration of aggressive behavior and bullying at home, school and public places. The research work reports performance of machine learning classifiers that were applied on video transcripts of YouTube videos to detect aggression. The dataset constructed for the purpose of research work, consists of English video scenes transcripts that were collected from the web and were annotated manually as violent and non-violent. Various experiments were performed on the dataset using different machine learning (ML) classifiers with different text preprocessing settings in RapidMiner and Python environments and thus predictive classifier models were constructed and tested. In RapidMiner environment, the SVM classifier model outperformed the other classifiers achieving highest accuracy of 79% after preprocessing step of removal of stop words. In Python programming environment, NB classifier outperformed the other classifiers in majority of experiments with different preprocessing settings, achieving highest accuracy of 82.5%, when stemming was performed in preprocessing stage along with other preprocessing steps. The automatic process of aggression detection in video scenes can be used by concerned authorities to enforce their cultural priorities.",
        "keywords": "Video transcript; Aggression detection; Machine learning; Vector Space Model; Term Frequency- Inverse Document Frequency; Natural Language Toolkit; Decision Tree; Naive Bayes; K-Nearest Neighbor; Support Vector Machine; Weka - RIpple-DOwn Rule learner",
        "released": 2020,
        "link": "https://doi.org/10.22937/IJCSNS.2020.20.09.24"
    },
    {
        "title": "Battery-free MakeCode: Accessible programming for intermittent computing",
        "abstract": "Hands-on computing has emerged as an exciting and accessible way to learn about computing and engineering in the physical world for students and makers of all ages. Current end-to-end approaches like Microsoft MakeCode require tethered or battery-powered devices like a micro:bit, limiting usefulness and applicability, as well as abdicating responsibility for teaching sustainable practices. Unfortunately, energy harvesting computing devices are usually only programmable by experts and require significant supporting toolchains and knowledge across multiple engineering and computing disciplines to work effectively. This paper bridges the gap between sustainable computing efforts, the maker movement, and novice-focused programming environments with MakeCode-Iceberg, a set of compiler extensions to Microsoft’s open-source MakeCode project. The extensions automatically and invisibly transform user code in any language supported (Blocks, JavaScript, Python)into a version that can safely and correctly execute across intermittent power failures caused by unreliable energy harvesting. Determining where, when, and what to save in a checkpoint on limited energy, time, and hardware budget is challenging. We leverage the unique intermediate representation of the MakeCode source-to-source compiler to design and deploy various checkpointing techniques. Our approach allows us to provide, for the first time, a fully web-based and toolchain-free environment to program intermittent computing devices, making battery-free operation accessible to all. We demonstrate new use cases with multiple energy harvesters, peripherals, and application domains: including a Smart Terrarium, Step Counter, and Combination Lock. MakeCode-Iceberg provides sustainable hands-on computing opportunities to a broad audience of makers and learners, democratizing access to energy harvesting and battery-free embedded systems.",
        "keywords": "Energy Harvesting; Intermittent Computing; Battery-free; Block based programming",
        "released": 2022,
        "link": "https://doi.org/10.1145/3517236"
    },
    {
        "title": "Online identification of nonlinear systems using neo-fuzzy supported brain emotional learning network",
        "abstract": "This paper proposes an online algorithm for identifying the nonlinear dynamical systems and is termed as neo-fuzzy based brain emotional learning plant identifier (NFBELPI). As the name suggests, the proposed identifier is a combination of brain emotional learning network and neo-fuzzy neurons. The integration of these two networks is realized in a way that retains the characteristics of both the networks while an enhanced performance is achieved at the same time. Precisely, the orbitofrontal cortex section of the brain emotional learning network is fused with neo-fuzzy neurons with a view to equip it with more knowledge than does the amygdala section possesses. The proposed identifier accepts n-input and m-output samples to generate an estimate of the plant output and employs a brain emotional learning algorithm to lower the estimation error by adjusting a total of ((n + m + 1) x p) + ( n + m + 2) weights, with p being the number of neo-fuzzy neurons. The proposal is validated in aMATLAB programming environment using a simulated Narendra dynamical plant as well as against the data recorded from real forced duffing oscillator. Comparison with a brain emotional learning plant identifier (BELPI) and some other state-of-the art identifiers in terms of root mean squared error (RMSE) criterion reveals the improved performance of the proposed identifier.",
        "keywords": "System identification; brain emotional learning; neo-fuzzy neurons; MATLAB",
        "released": 2020,
        "link": "https://doi.org/10.3233/JIFS-179689"
    },
    {
        "title": "Controlled manipulation of molecular samples with the nanoManipulator",
        "abstract": "Making further advances in such diverse problems as building more powerful computers, measuring material properties of biological samples, or exploring fundamental physical laws on the atomic level requires gaining access to the nanoworld. The nanoManipulator system adds a virtual-reality interface to an atomic-force microscope (AFM), thus providing a tool that can be used by scientists to image and manipulate nanometer-sized molecular structures in a controlled manner. As the AFM tip scans the sample, the tip-sample interaction forces are monitored, which. in turn, can yield information about the frictional, mechanical, material, and topological properties of the sample. Computer graphics are used to reconstruct the surface for the user, with color or contours overlaid to indicate additional data sets. Moreover, a force feedback stylus, which is connected to the tip via software, allows the user to directly interact with the macromolecules. This system is being used to investigate carbon nanotubes (CNT’s), deoxyribonucleic acid (DNA), fibrin, adeno-and tobacco mosaic virus. Nanotubes have been bent, translated, and rotated to understand their mechanical properties and to investigate friction on the molecular level. Using AFM lithography in combination with the nanoManipulator, the electromechanical properties of CNT’s ale being investigated. The rupture forces of DNA and fibrin fibers have been measured and the elastic moduli of viruses are being studied. It is now also possible to insert this system into a scanning electron microscope which pro,ides the user with continuous images of the sample, even while the AFM tip is being used for manipulations. Investigators are invited to apply to use the system as described on the web at http://www.cs.unc.edu/Research/nano/doc/biovisit.html.",
        "keywords": "adenovirus; atomic-force microscope; carbon nanotubes; deoxyribonucleic acid; fibrin; force; nanoManipulator; scanning electron microscope; scanning-force microscope; tobacco mosaic virus",
        "released": 2000,
        "link": "https://doi.org/10.1109/3516.847092"
    },
    {
        "title": "An effective evaluation of wavelength scheduling for various WDM-PON network designs with traffic protection provision",
        "abstract": "Recently, metropolitan and access communication networks have markedly developed by utilizing a variety of technologies. Their bearer communication infrastructures will be mostly exploiting the optical transmission medium where wavelength division multiplexing techniques will play an important role. This contribution discusses the symmetric sharing of common optical network resources in wavelength and time domains. Wavelength-Division Multiplexed Passive Optical Networks (WDM-PON) attract considerable attention regarding the next generation of optical metropolitan and access networks. The main purpose of this contribution is presented by the analysis of possible scheduling of wavelengths for our novel hybrid network topologies considered for WDM-PON networks. This contribution briefly deploys adequate Dynamic Wavelength Allocation (DWA) algorithms for selected WDM-PON network designs with the provision of traffic protection when only passive optical components in remote nodes are utilized. The main part of this study is focused on the use of wavelength scheduling methods for selected WDM-PON network designs. For evaluation of offline and online wavelength scheduling for novel hybrid network topologies, a simulation model realized in the Matlab programming environment allows to analyze interactions between various metropolitan and access parts in the Optical Distribution Network (ODN) related to advanced WDM-PON network designs. Finally, wavelength scheduling methods are compared from a viewpoint of utilization in advanced WDM-PON networks designs.",
        "keywords": "optical fiber communication; WDM-PON network design; wavelength channel allocation; scheduling algorithms; traffic protection",
        "released": 2021,
        "link": "https://doi.org/10.3390/sym13081540"
    },
    {
        "title": "Bridging open source tools and geoportals for interactive spatial data analytics",
        "abstract": "Geoportals have been the primary source of spatial information to researchers in diverse fields. Recent years have seen a growing trend to integrate spatial analysis and geovisual analytics inside Geoportals. Researchers could use the Geoportal to conduct basic analysis without offline processing. In practice, domain-specific analysis often requires researchers to integrate heterogeneous data sources, leverage new statistical models, or build their own customized models. These tasks are increasingly being tackled with open source tools in programming languages such as Python or R. However, it is unrealistic to incorporate the numerous open source tools in a Geoportal platform for data processing and analysis. This work provides an exploratory effort to bridge Geoportals and open source tools through Python scripting. The Geoportal demonstrated in this work is the Urban and Regional Explorer for China studies. A python package is provided to manipulate this platform in the local programming environment. The server side of the Geoportal implements a set of service endpoints that allows the package to upload, transform, and process user data and seamlessly integrate them into the existing datasets. A case study is provided that illustrated the use of this package to conduct integrated analyses of search engine data and baseline census data. This work attempts a new direction in Geoportal development, which could further promote the transformation of Geoportals into online analytical workbenches.",
        "keywords": "Geoportal; open source; Python; web GIS; spatial analysis",
        "released": 2019,
        "link": "https://doi.org/10.1080/10095020.2019.1645497"
    },
    {
        "title": "Smart shopper: An agent-based web-mining approach to internet shopping",
        "abstract": "With the rapid development of e-commerce, there is an immediate need for an autonomous and robust system that can offer decision-making support for customers who are looking for the cheapest, the most familiar, or the best-quality product. This paper presents an agent-based web-mining approach to Internet shopping. In contrast to the traditional multiagent systems, which use rule-based or case-based process flows to coordinate communications for system automation, we propose a fuzzy neural network to tackle the uncertainties in practical shopping activities, such as consumer preferences, product specification, product selection, price negotiation, purchase, delivery, after-sales service and evaluation. The fuzzy neural network provides an automatic and autonomous product classification and selection scheme to support fuzzy decision making by integrating fuzzy logic technology and the back propagation feed forward neural network. In addition, a new visual data model is introduced to overcome the limitations of the current web browsers that lack flexibility for customers to view products from different perspectives. Such a model also extends the conventional data warehouse schema to deal with intensive data volumes and complex transformations with a high degree of flexibility for multiperspective visualization and morphing capability in an interactive environment. Furthermore, an agent development tool named “Aglet” is used as a programming framework for system implementation. The integration of dynamic object visualization, interactive user interface and data mining decision support provides an effective technique to close the gap between the “real world” and the “cyber world” from a business perspective. The experimental results demonstrate the feasibility of the proposed approach for web-based business transactions.",
        "keywords": "feature selection; fuzzy neural network; Internet shopping; multiagent; web-mining",
        "released": 2003,
        "link": "https://doi.org/10.1109/TFUZZ.2003.809900"
    },
    {
        "title": "Fusion: A system for business users to manage program variability",
        "abstract": "In order to make software components more flexible and reusable, it is desirable to provide business users with facilities to assemble and control them without their needing programming knowledge. This paper describes a fully functional prototype middleware system where variability is externalized so that core applications need not be altered for anticipated changes. In this system, application behavior modification is fast and easy, making this middleware suitable for frequently changing programs.",
        "keywords": "Web site management/development tools; middleware/business logic; specialized application languages; domain-specific architectures; human factors in software design; user interfaces",
        "released": 2005,
        "link": "https://doi.org/10.1109/TSE.2005.82"
    },
    {
        "title": "Subacute pulmonary toxicity of glutaraldehyde aerosols in a human in vitro airway tissue model",
        "abstract": "Glutaraldehyde (GA) has been cleared by the Center for Devices and Radiological Health (CDRH) of the Food and Drug Administration (FDA) as a high-level disinfectant for disinfecting heat-sensitive medical equipment in hospitals and healthcare facilities. Inhalation exposure to GA is known to cause respiratory irritation and sensitization in animals and humans. To reproduce some of the known in vivo effects elicited by GA, we used a liquid aerosol exposure system and evaluated the tissue responses in a human in vitro airway epithelial tissue model. The cultures were treated at the air interface with various concentrations of GA aerosols on five consecutive days and changes in tissue function and structure were evaluated at select timepoints during the treatment phase and after a 7-day recovery period. Exposure to GA aerosols caused oxidative stress, inhibition of ciliary beating frequency, aberrant mucin production, and disturbance of cytokine and matrix metalloproteinase secretion, as well as morphological transformation. Some effects, such as those on goblet cells and ciliated cells, persisted following the 7-day recovery period. Of note, the functional and structural disturbances observed in GA-treated cultures resemble those found in ortho-phthaldehyde (OPA)-treated cultures. Furthermore, our in vitro findings on GA toxicity partially and qualitatively mimicked those reported in the animal and human survey studies. Taken together, observations from this study demonstrate that the human air-liquid-interface (ALI) airway tissue model, integrated with an in vitro exposure system that simulates human inhalation exposure, could be used for in vitro-based human hazard identification and the risk characterization of aerosolized chemicals.",
        "keywords": "air-liquid-interface (ALI) airway model; glutaraldehyde (GA); airway toxicity; cloud liquid aerosol generation and exposure system; medical device development tools",
        "released": 2022,
        "link": "https://doi.org/10.3390/ijms232012118"
    },
    {
        "title": "Evaluating the effects of microphysical complexity in idealised simulations of trade wind cumulus using the factorial method",
        "abstract": "The effect of microphysical and environmental factors on the development of precipitation in warm idealised cloud is explored using a kinematic modelling framework. A simple one-dimensional column model is used to drive a suite of microphysics schemes including a flexible multimoment bulk scheme (including both single and dual moment cloud liquid water) and a state-of-the-art bin-resolved scheme with explicit treatments of liquid and aerosol. The Factorial Method is employed to quantify and compare the sensitivities of each scheme under a set of controlled conditions, in order to isolate the effect of additional microphysical complexity in terms of the impact on surface precipitation. At relatively low updraught speeds, the sensitivity of the bulk schemes was found to depend on the assumptions made with regards the treatment of droplet activation. It was possible to achieve a much closer agreement between the single and dual moment bulk schemes by tuning the specified droplet number concentration in the single moment scheme, suggesting that a diagnostic representation of droplet number may be an acceptable alternative to the more expensive prognostic option. However the effect of changes in CCN concentration were found to produce a relatively stronger effect on precipitation in the bulk schemes compared to the bin scheme; this is believed to be a consequence of differences in the treatment of drop growth by collision and coalescence. Collectively, these results demonstrate the usefulness of the Factorial Method as a model development tool for quantitatively comparing and contrasting the behaviour of microphysics schemes of differing levels of complexity within a specified parameter space.",
        "keywords": "",
        "released": 2011,
        "link": "https://doi.org/10.5194/acp-11-2729-2011"
    },
    {
        "title": "A fully computational and reasonable representation for karyotypes",
        "abstract": "A Summary: The human karyotype has been used as a mechanism for describing and detecting gross abnormalities in the genome for many decades. It is used both for routine diagnostic purposes and for research to further our understanding of the causes of disease. Despite these important applications there has been no rigorous computational representation of the karyotype; rather an informal, string-based representation is used, making it hard to check, organize and search data of this form. In this article, we describe our use of OWL, the Ontology Web Language, to generate a fully computational representation of the karyotype; the development of this ontology represents a significant advance from the traditional bioinformatics use for tagging and navigation and has necessitated the development of a new ontology development environment called Tawny-OWL.",
        "keywords": "",
        "released": 2019,
        "link": "https://doi.org/10.1093/bioinformatics/btz440"
    },
    {
        "title": "OWL/XDD application profiles",
        "abstract": "An application profile specifies a set of terms, drawn from one or more standard namespaces, for annotation of data, and constrains their usage and interpretations in a particular local application. An approach to representation of and reasoning with application profiles based on the OWL and OWL/XDD languages is proposed. The former is a standard Web ontology language, while the latter is a definite-clause-style rule language that employs XML expressions as its underlying data structure. Semantic constraints are defined in terms of rules, which are represented as XDD clauses. Application of the approach to defining application profiles with fine-grained semantic constraints, involving implicit properties of metadata elements, is illustrated. A prototype application profile development environment equipped with metadata validation features has been implemented based on the proposed framework.",
        "keywords": "application profile; metadata schema; OWL; XDD",
        "released": 2007,
        "link": "https://doi.org/10.1093/ietisy/e90-d.10.1611"
    },
    {
        "title": "Vnode: Low-overhead transparent tracing of node.js-based microservice architectures",
        "abstract": "Tracing serves as a key method for evaluating the performance of microservices-based architectures, which are renowned for their scalability, resource efficiency, and high availability. Despite their advantages, these architectures often pose unique debugging challenges that necessitate trade-offs, including the burden of instrumentation overhead. With Node.js emerging as a leading development environment recognized for its rapidly growing ecosystem, there is a pressing need for innovative performance debugging approaches that reduce the telemetry data collection efforts and the overhead incurred by the environment’s instrumentation. In response, we introduce a new approach designed for transparent tracing and performance debugging of microservices in cloud settings. This approach is centered around our newly developed Internal Transparent Tracing and Context Reconstruction (ITTCR) technique. ITTCR is adept at correlating internal metrics from various distributed trace files to reconstruct the intricate execution contexts of microservices operating in a Node.js environment. Our method achieves transparency by directly instrumenting the Node.js virtual machine, enabling the collection and analysis of trace events in a transparent manner. This process facilitates the creation of visualization tools, enhancing the understanding and analysis of microservice performance in cloud environments. Compared to other methods, our approach incurs an overhead of approximately 5% on the system for the trace collection infrastructure while exhibiting minimal utilization of system resources during analysis execution. Experiments demonstrate that our technique scales well with very large trace files containing huge numbers of events and performs analyses in very acceptable timeframes.",
        "keywords": "cloud; microservices; distributed tracing; transparent tracing; trace analysis; debugging; performance; monitoring; Node.js; trace context",
        "released": 2024,
        "link": "https://doi.org/10.3390/fi16010013"
    },
    {
        "title": "RETRACTED: A web-based collaborative design architecture for developing immersive VR driving platform (retracted article)",
        "abstract": "In this study, based on the analysis of the dynamic nature of collaborative design process, a new framework of collaborative design for modelling an immersive VR automotive driving learning (imseADL) platform is described. This framework adopts an agent-based approach and relocates designers, system and the supporting agents in a unified knowledge representation scheme for imseADL design. This study presents the research issues and industrial requirements for such a system. Furthermore, a prototype system of the proposed framework is implemented and its feasibility is evaluated using a real design scenario whose objective is designing an imseADL. In this system, each virtual element or assembly is designed as an independent unit. The unit agent method is used as the basic system modules. To manage these unit agents, a web-based interface manager is provided. The scene explorer is a virtual element design space based on unit agents and the interface manager. To manage the collaborative session, a web-based design phase manager is proposed. In this situation, designers do not possess all the knowledge they need but instead rely on other organisations. In addition, this proposed system is an effective and valuable architecture for collaboration design in today’s product development environment.",
        "keywords": "collaborative design environment; agent-based approach; knowledge engineering",
        "released": 2010,
        "link": "https://doi.org/10.1080/0951192X.2010.490276"
    },
    {
        "title": "An integrated performance analysis tool for SPMD data-parallel programs",
        "abstract": "A new generation of data parallel languages have been proposed whereby a user specifies how data structures are to be distributed amongst the processor nodes of a distributed-memory machine. Based on this information, the compiler then generates code for the parallel application. Although this approach significantly simplifies the development of the initial version of a parallel application, selection of good data distributions leading to efficient computations is often quite difficult. Therefore, performance debuggers are needed to yield insights into the data distribution effects. On the other hand, most of the existing approaches to performance debugging are very general and thus do not provide the user feedback in terms of the high level programming model or the source code of the parallel application. In this paper, we describe a novel approach to performance debugging of data parallel programs. The design and implementation of a visual performance debugger is described that is specifically targeted to meet the performance debugging requirements of a data-parallel programming model based on user-specified data distributions. The performance debugger is part of an integrated programming environment, called EPPP, which also supports a data parallel compiler and a parallel architecture simulator. Thus development and performance debugging of an application may be done either on the real hardware or by using the simulator. The code for EPPP can be obtained free of cost (contact web address, http://www.crim.ca/apar).",
        "keywords": "parallel programming; distributed-memory multiprocessor; data distribution; performance debugger; programming environment",
        "released": 1997,
        "link": "https://doi.org/10.1016/S0167-8191(97)00043-4"
    },
    {
        "title": "Current advancements and prospects of enzymatic and non-enzymatic electrochemical glucose sensors",
        "abstract": "This review discusses the most current developments and future perspectives in enzymatic and non-enzymatic glucose sensors, which have notably evolved over the preceding quadrennial period. Furthermore, a thorough exploration encompassed the sensor’s intricate fabrication processes, the diverse range of materials employed, the underlying principles of detection, and an in-depth assessment of the sensors’ efficacy in detecting glucose levels within essential bodily fluids such as human blood serums, urine, saliva, and interstitial fluids. It is worth noting that the accurate quantification of glucose concentrations within human blood has been effectively achieved by utilizing classical enzymatic sensors harmoniously integrated with optical and electrochemical transduction mechanisms. Monitoring glucose levels in various mediums has attracted exceptional attention from industrial to academic researchers for diabetes management, food quality control, clinical medicine, and bio-process inspection. There has been an enormous demand for the creation of novel glucose sensors over the past ten years. Research has primarily concentrated on succeeding biocompatible and enhanced sensing abilities related to the present technologies, offering innovative avenues for more effective glucose sensors. Recent de-velopments in wearable optical and electrochemical sensors with low cost, high stability, point-of-care testing, and online tracking of glucose concentration levels in biological fluids can aid in managing and controlling diabetes globally. New nanomaterials and biomolecules that can be used in electrochemical sensor systems to identify glucose concentration levels are developed thanks to advances in nanoscience and nanotechnology. Both enzymatic and non-enzymatic glucose electrochemical sensors have garnered much interest recently and have made significant strides in detecting glucose levels. In this review, we summarise several categories of non -enzymatic glucose sensor materials, including composites, non-precious transition metals and their metal ox-ides, hydroxides, precious metals and their alloys, carbon-based materials, conducting polymers, metal-organic framework (MOF)-based electrocatalysts, and wearable device-based glucose sensors deeply.",
        "keywords": "Diabetes; Electrocatalyst; Glucose sensors; Nanomaterials; MOF; Wearable sensors",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.ijbiomac.2023.126680"
    },
    {
        "title": "Designing a data set for postpartum recall registry of women with gestational diabetes in recent pregnancy and its implementation in iranian urban health centers",
        "abstract": "Background and aims: Gestational diabetes has serious short and long-term consequences for both mother and child. Designing a standard data set and implementing a recall registry system provides opportunities for early interventions in women with a history of gestational diabetes. The present study aims to draft a data set for the gestational diabetes recall registry and its establishment in urban health centers. Methods: To design a data set for gestational diabetes postpartum recall registry a qualitative study has been done from April to June of 2019. In this stage, Information need assessment, Identification of data elements, development of registry software, and field-testing were done. Web-based software was designed in NET language and using a Visual Studio programming environment. Implementation of the postpartum recall registry was started in 6 health centers of Ahvaz city from August 2019. Results: During six months 163 women with gestational diabetes in current pregnancy were registered. The final data set for the postpartum recall registry included six main groups, 23 subclasses, and 188 data elements. Mandatory data were included 115 elements. Conclusion: A data set was finalized using a standard method and implemented in urban health centers for six months. Implementing a postpartum registry with standard elements can help manage data and plan for future interventions to reduce modifiable risk factors in this population. (C) 2020 Diabetes India. Published by Elsevier Ltd. All rights reserved.",
        "keywords": "Gestational diabetes mellitus; Registry; Data set; Health centers",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.dsx.2020.11.018"
    },
    {
        "title": "Patient adherence to antiviral treatment for chronic hepatitis b and c: A systematic review",
        "abstract": "Introduction. Poor adherence to treatment for various chronic diseases is a frequent phenomenon. Current guidelines for the treatment of chronic hepatitis B (HBV) and hepatitis C (HCV) recommend optimal adherence, since it has been suggested that poor adherence is associated with an increased risk of virological failure. We aimed to give an overview of studies exploring adherence to combination treatment (PEG-interferon plus ribavirin) for HCV and nucleos(t)ide analogues for HBV. Material and methods. A systematic review Was conducted using the databases PubMed, Embase, Cochrane Library and Web of Knowledge. Search terms included “adherence” or “compliance” combined with “hepatitis B”, “hepatitis C” or “viral hepatitis”. Results. The final selection included 19 studies (13 HCV, 6 HBV). Large differences in patient numbers and adherence assessment methods were found between the various studies. For HCV mean adherence varied from 27 to 97%, whereas the proportion of patients with >= 80% adherence varied from 27 to 96%. Mean adherence reported in HBV studies ranged from 81 to 99%, with 66 to 92% of patients being 100% adherent. For both HCV and HBV studies, the highest adherence rates were reported in studies using self-report whereas lower adherence rates were reported in studies using pharmacy claims. Poor adherence to treatment was associated with an increased risk of virological failure. Conclusion. Non-adherence to treatment in chronic viral hepatitis is not a frequent phenomenon. However, given the increased risk of virological failure in poorly adherent patients, clinicians should routinely address adherence issues in all patients treated for chronic viral hepatitis.",
        "keywords": "Compliance; Nucleos(t)ide analogues; Peg-interferon; Ribavirin; Viral hepatitis",
        "released": 2013,
        "link": "https://doi.org/10.1016/S1665-2681(19)31000-2"
    },
    {
        "title": "Application of flexible technical computing platform to electrification of automotive products",
        "abstract": "JTEKT CORPORATION is an automobile parts and machine tools manufacturer that sells steering systems, bearings, and driveline components worldwide that help to improve fuel efficiency. Electrification of these products has been accelerating, and JTEKT has focused on developing technologies to further contribute to compact, lightweight, and high-efficiency vehicles that are even more energy efficient. Electrification of automotive products is also steadily expanding in emerging markets. In particular, there are urgent demands to overcome such technical challenges as developing low-cost and ultra-lightweight electronic components that can operate in harsh conditions. Thus, while ensuring greater reliability in products than that conventionally achieved, JTEKT is working to quickly develop high-performance, highly functional car electronics. To achieve this, it has introduced the Flexible Technical Computing Platform (FTCP), a general design and development environment that utilizes Fujitsu’s Engineering Cloud, and is using it to construct its own development platform. This paper introduces the steps taken by JTEKT to construct this platform for use in developing car electronics.",
        "keywords": "",
        "released": 2012,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000310171300015"
    },
    {
        "title": "Current status of awake spine surgery: A bibliometric analysis",
        "abstract": "- BACKGROUND: Spine surgery accounts for a large proportion of neurosurgical procedures, with approximately 313 million spine surgeries conducted annually worldwide. Considering delayed recovery and postoperative complications that are commonly reported, there has been a recent shift toward minimally invasive spine procedures conducted under local anesthesia. Despite proven success, there exists a limited body of literature on the use of awake surgery in spinal procedures.- METHODS: A bibliometric analysis was conducted to map the current landscape of work in this field. 190 articles were identified from the Web of Science (Clarivate, NY) database. A comprehensive bibliometric analysis was performed on a narrowed list of the most relevant articles using Bibliometrix, an R-based programming tool.- RESULTS: There has been a rise in academic papers published on the topic of awake spine surgery since 2016, with an increase in publication count by approximately 18% annually and each article cited approximately ten times on average to date. The year 2022 saw an uptick in publications, with 9 throughout the entire year. The most impactful article, with a total of 95 citations, was published by Sairyo et al. 1 Thematic analysis revealed that the terms “lumbar spine “ and “stenosis “ are well-developed topics in the literature, whereas the topics of “complications, “ “fusion, “ and “cost-analysis “ are less well-developed topics.- CONCLUSIONS: This study provides a comprehensive overview of the mostcited articles in the field of awake spine surgery. Specifically, it identifies areas that are well represented in the literature and those which are underrepresented and should be areas of continued future research.",
        "keywords": "Awake spine surgery; Local anesthesia; Spinal anesthesia; Spine",
        "released": 2024,
        "link": "https://doi.org/10.1016/j.wNeu.2024.04.179"
    },
    {
        "title": "A taste for aliens: Contribution of a novel prey item to native fishes’ diet",
        "abstract": "Non-indigenous species (NIS) can alter food web structure and function in many ways. While the predatory and competitive roles of NIS in aquatic environments are commonly studied, their role as a prey item for native predators is often overlooked. As the northern Baltic Sea lacks native crabs, the omnivorous estuarine Harris mud crab (Rhithropanopeus harrisii) is a novel invader to the system and provides an opportunity to observe how the species enters the prey field of predatory fish. In fall 2013, 1185 stomachs from 17 fish species were dissected and analyzed for the presence of R. harrisii. Fishermen had previously reported finding crabs mostly in the stomachs of perch (Perca fluviatilis), a frequent catch in recreational and commercial fisheries, but our study also found large numbers of crabs in four-horned sculpins (Myoxocephalus quadricornis) and small numbers in other species’ stomachs (Rutilus rutilus, Leuciscus ide, Gymnocephalus cernuus, and Blicca bjoerkna). In the study area occupied by R. harrisii, four-horned sculpins were the most frequent predator, with 83% having at least one crab in their stomach. In comparison, 7% of perch and roach had consumed R. harrisii. Most crabs eaten were 10-12 mm (carapace width), despite broader size range available (1-26 mm). Predation on R. harrisii in this system may be limited by the predators’ gape size (i.e., physical feeding restriction). These results highlight the need to understand the role of novel invasive species as prey items for native species, ultimately increase understanding of whether native predators can control NIS populations.",
        "keywords": "Non-indigenous species; Novel invasion; Predation control; Food web; Baltic Sea; Rhithropanopeus harrisii",
        "released": 2019,
        "link": "https://doi.org/10.1007/s10530-019-02021-w"
    },
    {
        "title": "A cyber secure medical management system by using blockchain",
        "abstract": "In the pharmaceutical industry, problems like counterfeit drugs, including vaccines, and their supply chain management problems like transparency, immutability, and traceability exist. In the case of vaccines, it becomes more difficult to standardize and detect fake vaccines because the public has less awareness and knowledge about vaccines. Moreover, the increase in online pharmacies gives more opportunities for counterfeiting vaccines to enter the authentic supply chain management system. We present transparent, immutable and secure vaccine supply chain (TISVSchain), a framework based on blockchain to handle the issues of counterfeited vaccines and vaccine supply chain problems like transparency, immutability, and traceability. Our proposed framework can run both on the private and public blockchain. We have implemented the framework on public blockchain by using remix ide and the smart contracts designed by solidity language run on very low gas cost. We also carried out several experiments by changing the number of nodes and their block time to evaluate the performance of our framework in terms of transaction per second (TPS), gas cost, and propagation delay. Our proposed framework improves the security by using offline unique account addresses in blockchain-based frameworks and improves the overall efficiency of the framework by keeping the gas cost low, finding a way to decrease the number of lost blocks to keep low propagation delay, and keeping high TPS value. TISVSchain shows us promising results to improve vaccine supply chain management’s overall performance, security, and efficiency.",
        "keywords": "Blockchain; data management; secure data; supply chain; vaccine",
        "released": 2023,
        "link": "https://doi.org/10.1109/TCSS.2022.3215455"
    },
    {
        "title": "Collaborative ontology engineering: A survey",
        "abstract": "Building ontologies in a collaborative and increasingly community-driven fashion has become a central paradigm of modern ontology engineering. This understanding of ontologies and ontology engineering processes is the result of intensive theoretical and empirical research within the Semantic Web community, supported by technology developments such as Web 2.0. Over 6 years after the publication of the first methodology for collaborative ontology engineering, it is generally acknowledged that, in order to be useful, but also economically feasible, ontologies should be developed and maintained in a community-driven manner, with the help of fully-fledged environments providing dedicated support for collaboration and user participation. Wikis, and similar communication and collaboration platforms enabling ontology stakeholders to exchange ideas and discuss modeling decisions are probably the most important technological components of such environments. In addition, process-driven methodologies assist the ontology engineering team throughout the ontology life cycle, and provide empirically grounded best practices and guidelines for optimizing ontology development results in real-world projects. The goal of this article is to analyze the state of the art in the field of collaborative ontology engineering. We will survey several of the most outstanding methodologies, methods and techniques that have emerged in the last years, and present the most popular development environments, which can be utilized to carry out, or facilitate specific activities within the methodologies. A discussion of the open issues identified concludes the survey and provides a roadmap for future research and development in this lively and promising field.",
        "keywords": "",
        "released": 2014,
        "link": "https://doi.org/10.1017/S0269888913000192"
    },
    {
        "title": "Predictors of relapse after cessation of nucleos(t)ide analog treatment in HBeAg-negative chronic hepatitis b patients: A meta-analysis",
        "abstract": "Objectives: The aim of this study was to identify the predictors of relapse after the withdrawal of nucleos (t)ide analog (NA) therapy in patients with hepatitis B e antigen (HBeAg)-negative chronic hepatitis B (CHB). Methods: The PubMed, EMBASE, Cochrane Central Register of Controlled Trials, and Web of Science databases were searched through January 2019. A random-effects model meta-analysis was performed, with hazard ratios (HR) and 95% confidence intervals (CI) used as summary statistics. Results: Seventeen studies were included in the meta-analysis. Age (HR = 1.022 per year), baseline hepatitis B surface antigen (HBsAg) (HR = 1.509 per log IU/l), end of treatment (EOT) HBsAg level (HR = 1.896 per log IU/l), EOT HBsAg level >= 1000 IU/ml (HR = 1.749), and HBsAg decline from baseline to EOT (HR = 0.748 per log IU/l) were associated with virological relapse. The predictors of clinical relapse were baseline HBsAg level (HR = 1.312 per log IU/l), EOT HBsAg level (HR = 1.458 per log IU/l), EOT HBsAg level >= 100 IU/ml (HR = 3.199) or >= 1000 IU/ml (HR = 1.810), and duration of consolidation therapy (HR = 0.991 per month). Conclusions: This meta-analysis indicates that age, the duration of consolidation therapy, and levels of baseline and EOT HBsAg were factors predictive of relapse in HBeAg-negative CHB patients who discontinued NA treatment. (C) 2019 The Author(s). Published by Elsevier Ltd on behalf of International Society for Infectious Diseases.",
        "keywords": "Chronic hepatitis B; HBeAg-negative; Stopping therapy; Nucleot(s)ide analogs",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.ijid.2019.07.036"
    },
    {
        "title": "A hierarchical region-merging algorithm for 3-d segmentation of individual trees using UAV-LiDAR point clouds",
        "abstract": "Over an extended period, remote-sensing-based individual tree analysis has played a critical role in modern forest inventory and management research. The segmentation of individual trees from aerial point clouds usually depends on the characteristics of peak-like uplift on the crown surface; however, the performance inevitably decreases with increasing visibility of such features in point clouds, especially for high-density forests. Herein, we developed a novel hierarchical region-merging algorithm that first over-segmented the entire forest scene based on local density and then merged the over-segmented partitions into pairs through a stepwise optimal process to produce the final segmentation. In the region-merging method, a global merging cost was introduced to shift from local detection of crown features to use the overall compactness of forest point clouds. The experiments were conducted using unmanned aerial vehicle light detection and ranging (UAV-LiDAR) point clouds from three coniferous stands with different densities and a high-density coniferous and broad-leaved mixed stand. A total of 5510 field-measured trees in 36 plots were used to assess the accuracy of the proposed method. Our method achieved F-scores of 0.91, 0.88, 0.84, and 0.80 for low- (similar to 700 stems/ha), medium- (similar to 1000 stems/ha), and high-density (similar to 2000 stems/ha) conifer stands and coniferous and broad-leaved mixed forests (similar to 800 stems/ha), respectively. Compared with the classical individual tree segmentation methods (marker-controlled watershed segmentation and point cloud region-growing algorithm), our method obtained comparable performance in low-density conifer stands and superior performance in the other stands. Furthermore, the region-merging algorithm could detect 10% more suppressed trees on average, which led to an apparent improvement in detection accuracy. The proposed algorithm prop ides a flexible segmentation framework that could he further improved by a different design that merges costs or applies multiscale segmentation with different stopping criteria.",
        "keywords": "Hierarchical stepwise optimal; individual tree segmentation; light detection and ranging (LiDAR); region-merging; unmanned aerial vehicle (UAV)",
        "released": 2022,
        "link": "https://doi.org/10.1109/TGRS.2021.3121419"
    },
    {
        "title": "Towards a professional development tool for teachers of english in bilingual streams: The dynamics of beliefs and practices",
        "abstract": "This study highlights tensions and challenges experienced by language teachers in CLIL contexts. Using an example from the Netherlands, it explores the pedagogical and collaborative practices of Teachers of English in Bilingual streams (TEBs). The study shows how, using formal and practical theories (Borg and Burns 2008. “Integrating Grammar in Adult TESOL Classrooms.” Applied Linguistics 29 (3): 456-482. doi:10.1093/applin/amn020.), pedagogical and collaborative practices were formulated and used to investigate the beliefs and practices of language teachers in bilingual settings. The paper presents the operationalisation of 36 practices for TEBs and reports on an online survey investigating TEBs’ stated beliefs and practices. The findings suggest this set of practices has potential, both as a professional development tool for language teachers in bilingual education settings, and for further research. Results of the online survey revealed that the disciplinary identity of most Dutch TEBs leads to a focus on language, communication, literature, and language arts. TEBs are not necessarily aware of, and do not automatically consider, possibilities for expanding their own pedagogical practices in relation to subject-specific language or supporting and collaborating with their subject teacher colleagues. We suggest that policy guidelines, curricula development and teacher education programmes should pay more attention to the unique position of language teachers in these settings.",
        "keywords": "CLIL; language teacher; bilingual education; teacher beliefs; pedagogical practices; collaborative practices",
        "released": 2021,
        "link": "https://doi.org/10.1080/13670050.2018.1556244"
    },
    {
        "title": "FPGA-based acceleration of time series similarity prediction: From cloud to edge",
        "abstract": "With the proliferation of low-cost sensors and the Internet of Things, the rate of producing data far exceeds the compute and storage capabilities of today’s infrastructure. Much of this data takes the form of time series, and in response, there has been increasing interest in the creation of time series archives in the past decade, along with the development and deployment of novel analysis methods to process the data. The general strategy has been to apply a plurality of similarity search mechanisms to various subsets and subsequences of time series data to identify repeated patterns and anomalies; however, the computational demands of these approaches renders them incompatible with today’s power-constrained embedded CPUs. To address this challenge, we present FA-LAMP, an FPGA-accelerated implementation of the Learned Approximate Matrix Profile (LAMP) algorithm, which predicts the correlation between streaming data sampled in real-time and a representative time series dataset used for training. FA-LAMP lends itself as a real-time solution for time series analysis problems such as classification. We present the implementation of FA-LAMP on both edge- and cloud-based prototypes. On the edge devices, FA-LAMP integrates accelerated computation as close as possible to IoT sensors, thereby eliminating the need to transmit and store data in the cloud for posterior analysis. On the cloud-based accelerators, FA-LAMP can execute multiple LAMP models on the same board, allowing simultaneous processing of incoming data from multiple data sources across a network. LAMP employs a Convolutional Neural Network (CNN) for prediction. This work investigates the challenges and limitations of deploying CNNs on FPGAs using the Xilinx Deep Learning Processor Unit (DPU) and the Vitis AI development environment. We expose several technical limitations of the DPU, while providing amechanism to overcome them by attaching custom IP block accelerators to the architecture. We evaluate FA-LAMP using a low-cost Xilinx Ultra96-V2 FPGA as well as a cloud-based Xilinx Alveo U280 accelerator card and measure their performance against a prototypical LAMP deployment running on a Raspberry Pi 3, an Edge TPU, a GPU, a desktop CPU, and a server-class CPU. In the edge scenario, the Ultra96-V2 FPGA improved performance and energy consumption compared to the Raspberry Pi; in the cloud scenario, the server CPU and GPU outperformed the Alveo U280 accelerator card, while the desktop CPU achieved comparable performance; however, the Alveo card offered an order of magnitude lower energy consumption compared to the other four platforms. Our implementation is publicly available at https://github.com/aminiok1/lamp-alveo.",
        "keywords": "Field-programmable gate array (FPGA); time series; Matrix Profile",
        "released": 2023,
        "link": "https://doi.org/10.1145/3555810"
    },
    {
        "title": "Resistance and struggle in leadership development",
        "abstract": "That leadership development is a contested terrain, like any organizational terrain, can scarcely be considered a new idea, yet research into the intricacies of resistance in this context is very much in its infancy. This article takes recent critical scholarship on resistance as its starting point to explore the interdependencies of power, resistance and struggle in a leadership development environment. Drawing on extensive online interactions collected from an 18-month, cross-sector programme with emergent leaders, this article asks whether the different stakeholders in leadership development could benefit from a more open exploration of power and resistance. Such dynamics offer new insights into the relationship between participants and facilitators and raise a series of alternative questions, challenges and strategies for leadership development.",
        "keywords": "leadership development; learning; power; resistance; social constructionism",
        "released": 2014,
        "link": "https://doi.org/10.1177/0018726714521644"
    },
    {
        "title": "Association of <i>HHEX</i> and <i>SLC30A8</i> gene polymorphisms with gestational diabetes mellitus susceptibility: A meta-analysis",
        "abstract": "Genetics plays a role in the development of gestational diabetes mellitus (GDM), which poses serious risks to pregnant women and their children. Several studies have demonstrated a link between GDM susceptibility and rs13266634 C/T polymorphism in SLC30A8 gene and rs1111875 C/T and rs5015480 C/T, which are located near the linkage disequilibrium block containing the IDE, HHEX, and KIF11 genes. However, the results are conflicting. Therefore, we aimed to investigate the association between susceptibility to GDM and HHEX and SLC30A8 gene polymorphisms. PubMed, Web of Science, EBSCO, CNKI, Wanfang Data, VIP, and SCOPUS were used to search for research articles. The quality of the selected literature was evaluated using the Newcastle-Ottawa scale. A meta-analysis was performed using Stata 15.1. Allelic, dominant, recessive, homozygote, and heterozygote models were used for the analysis. Nine articles with 15 studies were included. (1) Four studies about HHEX rs1111875 showed that the C allele was associated with the susceptibility to GDM; (2) three studies on HHEX rs5015480 indicated that the C allele in rs5015480 was significantly associated with GDM; (3) eight studies about SLC30A8 rs13266634 showed that the C allele was significantly associated with the susceptibility to GDM; and (4) a subgroup analysis showed that the rs5015480 polymorphism in HHEX and rs13266634 polymorphism in SLC30A8 gene were associated with GDM susceptibility in Asians. The meta-analysis provided evidence that the C allele in rs1111875 and rs5015480 in HHEX and rs13266634 in SLC30A8 can increase the risk of GDM.PROSPERO registration number CRD42022342280.",
        "keywords": "HHEX; Gene polymorphism; Gestational diabetes mellitus; Meta-analysis",
        "released": 2023,
        "link": "https://doi.org/10.1007/s10528-023-10385-x"
    },
    {
        "title": "Assessing versatility of a generic end-to-end platform for IoT ecosystem applications",
        "abstract": "Availability of efficient development tools for data-rich IoT applications is becoming ever more important. Such tools should support cross-platform deployment and seamless and effective applicability in a variety of domains. In this view, we assessed the versatility of an edge-to-cloud system featuring Measurify, a framework for managing smart things. The framework exposes to developers a set of measurement-oriented resources that can be used in different contexts. The tool has been assessed in the development of end-to-end IoT applications in six Electronic and Information Technologies Engineering BSc theses that have highlighted the potential of such a system, both from a didactic and a professional point of view. The main design abstractions of the system (i.e., generic sensor configuration, simple language with chainable operations for processing data on the edge, seamless WiFi/GSM communication) allowed developers to be productive and focus on the application requirements and the high-level design choices needed to define the edge system (microcontroller and its sensors), avoiding the large set-up times necessary to start a solution from scratch. The experience also highlighted some usability issues that will be addressed in an upcoming release of the system.",
        "keywords": "IoT; edge computing; end-to-end systems; development tools; embedded systems and devices",
        "released": 2022,
        "link": "https://doi.org/10.3390/s22030713"
    },
    {
        "title": "Knowledge-empowered agent information system for privacy payoff in eCommerce",
        "abstract": "Today, many online companies are gathering information and assembling sophisticated databases that know a great deal of information about many people, generally without the knowledge of those people. Such endeavor has resulted in the unprecedented attrition of individual’s right to informational self-determination. On the one hand, Consumers are powerless to prevent the unauthorized dissemination of their personal information, and on the other, they are excluded from its profitable commercial exchange. This paper focuses on developing knowledge-empowered agent information system for privacy payoff as a means of rewarding consumers for sharing their personal information with online businesses. The design of this system is driven by the following argument: if consumers’ personal information is a valuable asset, should they not be entitled to benefit from their asset as well? The proposed information system is a multi-agent system where several agents employ various knowledge and requirements for personal information valuation and interaction capabilities that most users cannot do on their own. The agents in the information system bear the responsibility of working on behalf of consumers to categorize their personal data objects, report to consumers on online businesses’ trust and reputation, determine the value of their compensation using risk-based financial models, and finally negotiate for a payoff value in return for the dissemination of users’ information. The details of the system as well as a proof-of-concept implementation using JADE (Java Agent Development Environment) are presented here.",
        "keywords": "Knowledge; Information system; Agents; Privacy; eCommerce",
        "released": 2012,
        "link": "https://doi.org/10.1007/s10115-011-0415-3"
    },
    {
        "title": "Systematic review and meta-analysis: Impact of anti-viral therapy on portal hypertensive complications in HBV patients with advanced chronic liver disease",
        "abstract": "Background The efficacy of nucleos(t)ide analogs (NAs) in non-cirrhotic chronic hepatitis B (CHB) patients is well-established. However, their impact on complications of portal hypertension in advanced chronic liver disease (ACLD) is less well characterized. Methods MEDLINE/PubMed, EMBASE, Web of Science, Cochrane Central Register of Controlled Trials, and abstracts of major international hepatology meetings were searched for publications from Jan 1, 1995 to Nov 30, 2021. Randomized control trials and observational studies reporting the efficacy of NAs in ACLD patients were eligible. Pooled risk ratios (RRs) for outcomes of interest were calculated with a random-effect or fixed-effect model, as appropriate. Results Thirty-nine studies including 14,212 ACLD patients were included. NA treatment was associated with reduced risks of overall hepatic decompensation events (RR, 0.51; 95% confidence interval [CI]: 0.37-0.71), such as variceal bleeding (RR, 0.44; 95% CI: 0.26-0.74) and ascites (RR, 0.10; 95% CI: 0.01-1.59), on a trend-wise level. Moreover, the risks of hepatocellular carcinoma (HCC) (RR, 0.48; 95% CI: 0.30-0.75) and liver transplantation/death (RR, 0.36; 95% CI: 0.25-0.53) were also reduced by NA treatment and the first-line NAs were superior to non-first-line NAs in improving these outcomes (RR, 0.85; 95% CI: 0.75-0.97 and RR, 0.85; 95% CI: 0.73-0.99, respectively). Conclusion NA therapy lowers the risk of portal hypertension-related complications, including variceal bleeding, HCC, and liver transplantation/death.",
        "keywords": "Advanced chronic liver disease; Antiviral therapy; Meta-analysis; Systematic review",
        "released": 2022,
        "link": "https://doi.org/10.1007/s12072-022-10369-w"
    },
    {
        "title": "Open source as appropriate technology for global education",
        "abstract": "Economic arguments for the adoption of “open source” software in business have been widely discussed. In this paper we draw on personal experience in the UK, South Africa and Southeast Asia to forward compelling reasons why open source software should be considered as an appropriate and affordable alternative to the currently prevailing dependency on large commercial organisations and proprietary products in the field of education. The dynamic and responsive nature of “open source” software and the existence of freely available documentation and online communities offers an opportunity for educators, network administrators and software developers to participate in the development of resources appropriate to local needs while developing their own skills. We identify a range of critical development tools such as Perl and Linux, alongside a more specific application, Basic Support for Cooperative Work, which has great versatility for customising to fulfil specific educational needs and for the development of collaborative on-line learning communities. (C) 2002 Elsevier Science Ltd. All rights reserved.",
        "keywords": "open source software; appropriate technology; virtual communities; internet groupware; collaborative workspace",
        "released": 2002,
        "link": "https://doi.org/10.1016/S0738-0593(00)00077-8"
    },
    {
        "title": "Model-driven engineering of manufacturing automation software projects - a SysML-based approach",
        "abstract": "This paper comprises a SysML-based approach to support the model-driven engineering (MDE) of Manufacturing Automation Software Projects (MASP). The Systems Modeling Language (SysML) is adapted to define the SysML-AT (SysML for automation), a specialized language profile that covers (non-)functional requirements, corresponding software applications and properties of proprietary hardware components. Furthermore, SysML-AT supports an automated software generation for run-time environments conforming to IEC 61131-3. A prototypical tool support was realized for adapted SysML Parametric Diagrams (PD) inside an industrial automation software development tool. Coupling the model editor and online data from the provided run-time environment enables direct debugging inside the model. The approach was evaluated by several case studies and additional usability experiments. With the latter, the suitability of the MDE approach for future users was proven. (C) 2014 Published by Elsevier Ltd.",
        "keywords": "Model-based system and software engineering; Distributed systems; Manufacturing automation system",
        "released": 2014,
        "link": "https://doi.org/10.1016/j.mechatronics.2014.05.003"
    },
    {
        "title": "A conference-friendly, hands-on introduction to deep learning for radiology trainees",
        "abstract": "Artificial or augmented intelligence, machine learning, and deep learning will be an increasingly important part of clinical practice for the next generation of radiologists. It is therefore critical that radiology residents develop a practical understanding of deep learning in medical imaging. Certain aspects of deep learning are not intuitive and may be better understood through hands-on experience; however, the technical requirements for setting up a programming and computing environment for deep learning can pose a high barrier to entry for individuals with limited experience in computer programming and limited access to GPU-accelerated computing. To address these concerns, we implemented an introductory module for deep learning in medical imaging within a self-contained, web-hosted development environment. Our initial experience established the feasibility of guiding radiology trainees through the module within a 45-min period typical of educational conferences.",
        "keywords": "Machine learning; Deep learning; Medical education; Medical imaging",
        "released": 2021,
        "link": "https://doi.org/10.1007/s10278-021-00492-9"
    },
    {
        "title": "An approach to build XML-based domain specific languages solutions for client-side web applications",
        "abstract": "Domain-Specific Languages (DSLs) allow for the building of applications that ease the labour of both software engineers and domain experts thanks to the level of abstraction they provide. In cases where the domain is restricted to Client-Side Web Applications (CSWA), XML-based languages, frameworks and widgets are commonly combined in order to provide fast, robust and flexible solutions. This article presents an approach designed to create XML-based DSL solutions for CSWA that includes an evaluation engine, a programming model and a lightweight development environment. The approach is able to evaluate multiple XML-based DSL programs simultaneously to provide solutions to those Domain Specific Problems for CSWAs. To better demonstrate the capabilities and potential of this novel approach, we will employ a couple of case studies, namely Anisha and FeedPsi. (C) 2017 Elsevier Ltd. All rights reserved.",
        "keywords": "Domain-Specific languages; XML interpreter; JavaScript; Web Application; XML programing language",
        "released": 2017,
        "link": "https://doi.org/10.1016/j.cl.2017.04.002"
    },
    {
        "title": "Migration of legacy mumps applications to relational database servers",
        "abstract": "An extended implementation of the Mumps language is described that facilitates vendor neutral migration of legacy Mumps applications to SQL-based relational database servers. Implemented as a compiler, this system translates Mumps programs to operating system independent, standard C code for subsequent compilation to fully stand-alone, binary executables. Added built-in functions and support modules extend the native hierarchical Mumps database with access to industry standard, networked, relational database management servers (RDBMS) thus freeing Mumps applications from dependence upon vendor specific, proprietary, unstandardized database models. Unlike Mumps systems that have added captive, proprietary RDMBS access, the programs generated by this development environment can be used with any RDBMS system that supports common network access protocols. Additional features include a built-in web server interface and the ability to interoperate directly with programs and functions written in other languages.",
        "keywords": "Mumps; SQL; patient records; relational database server",
        "released": 2001,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000170227100011"
    },
    {
        "title": "A heuristic-based package-aware function scheduling approach for creating a trade-off between cold start time and cost in FaaS computing environments",
        "abstract": "With the migration of enterprise applications to microservices and containers, cloud service providers, starting with Amazon in 2014, announced a new computational model called function-as-a-service. In these platforms, developers create a set of fine-grained functions with shorter execution times instead of developing coarse-grained software. In addition, the management of system resources and servers is delegated to cloud service providers. This model has many benefits, such as reducing costs, improving resource utilization, and helping developers focus on the core logic of applications. But, still faces many challenges such as cost/usage balance, optimizing performance, programming models, using current development tools, containers’ cold start problem, saving data in caches, security issues, privacy concerns, and scheduling challenges like execution time prediction. In this paper, we focus on scheduling and cold start problems. Reducing the cold start time can result in better response times and hence a better experience for end-users. Compromise occurs when keeping warm operating environments which can reduce cold start times while increasing resource usage costs. The cold start problem is wildly studied, but in this work, we propose a novel dynamic waiting-time adjustment approach in which the waiting time of a container will change dynamically according to the situation. In this study, four different types of decisions for changing the waiting time are introduced and investigated. We aim to create a trade-off by using these decisions at runtime using a heuristic method. Functions’ invocation frequency, function dependency graph, maximum merging time, correct mergers, cost, and timeline are considered for the decision-making process. The performed evaluations demonstrate that the proposed approach results in a 32% improvement over the fixed-time method (i.e., the method used by Apache OpenWhisk). This comparison is made from the cumulative measurement viewpoint which is a combination of response time, turnaround time, cost, and utilization.",
        "keywords": "Scheduling; Cloud computing; Function-as-a-service; Serverless computing",
        "released": 2023,
        "link": "https://doi.org/10.1007/s11227-023-05128-z"
    },
    {
        "title": "Compositional algebra for interactive data access",
        "abstract": "An important issue for the success of a database application is the effectiveness of its interface. Frequently a relevant part of the programming effort is devoted to the generation of interfaces. The visual programming environments reduce only partly this effort, and in particular, things become more complicated when data coming from different sources (different views in the same database or even views from different databases or systems) are to be related and must cooperate in the data navigation and manipulation task. To overcome this problem we present a new database access paradigm based on an algebra on the domain of computational abstractions called “services” which include both dimensions: the data access computation and the user interaction. This means that the interaction is not implemented by using separated constructs as happens for traditional computational models; on the contrary, as the interaction is an integral part of the service paradigm, the user interaction is computed starting from the declarative specification of the data access itself. The combination of services in a service expression through the operators defined by the service algebra makes it possible to generate cooperating user interfaces for complex data navigation and manipulation. Through algebraic properties, which hold both from the data and user interface point of view, the service expressions can be simplified and optimized guaranteeing their initial semantics. The paper shows the application of the service algebra to the relational environment by means of a simple extension to SQL. Finally, the paper describes a tool based on a three tier architecture and on lava technology for developing and distributing services in Web environment. Services and combination of services expressed with the service algebra are automatically translated into Java objects, allowing the rapid development of platform independent data access services. (C) 2000 Published by Elsevier Science Ltd.",
        "keywords": "interactive data access; service; algebra; SQL; relational DBMS; Java; middleware",
        "released": 2000,
        "link": "https://doi.org/10.1016/S0306-4379(00)00023-5"
    },
    {
        "title": "Multispectral UAV data for detection of weeds in a citrus farm using machine learning and google earth engine: Case study of morocco",
        "abstract": "Accurate and timely weed mapping between and within trees is considered one of the major chal-lenges in the site-specific weed management systems. This research presents the first cloud com-puting approach based on the incorporation of multispectral Unmanned Aerial Vehicle (UAV) im-agery in the Google Earth Engine (GEE) programming environment with the aim of improving the mapping of weed patches between and within trees in a citrus farm. For this purpose, the UAV multispectral bands (red, green, blue, near infrared, and red edge), as well as the estimated vege-tation height from the UAV Digital Elevation Model (DEM) were analysed in terms of tree and weed discrimination and used as input into Random Forest (RF) and k-nearest neighbors (KNN) machine learning algorithms. From the DEM, the Digital Terrain Model (DTM) was estimated us-ing the Inverse Distance Weighted Interpolation (IDW) of the elevation values of dense points on the soil. The plant height was derived by subtracting the DTM from the DEM, resulting in the Canopy Height Model (CHM). The experimental results show that: (i) the combination of spectral bands and the CHM can classify both trees and weeds with an overall accuracy reached 96.87%; (ii) the RF classifier was more robust compared to KNN in the classification performance; (iii) when compared to the use of UAV spectral bands, the addition of the CHM can improve the accu-racy of crop classification by 13.36% (KNN) and 1.79% (RF). Furthermore, the integration of UAV imagery in the GEE was highly efficient in terms of automation of the UAV imagery process-ing.",
        "keywords": "Weed mapping; Multispectral UAV imagery; Google earth engine (GEE); Machine learning; Canopy height model (CHM)",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.rsase.2023.100941"
    },
    {
        "title": "Communication intervention in rett syndrome: A survey of speech language pathologists in swedish health services",
        "abstract": "Purpose: To investigate communication intervention that speech language pathologists (SLPs) provide to people with Rett syndrome. Methods: A web-based survey targeting all Swedish SLPs working with people currently receiving support from habilitation services. Results: The SLPs reportedly followed recommended practice in the following aspects: (1) Information on communicative function was collected from several sources, including observation in well-known settings and reports from the client s social network, (2) Multimodal communication was promoted and, (3) Responsive partner strategies were largely targeted in the intervention. However, few instruments or standard procedures were used and partner instruction was given informally. Most SLPs used communication aids in the intervention and their general impression of using communication aids was positive. Further, augmentative and alternative communication (AAC) was estimated to increase and clarify communicative contributions from the person. Conclusions: Communication aids were reported to have a positive influence on communicative functions. Swedish SLP services followed best practice in several aspects, but there are areas with potential for development. Tools and best practice guidelines are needed to support SLPs in the AAC process for clients with Rett syndrome.",
        "keywords": "Augmentative and alternative communication; communication; intervention; Rett syndrome; speech language pathology; Sweden",
        "released": 2015,
        "link": "https://doi.org/10.3109/09638288.2014.962109"
    },
    {
        "title": "<I>harvest</i>: An open platform for developing web-based biomedical data discovery and reporting applications",
        "abstract": "Biomedical researchers share a common challenge of making complex data understandable and accessible as they seek inherent relationships between attributes in disparate data types. Data discovery in this context is limited by a lack of query systems that efficiently show relationships between individual variables, but without the need to navigate underlying data models. We have addressed this need by developing Harvest, an open-source framework of modular components, and using it for the rapid development and deployment of custom data discovery software applications. Harvest incorporates visualizations of highly dimensional data in a web-based interface that promotes rapid exploration and export of any type of biomedical information, without exposing researchers to underlying data models. We evaluated Harvest with two cases: clinical data from pediatric cardiology and demonstration data from the OpenMRS project. Harvest’s architecture and public open-source code offer a set of rapid application development tools to build data discovery applications for domain-specific biomedical data repositories. All resources, including the OpenMRS demonstration, can be found at http://harvest.research.chop.edu",
        "keywords": "database; open-source; application development; data discovery; data visualization",
        "released": 2014,
        "link": "https://doi.org/10.1136/amiajnl-2013-001825"
    },
    {
        "title": "An open and scalable framework for enriching ontologies with natural language content",
        "abstract": "Knowledge Sharing is a crucial issue in the Semantic Web: SW services expose and share knowledge content which arise from distinct languages, locales, and personal perspectives; a great effort has been spent in these years, in the form of Knowledge Representation standards and communication protocols, with the objective of acquiring semantic consensus across distributed applications. However, neither ontology mapping algorithm nor knowledge mediator agent can easily find a way through ontologies as they are organized nowadays: concepts expressed by hardly recognizable labels, lexical ambiguity represented by phenomena like synonymy and polysemy and use of different natural languages which derive from different cultures, all together push for expressing ontological content in a linguistically motivated fashion. This paper presents our approach in establishing a framework for semi-automatic linguistic enrichment of ontologies, which led to the development of Ontoling, a plug-in for the popular ontology development tool Protege. We describe here its features and design aspects which characterize its current release.",
        "keywords": "",
        "released": 2006,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000239623800106"
    },
    {
        "title": "Decoding kinase-adverse event associations for small molecule kinase inhibitors",
        "abstract": "Small molecule kinase inhibitors (SMKIs) are being approved at a fast pace under expedited programs for anticancer treatment. In this study, we construct a multi-domain dataset from a total of 4638 patients in the registrational trials of 16 FDA-approved SMKIs and employ a machine-learning model to examine the relationships between kinase targets and adverse events (AEs). Internal and external (datasets from two independent SMKIs) validations have been conducted to verify the usefulness of the established model. We systematically evaluate the potential associations between 442 kinases with 2145 AEs and made publicly accessible an interactive web application “Identification of Kinase-Specific Signal” (https://gongj.shinyapps.io/ml4ki). The developed model (1) provides a platform for experimentalists to identify and verify undiscovered KI-AE pairs, (2) serves as a precision-medicine tool to mitigate individual patient safety risks by forecasting clinical safety signals and (3) can function as a modern drug development tool to screen and compare SMKI target therapies from the safety perspective.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1038/s41467-022-32033-5"
    },
    {
        "title": "Engineering platform and experimental protocol for design and evaluation of a neurally-controlled powered transfemoral prosthesis",
        "abstract": "To enable intuitive operation of powered artificial legs, an interface between user and prosthesis that can recognize the user’s movement intent is desired. A novel neural-machine interface (NMI) based on neuromuscular-mechanical fusion developed in our previous study has demonstrated a great potential to accurately identify the intended movement of transfemoral amputees. However, this interface has not yet been integrated with a powered prosthetic leg for true neural control. This study aimed to report (1) a flexible platform to implement and optimize neural control of powered lower limb prosthesis and (2) an experimental setup and protocol to evaluate neural prosthesis control on patients with lower limb amputations. First a platform based on a PC and a visual programming environment were developed to implement the prosthesis control algorithms, including NMI training algorithm, NMI online testing algorithm, and intrinsic control algorithm. To demonstrate the function of this platform, in this study the NMI based on neuromuscular-mechanical fusion was hierarchically integrated with intrinsic control of a prototypical transfemoral prosthesis. One patient with a unilateral transfemoral amputation was recruited to evaluate our implemented neural controller when performing activities, such as standing, level-ground walking, ramp ascent, and ramp descent continuously in the laboratory. A novel experimental setup and protocol were developed in order to test the new prosthesis control safely and efficiently. The presented proof-of-concept platform and experimental setup and protocol could aid the future development and application of neurally-controlled powered artificial legs.",
        "keywords": "Biomedical Engineering; Issue 89; neural control; powered transfemoral prosthesis; electromyography (EMG); neural-machine interface; experimental setup and protocol",
        "released": 2014,
        "link": "https://doi.org/10.3791/51059"
    },
    {
        "title": "Student perceptions of faculty caring in online nursing education: A mixed-methods study",
        "abstract": "Background: As the prevalence of online nursing education increased, so did the need for faculty to understand student perceptions of faculty behaviors that demonstrate caring and promote student success. Literature from both education and nursing journals supported this study. Objectives: Primary objectives were to identify how the value of caring is made visible in online learning, to understand how students prioritized faculty caring behaviors and to identify any significant differences in per-ceptions related to student demographics. A secondary objective was to provide professional development tools for faculty who teach nursing students in the online environment. Design: Mixed methods. Setting: A College of Nursing within a large public university in the Southeastern United States. Participants: One hundred and forty-one (141) nursing students pursuing graduate degrees (MSN or DNP) participated in the student survey and 15 participated in the focus groups/interviews; 28 faculty members responded to the survey. Methods: A validated survey tool was used to identify how students prioritized faculty caring behaviors. Facilitator-led focus groups were used to gain additional insights. Faculty members were surveyed to compare faculty and student priorities. Descriptive and correlational analyses were performed. Results: Graduate nursing students perceived a course that is well-designed, with clear instructions and communication, and a supportive environment, to be evidence of faculty caring. There were significant differ-ences in student responses when analyzed by demographic characteristics such as gender and race. Student perceptions aligned with previous research on this topic; faculty and student perceptions differed in some areas. Conclusion: Quantifying graduate student perceptions about faculty caring behaviors provided information that is used for faculty professional development. Further research is needed to explore perceptions of faculty caring in different student populations. Multi-site studies to explore race and gender differences in perception are also recommended.",
        "keywords": "Online education; Nursing; Caring behaviors; Faculty",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.nedt.2022.105328"
    },
    {
        "title": "Serious adverse events after cessation of nucleos(t)ide analogues in individuals with chronic hepatitis b: A systematic review and meta-analysis",
        "abstract": "Background & Aims: The risk of serious clinical outcomes following cessation of nucleos(t)ide analogues (NUCs) in individuals with chronic hepatitis B remains poorly characterized. This systematic review and meta-analysis aimed to evaluate current literature on this issue. Methods: We searched PubMed, Embase, and Web of Science for NUC stop studies that noted clinical outcomes published between January 1, 2006 and August 18, 2022. We performed meta-research analyses to examine the relationships of reported outcomes with study designs and characteristics and also pooled studies with non-overlapping populations to provide risk estimates for the proportions of (1) severe hepatitis flares or hepatic decompensation or (2) hepatitis flare-related death or liver transplantation. Results: The meta-research analysis included 50 studies of highly heterogeneous designs and characteristics. We found that reporting of safety outcomes varied widely according to outcome definition, follow-up duration, and sample size. Only ten studies prespecified safety events as the study outcome, and only four had an outcome definition to include hepatic insufficiency, a follow-up duration >12 months, and a sample size >100 patients. We further pooled 15 studies with 4,525 individuals and estimated that severe hepatitis flares or decompensation would occur in 1.21% (95% CI 0.70-2.08%), with significant heterogeneity (I2 = 54%, p <0.01), while hepatitis flare-related death or liver transplantation would occur in 0.37% (95% CI 0.20-0.67%), without significant heterogeneity (I2 = 0.00%, p = 1.00). Conclusions: Current literature on the risk of serious clinical outcomes following NUC cessation is very limited and highly heterogeneous. Pooled analyses of available data found approximately 1% of patients who stopped NUCs developed severe flares or hepatic decompensation. Impact and implications: Current literature regarding the safety concerns surrounding NUC cessation for individuals with chronic hepatitis B is limited and heterogeneous in designs and characteristics, and thus should be interpreted with great caution. Based on currently available data, the proportion of patients that develop severe hepatitis flares or hepatic decompensation was estimated at 1.21% and that of flare-related death or liver transplantation at 0.37%. Our findings are important for individuals receiving nucleos(t)ide analogues for hepatitis B virus infection because we not only pooled currently available data to estimate the risk of serious clinical adverse events following treatment cessation but also uncovered critical limitations of existing literature regarding the safety of finite therapy. & COPY; 2022 The Author(s). Published by Elsevier B.V. on behalf of European Association for the Study of the Liver (EASL). This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",
        "keywords": "Chronic hepatitis B; nucleos(t)ide analogue; finite therapy; outcome research",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.jhepr.2022.100617"
    },
    {
        "title": "Development of integrated automatic system of laser cladding for repairing of polycrystalline diamond compact bits",
        "abstract": "In order to improve the automatic level for laser-cladding repair of high value industrial equipment, such as polycrystalline diamond compact bit (PDC bit) applied in oil industry, a universal scheme of integrated automatic system for repairing is proposed in this paper, and the basic functional modules together with the executing order according to which each module runs are defined. There are two main technical points, i.e.,inspection and repairing, that need to be realized for such integrated automatic system. Therefore, according to the proposed scheme and the existing instruments, a dual-robot system, which includes two KUKA industrial robots, is adopted as the technological implementation, where one robot is used to carry a 3D scanner to reconstruct the PDC bit to realize inspection while the other is used to hold the laser to melt the special powder flowing to the damaged region of the bit to complete cladding. To realize automatic running of the whole integrated system, a hand-eye calibration method, namely three-point calibration, is then proposed, by which coordinates of point cloud of the damaged PDC bit detected by 3D scanner can be transformed to those of the coordinate system of the robot with the laser, so that the cladding path planned via cutting slice of the damaged region of the PDC bit in the upper computer software, the key of the integrated system developed by QT programming tool, can be tracked by laser head and then the damaged part of the PDC bit can be repaired. Finally, a laser-cladding experiment for repairing PDC bit is carried out and the feasibility of the proposed scheme of the integrated automatic system and the effectiveness of the dual-robot system implemented via KUKA robots are verified, According to existing literature, no papers about such integrated system for automatic laser cladding repair have been published.",
        "keywords": "laser cladding; dual-robot system; 3D reconstruction; integrated automatic system",
        "released": 2023,
        "link": "https://doi.org/10.3390/electronics12040900"
    },
    {
        "title": "Development of the community water model (CWatM v1.04) - a high-resolution hydrological model for global and regional assessment of integrated water resources management",
        "abstract": "We develop a new large-scale hydrological and water resources model, the Community Water Model (CWatM), which can simulate hydrology both globally and regionally at different resolutions from 30 arcmin to 30 arcsec at daily time steps. CWatM is open source in the Python programming environment and has a modular structure. It uses global, freely available data in the netCDF4 file format for reading, storage, and production of data in a compact way. CWatM includes general surface and groundwater hydrological processes but also takes into account human activities, such as water use and reservoir regulation, by calculating water demands, water use, and return flows. Reservoirs and lakes are included in the model scheme. CWatM is used in the framework of the Inter-Sectoral Impact Model Intercomparison Project (ISIMIP), which compares global model outputs. The flexible model structure allows for dynamic interaction with hydro-economic and water quality models for the assessment and evaluation of water management options. Furthermore, the novelty of CWatM is its combination of state-of-the-art hydrological modeling, modular programming, an online user manual and automatic source code documentation, global and regional assessments at different spatial resolutions, and a potential community to add to, change, and expand the open-source project. CWatM also strives to build a community learning environment which is able to freely use an open-source hydrological model and flexible coupling possibilities to other sectoral models, such as energy and agriculture.",
        "keywords": "",
        "released": 2020,
        "link": "https://doi.org/10.5194/gmd-13-3267-2020"
    },
    {
        "title": "Sorotoki: A matlab toolkit for design, modeling, and control of soft robots",
        "abstract": "In this paper, we present Sorotoki, an open-source toolkit in MATLAB that offers a comprehensive suite of tools for the design, modeling, and control of soft robots. The complexity involved in researching and building soft robots often stems from the interconnectedness of design and control aspects, which are rarely addressed together as a unified problem. To address such complex interdependencies in soft robotics, the Sorotoki toolkit provides a comprehensive and modular programming environment composed of seven Object-Oriented classes. These classes are designed to work together to solve a wide range of soft robotic problems, offering versatility and flexibility for its users. We provide here a comprehensive overview of the Sorotoki software architecture to highlight its usage and applications. The details and interconnections of each module are thoroughly described, collectively explaining how to gradually introduce modeling complexity for various soft robotic scenarios. The effectiveness of Sorotoki is also demonstrated through a range of case studies, including novel problem scenarios and established works widely recognized in the soft robotics community. These case studies cover a broad range of research problems, including: inverse design of soft actuators, passive and active soft locomotion, object manipulation with soft grippers, meta-materials, model reduction, model-based control of soft robots, and online shape estimation. Additionally, the toolkit provides access to four open-hardware soft robotic systems that can be fabricated using commercially available 3D printers. For more information about Sorotoki, readers are encouraged to visit: https://bjcaasenbrood.github.io/SorotokiCode/",
        "keywords": "Soft robotics; Mathematical models; Sensors; Robot sensing systems; Finite element analysis; Solid modeling; Matlab; Software design; software; design; modeling; control; matlab",
        "released": 2024,
        "link": "https://doi.org/10.1109/ACCESS.2024.3357351"
    },
    {
        "title": "Bayesian parentage analysis with systematic accountability of genotyping error, missing data and false matching",
        "abstract": "Motivation: The goal of any parentage analysis is to identify as many parent-offspring relationships as possible, while minimizing incorrect assignments. Existing methods can achieve these ends, but they require additional information in the form of demographic data, thousands of markers and/or estimates of genotyping error rates. For many non-model systems, it is simply not practical, cost-effective or logistically feasible to obtain this information. Here, we develop a Bayesian parentage method that only requires the sampled genotypes to account for genotyping error, missing data and false matches. Results: Extensive testing with microsatellite and SNP datasets reveals that our Bayesian parentage method reliably controls for the number of false assignments, irrespective of the genotyping error rate. When the number of loci is limiting, our approach maximizes the number of correct assignments by accounting for the frequencies of shared alleles. Comparisons with exclusion and likelihood-based methods on an empirical salmon dataset revealed that our Bayesian method had the highest ratio of correct to incorrect assignments. Availability: Our program SOLOMON is available as an R package from the CRAN website. SOLOMON comes with a fully functional graphical user interface, requiring no user knowledge about the R programming environment. In addition to performing Bayesian parentage analysis, SOLOMON includes Mendelian exclusion and a priori power analysis modules. Further information and user support can be found at https://sites.google.com/site/parent-agemethods/. Contact: christim@science.oregonstate.edu Supplementary information: Supplementary data are available at Bioinformatics online.",
        "keywords": "",
        "released": 2013,
        "link": "https://doi.org/10.1093/bioinformatics/btt039"
    },
    {
        "title": "Design and implementation of the multithread priority scheduler for smart devices based on virtual machine",
        "abstract": "Virtual machine is one of the core technologies for mobile, cloud and IoT technologies, and its purpose is to provide a platform independent runtime environment from the specific hardware. Because a virtual machine simplifies the fragmentized complex execution and development environments by device types or platform types, and offers consistent program development and common execution methods, the use scope of a virtual machine becomes wider. Smart Virtual Machine (SVM) is a software processor mountable for smart devices and embedded devices, and is a core module of smart cross platform to enhance content re-usability by providing the independent function of language/platform to users. Although, SVM supports programming languages including C/C++, Java and Objective C, and various execution environments such as OS and HTML5 including Android, iOS, Linux and Windows, SVM does not support functions like multithread, priority scheduling and thread synchronization within the contents. This paper makes thread within content possible to be differentially executed by supporting a thread priority scheduling function in SVM. Through this, a programmer can use multithread that SVM independently provides to a developed language and a device. In addition, the SVM preferential occupancy of thread, according to importance, has become possible by using priority that can be awarded to each thread.",
        "keywords": "Smart Virtual Machine (SVM); smart cross platform; multithread; priority scheduling; programming languages",
        "released": 2015,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000368651300034"
    },
    {
        "title": "Experiential learning in biomedical engineering education using wearable devices: A case study in a biomedical signals and systems analysis course",
        "abstract": "Biomedical engineering (BME) is one of the fastest-growing engineering fields worldwide. BME professionals are extensively employed in the health technology and healthcare industries. Hence, their education must prepare them to face the challenge of a rapidly evolving technological environment. Biomedical signals and systems analysis is essential to BME undergraduate education. Unfortunately, students often underestimate the importance of their courses as they do not perceive these courses’ practical applications in their future professional practice. In this study, we propose using blended learning spaces to develop new learning experiences in the context of a biomedical signals and systems analysis course to enhance students’ motivation and interest and the relevance of the materials learned. We created a learning experience based on wearable devices and cloud-based collaborative development environments such that the students turned daily-life scenarios into experiential learning spaces. Overall, our results suggest a positive impact on the students’ perceptions of their learning experience concerning relevance, motivation, and interest. Namely, the evidence shows a reduction in the variability of such perceptions. However, further research must confirm this potential impact. This confirmation is required given the monetary and time investment this pedagogical approach would require if it were to be implemented at a larger scale.",
        "keywords": "biomedical engineering; engineering education; experiential learning; wearable technology; wearable devices; higher education; educational innovation",
        "released": 2022,
        "link": "https://doi.org/10.3390/educsci12090598"
    },
    {
        "title": "Programming of a flexible computer simulation to visualize pharmacokinetic-pharmacodynamic models",
        "abstract": "Teaching pharmacokinetic-pharmacodynamic (PK/PD) models can be made more effective using computer simulations. We propose the programming of educational PK or PK/PD computer simulations as an alternative to the use of pre-built simulation software. This approach has the advantage of adaptability to non-standard or complicated PK or PK/PD models. Simplicity of the programming procedure was achieved by selecting the LabVIEW programming environment. An intuitive user interface to visualize the time courses of drug concentrations or effects can be obtained with pre-built elements. The environment uses a wiring analogy that resembles electrical circuit diagrams rather than abstract programming code. The goal of high interactivity of the simulation was attained by allowing the program to run in continuously repeating loops. This makes the program behave flexibly to the user input. The programming is described with the aid of a 2-compartment PK simulation. Examples of more sophisticated simulation programs are also given where the PK/PD simulation shows drug input, concentrations in plasma, and at effect site and the effects themselves as a function of time. A multicompartmental model of morphine, including metabolite kinetics and effects is also included. The programs are available for download from the World Wide Web at http:// www.klinik.uni-frankfurt.de/zpharm/kiin/ PKPDsimulation/content.html. For pharmacokineticists who only program occasionally, there is the possibility of building the computer simulation, together with the flexible interactive simulation algorithm for clinical pharmacological teaching in the field of PK/PD models.",
        "keywords": "PK/PD; pharmacokinetic models; computer simulation",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000187961600002"
    },
    {
        "title": "sigTOOL: A MATLAB-based environment for sharing laboratory-developed software to analyze biological signals",
        "abstract": "This paper describes a software package, named sigTOOL, for processing biological signals. The package runs in the MATLAB programming environment and has been designed to promote the sharing of laboratory-developed software across the worldwide web. As proof-of-concept of the design of the system, sigTOOL has been used to build an analysis application for dealing with neuroscience data complete with a user-friendly graphical user interface which implements a range of waveform and spike-train analysis functions. The interface allows many commonly used neuroscience data file formats to be loaded (including those of Alpha Omega, Cambridge Electronic Design, Cyberkinetics Inc., Molecular Devices, Nex Technologies and Plexon Instruments). Waveform analysis functions selectable from the interface support waveform averaging (mean and median), auto- and cross-correlation, power spectral analysis, coherence estimation, digital filtering (feedback and feedforward) and resampling. Spike-train analyses include interspike interval distributions, Poincare plots, event auto- and cross-correlations, spike-triggered averaging, stimulus driven and phase-related peri-event time histograms and rasters as well as frequencygrams. User-developed additions to sigTOOL that are archived and distributed electronically will be added to the sigTOOL interface on-the-fly, without the need to modify the core sigTOOL code. Full sigTOOL functionality will be provided to support the user-developed code, including the ability to record a user action history for batch processing of files and support for exporting the results of analyses to external graphics editing software and spreadsheet-based data processing packages. (C) 2008 Elsevier B.V. All rights reserved.",
        "keywords": "Software; Action potential; Spike-train analysis; Correlation; Averaging; Waveform; Spectral analysis",
        "released": 2009,
        "link": "https://doi.org/10.1016/j.jneumeth.2008.11.004"
    },
    {
        "title": "Live, online short-courses: A case study of innovative teacher professional development",
        "abstract": "Teachers are searching for new venues through which they may meet stringent professional development requirements. Under competitive funding from NASA’s (National Aeronautics and Space Administration) Office of Education and the NASA Explorer Schools Project, U. S. Satellite Laboratory, Inc. created a series of live, online, interactive short-courses. In this case study, a mixed methods analysis of a variety of data sources reveals that diverse educators from a variety of classroom contexts view the short-courses as a useful professional development tool, both as a vehicle for a teacher’s own professional growth and for classroom applications. Teachers were particularly interested in the ability to participate in a collaborative community of practice with other educators, instructors, and scientists from across the country, and they found the flexible design of the professional development to be useful. This short-course design offers promise for future professional development opportunities.",
        "keywords": "Online learning; teacher professional development; in-service science teachers; synchronous online learning",
        "released": 2010,
        "link": "https://doi.org/10.19173/irrodl.v11i1.758"
    },
    {
        "title": "Condition monitoring of rotary machinery using industrial IOT framework: Step to smart maintenance",
        "abstract": "Modern maintenance strategies, such as predictive and prescriptive maintenance, which derived from the concept of Industry and Maintenance 4.0, involve the application of the Industrial Internet of Things (IIoT) to connect maintenance objects enabling data collection and analysis that can help make better decisions on maintenance activities. Data collection is the initial step and the foundation of any modern Predictive or Prescriptive maintenance strategy because it collects data that can then be analysed to provide useful information about the state of maintenance objects. Condition monitoring of rotary equipment is one of the most popular maintenance methods because it can distinguish machine state between multiple fault types. The topic of this paper is the presentation of an automated system for data collection, processing and interpretation of rotary equipment state that is based on IIoT framework consisting of an IIoT accelerometer, edge and fog devices, web API and database. Additionally, ISO 10816-1 guidance has been followed to develop module for evaluation of vibration severity. The collected data is also visualized in a dashboard in a near-real time and shown to maintenance engineering, which is crucial for pattern monitoring. The developed system was launched in laboratory conditions using rotating equipment failure simulator to test the logic of data collection and processing. A proposed system has shown that it is capable of automated periodic data collection and processing from remote places which is achieved using Node RED programming environment and MQTT communication protocol that enables reliable, lightweight, and secure data transmission.",
        "keywords": "accelerometer; automated data collection; Industrial Internet of Things (IIoT); MQTT; Node RED",
        "released": 2022,
        "link": "https://doi.org/10.31803/tg-20220517173151"
    },
    {
        "title": "Distributed print on demand systems in the xpect framework",
        "abstract": "The Internet is an extremely rich source of online information and services. However, complex client requests, involving and combining several types of services, are difficult to handle without some form of support. This is particularly true in the case of electronic commerce, where complex transactions may involve several independent good providers, bankers, delivery services, etc. Hence the need for “brokering” services, whose offers combine in the best possible way offers coming from existing, specialized services publicly available on the Net, in order to match customers’ constraints. The Xpect framework for electronic commerce has been developed for that purpose. In this paper, we illustrate it through a case study in the context of distributed print-on-demand. We propose an architecture and implementation of the case study based on CLF, a distributed application development tool relying on a rich object model and its corresponding coordination scripting facility.",
        "keywords": "print-on-demand; multi-agent negotiation; electronic commerce brokering; workflow",
        "released": 1999,
        "link": "https://doi.org/10.1023/A:1008706628221"
    },
    {
        "title": "Effect of combination treatment based on interferon and nucleos(t)ide analogues on functional cure of chronic hepatitis b: A systematic review and meta-analysis",
        "abstract": "Background Priority of antiviral treatment for patients with chronic hepatitis B (CHB) is to increase the probability of functional cure. We aimed to synthesize evidence regarding the efficacy of different combination strategies of antiviral treatment based on interferon (IFN) and nucleos(t)ide analogues (NAs) in adults with CHB. Methods PubMed, Web of Science and Embase databases were searched from inception to May 26, 2019. Three types of combination strategies were studied: initial combination (IFN or NAs monotherapy as control), add-on (I: IFN add-on NAs vs. NAs; II: NAs add-on IFN vs. IFN), switch-to (I: IFN switch-to NAs vs. IFN; II: NAs switch-to IFN vs. NAs). Results Compared to NAs monotherapy, initial combination strategy improved the probability of HBeAg loss (RR: 1.62, 95% CI 1.33-1.97) and HBsAg loss (RR: 15.59, 95% CI 3.22-75.49), while compared to IFN monotherapy, no higher rates in the loss of HBsAg or HBeAg for initial combination. Compared to NAs monotherapy, IFN add-on NAs strategy had a higher rate of HBsAg loss (RR: 4.52, 95% CI 1.95-10.47), while compared to IFN monotherapy, NAs add-on IFN had a similar outcome. Compared to NAs monotherapy, NAs switch-to IFN strategy improved HBsAg loss (RR: 12.15, 95% CI 3.99-37.01); while compared to IFN monotherapy, IFN switch-to NAs had no improved rate of HBsAg clearance but higher rates in undetectable HBV DNA, and HBeAg loss. Conclusion IFN add-on NAs, or NAs switched to IFN could significantly improve the probability of HBsAg loss compared to NAs monotherapy.",
        "keywords": "Chronic hepatitis B; Interferon; Nucleos(t)ide analogues; Combination treatment strategies; HBsAg loss",
        "released": 2020,
        "link": "https://doi.org/10.1007/s12072-020-10099-x"
    },
    {
        "title": "Distributed print on demand systems in the XPect framework",
        "abstract": "The Internet is an extremely rich source of online information and services. However, complex client requests, involving and combining several types of services, are difficult to handle without some form of support. This is particularly true in the case of electronic commerce, where complex transactions may involve several independent good providers, bankers, delivery services, etc. Hence the need for “brokering” services, whose offers combine in the best possible way offers coming from existing, specialized services publicly available on the Net, in order to match customers’ constraints. The XPect framework for electronic commerce has been developed for that purpose; In this paper, we illustrate it through a case study in the context of distributed print-on-demand. We propose an architecture and implementation of the case study based on CLF, a distributed application development tool relying on a rich object model and its corresponding coordination scripting facility.",
        "keywords": "print-on-demand; multi-agent negotiation",
        "released": 1998,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000079185700011"
    },
    {
        "title": "A microarray analysis for differential gene expression in the soybean genome using bioconductor and r",
        "abstract": "This article describes specific procedures for conducting quality assessment of Affymetrix GeneChip soybean genome data and for performing analyses to determine differential gene expression using the open-source R programming environment in conjunction with the open-source Bioconductor software. We describe procedures for extracting those Affymetrix probe set IDs related specifically to the soybean genome on the Affymetrix soybean chip and demonstrate the use of exploratory plots including images of raw probe-level data, boxplots, density plots and M versus A plots. RNA degradation and recommended procedures from Affymetrix for quality control are discussed. An appropriate probe-level model provides an excellent quality assessment tool. To demonstrate this, we discuss and display chip pseudo-images of weights, residuals and signed residuals and additional probe-level modeling plots that may be used to identify aberrant chips. The Robust Multichip Averaging (RMA) procedure was used for background correction, normalization and summarization of the AffyBatch probe-level data to obtain expression level data and to discover differentially expressed genes. Examples of boxplots and MA plots are presented for the expression level data. Volcano plots and heatmaps are used to demonstrate the use of (log) fold changes in conjunction with ordinary and moderated t-statistics for determining interesting genes. We show, with real data, how implementation of functions in R and Bioconductor successfully identified differentially expressed genes that may play a role in soybean resistance to a fungal pathogen, Phakopsora pachyrhizi. Complete source code for performing all quality assessment and statistical procedures may be downloaded from our web source: http://css.ncifcrf.gov/services/download/MicroarraySoybean.zip”",
        "keywords": "microarray; Affymetrix GeneChip (R); probe-level data preprocessing; quality control; differential gene expression analysis",
        "released": 2007,
        "link": "https://doi.org/10.1093/bib/bbm043"
    },
    {
        "title": "Process-centric engineering web services in a distributed and collaborative environment",
        "abstract": "We propose a Web service oriented and process-centric framework for supporting collaborative engineering services. The proposed approach utilizes BPEL-based (Business Process Execution Language) process templates and coordination broker-based conversation support, which can support long-running engineering transactions, engineering service orchestration and choreography, conversation over the process, and context-awareness. The process template plays an important role as a federation of published engineering services. It is managed and executed by a process orchestration and choreography broker, which works as a service dispatching and aggregation agent. It is capable of federating processes, tools, operations, and knowledge bases into a dynamic and collaborative product development environment. The paper also discusses how to support collaboration over the running process using conversation policy. Further, the paper discusses how to utilize engineering contexts in support of querying and reasoning of engineering service-related knowledge. (c) 2006 Elsevier Ltd. All rights reserved.",
        "keywords": "BPEL4WS; engineering contexts; engineering Web services; process management; conversation policy",
        "released": 2006,
        "link": "https://doi.org/10.1016/j.cie.2006.03.004"
    },
    {
        "title": "The OpenModelica integrated environment for modeling, simulation, and model-based development",
        "abstract": "OpenModelica is a unique large-scale integrated open-source Modelica- and FMI-based modeling, simulation, optimization, model-based analysis and development environment. Moreover, the OpenModelica environment provides a number of facilities such as debugging; optimization; visualization and 3D animation; web-based model editing and simulation; scripting from Modelica, Python, Julia, and Matlab; efficient simulation and co-simulation of FMI-based models; compilation for embedded systems; Modelica-UML integration; requirement verification; and generation of parallel code for multi-core architectures. The environment is based on the equation-based object-oriented Modelica language and currently uses the MetaModelica extended version of Modelica for its model compiler implementation. This overview paper gives an up-to-date description of the capabilities of the system, short overviews of used open source symbolic and numeric algorithms with pointers to published literature, tool integration aspects, some lessons learned, and the main vision behind its development.",
        "keywords": "Modelica; OpenModelica; MetaModelica; FMI; modeling; simulation; optimization; development; environment; numeric; symbolic; compilation; embedded system; real-time",
        "released": 2020,
        "link": "https://doi.org/10.4173/mic.2020.4.1"
    },
    {
        "title": "Data management and visualization of coastal railway in wireless sensor network monitoring system",
        "abstract": "With the continuous progress of industrialization in our country, machinery and equipment are developing to higher frequency and larger scale. How to automate and monitor is the key parts which are widely distributed and in bad working environment in machinery and equipment. Therefore, the realization of scientific life prediction and fault diagnosis is an important issue, which is widely concerned by the industry at present. This paper studies the fusion method of equipment information data and geographic information data in the wireless sensor network (WSN), and realizes the fusion of WSN sensing data based on the proposed development environment. Then, we establish the database model and parses the data file. In addition, we realize data visualization in web environment based on the Silverlight module. Finally, the visual part of the coastal railway monitoring WSN system established in this paper is tested by function test and performance test.",
        "keywords": "Wireless sensor network; monitoring system; data management and visualization",
        "released": 2020,
        "link": "https://doi.org/10.2112/JCR-SI110-052.1"
    },
    {
        "title": "Context-based support to enhance developers’ learning of software security",
        "abstract": "Software security is an ongoing problem, largely due to a lack of security knowledge among software developers from diverse backgrounds. To counter this, security experts are attempting to offer a broad range of knowledge resources to enlighten developers about increasing cybersecurity threats. Unfortunately, the abundance of knowledge resources does not seem to have much of an impact on reducing the issue of software security. The ineffective teaching and learning approaches for software security have created difficulties for developers in learning security knowledge. This research employs a four-cycle of Design Science Research Methodology (DSRM) to integrate necessary elements in the development of a context-based learning system for security education and learning. The final artifact is an ontology-based web application that facilitates a contextualized learning process by providing security knowledge through contextual software cases. Through evaluation in pedagogical and software development environments, it is proven to contribute a viable solution to the problem domain. While these results are positive, the innovative context-based artifact benefits not only the domain of software engineering but also other educational fields, such as information security and computer security.",
        "keywords": "software security; security education; knowledge management; context-based; design science",
        "released": 2023,
        "link": "https://doi.org/10.3390/educsci13070631"
    },
    {
        "title": "Pattern-based translation of BPMN process models to BPEL web services",
        "abstract": "The business process modeling notation (BPMN) is a graph-oriented language primarily targeted at domain analysts and supported by many modeling tools. The business process execution language for Web services (BPEL) on the other hand is a mainly block-structured language targeted at software developers and supported by several execution platforms. Translating BPMN models into BPEL code is a necessary step towards standards-based business process development environments. This translation is challenging since BPMN and BPEL represent two fundamentally different classes of languages. Existing BPMN-to-BPEL translations rely on the identification of block-structured patterns in BPMN models that are mapped onto structured BPEL constructs. This article advances the state of the art in BPMN-to-BPEL translation by defining methods for identifying not only perfectly block-structured fragments in BPMN models, but quasi-structured fragments that can be turned into perfectly structured ones and flow-based acyclic fragments that can be mapped onto a combination of structured constructs and control links. Beyond its direct relevance in the context of BPMN and BPEL, this article addresses issues that arise generally when translating between graph-oriented and block-structured flow definition languages.",
        "keywords": "BPMN; BPEL; business process modeling; Web services",
        "released": 2008,
        "link": "https://doi.org/10.4018/jwsr.2008010103"
    },
    {
        "title": "A systematic mapping study of mobile application testing techniques",
        "abstract": "The importance of mobile application specific testing techniques and methods has been attracting much attention of software engineers over the past few years. This is due to the fact that mobile applications are different than traditional web and desktop applications, and more and more they are moving to being used in critical domains. Mobile applications require a different approach to application quality and dependability and require an effective testing approach to build high quality and more reliable software. We performed a systematic mapping study to categorize and to structure the research evidence that has been published in the area of mobile application testing techniques and challenges that they have reported. Seventy nine (79) empirical studies are mapped to a classification schema. Several research gaps are identified and specific key testing issues for practitioners are identified: there is a need for eliciting testing requirements early during development process; the need to conduct research in real-world development environments; specific testing techniques targeting application life-cycle conformance and mobile services testing; and comparative studies for security and usability testing. (C) 2016 Elsevier Inc. All rights reserved.",
        "keywords": "Systematic mapping; Mobile application testing; Software testing",
        "released": 2016,
        "link": "https://doi.org/10.1016/j.jss.2016.03.065"
    },
    {
        "title": "Multi-model driven collaborative development platform for service-oriented e-business systems",
        "abstract": "Enterprises need to be responsive to meet dynamic businesses and requirements. Service-oriented architecture and Web services can improve e-Business systems to be integrated and flexible. This paper proposes a multi-model driven collaborative development platform for service-oriented e-Business systems. The platform provides engineers/consultants with three views, i.e., business view, process view, and service view to support service-oriented software engineering, top-down business design and bottom-up service composite and development. The views are kept consistent through model-driven translation mechanisms. The platform employs three models, i.e., service meta-model, process model and business model to implement the translation. On the platform, business and technical consultants/engineers can use the views designated for their roles to collaborate for a service-oriented e-Business system based at the distributed sites of, e.g., IT vendors and their clients. The collaboration is featured with visual development and rapid demonstration. Comparingly, most of the mainstream development environments focus on object-oriented application development other than service-oriented business process modeling. The platform has been developed and deployed in an innovation centre to be evaluated by visiting customers. (C) 2007 Elsevier Ltd. All rights reserved.",
        "keywords": "service-oriented architecture; model-driven architecture; service meta-model; service-oriented software engineering",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.aei.2007.09.005"
    },
    {
        "title": "Real-time simulator and offline/online closed-loop test bed for power system modeling and development",
        "abstract": "The high complexity of modern power systems forces engineers to design, implement, and test different equipment, precisely. The real-time simulator is one of the best tools for a researcher to test and verify complex power systems in the laboratories. Based on this issue, they can study power system control methods as well as the effects of various models in the partially simulated system. This paper presents a low-cost and versatile power system simulator that is applicable for a wide range of real-time hardware in the loop (RT-HIL) simulations of power systems. To implement the proposed closed-loop simulator, all the essential parts of a power plant, including generator, turbine-governor, excitation system (automatic voltage regulator (AVR), and power system stabilizer (PSS)), grid interconnection and loads are implemented in MATLAB/Simulink environment. To achieve fast and reliable measurement functions, dedicated programs are developed in the LabVIEW real-time and related FPGA programming environments. The data transfer between the MATLAB/Simulink model and LABVIEW and its real-time hardware (PXIe-8133) is accomplished by building a *.dll file as an inbuilt function. A HIL simulation of the excitation system is implemented using a real excitation cabinet to test system capability and also investigate system reaction against disturbances. The performance of the proposed simulator is investigated under various scenarios such as excitation voltage set-point change, the step response of automatic voltage regulator, network three-phase fault, torque step, and step load changes. The close correspondence between HIL and MATLAB/Simulink simulations reveals the validity of the implemented model for the excitation system as well as the effectiveness, flexibility, versatility, applicability, and usefulness of the offline/online closed-loop simulator for power system modeling and development.",
        "keywords": "Excitation system; field-programmable gate arrays (FPGAs); hardware-in-the-loop (HIL); LabVIEW RT; MATLAB/Simulink; Power system analysis; Real-time simulator",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.ijepes.2020.106203"
    },
    {
        "title": "E-profiling r&d involvement/earnings of researchers: A G2E tool for performance management at CSIR-NML",
        "abstract": "Purpose Council of Scientific and Industrial Research (CSIR)-National Metallurgical Laboratory (CSIR-NML) has launched a number of initiatives in different perspectives of e-Government. The “Mandays-Involvement” website was implemented by the laboratory in Government-to-Employee (G2E) perspective i.e. facilitating its research and development (R&D) manpower by providing data with respect to their own performance parameters through a single window. The development and implementation of the website had two major objectives : (1) to provide a system to the researchers for tracking and improving their own performance with respect to mandays and external cash flow generation and (2) to equip the management with a tool to enhance the organizational performance and enable optimum employee utilization. Design/methodology/approach Software Development Life Cycle approach was followed for the web-based system development and iterative model was used. Open source web development tools i.e. Php, My-Sql and CSS were used for the system development. Findings The new system helped in detailed profiling of current and future assignment of the researchers so as to have a check over preference-based allocation of work and providing equal opportunities of work to all. Research limitations/implications The percentage average mandays utilization was stabilized after the implementation of the website. The percentage under and over engagements of researchers in R&D projects was controlled and reduced. Practical implications The system outputs are utilized for R&D Team formation, Project approvals and Annual Performance Evaluations. Originality/value This new information system acts as a decision support system that helps the management to align its organizational policies toward the Future Research Projects and R&D manpower.",
        "keywords": "R&D performance; Mandays involvement; G2E; R&D manpower planning; HRIS; Web-based information management system",
        "released": 2021,
        "link": "https://doi.org/10.1108/IJPPM-09-2019-0437"
    },
    {
        "title": "Learners’ technological acceptance of VR content development: A sequential 3-part use case study of diverse post-secondary students",
        "abstract": "Web-based virtual reality (VR) development tools are in ubiquitous use by software developers, and now, university (undergraduate) students, to move beyond using, to creating new and energizing VR content. Web-based VR (WebVR), among other libraries and frameworks, have risen as a low-cost platform for users to create rich and intuitive VR content and applications. However, the success of WebVR as an instructional tool relies on post-secondary students technological acceptance (TA), the intersectionality of a user’s perceived utility (PU) and perceived ease of use (PEOU, or convenience) with said technological tool. Yet, there is a dearth of exploratory studies of students’ experiences with the AR/VR development technologies to infer their TA. To ascertain the viability of WebVR tools for software engineering undergraduates in the classroom, this paper presents a 3-case contextual investigation of 38 undergraduate students tasked with creating VR content. In each use case, students were provided increasing freedom in their VR content development parameters. Results indicated that students demonstrated elements of technological acceptance in their selection of webVR and other platforms, and not only successfully creating rich and robust VR content (PU), but also executing these projects in a short period (PEOU). Other positive externalities observed were students exhibitions of soft skills (e.g. creativity, critical thinking) and different modes of demonstrating coding knowledge, which suggest further study. Discussed are the lessons learned from the WebVR and VR/AR interventions and recommendations for WebVR instruction. This work may be helpful for both learners and teachers using VR/AR in selecting, designing, and developing coursework materials, tools, and libraries.",
        "keywords": "A-frame; computer science course design; soft skills; technology acceptance model; undergraduate education; web-based virtual reality",
        "released": 2019,
        "link": "https://doi.org/10.1142/S1793351X19400154"
    },
    {
        "title": "Design of tennis mobile teaching assistant system based on ordinary differential equations",
        "abstract": "Tennis is a sport, which is mainly composed of many parts, such as mobile devices, software and hardware. With the rapid development of today’s time, people’s living standards are getting higher and higher, the traditional teaching mode can no longer meet the user’s needs well, so teachers need to help students improve their overall quality with the help of more advanced modern online teaching methods, therefore, the design of a practical and flexible mobile assistance system is necessary for universities nowadays. This paper mainly introduces the tennis auxiliary system based on ordinary differential equations, which is based on the database, and it has the functions of data collection, processing and display. This software is designed and implemented by using javaSwing technology and solidworks development tools. After that, SSH framework is used to create the application and backend management interface for later maintenance and update, so that the management can understand the students’ situation at any time and make corresponding decision analysis to better serve the users and improve the quality and level of tennis teaching.",
        "keywords": "Ordinary differential equations; Tennis; Teaching aid systems",
        "released": 2022,
        "link": "https://doi.org/10.2478/amns.2022.2.0152"
    },
    {
        "title": "Universal point cloud viewer based on unreal video game engine",
        "abstract": "Point clouds are an essential tool for documenting archaeological and architectural heritage, since they are the initial product that can be obtained from a laser scanner or photogrammetry survey. Despite being a very basic and discretized source of information containing only coloured points, they can be used as a first approach to the surveyed model through a virtual visualization or interactive tour. The management and visualization of point clouds is not an easy task, since it involves the representation of millions of points and installing specific viewers is mandatory. These special viewers are developed by each laser scanner manufacturer to visualize its own point cloud file format. Handling with them can result unfriendly and non-intuitive for a non-specialized public. Therefore, the main objective of this work is to develop a working methodology based on Unreal Engine, a video game development environment, to create self-executing visualization modules that can be shared with any user. These modules will allow simple and fluid navigation of point clouds without the need of installing any additional software or having specific knowledge. These self-executing modules will allow any kind of viewer, specialised or not, to visualize the point cloud in a first-person way or from a bird’s eye view. The exposed methodology has been tested using a survey of the Oliva Castle in Valencia (Spain), which is classified as a cultural interest asset. In order to illustrate the results of this research, some images of the visualization environment will be shown and the download link of the display module corresponding to the Oliva Castle will be provided.",
        "keywords": "Point Cloud; Unreal; Viewer",
        "released": 2023,
        "link": "https://doi.org/10.20365/disegnarecon.30.2023.13"
    },
    {
        "title": "The big data system, components, tools, and technologies: A survey",
        "abstract": "The traditional databases are not capable of handling unstructured data and high volumes of real-time datasets. Diverse datasets are unstructured lead to big data, and it is laborious to store, manage, process, analyze, visualize, and extract the useful insights from these datasets using traditional database approaches. However, many technical aspects exist in refining large heterogeneous datasets in the trend of big data. This paper aims to present a generalized view of complete big data system which includes several stages and key components of each stage in processing the big data. In particular, we compare and contrast various distributed file systems and MapReduce-supported NoSQL databases concerning certain parameters in data management process. Further, we present distinct distributed/cloud-based machine learning (ML) tools that play a key role to design, develop and deploy data models. The paper investigates case studies on distributed ML tools such as Mahout, Spark MLlib, and FlinkML. Further, we classify analytics based on the type of data, domain, and application. We distinguish various visualization tools pertaining three parameters: functionality, analysis capabilities, and supported development environment. Furthermore, we systematically investigate big data tools and technologies (Hadoop 3.0, Spark 2.3) including distributed/cloud-based stream processing tools in a comparative approach. Moreover, we discuss functionalities of several SQL Query tools on Hadoop based on 10 parameters. Finally, We present some critical points relevant to research directions and opportunities according to the current trend of big data. Investigating infrastructure tools for big data with recent developments provides a better understanding that how different tools and technologies apply to solve real-life applications.",
        "keywords": "Big data; Components of big data system; Distributed file systems; NoSQL databases; Visualization; SQL Query tools; Data analytics",
        "released": 2019,
        "link": "https://doi.org/10.1007/s10115-018-1248-0"
    },
    {
        "title": "Deep learning for heterogeneous medical data analysis",
        "abstract": "At present, how to make use of massive medical information resources to provide scientific decision-making for the diagnosis and treatment of diseases, summarize the curative effect of various treatment schemes, and better serve the decision-making management, medical treatment, and scientific research, has drawn more and more attention of researchers. Deep learning, as the focus of most concern by both academia and industry, has been effectively applied in many fields and has outperformed most of the machine learning methods. Under this background, deep learning based medical data analysis emerged. In this survey, we focus on reviewing and then categorizing the current development. Firstly, we fully discuss the scope, characteristic and structure of the heterogeneous medical data. Afterward and primarily, the main deep learning models involved in medical data analysis, including their variants and various hybrid models, as well as main tasks in medical data analysis are all analyzed and reviewed in a series of typical cases respectively. Finally, we provide a brief introduction to certain useful online resources of deep learning development tools.",
        "keywords": "Medical data analysis; Deep learning; Survey",
        "released": 2020,
        "link": "https://doi.org/10.1007/s11280-019-00764-z"
    },
    {
        "title": "Active distance learning of embedded systems",
        "abstract": "The move from face-to-face to distance learning poses a challenge for courses that rely on hands-on experience such as embedded systems. In this course, students need to work with hardware and software to achieve various learning objectives. For full advantage, the hands-on experience should be aligned with the acquisition of related concepts and procedural knowledge. The alignment of conceptual learning with hands-on experience is a big challenge, in general, and for distance learning, in particular. This article describes how different learning technologies can be integrated to achieve such alignment for embedded systems in a distance learning mode. A framework for active, lecture-free learning was established using a learning management system, YouTube, various web resources, a hardware kit, and a software development environment. The learning activities were implemented as ungraded quizzes on Moodle with different types of questions. These include review questions, conceptual questions, procedural questions, brainstorming questions, code analysis questions, and code creation questions. Our students used the provided hardware kit and the software development environment to complete the learning activities throughout the semester without listening to any live or recorded lecture from our end. This instructional design was evaluated by analyzing learning data generated by Moodle as well as self-report data. The results show high student engagement and positive perceptions of the course content and the learning method. We believe that the proposed pedagogical framework of this design is of general value and can be adopted in other engineering courses with similar requirements of hands-on experience in distance learning.",
        "keywords": "Computer aided instruction; Embedded systems; Hardware; Software; Education; Pandemics; Licenses; Distance learning; embedded systems; learning management systems; student engagement; lecture-free instruction; COVID-19",
        "released": 2021,
        "link": "https://doi.org/10.1109/ACCESS.2021.3065248"
    },
    {
        "title": "A smart home system based on embedded technology and face recognition technology",
        "abstract": "A smart home module program is designed based on Linux, in the ARM11 embedded development environment, and by using the UP-Magic 6410 development board. A touch-screen graphical interface for human-computer interaction is designed using QT. The sensor module is integrated into the underlying QT program, and the actuator module program performs external calls via QT, thus a complete set of smart home terminal is constituted. The function of remote video monitoring and the function of displaying module statuses on webpage are realized through connecting video cameras to the development board as well as constructing GoAhead web server and Spcaserv video server. A face recognition program is designed using MATLAB, a database for storing the face information for different users is designed, and they are connected to the smart home terminal. Based on face recognition, the identity information for the current user and the desired temperature are displayed, and the speed of the DC motor is changed. This system can also realize sound and light alarm. Alarm for harmful gas can be raised under the help of smoke sensor. The photosensitive sound switch module is used for simulating the voice control device so as to facilitate the user’s control of the DC motor switch. Infrared irradiation sensor is used for simulating the identity-card identification equipment, so as to limit the system users. Through the coordination of the above parts, a complete smart home system which is composed of a computer, embedded development environment, sensors, actuators and ancillary devices is finally established.",
        "keywords": "Smart home; embedded development; sensor; embedded web server; video monitoring; face recognition",
        "released": 2017,
        "link": "https://doi.org/10.1080/10798587.2016.1217634"
    },
    {
        "title": "Logistics coordination based on inventory management and transportation planning by third-party logistics (3PL)",
        "abstract": "Currently, there is still a strong trend in research and in the market connected with the role of 3PL (third-party logistics) companies and the trend of developing and creating coordination in logistics networks. The most important issue for the following paper is the examination of the 3PL enterprise, which is able to create the demand forecasts to gain the functions of logistics coordination in the fields of inventory management and transportation planning. This research paper focuses on showing the demand forecasting tool results for 29 chosen distribution networks (in this paper, the traditional forecasting methods were used based on time series exponential smoothing, ARIMA, machine learning and neural-network-based methods, created in the R programming environment). In the next steps, the forecasting results were compared in the conditions of transportation planning and inventory management (in the conditions of “future” dynamic ABC analysis). The forecasting function supports the inventory management and transportation planning activity of 3PL as a key element of logistics coordination in distribution networks. The proper way to transfer the results of forecasting to an ABC analysis and transportation planning concept is to create a cloud-based system supported by data from the WMS (warehouse management system), while providing the possibility of results visualization by using some BI (business intelligence) solutions or different tools to create managerial information dashboards. Currently, one of the most efficient models connected with logistics coordination is the centralized network with 3PL responsible for planning and executing logistics processes with the creation of additional value. As such, 3PL is able to create a similar forecast for different types of DN and with different aggregations (per SKU or per recipient). It could support coordination in DN from the point of view of inventory management and transportation planning.",
        "keywords": "3PL; distribution network; demand forecasting; inventory management; logistics service provider; R programming; transportation planning",
        "released": 2022,
        "link": "https://doi.org/10.3390/su14138134"
    },
    {
        "title": "Digital psychological platform for mass web-surveys",
        "abstract": "Web-surveys are one of the most popular forms of primary data collection used for various researches. However, mass surveys involve some challenges. It is required to consider different platforms and browsers, as well as different data transfer rates using connections in different regions of the country. Ensuring guaranteed data delivery in these conditions should determine the right choice of technologies for implementing web-surveys. The paper describes the solution to transfer a questionnaire to the client side in the form of an archive. This technological solution ensures independence from the data transfer rate and the stability of the communication connection with significant survey filling time. The conducted survey benefited the service of education psychologists under the federal Ministry of Education. School psychologists consciously took part in the survey, realizing the importance of their opinion for organizing and improving their professional activities. The desire to answer open-ended questions in detail created a part of the answers in the dataset, where there were several sentences about different aspects of professional activity. An important challenge of the problem is the Russian language, for which there are not as many tools as for the languages more widespread in the world. The survey involved 20,443 school psychologists from all regions of the Russian Federation, both from urban and rural areas. The answers did not contain spam, runaround answers, and so on as evidenced by the average response time. For the surveys, an authoring development tool DigitalPsyTools.ru was used. Dataset: http://dx.doi.org/10.17632/m32kz6jjcx.1 Dataset License: CC-BY-4.0.",
        "keywords": "mass web-surveys; LDA; text preprocessing; open questions",
        "released": 2020,
        "link": "https://doi.org/10.3390/data5040095"
    },
    {
        "title": "Effectiveness and safety of different doses of pioglitazone in psoriasis: A meta-analysis of randomized controlled trials",
        "abstract": "Background: Pioglitazone may be beneficial in the treatment of psoriasis. However, based on the effectiveness and safety considerations, it has not been widely used. To fully evaluate the strength of evidence supporting psoriasis treatment with pioglitazone, we conducted a meta-analysis of existing published studies. Methods: PubMed, Ovid, Cochrane Library, Google Scholar, and Web of Science databases were systematically searched before February 2019. Randomized controlled trials (RCTs) of pioglitazone administration compared with placebo, administered to patients with psoriasis for at least 10 weeks, and published in English were included. Quality of the included RCTs was identified by the modified Jadad scale. The quality of evidence for each outcome was evaluated using the GRADEpro Guideline Development Tool online software. Primary outcomes were proportion of patients showing psoriasis area and severity index (PASI) score improvement (>75%) and the mean percent change in PASI score from baseline to the end of treatment. Dichotomous data were analyzed using odds ratios (ORs) corresponding to the 95% confidence interval (CI), whereas continuous variables, expressed as mean and standard deviation, were analyzed using the mean differences (MD) with the 95% CI. Results: Six RCTs were analyzed. Meta-analysis showed that pioglitazone reduced the PASI scores in patients with psoriasis compared with the control group when administered at 30 mg per day (P < 0.001, MD = -3.82, 95% CI = -5.70, -1.93) and at 15 mg per day (P = 0.04, MD = -3.53, 95% CI = -6.86, -0.20). The PASI-75 of the pioglitazone group was significantly higher than that of the control group at 30 mg per day (P < 0.001, OR = 8.30, 95% CI = 3.99, 17.27) and at 15 mg per day (P = 0.03, OR = 2.96, 95% CI = 1.08, 8.06). No statistically significant differences in total adverse events were observed between the groups. There were no significant differences in common adverse reactions such as weight gain and elevated liver enzymes between the two pioglitazone groups. Conclusions: Use of pioglitazone in the current treatment of psoriasis is beneficial. The therapeutic effect of the daily 30 mg dose may be greater than that of the 15 mg dose per day with no significant change in the frequency of adverse reactions.",
        "keywords": "Meta-analysis; Psoriasis; Pioglitazone",
        "released": 2020,
        "link": "https://doi.org/10.1097/CM9.0000000000000642"
    },
    {
        "title": "Integrating web map service and building information modeling for location and transportation analysis in green building certification process",
        "abstract": "In green buildings design and construction, evaluating the sustainable effects of site location and transportation to the ecosystem and human life is a critical and difficult task. Works regarding these matters require experience, time, labor, and manual calculations. In recent years, many studies have been conducted to enhance the application of Building Information Modeling (BIM) in green building certifications. However, the application of BIM to site location and transportation analysis is usually considered impractical due to the lack of a powerful map application in present BIM products. The aim of this research is to develop a framework for the integration of BIM and Web Map Service (WMS) technologies for location and transportation analysis in green building certifications. Using Autodesk Revit API and Google Maps API as the development tools, this research converts the integration model into the BIM-integrated plugin in Autodesk Revit. The plugin is used to streamline the certification process of site location and transportation analysis in LEED (Leadership in Energy and Environmental Design), one of the most popular and globally recognized green building standards. (C) 2017 Elsevier B.V. All rights reserved.",
        "keywords": "Green building; Web map service; BIM; LEED; Sustainable sites",
        "released": 2017,
        "link": "https://doi.org/10.1016/j.autcon.2017.01.014"
    },
    {
        "title": "SODAS: Smart open data as a service for improving interconnectivity and data usability",
        "abstract": "In this study, we proposed Smart Open Data as a Service (SODAS) as a new open data platform based on the international standards Data Catalog Vocabulary (DCAT) and Comprehensive Knowledge Archive Network (CKAN) to facilitate the release and sharing of data. We first analyze the five problems in the legacy CKAN and then draw up corresponding solutions through three core strategies: CKAN expansion, DCATv2 support, and extendable DataMap. We then define four components and nine function blocks of SODAS for each core strategy. As a result, SODAS drives Open Data Portal, Open Data Reference Model, DataMap Publisher, and Analytics and Development Environment (ADE) Provisioning for connecting the defined function blocks. We confirm that each function works correctly through the SODAS Web portal, and then we apply SODAS to actual data distribution sites to prove its efficiency and practical use. SODAS is the first open data platform that provides secure interoperability between heterogeneous platforms based on international standards, and it enables domain-free data management with flexible metadata.",
        "keywords": "open data; SODAS; data hub; data distribution; data sharing; open data platform",
        "released": 2023,
        "link": "https://doi.org/10.3390/electronics12051237"
    },
    {
        "title": "SpidersRUs: Creating specialized search engines in multiple languages",
        "abstract": "While small-scale search engines in specific domains and languages are increasingly used by Web users, most existing search engine development tools do not support the development of search engines in languages other than English, cannot be integrated with other applications, or rely on proprietary software. A tool that supports search engine creation in multiple languages is thus highly desired. To study the research issues involved, we review related literature and suggest the criteria for an ideal search tool. We present the design of a toolkit, called SpidersRUs, developed for multilingual search engine creation. The design and implementation of the tool, consisting of a Spider module, an Indexer module, an Index Structure, a Search module, and a Graphical User Interface module, are discussed in detail. A sample user session and a case study on using the tool to develop a medical search engine in Chinese are also presented. The technical issues involved and the lessons learned in the project are then discussed. This study demonstrates that the proposed architecture is feasible in developing search engines easily in different languages such as Chinese, Spanish, Japanese, and Arabic. (c) 2007 Elsevier B.V. All rights reserved.",
        "keywords": "search engine development; multilingual search engines; information retrieval",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.dss.2007.07.006"
    },
    {
        "title": "Ontological approach application to the design of a geospatial experimental database for information support of research in precision agriculture",
        "abstract": "Thanks to the development of information technologies and computing resources, it became possible to obtain and process big data, including geospatial data. Most research in the field of precision farming is interdisciplinary in nature, with experimental field data used by disparate scientific groups. In this connection, it became necessary to develop a unified web-based system for storing, organizing, and exchanging experimental information between researchers. The first step in achieving this goal was to create a geospatial database. Since the system being developed in the future may require extensions, modifications, adjustments, integration into other projects, it seems appropriate to use the ontology to form the database structure. The most popular tools were used as the main tools: the ontology language OWL (Ontology Web Language), the Protege 5.5 development environment. The main initial information obtained in the course of experimental studies carried out at the biopolygon: weather data, agrochemical indicators (sampling of soil and plants with georeferencing), agrophysical parameters (humidity, electrical conductivity), remote sensing data. Based on the results of the analysis of the current state of research in the field of storage and systematization of experimental information in crop production, as well as a survey of ARI employees, a prototype of the database structure was formed based on the ontological approach. Nine parent classes were defined as the foundation: Field, Crop rotation - experience, Agrotechnology, Yield, Meteo, Ground samples, Orthophoto, Calendar, and Dictionary - units of measurement.",
        "keywords": "ontology; precision agriculture; field experiments; biopolygon; OWL; Protege",
        "released": 2022,
        "link": "https://doi.org/10.21638/11701/spbu10.2022.206"
    },
    {
        "title": "Consolidating metabolite identifiers to enable contextual and multi-platform metabolomics data analysis",
        "abstract": "Background: Analysis of data from high-throughput experiments depends on the availability of well-structured data that describe the assayed biomolecules. Procedures for obtaining and organizing such meta-data on genes, transcripts and proteins have been streamlined in many data analysis packages, but are still lacking for metabolites. Chemical identifiers are notoriously incoherent, encompassing a wide range of different referencing schemes with varying scope and coverage. Online chemical databases use multiple types of identifiers in parallel but lack a common primary key for reliable database consolidation. Connecting identifiers of analytes found in experimental data with the identifiers of their parent metabolites in public databases can therefore be very laborious. Results: Here we present a strategy and a software tool for integrating metabolite identifiers from local reference libraries and public databases that do not depend on a single common primary identifier. The program constructs groups of interconnected identifiers of analytes and metabolites to obtain a local metabolite-centric SQLite database. The created database can be used to map in-house identifiers and synonyms to external resources such as the KEGG database. New identifiers can be imported and directly integrated with existing data. Queries can be performed in a flexible way, both from the command line and from the statistical programming environment R, to obtain data set tailored identifier mappings. Conclusions: Efficient cross-referencing of metabolite identifiers is a key technology for metabolomics data analysis. We provide a practical and flexible solution to this task and an open-source program, the metabolite masking tool (MetMask), available at http://metmask.sourceforge.net, that implements our ideas.",
        "keywords": "",
        "released": 2010,
        "link": "https://doi.org/10.1186/1471-2105-11-214"
    },
    {
        "title": "Vis-a-vis: Visual exploration of visualization source code evolution",
        "abstract": "Developing an algorithm for a visualization prototype often involves the direct comparison of different development stages and design decisions, and even minor modifications may dramatically affect the results. While existing development tools provide visualizations for gaining general insight into performance and structural aspects of the source code, they neglect the central importance of result images unique to graphical algorithms. In this article, we present a novel approach that enables visualization programmers to simultaneously explore the evolution of their algorithm during the development phase together with its corresponding visual outcomes by providing an automatically updating meta visualization. Our interactive system allows for the direct comparison of all development states on both the visual and the source code level, by providing easy to use navigation and comparison tools. The on-the-fly construction of difference images, source code differences, and a visual representation of the source code structure further enhance the user’s insight into the states’ interconnected changes over time. Our solution is accessible via a web-based interface that provides GPU-accelerated live execution of C++ and GLSL code, as well as supporting a domain-specific programming language for scientific visualization.",
        "keywords": "Data visualization; Visualization; Tools; Software; Pipelines; Task analysis; History; Visualization system and toolkit design; user interfaces; integrating spatial and non-spatial data visualization; software visualization",
        "released": 2021,
        "link": "https://doi.org/10.1109/TVCG.2019.2963651"
    },
    {
        "title": "A WebGIS platform to monitor environmental conditions in ports and their surroundings in south eastern europe",
        "abstract": "The scope of this work is to describe the design and development of a web-based Geographic Information System (GIS) application and highlight its usefulness regarding monitoring and evaluating environmental conditions in several ports and their surroundings in the greater South East Europe (SEE). The system receives inputs and handles two kinds of data that are processed and illustrated through maps and graphs at various temporal and spatial scales in this informational platform. The aforementioned data consists of point measurements from stations operating in the area of SEE ports as well as satellite date sets derived monthly for a period of 10 to 12 years, in terms of sea surface temperature, chlorophyll a, and colored dissolved organic matter (CDOM). The WebGIS platform is based on the client-server model and uses Google Maps API services for data plotting. Advanced designing and development tools and methodologies are used. The available valuable data render the application into a trustful and accurate provider of visual environmental interest information regarding the main ports of southeastern Europe and their surroundings that would operate as a guide for an environmentally sustainable future of ports and sea corridors in SEE.",
        "keywords": "WebGIS; Remote sensing data; Monitoring; Environmental conditions; Seaport; Southeastern Europe",
        "released": 2015,
        "link": "https://doi.org/10.1007/s10661-015-4786-x"
    },
    {
        "title": "TOWARDS TRANSPARENT COMPUTING: CONTENT AUTHORING USING OPEN STANDARDS",
        "abstract": "With today’s rapid developments in digital technologies, technical obsolescence can occur much faster than in the past. Who would have predicted five years ago that Adobe’s Flash would have seen the rapid decline it has experienced as a development environment? Using open, internationally accepted standards for materials development is no absolute guarantee of longevity, but it does increase the likelihood that content will continue to be usable and that, if needed, conversion tools will be available. In this column I will be discussing approaches to the development of electronically delivered language learning materials that I believe are the least likely to face short-term obsolescence. I will be arguing in favor of multilaterally developed, open standards, supported by major industry players and educational standards bodies. Specifically, I will be looking at approaches for delivering learning materials through Web browsers or e-book readers (e-readers), so that created content works seamlessly across devices and platforms.",
        "keywords": "",
        "released": 2014,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000339448900001"
    },
    {
        "title": "Skill-based programming of complex robotic assembly tasks for industrial application",
        "abstract": "In recent years, a paradigm shift is underway as robots leave their typical application field and move into domains that have been untouched by robotic automation. These new kinds of automation systems allow more product variations, smaller life cycles, smaller batch sizes and pave the way from mass production to mass customization. This is due to completely new breed of safe robot technology but also novel ways of setting up new applications like e.g. kinesthetic programming. However, the topic of reducing the programming effort for complex tasks using natural modes of communication is still open. This paper addresses the key developments in this field, shows different ways of programming, and gives relevant use cases in industrial assembly. The technology coverage starts with an online workflow editor called XROB that allows easy-to-use setup of process workflows and related skill parameters. However, in order to reduce the programming effort, a novel way to demonstrate process trajectories by using instrumented hand guided process tools is presented. Finally, the paper gives an overview of a promising approach that allows programming without touching the robot just by demonstrating the process by an expert. The semantic relations between activities executed by the human and robot skills are captured to learn the task sequence of the assembly process. The acquired process knowledge is refined to execute robotic tasks with the help of an interactive graphical user interface (GUI). The system queries the user for feedback, asking for specific information to help the robot complete the task at hand. The given examples show the usability of flexible programming tools in the automation chain and the presented results provide strong evidence of the technological potential in the field.",
        "keywords": "skill-based learning; human-robot cooperation",
        "released": 2019,
        "link": "https://doi.org/10.1007/s00502-019-00741-4"
    },
    {
        "title": "Introducing the next generation of software inspection tools",
        "abstract": "The area of tool support for software inspection has been under active research since the early 1990’s. Although numerous implementations exist and development is still taking place, no tool has achieved a breakthrough. The main reason is that one tool usually demonstrates only one new idea, neglecting other features. A different approach must be taken, and software inspection tools should be seen as integral parts of the development environment. This paper categorises the existing tools into four generations based on the transition from traditional meeting support to asynchronous distributed inspections implemented with web technologies. Based on the analysis of 16 tools and our experiences, we summarize the most important features and add two new aspects to be notified when implementing the next generation of inspection tools for use in modem software development, flexibility and integration. The major focus is on comprehension, and we have taken the first steps at achieving this.",
        "keywords": "",
        "released": 2004,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000189505000017"
    },
    {
        "title": "RadNotes: A novel soft ware development tool for radiology education",
        "abstract": "RadNotes is a novel software development tool that enables physicians to develop teaching materials incorporating text and images in an intelligent, highly usable format, Projects undertaken in the RadNotes environment require neither programming expertise nor the assistance of a software engineer, The first of these projects, Thoracic Imaging, integrates image teaching files, concise disease and topic summaries, references, and flash card quizzes into a single program designed to provide an overview of chest radiology. RadNotes is intended to support the academic goals of teaching radiologists by enabling authors to create, edit, and electronically distribute image-oriented presentations, RadNotes also supports the educational goals of physicians who wish to quickly review selected imaging topics, as well as to develop a visual vocabulary of corresponding radiologic anatomy and pathologic conditions, Although Thoracic Imaging was developed with the aim of introducing chest radiology to residents, RadNotes can be used to develop tutorials and image-based tests for all levels; create corresponding World Wide Web sites; and organize notes, images, and references for individual use.",
        "keywords": "computers, educational aid; computers, multimedia; education",
        "released": 1997,
        "link": "https://doi.org/10.1148/radiographics.17.3.9153710"
    },
    {
        "title": "Useware engineering: A methodology for the development of user-friendly interfaces",
        "abstract": "Purpose - The development time for user interfaces is drastically reduced by today’s shorter product life cycles. Generally, at the end of the development phase there is not enough time or money left to enhance the usability of software. After the user interface has been developed, the users are left alone to handle any problems with the interface. Systematic development using engineering processes can help to overcome these usability problems. Engineering processes identify and consider clearly defined requirements during the development phase. This results in more usable products and a growing recognition of the importance of engineering processes. This article aims to introduce a structured Useware engineering process that increases the effectiveness and efficiency of user interface development. Design/methodology/approach - This article introduces a structured Useware engineering process, the application of which enhances user orientation which leads to higher acceptance and more practical user interfaces. Findings - The article finds that the current process is supported by various expert development tools for the production of user interfaces; these tools ensure a systematic and continuous data transferability from one phase to the next. The process has already provided remarkable results in the area of task orientation and usability within several different industrial sectors, e.g. medicine, manufacturing industry and process engineering. Further possibilities to apply this process are software for online catolog planning or lending and return machines. Originality/value - This article introduces a structured engineering process that increases the effectiveness and efficiency of user interface development which has the possibility of use in online catolog planning or lending and return machines.",
        "keywords": "computer software; lending services; user interfaces; user studies",
        "released": 2008,
        "link": "https://doi.org/10.1108/07378830810857852"
    },
    {
        "title": "Investigation of developer’s perceptions in XML schema development using textual and visual tool types",
        "abstract": "This paper analyses the influence of different tool types (visual or textual) on a developer’s perception of efficiency during XML Schema development. We conducted a controlled experiment that focused on discovering which XML Schema development tool type enables better efficiency and also engenders a friendlier environment for developers while developing XML Schemas. The experiment was conducted with 240 participants and divided into two practical parts (visually developing an XML Schema and manually using a mark-up language intelligent textual editor). After the experiment, the participants’ opinions were gathered via a web survey. In the survey, a technology acceptance model (TAM) was used as the basis for constructing the measurement items in order to answer the following questions: (1) which tool type is preferred and perceived as better, and (2) which variables influence the user’s perceptions and decisions. In this study, we searched for an optimal way of building XML Schemas. The general tendency of most participants was towards a visual tool, suggesting that visual support is perceived as more useful, and can create better results with less effort.",
        "keywords": "XML Schemas; XML documents; XML supporting tools",
        "released": 2014,
        "link": "https://doi.org/10.1142/S021819401450017X"
    },
    {
        "title": "Knowledge of professional healthcare providers about sickle cell disease: Impact of a distance education course",
        "abstract": "Objective: To assess the impact of the distance education course “Sickle Cell Disease: Primary Health Care Line” on knowledge acquisition of professional healthcare providers. Methods: A cross-sectional study was conducted with a quantitative approach at the Educational and Support Center for Hemoglobinopathies (Cehmob-MG), state of Minas Gerais, Brazil, in 2016. One hundred and fifty-three out of 300 professional healthcare providers were invited to participate in the proposed distance course. Of the participating professional healthcare providers, 72 (47%) successfully concluded the course (Group A), whereas 81 (53%) did not complete their course assignments and did not meet the minimum requirements for regular attendance (Group B). Knowledge acquisition was assessed with the Knowledge of Sickle Cell Disease Instrument, DFConhecimento, applied using the web tool eSurv. Univariate analysis by Poisson regression was employed to assess the influence of sociodemographic variables on the DFConhecimento score and to select variables to compose the initial multivariate regression model (p-value < 0.20). The analysis was performed in the statistical programming environment R. Results: The average score was 9.76 for Group A and 6.54 for Group B. The two groups were considered statistically different (p-value <0.05) for all items with the proportion of correct items being greater in Group A. Professional healthcare providers who concluded the course had a significantly higher DFConhecimento score (45%) when compared to those who did not successfully conclude the course. Conclusion: Participation in a distance education course on sickle cell disease had a positive impact on the acquisition of knowledge about the disease by professional healthcare providers. (C) 2018 Associacao Brasileira de Hematologia, Hemoterapia e Terapia Celular. Published by Elsevier Editora Ltda.",
        "keywords": "Sickle cell disease; Healthcare provider; Knowledge; Health education",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.htct.2018.06.004"
    },
    {
        "title": "Extending computational thinking into information and communication technology literacy measurement: Gender and grade issues",
        "abstract": "As Information and Communication Technology (ICT) literacy education has recently shifted to fostering computing thinking ability as well as ICT use, many countries are conducting research on national curriculum and evaluation. In this study, we measured Korean students’ ICT literacy levels by using the national measurement tool that assesses abilities of the IT (Information Technology) area and the CT (Computational Thinking) area. A research team revised an existing ICT literacy assessment tool for the IT test and developed a new CT test environment in which students could perform actual coding through a web-based programming tool such as Scratch. Additionally, after assessing ICT literacy levels, differences in ICT literacy levels by gender and grade were analyzed to provide evidence for national education policies. Approximately 23,000 elementary and middle school students participated in the 2018 national assessment of ICT literacy, accounting for 1% of the national population of students. The findings demonstrated that female students had higher literacy levels in most sub-factors of IT and CT areas. Additionally, in the areas of strengths and weaknesses, the ratio of below-basic achievement among male students was at least two times greater than that of female students. Nonetheless, male students scored higher on CT automation, a coding item that involved problem solving using Scratch. Looking at the difference according to grade level, the level improved as the school year increased in elementary school, but there was no difference in middle school. When analyzing the detailed elements of middle school students, the automation factor of seventh grade students was found to be higher than eighth and ninth grade students. Based on these results, this study discussed some implications for ICT and computing education in elementary and middle schools.",
        "keywords": "ICT literacy; computational thinking; elementary education; secondary education; 21st century abilities",
        "released": 2021,
        "link": "https://doi.org/10.1145/3427596"
    },
    {
        "title": "Research on sinter quality prediction system based on granger causality analysis and stacking integration algorithm",
        "abstract": "Sinter ore quality directly affects the stability of blast furnace production. In terms of both physical and chemical properties, the main indicators of sinter quality are the TFe content, alkalinity, and drum index. By analyzing the massive historical data on the sinter production of a steel company, this study proposes a sinter quality prediction system based on Granger causality analysis and a stacking integration algorithm. First, based on real historical data of sintering production in steel enterprises (including coal gas pressure, ignition temperature, combustion air pressure, etc.), data preprocessing of raw data was carried out using a combination of feature engineering and the sintering process. Second, Pearson correlation analysis, Spearman correlation analysis, and Granger causality analysis were used to screen out the characteristic parameters with a strong influence on the target variable of sinter quality (drum Index, TFe, alkalinity). Third, a prediction model for sinter quality parameters was developed using a stacking integration algorithm pair for training. Finally, a program development tool was used to realize the establishment and online operation of a sinter ore quality prediction system. The test results showed that the predicted goodness of fit of the model for the TFe content, alkalinity (R), and drum index were 0.942, 0.958, and 0.987, respectively, and the model calculation time met the actual production requirements. By establishing a suitable model and running the program online, the real-time prediction of the main indicators of sinter quality was realized to guide production promptly.",
        "keywords": "sintering quality; Granger causality analysis; stacking integration algorithm",
        "released": 2023,
        "link": "https://doi.org/10.3390/met13020419"
    },
    {
        "title": "An integrated approach to rapid product development for embedded automotive control systems",
        "abstract": "This paper describes ASCET-SD, a development environment for embedded control systems. Using a new approach, involving flexible modeling, automatic code generation, and offline as well as online simulation, ASCET-SD supports every step of the development process. After a discussion of the development process for embedded control systems, the different kinds of modeling supported by ASCET-SD are presented in detail. The novel features are the support for several modeling philosophies, controller refinement and optimization adapted for use in the embedded control design. The next main point described in the paper is the automatic code generation supported by ASCET-SD, which closes the time-consuming and expensive gap between flexible algorithm development, prototyping and series production. Finally, an overview is given of the whole experimentation environment, the experimental hardware, the database management and the documentation generator. (C) 1998 Elsevier Science Ltd. Ail rights reserved.",
        "keywords": "ASCET-SD; automotive control; database; code generation; code optimization; embedded systems; hardware-in-the-loop (HIL); modeling; operating systems; quantization; simulation",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0967-0661(98)00048-3"
    },
    {
        "title": "A multi-agent based approach to dynamic scheduling with flexible processing capabilities",
        "abstract": "A multi-agent based system is proposed to simultaneous scheduling of flexible machine groups and material handling system working under a manufacturing dynamic environment. The proposed model is designed by means of methodology and programmed in agent based systems development environment. Each agent in the model is autonomous and has an ability to cooperate and negotiate with the other agents in the system. Due to these abilities of agents, the structure of the system is more suitable to handle dynamic events. The proposed dynamic scheduling system is tested on several test problems the literature and the results are quite satisfactory because it generates effective schedules for both dynamic cases in the real time and static problem sets. Although the model is designed as an online method and has a dynamic structure, obtained schedule performance parameters are very close to those obtained from offline optimization based algorithms.",
        "keywords": "Multi-agent systems; Dynamic scheduling; Material handling; Machine flexibility",
        "released": 2017,
        "link": "https://doi.org/10.1007/s10845-015-1069-x"
    },
    {
        "title": "AN AUTOMATED GRADING FRAMEWORK FOR THE MOBILE DEVELOPMENT PROGRAMMING LANGUAGE KOTLIN",
        "abstract": "With the recent rise of the Kotlin programming language as the main contender for Android mobile development, very few courses in Higher Education Institutions exist which incorporate Kotlin as one of the main languages. In addition, various online course platforms which offer learning Kotlin are still very low in number, and the ones that do exist are expensive. In this paper, an e-learning framework for the Kotlin programming language is presented, supporting automatic grading of given assessments. This framework is aimed at students who already have basic knowledge of Java (or similar) programming languages, and want to switch to mobile development. The solution focuses on the development of an interactive course in Kotlin. Furthermore, to compare our solution with commercially available ones, we point out the disadvantages of currently available Kotlin courses, such as the level of previous knowledge needed, or a need for a specific development environment.",
        "keywords": "autograders; e-Learning; Java; Kotlin; mobile development",
        "released": 2023,
        "link": "https://doi.org/10.24874/IJQR17.02-01"
    },
    {
        "title": "Potential antiviral activities of chrysin against hepatitis b virus",
        "abstract": "Background Interferon and nucleos(t)ide analogues are current therapeutic treatments for chronic Hepatitis B virus (HBV) infection with the limitations of a functional cure. Chrysin (5, 7-dihydroxyflavone) is a natural flavonoid, known for its antiviral and hepatoprotective activities. However, its anti-HBV activity is unexplored.Methods In the present study, the anti-hepatitis B activity of chrysin was investigated using the in vitro experimental cell culture model, HepG2 cells. In silico studies were performed where chrysin and lamivudine (used here as a positive control) were docked with high mobility group box 1 protein (HMGB1). For the in vitro studies, wild type HBV genome construct (pHBV 1.3X) was transiently transfected in HepG2. In culture supernatant samples, HBV surface antigen (HBsAg) and Hepatitis B e antigen (HBeAg) were measured by enzyme-linked immunosorbent assay (ELISA). Secreted HBV DNA and intracellular covalently closed circular DNA (cccDNA) were measured by SYBR green real-time PCR. The 3D crystal structure of HMGB1 (1AAB) protein was developed and docked with the chrysin and lamivudine. In silico drug-likeness, Absorption, Distribution, Metabolism, Excretion and Toxicity (ADMET) properties of finest ligands were performed by using SwissADME and admetSAR web servers.Results Data showed that chrysin significantly decreases HBeAg, HBsAg secretion, supernatant HBV DNA and cccDNA, in a dose dependent manner. The docking studies demonstrated HMGB1 as an important target for chrysin as compared to lamivudine. Chrysin revealed high binding affinity and formed a firm kissing complex with HMGB1 ( increment delta G = - 5.7 kcal/mol), as compared to lamivudine ( increment delta G = - 4.3 kcal/mol), which might be responsible for its antiviral activity.Conclusions The outcome of our study establishes chrysin as a new antiviral against HBV infection. However, using chrysin to treat chronic HBV disease needs further endorsement and optimization by in vivo studies in animal models.",
        "keywords": "Hepatitis B virus; Chrysin; CccDNA; HMGB1; In silico",
        "released": 2023,
        "link": "https://doi.org/10.1186/s13099-023-00531-6"
    },
    {
        "title": "The effectiveness of TDF versus ETV on incidence of HCC in CHB patients: A meta analysis",
        "abstract": "BackgroundIt has been proved that nucleos(t) ide analogues (NAs) therapy could improve underlying liver disease and reduce the incidence of hepatitis B virus (HBV)-related hepatocellular carcinoma (HCC). However, the difference of effectiveness in reducing HCC occurrence between tenofovir (TDF) and enticavir (ETV), two first-line NAs drugs, is still little known. This meta analysis aims to assess the efficacy in reducing incidence of HCC comparing tenofovir monotherapy with entecavir monotherapy among chronic hepatitis B (CHB) patients by analyzing their long-term clinical outcomes.MethodsDatabases including PubMed, Embase, Cochrane Central Register of Controlled Trial, and ISI Web of Science were fully investigated according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. For the included articles, two of the authors independently extracted and confirmed relevant data. Review Manager software (RevMan 5.3) was using for meta analysis.ResultsSeven articles with 3698 patients were finally included in this research, 1574 in tenofovir group and 2124 in entecavir group. For meta analysis, the incidence of HCC was significantly lower among the tenofovir group than entecavir group [rate ratio (95% CI) of 0.66 (0.49, 0.89), P=0.008], while there was no statistical significance in incidence of death or transplantation [rate ratio (95% CI) of 0.78 (0.55, 1.13), P=0.19], encephalopathy [risk ratio (95% CI) of 0.72 (0.45, 1.13), P=0.15] or variceal bleeding [risk ratio (95% CI) of 0.71 (0.34, 1.50), P=0.37] between the two groups.ConclusionThere is a better effect of tenofovir in reducing HCC incidence than entecavir, which indicates tenofovir should be used more widely while treating chronic hepatitis B patients. However before applying, randomized controlled trial and large prospective cohort study should be performed in the future.",
        "keywords": "Entecavir; Tenofovir; Chronic hepatitis B; Hepatocellular carcinoma; Meta-analysis",
        "released": 2019,
        "link": "https://doi.org/10.1186/s12885-019-5735-9"
    },
    {
        "title": "A combination of native LC-MS approaches for the comprehensive characterization of the antibody-drug conjugate trastuzumab deruxtecan",
        "abstract": "Background: Native mass spectrometry (nMS) approaches appear attractive to complement bottom-up strategies traditionally used in biopharmaceutical industries thanks to their quite straightforward and rapid workflows, especially through online hyphenation of nondenaturing liquid chromatography (LC) to nMS. The present work provides an overview of the state-of-the-art chromatographic tools available for the detailed characterization of monoclonal antibody (mAb) formats, exemplified on the antibody-drug conjugate (ADC) trastuzumab deruxtecan (T-DXd). Methods: T-DXd was first characterized by conventional reversed phase LC (rpLC) and peptide mapping. Couplings of size exclusion chromatography (SEC), cation exchange chromatography (CEX), and hydrophobic interaction chromatography (HIC) to nMS were used to gain further insights into size, hydrophobic, and charge variants of T-DXd and its parental mAb trastuzumab, at intact and middle-up levels. Results: SEC-nMS first offered a direct snapshot of the homogeneous conjugation of T-DXd, with an average drug-to-antibody ratio (DAR) of 8 in agreement with a conjugation on cysteines after reduction of all interchain disulfide bonds. Moreover, SEC-nMS afforded precise identification and quantification of aggregates and fragments. Middle-up level experiments performed after IdeS digestion confirmed that drug conjugation occurs in the Fab region of the mAb, as seen with rpLC. HIC separated two DAR8 species that could not be differentiated by nMS. Although middle-up HIC-nMS proved to be more informative for oxidized forms, the identification of minor variants was still difficult because of poor MS signal quality, showing how the coupling of HIC to nMS remains challenging. Lastly, middle-up CEX-nMS provided accurate determination and localization of post-translational modifications, with several acidic/basic variants within Fab and Fc regions of T-DXd that were also identified by peptide mapping. Conclusions: This study illustrates the strengths and drawbacks of each LC-nMS coupling. By combining SEC-, HIC-, and CEX-nMS, we were able to achieve a comprehensive characterization of T-DXd without extensive sample preparation prior to MS analysis.",
        "keywords": "native mass spectrometry (MS); liquid chromatography (LC); size exclusion chromatography (SEC); cation exchange chro-matography (CEX); hydrophobic interaction chromatography (HIC); higher order structures; biotherapeutics; antibody-drug conjugate (ADC)",
        "released": 2022,
        "link": "https://doi.org/10.31083/j.fbl2710290"
    },
    {
        "title": "Six-degree-of-freedom manipulator wireless control system based on internet of things",
        "abstract": "The robotic arm is an automatic control device that imitates the function of a human arm and accurately recognizes specific points in the three-dimensional space according to received instructions. This article aims to study the wireless control system of six-degree-of-freedom manipulator under the Internet of Things. In this paper, the Arduino material platform is used as the main controller, combined with Arduino IDE to complete the six-degree-of-freedom manipulator and main load control program, and use APPinventor to develop Android applications, and realize the wireless control of the manipulator through Bluetooth mode. In addition, the robot arm Jacobi control algorithm is used to describe the differential mapping relationship between the robot arm motion and the camera image space, and the image is updated online through the calculation method to improve the accuracy of the robot arm light control server. Mark the control of each joint, and then collect these control signals and the position information of the six joints. In order to ensure the richness of the entry signs and the safety of the equipment, the control signal given here is a random signal, and the output signal is six connected position signals.A total of 400 input and output sample sets were used. The first 200 units were used for training and the last 200 were used for testing. The experimental results in this paper show that the performance of the six-degree-of-freedom manipulator based on the Internet of Things is 25% higher than that of the traditional machine, and the stability of the wireless control system is also improved by 18% compared with before.",
        "keywords": "Internet of Things; Six degrees of freedom robotic arm; Wireless control system; Wireless communication; Wireless sensor technology",
        "released": 2021,
        "link": "https://doi.org/10.1007/s12652-021-03360-0"
    },
    {
        "title": "The impact of nucleos(t)ide analogs off-therapy among chronic hepatitis b patients: A systematic review and meta-analysis",
        "abstract": "Background and Aim: Although most chronic hepatitis B (CHB) patients achieve effective virological suppression after receiving long-term nucleos(t)ide analogs (Nucs) therapy, the safety of off-therapy is controversial under the monitor. Methods: We identified studies through searching PubMed, Embase, Cochrane Library, and Web of Science from January 1990 to February 2021. The eligible studies compare the long outcomes between discontinued and continued Nucs treatments groups among CHB patients. This study was conducted to investigate long-term outcomes, including biochemical, serological, and virological outcomes, as well as hepatocellular carcinoma (HCC) development rate between discontinued and maintained Nucs therapy groups among CHB patients. Results: Five eligible studies covering 1,425 patients were selected for meta-analysis. Our result exhibits that patients with Nucs off-treatment have a higher risk of alanine aminotransferase (ALT) flares-up than those who continued Nucs therapy under the monitor (OR = 9.39, 95%CI = 3.87-22.78). Nucs off-therapy patients have a higher virological bound incidence (OR = 617.96, 95%CI = 112.48-3,395.14) and a higher HBV DNA level (OR = 9.39, 95%CI = 3.87-22.78) than those who continued Nucs therapy. There was no statistically significant difference in the risk of hyperbilirubinaemia, hepatic decompensation, and HCC development between both two groups. Patients in Nucs off-therapy group demonstrate a higher HBsAg loss rate than those in the continued group (OR = 7.10, 95%CI = 6.68-13.69). Conclusions: Nucs off-therapy patients may exhibit a higher chance of achieving HBsAg loss than those who continue Nucs therapy. It requires close monitoring after Nucs off-therapy and timely restarting of Nucs therapy when ALT concentrations increase.",
        "keywords": "chronic hepatitis B; off-treatment; maintained; nucleos(t)ide analogs; meta-analysis",
        "released": 2021,
        "link": "https://doi.org/10.3389/fpubh.2021.709220"
    },
    {
        "title": "Accuracy evaluation of dental CBCT and scanned model registration method based on pulp horn mapping surface: An in vitro proof-of-concept",
        "abstract": "Background and aim 3D fusion model of cone-beam computed tomography (CBCT) and oral scanned data can be used for the accurate design of root canal access and guide plates in root canal therapy (RCT). However, the pose accuracy of the dental pulp and crown in data registration has not been investigated, which affects the precise implementation of clinical planning goals. We aimed to establish a novel registration method based on pulp horn mapping surface (PHMSR), to evaluate the accuracy of PHMSR versus traditional methods for crown-pulp registration of CBCT and oral scan data. Materials and methods This vitro study collected 8 groups of oral scanned and CBCT data in which the left mandibular teeth were not missing, No. 35 and No. 36 teeth were selected as the target teeth. The CBCT and scanned model were processed to generate equivalent point clouds. For the PHMSR method, the similarity between the feature directions of the pulp horn and the surface normal vectors of the crown were used to determine the mapping points in the CBCT point cloud that have a great influence on the pulp pose. The small surface with adjustable parameters is reconstructed near the mapping point of the crown, and the new matching point pairs between the point and the mapping surface are searched. The sparse iterative closest point (ICP) algorithm is used to solve the new matching point pairs. Then, in the C + + programming environment with a point cloud library (PCL), the PHMSR, the traditional sparse ICP, ICP, and coherent point drift (CPD) algorithms are used to register the point clouds under two different initial deviations. The root square mean error (RSME) of the crown, crown-pulp orientation deviation (CPOD), and position deviation (CPPD) were calculated to evaluate the registration accuracy. The significance between the groups was tested by a two-tailed paired t-test (p < 0.05). Results The crown RSME values of the sparse ICP method (0.257), the ICP method (0.217), and the CPD method (0.209) were not significantly different from the PHMSR method (0.250). The CPOD and CPPD values of the sparse ICP method (4.089 and 0.133), the ICP method (1.787 and 0.700), and the CPD method (1.665 and 0.718) than for the PHMSR method, which suggests that the accuracy of crown-pulp registration is higher with the PHMSR method. Conclusion Compared with the traditional method, the PHMSR method has a smaller crown-pulp registration accuracy and a clinically acceptable deviation range, these results support the use of PHMSR method instead of the traditional method for clinical planning of root canal therapy.",
        "keywords": "Root canal therapy; Digital registration; CBCT; Scanned model; Crown-pulp pose deviation",
        "released": 2024,
        "link": "https://doi.org/10.1186/s12903-024-04565-3"
    },
    {
        "title": "Real-time target detection system for intelligent vehicles based on multi-source data fusion",
        "abstract": "To improve the identification accuracy of target detection for intelligent vehicles, a real-time target detection system based on the multi-source fusion method is proposed. Based on the ROS melodic software development environment and the NVIDIA Xavier hardware development platform, this system integrates sensing devices such as millimeter-wave radar and camera, and it can realize functions such as real-time target detection and tracking. At first, the image data can be processed by the You Only Look Once v5 network, which can increase the speed and accuracy of identification; secondly, the millimeter-wave radar data are processed to provide a more accurate distance and velocity of the targets. Meanwhile, in order to improve the accuracy of the system, the sensor fusion method is used. The radar point cloud is projected onto the image, then through space-time synchronization, region of interest (ROI) identification, and data association, the target-tracking information is presented. At last, field tests of the system are conducted, the results of which indicate that the system has a more accurate recognition effect and scene adaptation ability in complex scenes.",
        "keywords": "machine vision; millimeter-wave radar; multi-source data fusion; YOLOv5 algorithm; target detection",
        "released": 2023,
        "link": "https://doi.org/10.3390/s23041823"
    },
    {
        "title": "Validation of a development methodology and tool for IoT-based systems through a case study for visually impaired people",
        "abstract": "In this article, we validate the Test-Driven Development Methodology for Internet of Things (IoT)based Systems (TDDM4IoTS) and its companion tool, called Test-Driven Development Tool for IoT-based Systems (TDDT4IoTS). TDDM4IoTS consists of 11 stages, including activities ranging from system requirements gathering to system maintenance. To evaluate the effectiveness of TDDM4IoTS and TDDT4IoTS, in the last four academic years from 2019, System Engineering students have developed several IoT-based systems as part of their training, from the sixth semester (third academic year). N & SIM;awi (phonetically, Gnawi), which is the case study presented herein, is one of them, and intends to assist visually impaired people to move through open environments. N & SIM;awi consists of a device, a mobile application and a web application. The device interacts with the environment and issues alerts to the user whenever it recognizes obstacles in their path. The mobile application targets two user roles: assisted person and caregiver. Assisted people can use the device and log in into a server when they leave home, so that the mobile application identifies and notifies obstacles in their path. All the collected data is gathered into the server, so that caregivers receive notifications and can monitor the location of their assisted people at any place and time. The web application allows caregivers to query and view more extensive information (details of events, trajectories, etc.). TDDM4IoTS has been evaluated regarding both the roles of the project members and the development cycle stages. A survey was used to evaluate the methodology. Out of a total of 47 respondents, 30 had used TDDM4IoTS and 96.66% of them were very satisfied or satisfied, with nobody unsatisfied.",
        "keywords": "Internet of Things (IoT); Development methodology; Test-driven development; Obstacle detection; Obstacle identification; Visually impaired people",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.iot.2023.100900"
    },
    {
        "title": "Real-time photovoltaic energy assessment using a GSM-based smart monitoring system: Addressing the impact of climate change on solar energy estimation software",
        "abstract": "Climate change is altering weather patterns, leading to increased variability in cloud cover, precipitation, and temperature. These changing patterns make it more challenging to accurately forecast solar radiation levels, which directly impact solar energy generation. This study, evaluates the solar energy potential using a real-time GSM-based communication system combined with an off-grid PV power plant in Tehran and Ahvaz, two regions in Iran with contrasting climates. To measure the output voltage, C++ language programming was used in Arduino_IDE programming language was utilized along with an Arduino Nano board, ATmega328P microcontroller, and sensors for measuring current, voltage, temperature, humidity, and time. The recorded data was sent through short message service. The results were compared with simulations from PVsyst, PVGIS, and the Global Solar Atlas. The findings revealed that the PV power production potential in Tehran, which is characterized by a cold and wet climate, was 4.101 kwh.kwp-1 with a capacity factor of 17.09%. In contrast, Ahvas, with its warm and dry climate, had a potential of 3.271 kwh.kwp-1 and a capacity factor of 13.63%. These values differed significantly from the estimates provided by PVsyst and the Global Solar Atlas. The Global Solar Atlas overestimated the PV potential by 15% and 18.2% in Tehran and Ahvaz, respectively, while PVsyst underestimated it by more than 15% in both locations. The real-time energy production was measured at 246.1 Wh and 196.27 Wh, compared to the estimated values of 283 Wh and 232 Wh from the Global Solar Atlas, 178.36 Wh and 142.63 Wh from PVsyst, and 221.4 Wh and 196.9 Wh from PVGIS in Tehran and Ahvaz, respectively. These results underscore the importance of accounting for recent climate changes when predicting near real-time solar generation, particularly in remote areas and regions that deviate from global trends.(c) 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",
        "keywords": "PV energy; GSM; Arduino; PVGIS; Solar atlas",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.egyr.2023.09.038"
    },
    {
        "title": "(Implementation of a system to evaluate the coverage of the sigfox network inside buildings)",
        "abstract": "Low-energy wireless wide-area networks are a key technology for the development of the Internet of Things. Like any wireless communications system, it requires that during the design the place where the network will be deployed be inspected to determine the locations where the signal is weak. Sigfox is a network with wireless access that provides the connectivity service for the Internet of Things. When the sensor nodes are located inside buildings, the level of the received signal can vary, due to internal obstructions and interference, so it is necessary to have systems that take into account the technical characteristics of the Sigfox network, to measure the levels. signal strength inside buildings and identify locations where there is no signal. The implemented system allows obtaining information on the signal levels inside the buildings in order to identify the locations in which the nodes do not have connectivity with the network and find solutions to this problem before implementing the network. The system implementation uses Sipy nodes programmed with Pymark and the Sigfox cloud, while the application development uses API and APIREST in the Visual Studio development environment.",
        "keywords": "SigFox; signal; power; coverage",
        "released": 2023,
        "link": "https://doi.org/10.29019/enfoqueute.859"
    },
    {
        "title": "Mobile platform products supporting use of smart devices",
        "abstract": "Smart devices such as smartphones and tablets are coming to be applied in a wide variety of business scenarios both inside and outside the company thanks to their mobile and portable features. This revolution in work style centered about the use of smart devices was perhaps inevitable, but there are still many needs in system management and development such as enhanced security measures and efficient software development methods under multiple OSs. To meet these needs, Fujitsu provides a middleware product called FUJITSU Software Interstage Mobile Application Server (IMAPS) and a service platform called FUJITSU Cloud PaaS MobileSUITE (MobileSUITE). IMAPS enables the efficient development and operation of applications for smart devices by providing a multi-platform development environment, a secure application execution infrastructure, and a business operation environment for improving on-site productivity. MobileSUITE provides services that enable functions for content delivery and digital catalogs to be used immediately and also provides mobile device management (MDM) Functions. Using these MobileSUITE services and functions in combination enables prompt support of situations requiring security measures. This paper introduces the features of these two products and presents examples of their application.",
        "keywords": "",
        "released": 2015,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000353006600004"
    },
    {
        "title": "Modeling stack overflow tags and topics as a hierarchy of concepts",
        "abstract": "Developers rely on online Q&A forums to look up technical solutions, to pose questions on implementation problems, and to enhance their community profile by contributing answers. Many popular developer communication platforms, such as the Stack Overflow Q&A forum, require threads of discussion to be tagged by their contributors for easier lookup in both asking and answering questions. In this paper, we propose to leverage Stack Overflow’s tags to create a hierarchical organization of concepts discussed on this platform. The resulting concept hierarchy couples tags with a model of their relevancy to prospective questions and answers. For this purpose, we configure and apply a supervised multi-label hierarchical topic model to Stack Overflow questions and demonstrate the quality of the model in several ways: by identifying tag synonyms, by tagging previously unseen Stack Overflow posts, and by exploring how the hierarchy could aid exploratory searches of the corpus. The results suggest that when traversing the inferred hierarchical concept model of Stack Overflow the questions become more specific as one explores down the hierarchy and more diverse as one jumps to different branches. The results also indicate that the model is an improvement over the baseline for the detection of tag synonyms and that the model could enhance existing ensemble methods for suggesting tags for new questions. The paper indicates that the concept hierarchy as a modeling imperative can create a useful representation of the Stack Overflow corpus. This hierarchy can be in turn integrated into development tools which rely on information retrieval and natural language processing, and thereby help developers more efficiently navigate crowd-sourced online documentation. (C) 2019 Elsevier Inc. All rights reserved.",
        "keywords": "Concept hierarchy; Hierarchical topic model; Stack overflow; Tag synonym identification; Tag prediction; Entropy-based search evaluation",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.jss.2019.07.033"
    },
    {
        "title": "<I>terra</i>GIS - a web GIS for delivery of digital soil maps in cotton-growing areas of australia",
        "abstract": "Effective management of soil requires the spatial distribution of its various physical, chemical and hydrological properties. This is because properties, for example clay content, determine the ability of soil to hold cations and retain water. However, data acquisition is labour intensive and time-consuming. To add value to the limited soil data, remote sensing (e.g. airborne gamma-ray spectrometry) and proximal sensing, such as electromagnetic (EM) induction, are being used as ancillary data. Here, we provide examples of developing Digital Soil Maps (DSM) of soil physical, chemical and hydrological properties, for seven cotton-growing areas of southeastern Australia, by coupling soil data with remote and proximal sensed ancillary data. A greater challenge is how to get these DSM to a stakeholder in a way that is useful for practical soil use and management. This study describes how we facilitate access to the DSMs, using a simple-to-use web GIS platform, called terraGIS. The platform is underpinned by Google Maps API, which is an open-source development environment for building spatially enabled Internet applications. In conclusion, we consider that terraGIS and the supporting information, available on the sister web page (), allow easy access to explanation of DSM of soil properties, which are relevant to cotton growers, farm managers, consultants, extension staff, researchers, state and federal government agency personnel and policy analysts. Future work should be aimed at developing error budget maps to identify where additional soil and/or ancillary data is required to improve the accuracy of the DSMs.",
        "keywords": "Electromagnetic induction; gamma-ray spectrometry; GIS; digital soil mapping; Google Maps; soil conservation",
        "released": 2017,
        "link": "https://doi.org/10.1111/sum.12383"
    },
    {
        "title": "SimBSI: An open-source simulink library for developing closed-loop brain signal interfaces in animals and humans",
        "abstract": "Objective. A promising application of BCI technology is in the development of personalized therapies that can target neural circuits linked to mental or physical disabilities. Typical BCIs, however, offer limited value due to simplistic designs and poor understanding of the conditions being treated. Building BCIs on more solid grounds may require the characterization of the brain dynamics supporting cognition and behavior at multiple scales, from single-cell and local field potential (LFP) recordings in animals to non-invasive electroencephalography (EEG) in humans. Despite recent efforts, a unifying software framework to support closed-loop studies in both animals and humans is still lacking. The objective of this paper is to develop such a unifying neurotechnological software framework. Approach. Here we develop the Simulink for Brain Signal Interfaces library (SimBSI). Simulink is a mature graphical programming environment within MATLAB that has gained traction for processing electrophysiological data. SimBSI adds to this ecosystem: 1) advanced human EEG source imaging, 2) cross-species multimodal data acquisition based on the Lab Streaming Layer library, and 3) a graphical experimental design platform. Main results. We use several examples to demonstrate the capabilities of the library, ranging from simple signal processing, to online EEG source imaging, cognitive task design, and closed-loop neuromodulation. We further demonstrate the simplicity of developing a sophisticated experimental environment for rodents within this environment. Significance. With the SimBSI library we hope to aid BCI practitioners of dissimilar backgrounds in the development of, much needed, single and cross-species closed-loop neuroscientific experiments. These experiments may provide the necessary mechanistic data for BCIs to become effective therapeutic tools.",
        "keywords": "Simulink; Lab Streaming Layer; online EEG source imaging; closed-loop; brain-computer interface; cross-species",
        "released": 2020,
        "link": "https://doi.org/10.1088/2057-1976/ab6e20"
    },
    {
        "title": "eLearniXML: Towards a model-based approach for the development of e-learning systems considering quality",
        "abstract": "With the evolution of technology, and especially of the Internet, a growing interest has appeared for online education. The many advantages of e-Learning have made this teaching philosophy an ideal partner for teachers, either as a complement to regular education or as a substitute for traditional education. The development of an e-Learning system poses extra challenges for software developers, since there are other facets, such as contents and user tracking, not usually considered in software development methodologies. In this paper eLearniXML approach to the development of e-Learning systems is presented. This approach enriches the development of e-Learning systems method proposed in ADDIE with the model-based development of user interfaces and software quality consideration. By doing so, we aim at the development of, what we have named, a Model-Based Instructional Development Environment (MB-ISDE), to include e-Learning development in the current trends of model-based software development. (C) 2009 Elsevier Ltd. All rights reserved.",
        "keywords": "e-Learning; Model-based design of user interfaces; MB-ISDE",
        "released": 2009,
        "link": "https://doi.org/10.1016/j.advengsoft.2009.01.019"
    },
    {
        "title": "Inference of development activities from interaction with uninstrumented applications",
        "abstract": "Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.",
        "keywords": "Software development; Developers’ interaction data; Condition Random Field",
        "released": 2018,
        "link": "https://doi.org/10.1007/s10664-017-9547-8"
    },
    {
        "title": "PPAM-mIoMT: A privacy-preserving authentication with device verification for securing healthcare systems in 5G networks",
        "abstract": "In 5G, the next generation of technologies like the Internet of Medical Things (IoMT) cooperate with networking artifacts to provide seamless connectivity. To provide effective information sharing, the Internet infrastructure integrates various components of medical information systems such as computing devices, software tools, and application services. The information systems access a massive Internet of Medical Things (mIoMT) to discover a series of network infrastructures with technological innovations to offer remote diagnosis and treatment. In collecting healthcare information, innovative applications utilize medical sensor networks to drive extensive information processing including mobility support. However, the convergence of IoT and the cloud addresses the security gap while centralizing the devices into edge computing systems to protect the transmission flow through the device gateway. Considering this fact, an authentication scheme with privacy preservation was proposed using a key exchange protocol and digital signature by Soleymani et al. to guarantee data confidentiality and integrity. However, this scheme is still incapable of validating the integrity of data with the edge systems properly. Thus, this paper presents privacy-preserving authentication with device verification (PP-ADV) for securing healthcare systems in 5G networks. To protect the device privacy and server identities with registered users, the proposed PP-ADV utilizes two cryptographic primitives namely elliptic curve arithmetic and a collision-free hash function to achieve the significant properties of key agreement protocol including authenticity and confidentiality. Formal and informal analyses show that the proposed PPADV can restrict vulnerabilities such as privileged insider and forgery to achieve high-level privacy protection with seamless verification to improve system efficiency compared with other state-of-the-art approaches. Also, a board simulation testbed was designed using Raspberry Pi with Arduino IDE to examine the quality metrics including data transmission ratio, authentication delay, and throughput rate. The testbed analysis demonstrates that the proposed PP-ADV gains less delay approximate to 292.28s and improved throughput approximate to 87.73 to meet the design criteria of 5G-enabled remote healthcare systems.",
        "keywords": "Internet of medical things; 5G; Healthcare; Privacy-preserving; Authentication; Security",
        "released": 2024,
        "link": "https://doi.org/10.1007/s10207-023-00762-3"
    },
    {
        "title": "A VR-based simulation system for glass pressing",
        "abstract": "The integration with virtual reality is a new boost to CAD and CAE. This paper presents a research effort aimed at creating a desktop-based, low-cost and independent VR-based simulation system for glass pressing, which integrates the forming process simulation and mold motion simulation. The skeletal animation technique is employed for Motion Simulation of the mold The stereoscopic view of the virtual system is achieved by Crystal Eyes shutter glasses, with C++ language and OpenGL support library selected as the development tool. With the stereoscopic display of the mold design, pressing motion and numerical CAE results, the system enables engineers to gain a cohesive view of mold structure, assembly issues, and possible forming faults. it can also support training of students and engineers to acquire the theoretical know-how, practical skills, and the problem troubleshooting techniques of glass pressing. (C) 2008 Wiley Periodicals, Inc. Comput Appl Eng Educ 16: 315-320. 2008: Published online in Wiley InterScience (www.interscience.wiley.com): DOI 10.1002/cae.20216",
        "keywords": "glass pressing; virtual reality; numerical simulation; skeletal animation",
        "released": 2008,
        "link": "https://doi.org/10.1002/cae.20216"
    },
    {
        "title": "Increasing quality and managing complexity in neuroinformatics software development with continuous integration",
        "abstract": "High quality neuroscience research requires accurate, reliable and well maintained neuroinformatics applications. As software projects become larger, offering more functionality and developing a denser web of interdependence between their component parts, we need more sophisticated methods to manage their complexity. If complexity is allowed to get out of hand, either the quality of the software or the speed of development suffer, and in many cases both. To address this issue, here we develop a scalable, low-cost and open source solution for continuous integration (CI), a technique which ensures the quality of changes to the code base during the development procedure, rather than relying on a pre-release integration phase. We demonstrate that a CI-based workflow, due to rapid feedback about code integration problems and tracking of code health measures, enabled substantial increases in productivity for a major neuroinformatics project and additional benefits for three further projects. Beyond the scope of the current study, we identify multiple areas in which CI can be employed to further increase the quality of neuroinformatics projects by improving development practices and incorporating appropriate development tools. Finally, we discuss what measures can be taken to lower the barrier for developers of neuroinformatics applications to adopt this useful technique.",
        "keywords": "software development; testing; process management; quality control; complexity management",
        "released": 2013,
        "link": "https://doi.org/10.3389/fninf.2012.00031"
    },
    {
        "title": "Capability-based localization of distributed and heterogeneous queries",
        "abstract": "One key aspect of data-centric applications is the manipulation of data stored in persistent repositories, which is moving fast from querying a centralized relational database to the ad-hoc combination of constellations of data sources. The extension of general purpose languages with query operations is increasingly popular, as a tool to improve reasoning and optimizing capabilities of interpreters and compilers. However, not much is being done to integrate and orchestrate different and separate sources of data. We present a data manipulation language that abstracts the nature and location of data-sources. We define its semantics and a type directed query localization mechanism to be used in development tools for heterogeneous environments to efficiently compile them into native queries. We introduce a localization procedure based on rewriting of query expressions that is confluent, terminating and provides the maximum mapping between site capabilities and the structure of the query. We provide formal type safety results that support the sound distribution of query fragments over remote sites. Our approach is also suitable for an interactive query construction environment by rich user interfaces that provide immediate feedback on data manipulation operations. This approach is currently the base for the data layer of a development platform for mobile and web applications.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1017/S095679681700017X"
    },
    {
        "title": "Translation, cross-cultural adaptation and validation of the sickle cell self-efficacy scale (SCSES)",
        "abstract": "Objective: To translate, cross-culturally adapt and validate the Sickle Cell Self-Efficacy Scale for application in the Brazilian cultural context.Methods: This is a methodological study performed in 6 steps: 1-Forward translation; 2 Translation synthesis; 3-Back-translation; 4-Assessment by expert committee, with computation of the Content Validity Index (CVI); 5-Cultural adequacy (pre-test); 6- Reproducibility. A pre-test was performed with the participation of 10 adolescents/young adults with sickle cell disease through a telephone call and their responses were recorded on a form in a web platform. The instrument validation step was carried out with 55 adolescents/young adults with sickle cell disease, 43 of them having participated in the retest. The analysis of internal consistency and reproducibility was calculated using the Cronbach’s alpha coeffi-cient and the Intraclass Correlation Coefficient (ICC), in the R statistical programming environment.Results: The translated instrument had good acceptance among the experts, reaching an average CVI of 1.0. In assessing reliability, the scale showed acceptable internal consistency, with a Cronbach’s alpha of 0.84. In the agreement analysis, the ICC was 0.923 (95% CI: 0.857 to 0.958), which indicates good temporal reproducibility. Conclusions: Following the process of translation, cross-cultural adaptation and validation, we obtained the Brazilian version of the Sickle Cell Self-Efficacy Scale, considered valid and reliable to be applied to adolescents and young adults with sickle cell disease in Brazil.& COPY; 2022 Published by Elsevier Espana, S.L.U. on behalf of Associacao Brasileira de Hematologia, Hemoterapia e Terapia Celular. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",
        "keywords": "Sickle cell disease; Self-ef ficacy; Adolescent; Young adult; Surveys and questionnaires; Validation studies",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.htct.2022.02.010"
    },
    {
        "title": "The object-JavaScript language",
        "abstract": "Complex graphical user interfaces (GUIs) that support a large amount of user interaction require a fast response time, a rich set of building blocks for an esthetic look-and-feel, and a development environment that supports ongoing change. On the World Wide Web, client-side technologies offer more of these features than do server-side solutions. Java and JavaScript are the two most popular languages used for client-side GUI implementations. Java implementations require a user to download a plug-in that contains a virtual machine to execute the Java byte-code. The installation and maintenance of this plug-in is sometimes an unsurmountable barrier to using Java. JavaScript lacks some of the desirable features of Java, such as easy to use object-oriented features and having a GUI class Library, but does not require a plug-in, We have enhanced JavaScript by implementing a new language Object-JavaScript (OJS) and by providing an OJS library of GUI components, thus making it a viable alternative to Java. Copyright (C) 2000 John Wiley & Sons, Ltd.",
        "keywords": "Java; JavaScript; Object-JavaScript; object-oriented programming",
        "released": 2000,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000165322500003"
    },
    {
        "title": "Open ecosystem for future industrial internet of things (IIoT): Architecture and application",
        "abstract": "Advanced sensing, data analysis and communication techniques have led to the emergence and tremendous development of industrial internet of things (IIoT) in recent years, which raises revolution in condition monitoring and maintenance for electrical assets. An open ecosystem for future IIoT is proposed in this paper and the architecture of the open ecosystem is discussed. An open development environment needs to be established for users to interact with power devices and servers freely via web or mobile applications installed on user terminals, increasing IIoT scalability and flexibility. The core technologies of open ecosystem in future IIoT are discussed, which include comprehensive sensing techniques, wide-area communication tools, Big Data service infrastructure, data analysis algorithms and intelligent maintenance schemes. An application of future IIoT ecosystem in wind farm maintenance is then presented. It is demonstrated that the efficiency and effectiveness of wind farm maintenance can be improved with the support of open ecosystem of future IIoT, providing an innovative insight for electrical assets monitoring and maintenance with high reliability.",
        "keywords": "Industrial internet of things (IIoT); IIoT-based development environment; open ecosystem; wind farm maintenance",
        "released": 2020,
        "link": "https://doi.org/10.17775/CSEEJPES.2019.01810"
    },
    {
        "title": "An aggregate resource model for the provision of dynamic “resource-aware” planning",
        "abstract": "The realization of agile enterprises requires substantial development of the underpinning modelling, information management and knowledge representation technologies. This paper introduces a resource model that has been developed to support the dynamic, aggregate planning of manufacturing operations within large, complex production networks during the formative stages of design. The main goal of aggregate planning is the measurement of product manufacturability and the evaluation of alternative design configurations and manufacturing scenarios, through the allocation of multiple parts to remote facilities within the supply network. The term “resource aware” planning is used to indicate the creation of a dynamic interrelationship between the planning entities (products and processes) and the enterprise resources, including humans and machines. The technologies employed for implementing the pilot methods include the aggregate resource modelling methodology, a web-centric co-development environment, unique methods for enriching planning entities with knowledge and evolutionary computing methods for the rapid generation and optimization of production routes. A pilot resource aware planning system has been implemented that supports many innovative modes of operation, the initial testing of which was very encouraging.",
        "keywords": "agile manufacture; resource modelling; enterprise planning; CAPP; distributed enterprise",
        "released": 2003,
        "link": "https://doi.org/10.1243/095440503322617234"
    },
    {
        "title": "Global names: Support for managing software in a world of virtual organizations",
        "abstract": "Emerging technologies such as the Internet, the World Wide Web, Java (TM) technology, and software components are accelerating product life cycles and encouraging collaboration across organizational boundaries. The familiar coordination problems of large scale software development reappear in a context where tools used by collaborators must be less tightly coupled to one another than before. To the traditional notion of scale, based on the size of software systems, must be added a new dimension of scale: organizational complexity. Designing configuration management systems that scale well over both dimensions requires difficult trade-offs between reliability and flexibility. At the heart of these trade-offs is the aggregate information shared by collaborators: how it is represented, maintained, and understood by the people and tools using it. While designing a prototype development environment intended to scale in both dimensions, we have revisited the role played by naming. A proposed extension to the prototype’s naming system addresses issues such as which objects should be named and how the shared naming system is constructed.",
        "keywords": "",
        "released": 1999,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000170800400011"
    },
    {
        "title": "EduGene: A UIDP-based educational app generator for multiple devices and platforms",
        "abstract": "Education is key to a just and progressive society. It is specially provided by schools and reflects on various areas of our lives. The importance of education has led to new schemes in the development of educational applications. The goal of this research is to propose the design of an architecture for educational application development that relies on a set of user interface design patterns (UIDPs) to facilitate the laborious, time-consuming task of application development. To this end, this article presents the EduGene as the proof of concept. Applications generated with EduGene are compatible with four operating systems, Android (TM), Firefox (R) OS, macOS (R), and Windows Phone (R), and with the Web. Furthermore, EduGene is compatible with three types of devices: mobile (smartphones and tablets), desktop, and television. The evaluation results prove that EduGene is a user-friendly educational application generator, since it provides an intuitive interface that facilitates user interaction. The e-learning tools selected to perform EduGene’s comparative evaluation are Moodle, Sakai, and ATutor. It is important to mention that the goal of the evaluation was only to highlight the benefits of EduGene as an educational app development tool.",
        "keywords": "",
        "released": 2019,
        "link": "https://doi.org/10.1080/10447318.2018.1459347"
    },
    {
        "title": "A generic method for intact and subunit level characterization of mAb charge variants by native mass spectrometry",
        "abstract": "Monoclonal antibodies (mAbs) are heterogeneous macromolecules that display a complex isoform profile as a result of the large series of modifications they can undergo. Product-related charge variants that are associated with a loss of biological activity or affected half-life and immunogenicity are especially important. Consequently, they are often considered critical quality attributes such that acceptance criteria and controls should be established. The characterization of rnAbs charge variants has long been a time and resource consuming task. Recent successes in the use of salt mediated pH gradient ion exchange chromatography with volatile mobile phases have shown there to be significant promise in using online mass spectrometric (MS) detection to facilitate peak detection. In this study, a newly developed 3 gm non-porous cation exchange column technology was investigated for its capability to be hyphenated to MS for the purpose of characterizing mAb charge variants. A 2 min ID format was selected for the ease of configuring it to classical MS ESI ion sources. A monoclonal antibody reference material from NIST (FtM 8671; NISTinAb) was used in its intact and IdeS/IgdE-digested forms to test for column performance and MS sensitivity. Furthermore, three different mAbs with highly basic isoelectric points (pI) were analyzed in their native and proteolyzed forms to demonstrate the straightforward application of the developed technique even with mAbs having strong retention on cation exchange media. The MS detection of low-abundance charge variant species ( < 0.1%) demonstrated there to be acceptable sensitivity and dynamic range even from routine analyses. The capability of the column to separate different mAbs having high basic pI was demonstrated, and it was found that slight adjustment of ammonium acetate concentration in the eluent can be a convenient way to rapidly optimize a separation if necessary. Linearity was shown to exist between protein mass loads of 2.5 and 50 mu g while an optimal balance between chromatographic resolution and MS sensitivity was observed between 5 and 10 mu g. Excellent run-to-run and column-to-column repeatability was achieved in terms of retention times, resolution and recovery.",
        "keywords": "Ion exchange chromatography; Charge variant; mAbs; Native mass spectrometry; Non-covalent",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.jchromb.2019.121814"
    },
    {
        "title": "A game engine designed to simplify 2D video game development",
        "abstract": "In recent years, the increasing popularity of casual games for mobile and web has promoted the development of new editors to make video games easier to create. The development of these interactive applications is on its way to becoming democratized, so that anyone who is interested, without any advanced knowledge of programming, can create them for devices such as mobile phones or consoles. Nevertheless, most game development environments rely on the traditional way of programming and need advanced technical skills, even despite today’s improvements. This paper presents a new 2D game engine that reduces the complexity of video game development processes. The game specification has been simplified, decreasing the complexity of the engine architecture and introducing a very easy-to-use editing environment for game creation. The engine presented here allows the behaviour of the game objects to be defined using a very small set of conditions and actions, without the need to use complex data structures. Some experiments have been designed in order to validate its ease of use and its capacity in the creation of a wide variety of games. To test it, users with little experience in programming have developed arcade games using the presented environment as a proof of its easiness with respect to other comparable software. Results obtained endorse the concept and the hypothesis of its easiness of use and demonstrate the engine potential.",
        "keywords": "Game Engine; 2D video games; Game editor; Game logic",
        "released": 2020,
        "link": "https://doi.org/10.1007/s11042-019-08433-z"
    },
    {
        "title": "Clinical evaluation of maxillary sinus floor elevation with or without bone grafts: A systematic review and meta-analysis of randomised controlled trials with trial sequential analysis",
        "abstract": "Introduction: Our goal was to systematically review the current evidence comparing the relative effectiveness of two maxillary sinus floor elevation (MSFE) approaches (internal and external) without bone grafts with that of conventional/grafted MSFE in patients undergoing implantation in the posterior maxilla. Material and methods: Medical databases (PubMed/Medline, Embase, Web of Science, and Cochrane Library) were searched for randomised controlled trials published between January 1980 and May 2023. A manual search of implant-related journals was also performed. Studies published in English that reported the clinical outcomes of MSFE with or without bone material were included. The risk of bias was assessed using the Cochrane Handbook Risk Assessment Tool. Meta-analyses and trial sequence analyses were performed on the included trials. Meta -regression analysis was performed using pre-selected covariates to account for substantial heterogeneity. The certainty of evidence for clinical outcomes was assessed using GRADEpro GDT online (Guideline Development Tool). Results: Seventeen studies, including 547 sinuses and 696 implants, were pooled for the meta -analysis. The meta -analysis showed no statistically significant difference between MSFE without bone grafts and conventional MSFE in terms of the implant survival rate in the short term ( n = 11, I 2 = 0%, risk difference (RD): 0.03, 95% confidence intervals (CI): -0.01-0.07, p = 0.17, required information size (RIS) = 307). Although conventional MSFE had a higher endo-sinus bone gain ( n = 13, I 2 = 89%, weighted mean difference (WMD): -1.24, 95% CI: -1.91- -0.57, p = 0.0003, RIS = 461), this was not a determining factor in implant survival. No difference in perforation ( n = 13, I 2 = 0%, RD = 0.03, 95% CI: -0.02-0.09, p = 0.99, RIS = 223) and marginal bone loss ( n = 4, I 2 = 0%, WMD = 0.05, 95% CI: -0.14-0.23, p = 0.62, no RIS) was detected between the two groups using meta -analysis. The pooled results of the implant stability quotient between the two groups were not robust on sensitivity analysis. Because of the limited studies reporting on the visual analogue scale, surgical time, treatment costs, and bone density, qualitative analysis was conducted for these outcomes. Conclusions: This systematic review revealed that both non-graft and grafted MSFE had high implant survival rates. Owing to the moderate strength of the evidence and short -term follow-up, the results should be interpreted with caution.",
        "keywords": "maxillary sinus floor elevation; dental implants; bone grafts; meta-analysis; trial sequential analysis",
        "released": 2024,
        "link": "https://doi.org/10.5114/aoms/174648"
    },
    {
        "title": "Security knowledge representation artifacts for creating secure IT systems",
        "abstract": "The creation of secure applications is more than ever a complex task because it requires from system engineers increasing levels of knowledge in security requirements, design and implementation. In fact, the fast increasing size and volatility of this knowledge has reached a point in which it is unrealistic to expect that system engineers can keep up to date with it. The most prominent paradigm for addressing this problem is the use of security patterns to communicate security knowledge from experts to system designers. This, and other security artifacts, have proved their utility and benefits in the past years, improving the way security is taken into account by system engineers and developers. On the other hand, these artifacts have some limitations that have prevented them from becoming more widespread. In particular, security patterns are human-oriented and as such heavily based on natural language, which implies intrinsic high degrees of imprecision and ambiguity. In our opinion, we need to make the move from purely human oriented artifacts to hybrid artifacts that convey information for both humans (engineers and designers) and computer tools (engineering and development environments). Therefore, we have created a new security knowledge representation artifact that aims to cover the needs of system engineers and help them not only in applying a solution, but also in understanding the security aspects of a given domain as a highly-related set of security concepts (e.g. properties, requirements, solutions, etc.). This artifact, called Domain Security Metamodel (DSM), is, as its name suggests, domain-specific and contains information about all security aspects that are relevant in a specific domain (e.g. embedded systems, web services, etc.). The DSM contains security solutions that implement the security properties of the specific domains. That way, when users apply them into their system models the solutions for development time can be integrated directly and naturally. In order to describe our approach in a useful way we use a running example based on the Web Service Security (WS-Security) specification. (C) 2016 Elsevier Ltd. All rights reserved.",
        "keywords": "Security engineering process; Security modeling; Security solutions; Domain specific tools; Security requirements",
        "released": 2017,
        "link": "https://doi.org/10.1016/j.cose.2016.09.001"
    },
    {
        "title": "BioC viewer: A web-based tool for displaying and merging annotations in BioC",
        "abstract": "BioC is an XML-based format designed to provide interoperability for text mining tools and manual curation results. A challenge of BioC as a standard format is to align annotations from multiple systems. Ideally, this should not be a major problem if users follow guidelines given by BioC key files. Nevertheless, the misalignment between text and annotations happens quite often because different systems tend to use different software development environments, e.g. ASCII vs. Unicode. We first implemented the BioC Viewer to assist BioGRID curators as a part of the BioCreative V BioC track (Collaborative Biocurator Assistant Task). For the BioC track, the BioC Viewer helped curate protein-protein interaction and genetic interaction pairs appearing in full-text articles. Here, we describe the BioC Viewer itself as well as improvements made to the BioC Viewer since the BioCreative V Workshop to address the misalignment issue of BioC annotations. While uploading BioC files, a BioC merge process is offered when there are files from the same full-text article. If there is a mismatch between an annotated offset and text, the BioC Viewer adjusts the offset to correctly align with the text. The BioC Viewer has a user-friendly interface, where most operations can be performed within a few mouse clicks. The feedback from BioGRID curators has been positive for the web interface, particularly for its usability and learnability.",
        "keywords": "",
        "released": 2016,
        "link": "https://doi.org/10.1093/database/baw106"
    },
    {
        "title": "KETOS: Clinical decision support and machine learning as a service - a training and deployment platform based on docker, OMOP-CDM, and FHIR web services",
        "abstract": "Background and objective To take full advantage of decision support, machine learning, and patient-level prediction models, it is important that models are not only created, but also deployed in a clinical setting. The KETOS platform demonstrated in this work implements a tool for researchers allowing them to perform statistical analyses and deploy resulting models in a secure environment. Methods The proposed system uses Docker virtualization to provide researchers with reproducible data analysis and development environments, accessible via Jupyter Notebook, to perform statistical analysis and develop, train and deploy models based on standardized input data. The platform is built in a modular fashion and interfaces with web services using the Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR) standard to access patient data. In our prototypical implementation we use an OMOP common data model (OMOP-CDM) database. The architecture supports the entire research lifecycle from creating a data analysis environment, retrieving data, and training to final deployment in a hospital setting. Results We evaluated the platform by establishing and deploying an analysis and end user application for hemoglobin reference intervals within the University Hospital Erlangen. To demonstrate the potential of the system to deploy arbitrary models, we loaded a colorectal cancer dataset into an OMOP database and built machine learning models to predict patient outcomes and made them available via a web service. We demonstrated both the integration with FHIR as well as an example end user application. Finally, we integrated the platform with the open source DataSHIELD architecture to allow for distributed privacy preserving data analysis and training across networks of hospitals. Conclusion The KETOS platform takes a novel approach to data analysis, training and deploying decision support models in a hospital or healthcare setting. It does so in a secure and privacy-preserving manner, combining the flexibility of Docker virtualization with the advantages of standardized vocabularies, a widely applied database schema (OMOP-CDM), and a standardized way to exchange medical data (FHIR).",
        "keywords": "",
        "released": 2019,
        "link": "https://doi.org/10.1371/journal.pone.0223010"
    },
    {
        "title": "GIS software package “epidemiological atlas of russia” on current infectious diseases",
        "abstract": "The aim of the study is to develop a GIS software package “Epidemiological Atlas of Russia” on topical infectious and parasitic diseases in the Russian Federation to create an open and publicly accessible information resource allowing to improve the quality of morbidity epidemiological monitoring and analysis. Materials and Methods. The GIS software package “Epidemiological Atlas of Russia” was designed for data monitoring, epidemiological analysis, and cartographic visualization and was implemented as a web resource consisting of a web application, a package administration module, and a database management system. The following development tools were used to create the package: JavaScript, PHP, additional mapping libraries (Leaflet, OpenStreetMap), MySQL database management systems, Visual Basic.NET. The primary information for the database was taken from official federal statistical observation forms No.1 and No.2 “Information on infectious and parasitic diseases”. Results. Analytical methods and GIS technologies used in epidemiological practice were evaluated, optimal technical solutions based on the experience of developing the “Epidemiological Atlas of the Volga Federal District” were selected. A versatile database structure was designed and developed to create an array of input and output statistical values of an epidemiological nature. Original algorithms were created to obtain and evaluate epidemiological indicators. Web application “Epidemiological Atlas of Russia” was developed to present, analyze, and visualize information on infectious and parasitic diseases in the subjects of a district, federal districts, and the Russian Federation as a whole. It allows to work with report forms of the Ministry of Health to organize federal statistical monitoring in the field of health protection and with laboratory studies results to create thematic modules providing detailed information on individual nosologies. Initial data were temporally broken down by months, and spatially, by Russian Federation subjects. All visualization results were dynamically updated and generated based on user’s interactive request. Conclusion. GIS software package “Epidemiological Atlas of Russia” was developed as an open and publicly accessible information resource and is designed to improve the quality of epidemiological monitoring, operational and retrospective epidemiological analysis of the incidence of current infectious and parasitic diseases in the Russian Federation. The package is intended for use in federal executive authorities, in supervisory authorities and institutions of Rospotrebnadzor, in medical organizations of the Ministry of Health of the Russian Federation and is in line with the state policy aimed to introduce modern technologies into practice.",
        "keywords": "analysis of infectious/somatic morbidity; GIS technologies; geoinformation systems in epidemiology; database structure; algorithms for calculating epidemiological indicators",
        "released": 2023,
        "link": "https://doi.org/10.17691/stm2023.15.6.03"
    },
    {
        "title": "Predictable cloud computing",
        "abstract": "The standard tools for cloud computingprocessor and network virtualizationmake it difficult to achieve dependability, both in terms of real time operations and fault tolerance. Virtualization multiplexes virtual resources onto physical ones, typically by time division or statistical multiplexing. Time, in the virtual machine, is therefore as virtual as the machine itself. And fault tolerance is difficult to achieve when redundancy and independent failure in the virtual environment do not necessarily map to those properties in the physical environment. Virtualization adds a level of indirection that creates overhead, and makes it all but impossible to achieve predictable performance. Osprey uses an alternative to virtualization that achieves the same goals of scalability and flexibility but carries neither the overhead of virtualization, nor the restrictions on dependability. The result is a programming environment that achieves most of the compatibility offered by traditional virtualization efforts and provides much better and much more predictable performance. One technique we use is called Library OS, which stems from high-performance computing. The technique consists of linking applications with a library that implements most services normally provided by the operating system, creating an application that can run practically stand alone, or at least with a very minimal operating system. The Library OS approach moves the boundary between application and operating system down to a level where interactions with the operating system consist of sending/receiving messages (e.g., network packets) and scheduling resources (processor, memory, network bandwidth, and device access). These interactions, as we demonstrate, form a relatively weak bond between an application and the particular instance of the operating system on which it runsone that can be broken and re-established elsewhere. In fact, we make sure this is the case. Legacy applications that cannot be recompiled or relinked can make use of a Library OS server that runs as a tandem process along with the legacy application processes. System calls from the legacy process are catapulted into the Library OS server which executes them. Applications can still migrate, taking their server process along with them. (c) 2012 Alcatel-Lucent.",
        "keywords": "",
        "released": 2012,
        "link": "https://doi.org/10.1002/bltj.21542"
    },
    {
        "title": "Experience in integrating java with c# and .NET",
        "abstract": "Java programmers cannot help but be aware of the advent of C#, the .NET network environment, and a host of new supporting technologies, such as Web services. Before taking the big step of moving all development to a new environment, programmers will want to know what are the advantages of C# as a language over Java, and whether the new and interesting features of C# and NET can be incorporated into existing Java software. This paper surveys the advantages of C# and then presents and evaluates experience with connecting it to Java in a variety of ways. The first way provides evidence that Java can be linked to C# at the native code level, albeit through C++ wrappers. The second is a means for retaining the useful applet feature of Java in the server-side architecture of Web services written in C#. The third is by providing a common XML-based class for the development of graphical user interfaces (GUIs), which can be incorporated into Java or C#. An added advantage of this system, called Views, is that it can run independently of the resource-intensive development environment that would otherwise be needed for using C#. A major advantage of the methods described in this paper is that in all cases the Java program is not affected by the fact that it is interfacing with C#. The paper concludes that there are many common shared technologies that bring Java and C# close together, and that innovative ways of using others can open up opportunities not hitherto imagined. Copyright (c) 2005 John Wiley & Sons, Ltd.",
        "keywords": "integration; Java; C#; XML; native code; web services; GUI",
        "released": 2005,
        "link": "https://doi.org/10.1002/cpe.858"
    },
    {
        "title": "Training packages for the use of child development tools in low/middle-income countries: A review",
        "abstract": "Background We are now moving beyond the focus of “child survival” to an era which promotes children thriving and developing rather than simply “surviving”. In doing so, we are becoming more aware of the large variation of child development screening tools available globally, but in particular, those in low/middle-income countries (LMICs). Methods This narrative review identifies 24 child development tools used in LMICs. We aimed to identify information on training accessibility and training design, assessment methods and cost of training. For those tools with no training information identified or for any tools identified as providing online training, the tool author was contacted individually to obtain information on the features of the tool’s training package. Results Information on training features was identified for 18 tools. All of the tools are identified as screening tools with some also identified as surveillance or assessment tools. The training material for the majority of the tools was not readily accessible and most training packages were proprietary and only available with a face-to-face training design. Other training options included a user manual, training videos or training through an online platform. Conclusions Training is a key factor when selecting a child development screening or surveillance tool particularly in a low-income or middle-income setting where funds may be limited. The accessibility of training can have a key impact on the implementation and utilisation of tools desperately needed for use in LMICs. Essential to improvements in overall child health is an emphasis on helping children thrive as well as survive. This new emphasis will require a well-trained healthcare workforce in this regard. This paper reviews child development training packages in resource-limited settings.",
        "keywords": "child development; global health; paediatrics",
        "released": 2023,
        "link": "https://doi.org/10.1136/archdischild-2022-323814"
    },
    {
        "title": "Pace v0.2: A python-based performance-portable atmospheric model",
        "abstract": "Progress in leveraging current and emerging high-performance computing infrastructures using traditional weather and climate models has been slow. This has become known more broadly as the software productivity gap. With the end of Moore’s law driving forward rapid specialization of hardware architectures, building simulation codes on a low-level language with hardware-specific optimizations is a significant risk. As a solution, we present Pace, an implementation of the nonhydrostatic FV3 dynamical core and GFDL cloud microphysics scheme which is entirely Python-based. In order to achieve high performance on a diverse set of hardware architectures, Pace is written using the GT4Py domain-specific language. We demonstrate that with this approach we can achieve portability and performance, while significantly improving the readability and maintainability of the code as compared to the Fortran reference implementation. We show that Pace can run at scale on leadership-class supercomputers and achieve performance speeds 3.5-4 times faster than the Fortran code on GPU-accelerated supercomputers. Furthermore, we demonstrate how a Python-based simulation code facilitates existing or enables entirely new use cases and workflows. Pace demonstrates how a high-level language can insulate us from disruptive changes, provide a more productive development environment, and facilitate the integration with new technologies such as machine learning.",
        "keywords": "",
        "released": 2023,
        "link": "https://doi.org/10.5194/gmd-16-2719-2023"
    },
    {
        "title": "Factors affecting the organizational adoption of service-oriented architecture (SOA)",
        "abstract": "Service-oriented architecture (SOA) takes an architectural approach to designing and implementing IT solutions. Although it is fast emerging as one of the major architectural styles to execute enterprise architecture management, academic empirical research on SOA adoption is scarce, with most studies focusing on qualitative analysis. This study investigates organizational SOA adoption in South Africa and combines the perspectives of the diffusion of innovations theory and the technology-organization-environment framework into one comprehensive model of SOA adoption. In order to validate the research instrument and to gauge the state of SOA adoption, an online survey was conducted among enterprise architects in South African organizations. The survey provides insights in the perceived risks, obstacles but also expected benefits of SOA adoption. The results also highlight a number of factors significantly influencing SOA adoption in South Africa. Use of multiple standards and platforms, compatibility, top management support, good governance and strategy, adequate human and financial resources, vendor support for integration and development tools are all significant factors for a fruitful SOA implementation. Finally, all of the above adoption factors as well as cost and complexity were also found to correlate significantly with the degree of success of the SOA implementation as perceived by the IT or EA department.",
        "keywords": "Service-oriented architecture (SOA); SOA and EAM; Service-oriented applications architecture; Enterprise architecture; IT adoption; SOA success; South Africa",
        "released": 2014,
        "link": "https://doi.org/10.1007/s10257-012-0212-x"
    },
    {
        "title": "A roadmap to robot motion planning software development",
        "abstract": "PhD programs and graduate studies in robotics usually include motion planning among its main subjects. Students that focus their research in this subject find themselves trapped in the necessity of programming an environment where to test and validate their theoretic contributions. The programming of this robot motion planning environment is a big challenge. It requires on the one hand good programming skills involving the use of software development tools, programming paradigms, or the knowledge of computational complexity and efficiency issues. On the other hand it requires coping with different related issues like the modeling of objects, computational geometry problems and graphical representations and interfaces. The mastering of all these techniques is good for the curricula of roboticists with a motion planning profile. Nevertheless, the time and effort devoted to this end must remain reasonable. Within this framework, the aim of this paper is to provide the students with a roadmap to help them in the development of the software tools needed to test and validate their robot motion planners. The proposals are made within the scope of multi-platform open source code. (C) 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 18: 651-660, 2010; View this article online at wileyonlinelibrary.com; DOI 10.1002/cae.20269",
        "keywords": "robotics; software development; path planning",
        "released": 2010,
        "link": "https://doi.org/10.1002/cae.20269"
    },
    {
        "title": "psc2code: Denoising code extraction from programming screencasts",
        "abstract": "Progranuning screencasts have become a pervasive resource on the Internet, which help developers learn new progranuning technologies or skills. The source code in progranuning screencasts is an important and valuable information for developers. But the streaming nature of programming screencasts (i.e., a sequence of screen-captured images) limits the ways that developers can interact with the source code in the screencasts. Many studies use the Optical Character Recognition (OCR) technique to convert screen images (also referred to as video frames) into textual content, which can then be indexed and searched easily. However, noisy screen images significantly affect the quality of source code extracted by OCR, for example, no-code frames (e.g., PowerPoint slides, web pages of API specification), non-code regions (e.g., Package Explorer view, Console view), and noisy code regions with code in completion suggestion popups. Furthermore, due to the code characteristics (e.g., long compound identifiers like ItemListener), even professional OCR tools cannot extract source code without errors from screen images. The noisy OCRed source code will negatively affect the downstream applications, such as the effective search and navigation of the source code content in programming screencasts. In this article, we propose an approach named psc2code to denoise the process of extracting source code from programming screencasts. First, psc2code leverages the Convolutional Neural Network (CNN) based image classification to remove non-code and noisy-code frames. Then, psc2code performs edge detection and clustering-based image segmentation to detect sub-windows in a code frame, and based on the detected subwindows, it identifies and crops the screen region that is most likely to be a code editor. Finally, psc2code calls the API of a professional OCR tool to extract source code from the cropped code regions and leverages the OCRed cross-frame information in the programming screencast and the statistical language model of a large corpus of source code to correct errors in the OCRed source code. We conduct an experiment on 1,142 programming screencasts from YouTube. We find that our CNN-based image classification technique can effectively remove the non-code and noisy-code frames, which achieves an F1-score of 0.95 on the valid code frames. We also find that psc2code can significantly improve the quality of the OCRed source code by truly correcting about half of incorrectly OCRed words. Based on the source code denoised by psc2code, we implement two applications: (1) a programming screencast search engine; (2) an interaction-enhanced programming screencast watching tool. Based on the source code extracted from the 1,142 collected programming screencasts, our experiments show that our programming screencast search engine achieves the precision@5, 10, and 20 of 0.93, 0.81, and 0.63, respectively. We also conduct a user study of our interaction-enhanced programming screencast watching tool with 10 participants. This user study shows that our interaction-enhanced watching tool can help participants learn the knowledge in the programming video more efficiently and effectively.",
        "keywords": "Programming videos; deep learning; code search",
        "released": 2020,
        "link": "https://doi.org/10.1145/3392093"
    },
    {
        "title": "Virtual reality surgical simulator software development tools",
        "abstract": "Virtual reality (VR) surgical simulations are among the most difficult software applications to develop mainly because of the type of user interactions that they must support. Surgery typically includes precise cutting of often intricate structures. Modelling these structures and accurately simulating their response to user interaction requires many software components to effectively work in unison. Some of these components are readily available but are tailored to more common applications such as computer games or open-world simulations such as flight-simulators. This article explores the software libraries that are currently available to developers of VR surgical simulation software. Like computer games and other VR simulations, VR surgical simulations require real-time lighting and rendering systems and physics-based interactions. However, in addition they require haptic interaction with cut-able and deformable soft-tissue, a key requirement that is not supported by the majority of the available tools. In this article, we introduce currently available software development tools and the specific benefits and limitations that can be encountered when using them to develop VR surgical simulations. We also provide a detailed review of collision detection libraries that are central to achieving reliable haptic rendering. Journal of Simulation (2013) 7, 101-108. doi:10.1057/jos.2012.22; published online 14 December 2012",
        "keywords": "virtual reality; simulation; surgery; software",
        "released": 2013,
        "link": "https://doi.org/10.1057/jos.2012.22"
    },
    {
        "title": "Automated deployment mechanism of containerized communication micro-services for smart manufacturing applications",
        "abstract": "To swiftly and reliably monitor various information and the operational status of machinery across large smart manufacturing sites, this study introduces the use of LPWAN multi-mode network communication technology. This technology can automatically switch between LoRa and NB-IoT modes based on signal strength to ensure communication stability. Through wireless communication technologies such as LoRa and NB-IoT, the status information of machinery can be transmitted back to the cloud in real-time, facilitating user management. However, LPWAN multi-mode network communication modules often adopt a monolithic architecture, making maintenance and upgrades more difficult. When an application within the module needs upgrading, not only must the consistency between the development environment and the execution environment be ensured, but also a significant amount of time and resources must be spent on on-site deployment. With the development of cloud computing and virtualization technologies, containerized microservices architecture, which focuses on replacing functional modules with services, is set to become the mainstream for future industrial applications. Therefore, this study proposes a remote communication architecture based on container and microservices technologies. Utilizing the concept of microservices, this architecture divides LPWAN multi-mode network communication modules based on different functionalities and offers them to users in a more flexible service manner through containerization technology. This study also designs a mechanism to automate the entire service construction process, followed by the implementation of communication services’ automatic deployment through container management tools. Compared to manual deployment, this significantly reduces the waste of time and human resources. Finally, this study uses a large mobile pumping unit as a practical application case to verify the feasibility of the proposed architecture. In the context of flood prevention and disaster relief, large mobile pumping units are widely used to solve flooding issues. These pumps are often deployed in dangerous areas with poor signal reception, thereby also validating the value of the proposed architecture.",
        "keywords": "Micro-services; Docker container technology; cloud information monitoring system; Internet of Things; edge computing",
        "released": 2024,
        "link": "https://doi.org/10.1177/09544054241249777"
    },
    {
        "title": "Interactive web environment for collaborative and extensible diagram based learning",
        "abstract": "Nowadays there is a growing need of ubiquity for learning, research and development tools, due to the portability and availability problems concerning traditional desktop applications. In this paper, we suggest an approach to avoid any further download or installation. The main goal is to offer a collaborative and extensible web environment which will cover a series of domains highly demanded by different kinds of working groups, in which it is crucial to have tools which facilitate the exchange of information and the collaboration among their members. The result of those interactions would be the development of one or several diagrams accessible from any geographical location, independently of the device employed. The environment can be adapted through personalized components, depending on the type of diagram that the user wants to interact with and the users can also create new elements or search and share components with other users of the community. By means of this environment, it will be possible to do research on the usability of collaborative tools for design diagrams, as well as research on the psychology of group interactions, assessing the results coming from. the employment of known methodologies, techniques, paradigms or patterns, both at an individual and at a collaborative group level. Crown Copyright (C) 2009 Published by Elsevier Ltd. All rights reserved.",
        "keywords": "Collaborative; Extensible; Interactive; Web environment; Diagram design; Groupware usability; User tracking; Graph visualization; Semantic web",
        "released": 2010,
        "link": "https://doi.org/10.1016/j.chb.2009.10.003"
    },
    {
        "title": "Graph grammars according to the type of input and manipulated data: A survey",
        "abstract": "Graph grammars which generate graphs are a generalization of Chomsky grammars that generate strings. During the last decades there has been a remarkable development of graph grammars. Due to their wide diversity of applications, graph grammars have received a particular attention from many scientists and researchers. There has been applications of graph grammars in several areas such as pattern recognition, data base systems, biological developments in organisms, semantics of programming languages, compiler construction, software development environments, etc. In the literature, in some surveys, graph grammars have been studied and classified according to some criteria such as: parallel or sequential applicability of rules, embedding mechanism, type of generated graphs, etc. In addition to this, as data play an important role more and more in different domains, we survey in this paper the vast field of graph grammars by classifying them according to three criteria: the number of manipulated data (single or multiple types), the nature of data (structured or unstructured), and finally the kind of data (images, graphs, patterns, etc.). In particular, we consider that a graph grammar is well defined by five components instead of four, namely: type of generated graphs (T-G), a start graph (Z), a set of production rules (P), a set of additional specifications of the rules (A), and the criterion that we additionally consider which is the type of input and manipulated data (TD). This proposed formalism, especially with the added fifth component, may serve to overcome some issues related to Big Data and Cloud Computing domains. (C) 2018 Elsevier Inc. All rights reserved.",
        "keywords": "Graph grammar; Type of input and manipulated data; Type of generated graph; Big Data; Cloud computing; Application",
        "released": 2018,
        "link": "https://doi.org/10.1016/j.cosrev.2018.04.001"
    },
    {
        "title": "The effect of textbook analysis as a teacher professional development tool on teacher understanding of nature of science",
        "abstract": "This article reports on the effect of textbook analysis as a tool of teacher professional development on nature of science (NOS) understanding of 10 science teachers in South Africa. The teacher professional development program (TPDP) was based on an explicit reflective methodology of textbook analysis and conducted online due to the Covid-induced lockdown. NOS understanding of the participant teachers was documented pre-training and post-training using a questionnaire designed by the researchers, termed the IFVNOS questionnaire. This tool was formulated based on the views of nature of science questionnaire version C (VNOSC) and the reconceptualised family resemblance approach (RFN) questionnaire. The same tool was used pre- and post-training. A comparison was made of the pre- and post-training results and it was found that there was a general individual increase in NOS understanding in 9 of the 10 teachers. The creative, scientific knowledge, science methods and ethical practices NOS aspects showed the greatest improvement in understanding by the teachers as a collective, whilst inferential NOS showed no overall change in understanding. This study showed that textbook analysis can be used as a professional development tool to improve NOS understanding of in-service science teachers.",
        "keywords": "",
        "released": 2023,
        "link": "https://doi.org/10.1007/s11191-023-00442-7"
    },
    {
        "title": "Global subjects or objects of globalisation? The promotion of global citizenship in organisations offering sport for development and/or peace programmes",
        "abstract": "Sport for Development and Peace (sdp) has been adopted as a “development tool” by Western development practitioners and a growing number of development organisations. Sport is frequently referred to as a “global language” and used to promote international awareness and cross-cultural understandingtwo key themes in global citizenship literature. In this paper I examine the language adopted by organisations promoting sdpspecifically, what sdp organisations say they do as well as the nature and implications of their discourses. Drawing on a large and growing body of literature on global citizenship and post-structuralism, and on post-colonial critiques, I argue that sdp narratives have the potential to reinforce the “Othering” of community members in developing countries and may contribute to paternalistic conceptions of development assistance. In so doing, they weaken the potential for more inclusive and egalitarian forms of global citizenship. The article examines the discourse of sdp organisational material found online and analyses it in the context of broader sport and colonialism literature. The work of SDP organisations is further examined in relation to global citizenship discourse with a focus on the production and projectionof global subjects, or objects of globalisation, and what this means for development “beneficiaries”.",
        "keywords": "",
        "released": 2011,
        "link": "https://doi.org/10.1080/01436597.2011.573946"
    },
    {
        "title": "Pegylated interferon treatment for the effective clearance of hepatitis b surface antigen in inactive HBsAg carriers: A meta-analysis",
        "abstract": "Background: Expanding antiviral therapy to benefit more populations and optimizing treatment to improve prognoses are two main objectives in current guidelines on antiviral therapy. However, the guidelines do not recommend antiviral therapy for inactive hepatitis B surface antigen (HBsAg) carriers (IHCs). Recent studies have shown that antiviral therapy is effective with good treatment outcomes in IHC populations. We conducted a systematic review and meta-analysis of HBsAg clearance and conversion in IHCs. Methods: We searched PubMed, Embase, Medline, and Web of Science to retrieve articles on HBsAg clearance in IHCs published between January 2000 and August 2021. Data were collected and analysed using the random-effects model for meta-analysis. Results: A total of 1029 IHCs from 11 studies were included in this analysis. The overall HBsAg clearance rate was 47% (95% confidence interval (CI): 31% - 64%), with a conversion rate of 26% (95% CI: 15% - 38%) after 48 weeks of Pegylated interferon (Peg-IFN) treatment. In the control group (including nucleos(t)ide analogue (NA) treatment or no treatment), the overall HBsAg clearance rate was only 1.54% (95% CI: 0.56% - 3.00%), which was markedly lower than that in the Peg-IFN group. Further analysis showed that a low baseline HBsAg level and long treatment duration contributed to a higher HBsAg clearance rate. Conclusion: This study showed that treatment of IHCs can be considered to achieve a clinical cure for chronic hepatitis B virus (HBV) infection. After Peg-IFN treatment, the HBsAg clearance rate was 47%, and the conversion rate was 26%, which are markedly higher than those reported by previous studies on Peg-IFN treatment in patients with chronic hepatitis B (CHB). A low baseline HBsAg level and long treatment duration were associated with HBsAg clearance in IHCs. Therefore, antiviral therapy is applicable for IHCs, a population who may be clinically cured. Systematic Review Registrationhttp://www.crd.york.ac.uk/PROSPERO, CRD): CRD42021259889.",
        "keywords": "HBV; HBsAg; high clearance rate; IHC; meta-analysis",
        "released": 2021,
        "link": "https://doi.org/10.3389/fimmu.2021.779347"
    },
    {
        "title": "Copyright law under siege: An inquiry into the legitimacy of copyright protection in the context of the global digital divide",
        "abstract": "in the digital age, information technology wields immense power to disseminate an unprecedented amount of information and knowledge inexpensively and ubiquitously. Information technology is no longer a luxury, but a development tool and a crucial means of information and knowledge exchange in the digital age. Therefore, people in the developing countries are offered unprecedented opportunities to acquire knowledge and enhance their educational systems. By using distance-learning technologies, research databases, or online digital libraries, it is estimated that researchers or students in developing countries would be enabled to get access to virtually the same electronic journals, books, and databases as their counterparts at the world’s leading educational institutions. Hence, the universal realization of the right to development, the right to education, and the right to freedom of expression, enshrined in the major international and regional instruments for the protection of human rights, will be significantly reinforced in the digital age. Due to the widening digital divide and ever-expanding protection of copyright, however, this article argues that, contrary to optimistic predictions, people in the less developed countries are unlikely to become the beneficiaries of the information technology revolution.",
        "keywords": "",
        "released": 2005,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000229001000003"
    },
    {
        "title": "xBCI: A generic platform for development of an online BCI system",
        "abstract": "A generic platform for realizing an online brain-computer interface (BCI) named xBCI was developed. The platform consists of several functional modules (components), such as data acquisition, storage, mathematical operations, signal processing, network communication, data visualization, experiment control, and real-time feedback presentation. Users can easily build their own BCI systems by combining the components on a graphical-user-interface (GUI) based diagram editor. They can also extend the platform by adding components as plug-ins or by creating components using a scripting language. The platform works on multiple operating systems and supports parallel (multi-threaded) data processing and data transfer to other PCs through a network transmission control protocol/internet protocol or user datagram protocol (TCP/IP or UDP). A BCI system based on motor imagery and a steady-state visual evoked potential (SSVEP) based BCI system were constructed and tested on the platform. The results show that the platform is able to process multichannel brain signals in real time. The platform provides users with an easy-to-use system development tool and reduces the time needed to develop a BC! system. (C) 2010 Institute of Electrical Engineers of Japan. Published by John Wiley & Sons, Inc.",
        "keywords": "brain-computer interface (BCI); generic platform; electroencephalography (EEG)",
        "released": 2010,
        "link": "https://doi.org/10.1002/tee.20560"
    },
    {
        "title": "Mining unit tests for discovery and migration of math APIs",
        "abstract": "Today’s programming languages are supported by powerful third-party APIs. For a given application domain, it is common to have many competing APIs that provide similar functionality. Programmer productivity therefore depends heavily on the programmer’s ability to discover suitable APIs both during an initial coding phase, as well as during software maintenance. The aim of this work is to support the discovery and migration of math APIs. Math APIs are at the heart of many application domains ranging from machine learning to scientific computations. Our approach, called MATHFINDER, combines executable specifications of mathematical computations with unit tests (operational specifications) of API methods. Given a math expression, MATHFINDER synthesizes pseudo-code comprised of API methods to compute the expression by mining unit tests of the API methods. We present a sequential version of our unit test mining algorithm and also design a more scalable data-parallel version. We perform extensive evaluation of MATHFINDER (1) for API discovery, where math algorithms are to be implemented from scratch and (2) for API migration, where client programs utilizing a math API are to be migrated to another API. We evaluated the precision and recall of MATHFINDER on a diverse collection of math expressions, culled from algorithms used in a wide range of application areas such as control systems and structural dynamics. In a user study to evaluate the productivity gains obtained by using MATHFINDER for API discovery, the programmers who used MATHFINDER finished their programming tasks twice as fast as their counterparts who used the usual techniques like web and code search, IDE code completion, and manual inspection of library documentation. For the problem of API migration, as a case study, we used MATHFINDER to migrate Weka, a popular machine learning library. Overall, our evaluation shows that MATHFINDER is easy to use, provides highly precise results across several math APIs and application domains even with a small number of unit tests per method, and scales to large collections of unit tests.",
        "keywords": "Algorithms; Design; Experimentation; API discovery; API migration; mathematical computation; mining; unit tests",
        "released": 2014,
        "link": "https://doi.org/10.1145/2629506"
    },
    {
        "title": "Designing and building an on-line community: The struggle to support sociability in the inquiry learning forum",
        "abstract": "In this paper we describe the sociotechnical structures of the Inquiry Learning Forum (ILF), a Web-based professional development tool designed to support a community of inservice and preservice mathematics and science teachers creating, sharing, and improving inquiry-based pedagogical practices, Founded in our previous research and consistent with our pedagogical commitment, the technical structures of the ILF have been designed around a “visiting-the-classroom” metaphor. This decision was based on our belief that teachers need to be full participants in, and owners of, their virtual space for meaningful interaction to occur. The hallmark of this environment is that teachers with a broad range of experience and expertise can come together in an on-line environment to observe, discuss, and reflect on pedagogical theory and practice anchored to actual teaching vignettes. The goal of this paper is to share how we instantiated our pedagogical commitments and to describe the challenges we faced during the design, development, implementation, and analysis of the ILF. Toward this end, we walk the reader through our design and implementation process, highlighting our change in focus from usability to sociability issues, and movement from conceiving the ILF as an electronic structure to a sociotechnical interaction network.",
        "keywords": "",
        "released": 2001,
        "link": "https://doi.org/10.1007/BF02504948"
    },
    {
        "title": "Identification and analysis of the highly cited knowledge base of sustainability science",
        "abstract": "We investigated the interdisciplinary “pillars” of scientific knowledge on which the emerging field of sustainability science is founded, using a bibliometric approach and data from the Web of Science database. To find this scientific basis, we first located publications that represent a relevant part of sustainability science and then extracted the set of best cited publications, which we called the highly cited knowledge base (HCKB). To find the research orientation in this set, we inspected the occurrence of fields and contrasted this with the occurrence of fields in other publication sets relevant to sustainability science. We also created a network of co-cited HCKB publications using the seed set citations, extracted communities or clusters in this network and visualised the result. Additionally, we inspected the most cited publications in these HCKB clusters. We found that themes related to the three pillars of sustainable development (environment, economy and sociology) are all present in the HCKB, although social science (not including economics) is less visible. Finally, we found increasing diversity of fields and clusters in the citations of the seed set, indicating that the field of sustainability science is not yet moving into a more transdisciplinary state.",
        "keywords": "Sustainability science; Knowledge base; Bibliometric analysis; Network visualisation; Clustering; Trends; Transdisciplinarity",
        "released": 2013,
        "link": "https://doi.org/10.1007/s11625-012-0185-1"
    },
    {
        "title": "Spaced education faculty development may not improve faculty teaching performance ratings in a surgery department",
        "abstract": "OBJECTIVES: To determine the effectiveness of spaced education as a faculty development tool designed to improve teaching skills in a surgery department. DESIGN: Faculty members were randomized to receive either weekly spaced education e-mails with content designed to improve teaching skills (group A) or no e-mails (group B). Using qualitative and quantitative surveys, we assessed both medical students’ perception of faculty members’ teaching effectiveness and faculty members’ perception of the usefulness of the spaced education e-mails. SETTING: Academic medical center. PARTICIPANTS: Twenty-nine surgery faculty members with teaching responsibility for medical students in their Core Surgery Clerkship. RESULTS: All 41 medical students who rotated through the Core Surgery Clerkship rated the quality of teaching for each faculty members; 172 online rating surveys were completed. Overall, faculty members received high ratings on the teaching skills included on the surveys. Additionally, no significant differences were found between the perceived skill level of the faculty members who received the weekly e-mails and those who did not. Specifically, 53.8% and 54% (p = 0.47) of the faculty were felt to deliver feedback more than three times per week; 87.1% and 89.9% (p = 0.15) of faculty were felt to deliver useful feedback; 89.2% and 90.8% (p = 0.71) of faculty were perceived to encourage student autonomy; and 78.1% and 81.9% (p = 0.89) of faculty were felt to set clear learning expectations for students. Postprogram comments from faculty revealed they did not find the e-mails useful as a faculty development tool. CONCLUSIONS: Students perceived high levels of teaching skills among the clinical faculty. Faculty members who received e-mail-based spaced education-based faculty development were not rated to be more effective teachers on the student surveys. Electronically based faculty development does not satisfy faculty expectations. (J Surg 69:52-57. (C) 2012 Association of Program Directors in Surgery. Published by Elsevier Inc. All rights reserved.)",
        "keywords": "medical students; faculty development; spaced education; teaching; feedback; surgery",
        "released": 2012,
        "link": "https://doi.org/10.1016/j.jsurg.2011.06.013"
    },
    {
        "title": "Constructing home monitoring system with node-RED",
        "abstract": "The aim of this study is to design a home monitoring system with a simple and clear visual interface that integrates home appliances, disaster prevention, and surveillance facilities. Furthermore, the Node-RED visual development environment was utilized to create the system, and the users will be able to configure the environmental parameters by themselves with its simple and graphical programming interface, to customize the user interface. In this study, we utilize the Node-RED development software installed on a personal computer and the Message Queuing Telemetry Transport (MQTT) to communicate and exchange data with home devices under the publish/subscribe mode, so that the stability of data transmissions can be ensured while more resources are saved; in addition, MySQL database management tools are used to access the data stored in the home devices for the statistics and analysis of changes in environmental data. Moreover, the HyperText Transfer Protocol (HTTP) is also used to acquire various information from the government’s open source data platform, and the information will be integrated and displayed on the Node-RED webpage interface. Since Node-RED uses a webpage interface, the devices that can use web browsers to view webpages, such as computers and smart phones, can thus be used to remotely monitor the Node-RED program. Finally, we utilize the Node-RED development software installed on a personal computer and MQTT to communicate and exchange data with the home devices under the publish/subscribe mode. The smart home system is divided into input and output parts, which control the electrical appliances and read the sensor information. Ten switches on the Web control interface are used to control the living room, kitchen, balcony, and bedroom light switches, air-conditioning, heating, exhaust fans, outdoor lights, doors, and alarm switches.",
        "keywords": "home monitoring system; Node-RED; MQTT; database; remote monitoring",
        "released": 2020,
        "link": "https://doi.org/10.18494/SAM.2020.2686"
    },
    {
        "title": "Living history and the communication of archaeology with the public",
        "abstract": "Living history as a term encompasses a wide range of attempts at presenting the facts, objects, persons, and events from history. It can be defined as a pre-planned, historically researched, and appropriately equipped educational and touristic activity in which the participants follow a strict plan which brings some aspects of a historical event or period to life. It was present in classical Rome, and perhaps we can also speak of some earlier phenomena. In these cases, all the way to the very 20th century, living history usually did not have an educational purpose, it had religious or political, and propaganda purpose. We know of re-enactments of historical battles in Roman amphitheatres and other venues, be it land or naval battles (naumachiae). The medieval period also follows this pattern and there were new re-enactments of famous battles. Only from the 19th century onwards can we speak of the educational character that these events start to acquire, although political propaganda is still not completely removed from these activities even today, unfortunately. Today, this type of interpretation of history is approached through postmodern thought. Living history is analysed in the context of the change of today’s approach to the past, as a community’s attempt of constructing individual or group identity, living history as a manifestation of the search of our own, sensory, direct contact with the past and its interpretations, living history as a manifestation of commercializing the past, living history as a new form of teaching about the past, living history as a ludic phenomenon, living history as an element of the contemporary culture of history, living history as a form of new spirituality, and finally, living history as a form of popularization of archaeology (Pawleta 2018: 1025). The most common reasons for using techniques of living history are an efficient interpretation of material culture, testing of an assumption from archaeology or collecting data for research in history and ethnology, as well as simple participation in a recreational activity that can also have an educational purpose. The main tool of living history is interpretation as a communication process between the audience and the venue of living history through the direct experience of interaction with people and objects. Living history covers re-enactment of battles, but also scenes from everyday life, with the use of costumes and replicas produced using contemporary technology or methods of experimental archaeology. Experimental archaeology undeniably benefits from the techniques of living history, and archaeologists specialized in this subfield of archaeology are often members of living history societies. With various reconstructions of objects, venues, and activities, living history helps with research of all periods around the world, from the use of Palaeolithic spear-throwers, textile manufacturing, and the interpretation of the Iron Age in Europe, to the use of Roman shoes on long marches in full gear or construction of medieval forts. Besides in experimental archaeology and science in general, living history is a very efficient technique of popularization of archaeology and history as disciplines, and in the world, it proved to be the best interpretative tool and attracting factor for audiences to various events and archaeological parks across Europe. The technique of living history is used more and more in recent decades in Croatia as well, as part of events with the goal of popularization of archaeology, such as the antiquity festival “Sepomaia Viva”, “Dani Andautonije” (Andautonia days), “Tisucljeca kulinarstva” (Millennia of culinary art), “Dani Ad Turresa” (Ad Turres days), “Cibalitanska noc” (Night in Cibalae), “Rimske noci u Naroni” (Roman nights in Narona), “Burnumske ide” (Ides of Burnum), “Bakanalije” (Bacchanalia) festival, “Halstatski dani” (Hallstatt days) event, and others. Recently, new forms of popularization of historical and archaeological disciplines thought the cooperation of archaeology and living history in Croatia, as can be seen in the example of the “Legio VI Herculia” project of the Institute of Archaeology and Guardians of Zagreb association directed towards the reconstruction of military and civil life of Roman legion on the Roman Limes on the Danube river at the beginning of the 4th century. This project creates new content on multiple levels (live, through multimedia, online, interactively and in other ways) by presenting to Croatian, European and world public a so-far mostly unknown or inaccurately interpreted and presented historical period in Croatia - the period of the Roman rule at the beginning of the 4th century - using state- of-the-art methods. Moreover, this project shows tendencies of linking with current European and beyond projects (scientific, as well as the living history ones), but also the projects of local communities, creating a new kind of cultural cohesion in popularization and representation of historical content in cutting-edge methods.",
        "keywords": "historical re-enactment; experimental archaeology; guided interpretation; living history museum; museum theatre",
        "released": 2021,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000741198000003"
    },
    {
        "title": "An educator’s toolkit for virtual simulation: A usability study",
        "abstract": "Background: The rapid change brought on by COVID-19 meant that many educators adopted virtual simulation quickly, often without having a strong background in the pedagogical principles of virtual simulation. To address this resource gap, a team of 21 Ontario virtual simulation-experienced educators created the freely available, online, interprofessional Virtual Simulation Educator’s Toolkit. The Toolkit provides the theory, practical strategies and resources needed to teach effectively with virtual simulation. Because the Toolkit would include new content, resources and numerous untested design elements, the team conducted a usability test. Design: A usability study, that followed a three-step process, was conducted. Participants were asked to carefully review the Toolkit, including all interactive components and complete an online survey based on the Technology Acceptance Model, to measure the ease of use and utility of the Toolkit. Lastly, participants were interviewed. Participants: In keeping with sampling principles for usability studies, twenty experienced virtual simulation educators from different disciplines participated in the study. Methods: A validated survey and one-on-one interviews were used to measure educators’ perceptions of the ease of use and utility of the Toolkit and their recommendations to improve it. Quantitative data were analyzed using descriptive statistics and qualitative data were coded and themes developed. Conclusion: The score of 90 % on the Toolkit Experience Survey indicated that participants found the Toolkit both easy to use and useful. Results from the interviews indicated that a practical, accessible professional development tool is urgently needed to support education with virtual simulation. Because the Toolkit introduces educators to different ways of implementing virtual simulation, as well as options, process considerations, and tools to enhance their teaching with virtual simulation, different disciplines and educators with different levels of simulation experience may benefit by using it.",
        "keywords": "Virtual simulation; Healthcare professions education; Simulation pedagogy; Usability study",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.nedt.2022.105600"
    },
    {
        "title": "eHealth policy framework in low and lower middle-income countries; a PRISMA systematic review and analysis",
        "abstract": "Background Low and lower middle-income countries suffer lack of healthcare providers and proper workforce education programs, a greater spread of illnesses, poor surveillance, efficient management, etc., which are addressable by a central policy framework implementation. Accordingly, an eHealth policy framework is required specifically for these countries to successfully implement eHealth solutions. This study explores existing frameworks and fills the gap by proposing an eHealth policy framework in the context of developing countries.Methods This PRISMA-based (PRISMA Preferred Reporting Items For Systematic Reviews and Meta-Analyses) systematic review used Google Scholar, IEEE, Web of Science, and PubMed latest on 23(rd) May 2022, explored 83 publications regarding eHealth policy frameworks, and extracted 11 publications scrutinizing eHealth policy frameworks in their title, abstract, or keywords. These publications were analyzed by using both expert opinion and Rstudio programming tools. They were explored based on their developing/developed countries’ context, research approach, main contribution, constructs/dimensions of the framework, and related categories. In addition, by using cloudword and latent semantic space techniques, the most discussed concepts and targeted keywords were explored and a correlation test was conducted to depict the important concepts mentioned in the related literature and extract their relation with the targeted keywords in the interest of this study.Results Most of these publications do not develop or synthesize new frameworks for eHealth policy implementation, but rather introduce eHealth implementation frameworks, explain policy dimensions, identify and extract relevant components of existing frameworks or point out legal or other relevant eHealth implementation issues.Conclusion After a thorough exploration of related literature, this study identified the main factors affecting an effective eHealth policy framework, found a gap in the context of developing countries, and proposed a four-step eHealth policy implementation guideline for successful implementation of eHealth in the context of developing. The limitation of this study is the lack of a proper amount of practically implemented eHealth policy framework cases in developing countries published in the literature for the review. Ultimately, this study is part of the BETTEReHEALTH (More information about the BETTEReHEALTH project at ) project funded by the European Union Horizon’s 2020 under agreement number 101017450.",
        "keywords": "eHealth policy framework; Developing countries; Low and Lower-Middle Income Countries; PRISMA systematic review",
        "released": 2023,
        "link": "https://doi.org/10.1186/s12913-023-09325-7"
    },
    {
        "title": "A field study of the wheel - a usability engineering process model",
        "abstract": "Interactive system developers are increasingly including usability engineering as an integral part of interactive system development. With recognition of the importance of usability come attempts to structure this new aspect of overall system development, leading to a variety of processes and methodologies. Unfortunately, these processes often lack flexibility, customizability, completeness, and breadth of coverage. This paper describes our development of a meta process or process model that we call the Wheel. This innovative approach to creating and tailoring usability engineering processes addresses these shortcomings, and describes an evaluation of its application in a real-world commercial development environment. The Wheel process model for usability engineering is not a process itself, but instead, it provides a general framework into which developers can fit specific existing or new techniques, methods, or activities to apply “best usability practices”. It grew out of our examination. adaptation. and extension of several existing usability engineering and software methodologies. The methods that most strongly guided creation of the Wheel were the LUCID framework of interaction design, the Star life cycle of usability engineering, and the waterfall and spiral models of software engineering. The resulting process model assumes the form of a sequence of distinct cycles (each of which produces a product form), allowing developers to focus on each cycle separately. Each cycle has the same four activity types: Analyze. Design. Implement, and Evaluate. Each activity type in a cycle is instantiated using an existing usability engineering technique. Working with an Internet technology company in northern Virginia under grant sponsorship from the Virginia Center for Innovative Technology (a research and development incubator for the Commonwealth of Virginia), we instantiated the Wheel process model and used it to develop a Web-based device management system. The process model performed remarkably well for this development environment. overcoming the tight constraints of budget and schedule cuts to produce an excellent process instance that resulted in a demonstration prototype of the company’s target system. (C) 2005 Elsevier Inc. All rights reserved.",
        "keywords": "usability engineering; interaction design; process model; interactive systems development; user-centered design; field study",
        "released": 2006,
        "link": "https://doi.org/10.1016/j.jss.2005.08.023"
    },
    {
        "title": "Effects of tai chi on health status in adults with chronic heart failure: A systematic review and meta-analysis",
        "abstract": "Background: Chronic heart failure (CHF) is among the top causes of cardiovascular morbidity, and most patients with CHF have poor health status. Tai Chi, a mind-body exercise that originated in China, is beneficial for health status. This study was conducted to evaluate the effects of Tai Chi on health status in adults with CHF. Methods: The Cochrane Library, PubMed, Embase, Web of Science, China National Knowledge Infrastructure, Wanfang Database, Chinese Biomedical Database, and Chinese Scientific Journal Database were searched from the inception to 22 October 2021. This meta-analysis was performed using the fixed- or random-effects model. Continuous outcomes were carried out using mean difference (MD) or standardized mean difference (SMD) with 95% confidence interval (CI). Dichotomous outcomes were determined using risk ratio (RR) with 95%CI. The Grading of Recommendations, Assessment, Development and Evaluations (GRADE)pro Guideline Development Tool (GDT) online software was used to present outcome-specific information regarding overall certainty of evidence from studies. Results: In total, 15 studies including 1,236 participants were finally included. Compared with usual care alone, Tai Chi combined with usual care achieved efficacy in improving Minnesota Living with Heart Failure Questionnaire (MD = -8.51; 95% CI: -10.32 to -6.70; p < 0.00001), 6-min walk test (MD = 43.47; 95% CI: 33.38 to 54.10; p < 0.00001), left ventricular ejection fraction (MD = 6.07; 95% CI: 3.44 to 8.70; p < 0.00001), B-type natriuretic peptide/N-terminal fragment of pro-BNP (SMD = -1.12; 95% CI: -1.70 to -0.54; p = 0.0002), Hamilton Depression Rating Scale (MD = -2.89; 95% CI: -4.87 to -0.91; p = 0.004), Pittsburgh Sleep Quality Index (MD = -2.25; 95% CI: -3.88 to -0.61; p = 0.007), timed up and go test (MD = -1.34; 95% CI: -2.50 to -0.19; p = 0.02), and reduced the risk of heart failure hospitalization (RR = 0.47; 95% CI: 0.25 to 0.88; p = 0.02). However, there was no difference in the outcome of peak oxygen uptake (MD = 1.38; 95% CI: -1.51 to 4.28; p = 0.35). All-cause mortality or cardiovascular death could not be evaluated due to insufficient data. The certainty of evidence ranged from very low to moderate due to the risk of bias, inconsistency, imprecision, and publication bias. Conclusion: Tai Chi might be safe and showed beneficial effects on health status in patients with CHF. However, more high-quality and long-term studies are still needed to further evaluate the effects of Tai Chi.",
        "keywords": "Tai Chi; chronic heart failure; health status; systematic review; meta-analysis",
        "released": 2022,
        "link": "https://doi.org/10.3389/fcvm.2022.953657"
    },
    {
        "title": "Automatically visualise and analyse data on pathways using PathVisioRPC from any programming environment",
        "abstract": "Background: Biological pathways are descriptive diagrams of biological processes widely used for functional analysis of differentially expressed genes or proteins. Primary data analysis, such as quality control, normalisation, and statistical analysis, is often performed in scripting languages like R, Perl, and Python. Subsequent pathway analysis is usually performed using dedicated external applications. Workflows involving manual use of multiple environments are time consuming and error prone. Therefore, tools are needed that enable pathway analysis directly within the same scripting languages used for primary data analyses. Existing tools have limited capability in terms of available pathway content, pathway editing and visualisation options, and export file formats. Consequently, making the full-fledged pathway analysis tool PathVisio available from various scripting languages will benefit researchers. Results: We developed PathVisioRPC, an XMLRPC interface for the pathway analysis software PathVisio. PathVisioRPC enables creating and editing biological pathways, visualising data on pathways, performing pathway statistics, and exporting results in several image formats in multiple programming environments. We demonstrate PathVisioRPC functionalities using examples in Python. Subsequently, we analyse a publicly available NCBI GEO gene expression dataset studying tumour bearing mice treated with cyclophosphamide in R. The R scripts demonstrate how calls to existing R packages for data processing and calls to PathVisioRPC can directly work together. To further support R users, we have created RPathVisio simplifying the use of PathVisioRPC in this environment. We have also created a pathway module for the microarray data analysis portal ArrayAnalysis.org that calls the PathVisioRPC interface to perform pathway analysis. This module allows users to use PathVisio functionality online without having to download and install the software and exemplifies how the PathVisioRPC interface can be used by data analysis pipelines for functional analysis of processed genomics data. Conclusions: PathVisioRPC enables data visualisation and pathway analysis directly from within various analytical environments used for preliminary analyses. It supports the use of existing pathways from WikiPathways or pathways created using the RPC itself. It also enables automation of tasks performed using PathVisio, making it useful to PathVisio users performing repeated visualisation and analysis tasks. PathVisioRPC is freely available for academic and commercial use at http://projects.bigcat.unimaas.nl/pathvisiorpc.",
        "keywords": "Automation; Biological pathways; Data visualisation; Multi-omics; Pathway analysis; Pathway building; R package; Workflow integration",
        "released": 2015,
        "link": "https://doi.org/10.1186/s12859-015-0708-8"
    },
    {
        "title": "Robert h. Waterman, jr., on being smart and lucky",
        "abstract": "Robert Waterman spent 21 years at McKinsey & Co., where he was a senior director. Since In Search of Excellence he has continued his research and writing. He has written The Renewal Factor: How the Best Get and Keep The Competitive Edge (1987), Adhocracy: The Power to Change (1990), and What America Does Right: Learning from Companies That Put People First (1994). In these studies Bob has stayed true to his belief that the keys to excellent performance are found in human interactions. In particular, these books highlight the roles of empowerment and leadership in effective managers. The willingness to give power to others and the ability to mentor and coach others are common themes in the successful managers and organizations he has studied. He was also instrumental in founding MindSteps, Inc., a leading provider of online career self-assessment and development tools for individuals and corporations. In January of 2000, MindSteps merged with University ProNet, a ten-year-old venture founded by the alumni associations of Stanford, MIT, UC Berkeley, and UCLA. to form eProNet, where Waterman serves as co-chairman of the board. He is also a founding director of AES Corporation, and he serves on the boards of McKesson Corporation and Boise Cascade Corporation. He is a trustee of the World Wildlife Fund and an advisor to the National Academy of Sciences.",
        "keywords": "",
        "released": 2002,
        "link": "https://doi.org/10.5465/AME.2002.6640158"
    },
    {
        "title": "Influence of the urban built environment on physical and mental health of the elderly under the background of big data",
        "abstract": "With the advent of the information technology revolution and the Internet era, information technology is gradually occupying an important position and becoming an important strategic factor in economic development. As an emerging technology that has been developing continuously in recent years, big data is becoming an important industry to improve the innovation and development of the urban economy. Like AI technology, cloud computing, and the Internet, big data has become an important application technology for economic growth and economic efficiency improvement in today’s world. It is an effective means of progress and development in a region and an important strategic resource. As a new technology, big data has attracted more and more attention from all walks of life. Many companies have turned their attention to developing big data for economic benefits. “Enjoy your old age” is the yearning of every old man and his family. In recent years, the national level has been committed to “creating an urban built environment for the elderly to achieve healthy aging.” From the perspective of promoting the physical and mental health of the elderly, this paper analyzes the impact of the urban built environment on the physical and mental health of the elderly based on the needs of the elderly and puts forward countermeasures and suggestions based on the current status and existing problems of the urban built environment for the elderly. Based on the combined data analysis method and technology in big data, this paper conducted a field questionnaire survey on a total of 4,000 elderly people in urban and rural areas by means of the questionnaire survey. It is found that the existing problems of the built environment in the old cities include scattered content, one-sided understanding, and rigid design. According to the problems, the solutions of building consensus, paying attention to planning, combining urban characteristics, and the joint efforts of all sectors of society are put forward. And programming tools are used to combine formulas and analyze related data in detail. The analysis results show that the physical and mental health index of the elderly is highly correlated with factors such as changes in the consensus degree of the urban built environment, urban built environment planning, urban built environment policy support, and multiparty efforts in the urban built environment. Changes show a positive change.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1155/2022/4266723"
    },
    {
        "title": "Development and evaluation of online education to increase the forensic relevance of oral health records",
        "abstract": "BackgroundHuman identification can be reliably established by dental comparison; success is significantly impacted by inadequate ante-mortem information. Previous Australian research revealed suboptimal recording of features important for forensic dental identification and compliance with Dental Board of Australia guidelines. We hence created and evaluated an online education programme aimed at improving oral health practitioner recording. MethodsAn interactive learning module (ILM) was constructed and released to three focus groups representing practitioners with varying experience levels: Australian Society of Forensic Odontology members, third year dental students and the wider dental community. Pre- and post-participation perceptions were recorded, with percentage, mean, broad agreement, standard deviation and statistical significance between responses determined. ResultsImproved recognition of importance of record keeping, knowledge, confidence, skill and motivation to learn was seen following ILM interaction. This was particularly significant for students, participants with 3-5 years of experience in their current occupation and those whose highest level of education was achieved in Australia. ConclusionsThe ILM increased self-reported awareness, understanding and attitude of participants with different levels of case note recording experience; this can improve recording practises and aid forensic dental identification if utilized in undergraduate teaching and as a continuing professional development tool for dental practitioners.",
        "keywords": "Case notes; dental; education; forensic; identification",
        "released": 2018,
        "link": "https://doi.org/10.1111/adj.12545"
    },
    {
        "title": "Comparison of STFT and wavelet transform methods in determining epileptic seizure activity in EEG signals for real-time application",
        "abstract": "Electroencephalography (EEG) is widely used in clinical settings to investigate neuropathology. Since EEG signals contain a wealth of information about brain functions, there are many approaches to analyzing EEG signals with spectral techniques. In this study, the short-time Fourier transform (STFT) and wavelet transform (WT) were applied to EEG signals obtained from a normal child and from a child having an epileptic seizure. For this purpose, we developed a program using Labview software. Labview is an application development environment that uses a graphical language G, usable with an online applicable National Instruments data acquisition card. In order to obtain clinically interpretable results, frequency band activities of delta, theta, alpha and beta signals were mapped onto frequency-time axes using the STFT, and 3D WT representations were obtained using the continuous wavelet transform (CWT). Both results were compared, and it was determined that the STFT was more applicable for real-time processing of EEG signals, due to its short process time. However, the CWT still had good resolution and performance high enough for use in clinical and research settings. (c) 2004 Elsevier Ltd. All rights reserved.",
        "keywords": "EEG; wavelet transform; STFT; epileptic seizure",
        "released": 2005,
        "link": "https://doi.org/10.1016/j.compbiomed.2004.05.001"
    },
    {
        "title": "Adaptive control of two-wheeled mobile balance robot capable to adapt different surfaces using a novel artificial neural network-based real-time switching dynamic controller",
        "abstract": "In this article, a novel real-time artificial neural network-based adaptable switching dynamic controller is developed and practically implemented. It will be used for real-time control of two-wheeled balance robot which can balance itself upright position on different surfaces. In order to examine the efficiency of the proposed controller, a two-wheeled mobile balance robot is designed and a test platformfor experimental setup ismade for balance problemon different surfaces. In a developed adaptive controller algorithm which is capable to adapt different surfaces, mean absolute target angle deviation error, mean absolute target displacement deviation error andmean absolute controller output data areemployed for surface estimation by using artificial neural network. In a designed two-wheeledmobile balance robot system, robot tilt angle is estimated viaKalman filter from accelerometer and gyroscope sensor signals. Furthermore, a visual robot control interface is developed in C++ software development environment so that robot controller parameters can be changed as desired. In addition, robot balance angle, linear displacement and controller output can be observed online on personal computer. According to the real-time experimental results, the proposed novel type controller gives more effective results than the classic ones.",
        "keywords": "Adaptive switching controller; ANN-based surface estimation; sensor fusion; two-wheeled mobile balance robot",
        "released": 2017,
        "link": "https://doi.org/10.1177/1729881417700893"
    },
    {
        "title": "Developing predictable and flexible distributed real-time systems",
        "abstract": "Predictability is considered the most distinguishing characteristic of real-time systems. Besides that, adaptability is also a very important attribute because RT systems are usually designed for long life cycles, during which they will have to cope with change. This paper presents the STER real-time software development environment, designed to support the production of predictable, and yet flexible, distributed real-time systems. Flexibility is one of the main concerns of STER’s programming model, based on the construction of reusable software modules. Modules can be reused with different timing constraints without having to be recoded, since the specification of their timing constraints is decoupled from their implementation. Predictable temporal behavior is obtained by an integrated allocation and scheduling strategy that involves off-line and online schedulers. The off-line scheduler tries to satisfy timing, precedence and allocation constraints of periodic hard real-time tasks, and to give the necessary leeway for the dynamic scheduling of aperiodic tasks. The paper shows how distributed programs are translated to scheduling graphs, and gives the results of some experiments conducted to evaluate the performance of the off-line algorithm. (C) 1998 Elsevier Science Ltd. All Fights reserved.",
        "keywords": "real-time systems; predictability; configuration language; task graphs; scheduling",
        "released": 1998,
        "link": "https://doi.org/10.1016/S0967-0661(97)10048-X"
    },
    {
        "title": "ELECANS-an integrated model development environment for multiscale cancer systems biology",
        "abstract": "Motivation: Computational multiscale models help cancer biologists to study the spatiotemporal dynamics of complex biological systems and to reveal the underlying mechanism of emergent properties. Results: To facilitate the construction of such models, we have developed a next generation modelling platform for cancer systems biology, termed “ELECANS” (electronic cancer system). It is equipped with a graphical user interface-based development environment for multiscale modelling along with a software development kit such that hierarchically complex biological systems can be conveniently modelled and simulated by using the graphical user interface/software development kit combination. Associated software accessories can also help users to perform post-processing of the simulation data for visualization and further analysis. In summary, ELECANS is a new modelling platform for cancer systems biology and provides a convenient and flexible modelling and simulation environment that is particularly useful for those without an intensive programming background. Availability and implementation: ELECANS, its associated software accessories, demo examples, documentation and issues database are freely available at http://sbie.kaist.ac.kr/sub_0204.php Contact: ckh@kaist.ac.kr Supplementary information: Supplementary data are available at Bioinformatics online.",
        "keywords": "",
        "released": 2013,
        "link": "https://doi.org/10.1093/bioinformatics/btt063"
    },
    {
        "title": "Development of an expert system for on-line ventilation network analysis and graphic representation of mine ventilation parameters",
        "abstract": "The expert system component of Knowledge Base System supports the analysis and interpretation phases of ventilation design with an algorithmic program used for the solution of alternate design. The paper describes the “ES-VENT” knowledge based system developed at Central Mining Research Institute, Dhanbad, India for an on-line diagnosing of the ventilation problems using the “IITMRULE” expert system development tool. In the “IITMRULE” expert system tool, it has been decided to use a backward chaining reasoning process to reach the goal state i.e. the final conclusion. ES-VENT system is made of three modules: online Data, Diagnostics and KBS, Each module is being a collection of several programs. The paper mainly deals with the parameters affecting the ventilation of underground mines such as air-velocity, air pressure, temperature, dust, and humidity and gas contents. The graphical presentation of these parameters on the computer and the interactive part of the developed program is expected to help a ventilation officer to determine the changes in a ventilation circuit necessary to bring the mine ventilation to the desired value. once such changes are physically effected, the display will indicate if the changes have brought the desired effect or not on ventilation.",
        "keywords": "ventilation network; on-line data; expert system; intelligent system; knowledge base; inference engine",
        "released": 2002,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000177807200002"
    },
    {
        "title": "Geena 2, improved automated analysis of MALDI/TOF mass spectra",
        "abstract": "Background: Mass spectrometry (MS) is producing high volumes of data supporting oncological sciences, especially for translational research. Most of related elaborations can be carried out by combining existing tools at different levels, but little is currently available for the automation of the fundamental steps. For the analysis of MALDI/TOF spectra, a number of pre-processing steps are required, including joining of isotopic abundances for a given molecular species, normalization of signals against an internal standard, background noise removal, averaging multiple spectra from the same sample, and aligning spectra from different samples. In this paper, we present Geena 2, a public software tool for the automated execution of these pre-processing steps for MALDI/TOF spectra. Results: Geena 2 has been developed in a Linux-Apache-MySQL-PHP web development environment, with scripts in PHP and Perl. Input and output are managed as simple formats that can be consumed by any database system and spreadsheet software. Input data may also be stored in a MySQL database. Processing methods are based on original heuristic algorithms which are introduced in the paper. Three simple and intuitive web interfaces are available: the Standard Search Interface, which allows a complete control over all parameters, the Bright Search Interface, which leaves to the user the possibility to tune parameters for alignment of spectra, and the Quick Search Interface, which limits the number of parameters to a minimum by using default values for the majority of parameters. Geena 2 has been utilized, in conjunction with a statistical analysis tool, in three published experimental works: a proteomic study on the effects of long-term cryopreservation on the low molecular weight fraction of serum proteome, and two retrospective serum proteomic studies, one on the risk of developing breat cancer in patients affected by gross cystic disease of the breast (GCDB) and the other for the identification of a predictor of breast cancer mortality following breast cancer surgery, whose results were validated by ELISA, a completely alternative method. Conclusions: Geena 2 is a public tool for the automated pre-processing of MS data originated by MALDI/TOF instruments, with a simple and intuitive web interface. It is now under active development for the inclusion of further filtering options and for the adoption of standard formats for MS spectra.",
        "keywords": "Computational Proteomics; Computational Mass Spectrometry; MALDI/TOF mass spectrometry; MS spectra comparison/alignment; Automation of data analysis; Web application; LAMP",
        "released": 2016,
        "link": "https://doi.org/10.1186/s12859-016-0911-2"
    },
    {
        "title": "Virtual standards development environments for concurrent standardization process",
        "abstract": "Recently, the increased handling of online standards information has emerged as an important feature of information and communications technology (ICT) standardization. In order to meet market needs for on-time standards deployment, most standards organizations are actively seeking more efficient ways of standardization using electronic means in order to accelerate the standards making process. This paper suggests a virtual standard development environment designed for standards developers to carry out their standards-related activities on-line. In this paper, we outline a conceptual model of a concurrent standardization process and describe the design and implementation of an Extranet-based network system called standards information cooperative network (SICN). The system was created with a view to fostering faster standards development with functionalities such as a virtual management of networked standards developers, collaboration support tools, a workflow-based electronic signature system, and dynamic links for ready retrieval of standards information stored in a database. We conclude this paper with an introduction to the concept of a virtual standards development organization (VSDO) that supports all the features needed by the relevant standards making bodies to carry out their activities in a dynamic on-line environment.",
        "keywords": "",
        "released": 1999,
        "link": "https://doi.org/10.4218/etrij.99.0199.0104"
    },
    {
        "title": "Integration of computer virtual reality technology to college physical education",
        "abstract": "The progress of the times has brought about a leap forward in people’s thinking. Under the rapid economic development environment, the physical field of young people has been unable to withstand the current teaching system, and many problems of poor physical fitness have emerged. In order to solve similar problems, to improve the physical quality of young people, it is bound to find a new teaching method different from the traditional physical education teaching in a special environment, The research on the integration of virtual reality technology and teaching has been rolling forward and never stopped in recent years. With the continuous upgrading of virtual reality technology, virtual reality devices that can be used have already joined the ranks of families. Let these virtual reality devices connect to the Internet through the WEB application settings, and then design according to different situations has become a reality. This research is an application development based on Web virtual reality technology, including network virtual reality technology, application modeling design and the use of Internet connection. With reference to this application, physical education teaching and classroom practice data, this research mainly introduces the current research situation of virtual reality technology and teaching, and establishes the model based on the integration of Web application design and virtual reality technology, Extract, including but not limited to the number of projects, activities and experiences, use the Internet of Things data upload and data processing technology to complete the data screening, obtain valuable data, and then use these data for the practical application value of virtual reality technology in physical education teaching. Complete the improvement. It is in line with the integration of modern virtual reality technology and physical education teaching to obtain data that can reflect the real situation, then conduct prac-tical teaching, obtain reliable practical data, and evaluate teaching. It can be clearly seen from the research results that the combination of virtual reality technology and physical education teaching is of great significance to improve students’ interest in learning and enthusiasm for sports, but there are also some shortcomings, such as different teaching steps and goals will affect students’ enthusiasm for learning. Therefore, further improvement is needed to find a stable way to improve students’ enthusiasm for learning. The integration of virtual reality technology and physical education teaching is proposed.",
        "keywords": "Virtual reality; WEB application program; Internet of Things; integration of sports and education; efficient teaching; application analysis",
        "released": 2022,
        "link": "https://doi.org/10.13052/jwe1540-9589.2173"
    },
    {
        "title": "Relationship between digestive diseases and COVID-19 severity and mortality a protocol for systematic review and meta-analysis",
        "abstract": "Background: Digestive diseases have been often reported in COVID-19 patients, but whether COVID-19 patients with existing digestive comorbidities are at an increased risk of serious disease and death remains unclear. This study aims to evaluate the association between digestive diseases and COVID-19 severity and mortality. Methods: PubMed, Embase.com, the Cochrane Central Register of Controlled Trials, Web of Science, China National Knowledge Infrastructure, Wanfang, and SinoMed will be searched to identify relevant studies up to October 1, 2020. We will use the Newcastle-Ottawa quality assessment scale to assess the quality of included studies. We will use Stata to perform pairwise meta-analyses using the random-effects model with the inverse variance method to estimate the association between digestive diseases and the mortality and severity of COVID-19. Subgroup analyses and sensitivity analyses will be conducted to investigate the sources of heterogeneity. We will create a “Summary of findings” table presenting our primary and secondary outcomes using the GRADEpro Guideline Development Tool software. Results: The results of this study will be published in a peer-reviewed journal. Conclusions: This study will comprehensively evaluate the association between digestive diseases and the severity and mortality of patients with COVID-19. The results of this study will provide high-quality evidence to support clinical practice and guidelines development.",
        "keywords": "COVID-19; gastrointestinal disease; liver disease; meta-analysis; SARS-CoV-2",
        "released": 2020,
        "link": "https://doi.org/10.1097/MD.0000000000023353"
    },
    {
        "title": "Developing an agent-based workflow management system for collaborative product design",
        "abstract": "Purpose - The purpose of this research is to develop a prototype of agent-based intelligent workflow system for product design collaboration in a distributed network environment. Design/methodology/approach - This research separates the collaborative workflow enactment mechanisms from the collaborative workflow building tools for flexible workflow management. Applying the XML/RDF (resource description framework) ontology schema, workflow logic is described in a standard representation. Lastly, a case study in collaborative system-on-chip (SoC) design is depicted to demonstrate the agent-based workflow system for the design collaboration on the web. Findings - Agent technology can overcome the difficulty of interoperability in cross-platform, distributed environment with standard RDF data schema. Control and update of workflow functions become flexible and versatile by simply modifying agent reasoning and behaviors. Research limitations/implications - When business partners want to collaborate, how to integrate agents in different workflows becomes a critical issues. Practical implications - Agent technology can facilitate design cooperation and teamwork communication in a collaborative, transparent product development environment. Originality/value - This research establishes generalized flow logic RDF models and an agent-based intelligent workflow management system, called AWfMS, based on the RDF schema of workflow definition. AWfMS minimizes barriers in the distributed design process and hence increases design cooperations among partners.",
        "keywords": "work flow; intelligent agents; product design; product design systems",
        "released": 2006,
        "link": "https://doi.org/10.1108/02635570610666449"
    },
    {
        "title": "Bibliometric analysis of green marketing research from 1977 to 2020",
        "abstract": "The scientific community’s growing interest in green marketing research can be traced through the abundance of published literature on the topic. However, there is still a lack of a comprehensive and systematic research focusing on the evolution of this field. The main objective of this paper was to consolidate the state-of-the-art research on green marketing through a bibliometric study of articles published from 1977 to 2020, and to analyze and present the results from the perspective of growing trends in the field; productive and influential countries, institutions, authors, articles, and research journals; keywords; authorship patterns; and international collaborations. The study results indicate a gradual increase in green marketing research from 1977 to 2020, more so in the last five years. A total of 1025 scholarly documents were published in 634 journals during this period and listed in the Web of Science (WOS). The top 10 most productive countries were then analyzed. A great number of institutions from both developing countries (in terms of article count) and developed countries (in terms of citation scores), were compared. Finally, based on keywords and a three-factor analysis, it was concluded that green marketing, sustainability, sustainable development, environment marketing and sustainable marketing have attracted extensive attention during the past decade as keywords.",
        "keywords": "bibliometric green marketing; ecological marketing; sustainable marketing; environmental marketing; citation analysis; green marketing",
        "released": 2021,
        "link": "https://doi.org/10.3390/publications9010001"
    },
    {
        "title": "Agent-based workflow management in collaborative product development on the internet",
        "abstract": "Product development is collaborative, involving multi-disciplinary functions and heterogeneous tools. Teamwork is essential through seamless tool integration and better co-ordination of human activities. This paper proposes to use workflow management as a mechanism to facilitate the teamwork in a collaborative product development environment where remote Web-based Decision Support Systems (TeleDSS) are extensively used by team members who are geographically distributed. The workflow of a project is modelled as a network. Its nodes correspond to the work (packages), and its edges to flows of control and data. The concept of agents is introduced to define nodes and the concept of messages to define edges. As a sandwich layer, agents act as special-purpose application clients for the remote TeleDSS application servers. Agents are delegated to manipulate the corresponding TeleDSS on behalf of their human users. Details of the proceedings are recorded by agents as their properties for future references or shared uses by other team members. Through flow messages, agents are able to share input and output data and request for remote services. One of the major contributions is that agents, once defined, can be reused for different projects without any changes. (C) 2000 Elsevier Science Ltd. All rights reserved.",
        "keywords": "collaborative product development; internet; web; agent; workflow",
        "released": 2000,
        "link": "https://doi.org/10.1016/S0010-4485(99)00096-2"
    },
    {
        "title": "PTAD: A web-based climate service for building design adaptation",
        "abstract": "Projections Tool for Architectural Design (PTAD) is a web-based climate service product available at www.ptad. mju.ac.th. Targeted at building owners and designers, it is intended to support these decision makers in building design to reduce the impacts of climate change. This study presents the technical environment of PTAD and provides a conceptual basis for linking academic information on climate change to building-related data with the cooperation of the relevant authorities. The PTAD development concept was divided into three parts: climate change impact assessment in small areas (i.e., at a construction site scale), building damage analysis of the impacts, and climate-resilient recommendations for building design. Tools such as GIS, HEC-RAS, and ERDAS and the existing outputs of regional climate models were used as the baseline data for analyzing the impacts that may occur at the study site. The impacts in small areas and design strategies to reduce the potential damage were analyzed with the Delphi technique using existing standards and guidelines. A 3-tier architecture was employed for the production and development environments by modularizing the user interface, service object, and data storage layers. The impact data values, namely, the highest annual surface temperatures and flood characteristics, including the maximum flood level, flood duration, and water velocity from 1998 to 2047, were displayed as markers on the satellite maps, tables, and figures. The impacts can be linked to architectural design guidelines to help design buildings that mitigate some of the impacts of climate change.",
        "keywords": "Climate change; Design tool; Floods; Microclimate; Climate-resilient architectural design",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.cliser.2021.100279"
    },
    {
        "title": "Innovative design of an art teaching quality evaluation system based on big data and an association rule algorithm from the perspective of sustainable development",
        "abstract": "Online art courses suffer from poor course production, limited sustainable use and development, and a significant degree of course similarity. It is important to figure out how to create an efficient system for evaluating art instruction. In this article, we look at the current institutions of higher art education and how to use data mining techniques in the system for judging the quality of teaching. Using an association rule algorithm to solve the current teaching quality evaluation of rationality and subjectivity and a teaching quality evaluation algorithm based on association were some of the solutions used, and data mining effectively dredged out the indicators of the institutions of higher art education’s teaching quality evaluation system. The widely used development tools, SQL Server and ASP.NET, were used to build and implement the teaching quality assessment system through a variety of indications of teaching quality evaluation in a school. The main contribution of this paper is the development of an effective teaching quality evaluation system index for higher art colleges, as well as the development of teacher-student interaction based on the comprehensive support of the Internet, in order to identify the factors influencing the teaching quality of design art colleges. The administrators of higher art education institutions will receive analytical materials for making decisions from this study that are based on science and objectivity. The results of the experimental process were quite effective.",
        "keywords": "Data mining; Association rule algorithm; Teaching quality evaluation system; Innovative design; Sustainable development",
        "released": 2023,
        "link": "https://doi.org/10.1007/s00500-023-08028-9"
    },
    {
        "title": "Development and application of chinese medical ontology for diabetes mellitus",
        "abstract": "ObjectiveTo develop a Chinese Diabetes Mellitus Ontology (CDMO) and explore methods for constructing high-quality Chinese biomedical ontologies.Materials and methodsWe used various data sources, including Chinese clinical practice guidelines, expert consensus, literature, and hospital information system database schema, to build the CDMO. We combined top-down and bottom-up strategies and integrated text mining and cross-lingual ontology mapping. The ontology was validated by clinical experts and ontology development tools, and its application was validated through clinical decision support and Chinese natural language medical question answering.ResultsThe current CDMO consists of 3,752 classes, 182 fine-grained object properties with hierarchical relationships, 108 annotation properties, and over 12,000 mappings to other well-known medical ontologies in English. Based on the CDMO and clinical practice guidelines, we developed 200 rules for diabetes diagnosis, treatment, diet, and medication recommendations using the Semantic Web Rule Language. By injecting ontology knowledge, CDMO enhances the performance of the T5 model on a real-world Chinese medical question answering dataset related to diabetes.ConclusionCDMO has fine-grained semantic relationships and extensive annotation information, providing a foundation for medical artificial intelligence applications in Chinese contexts, including the construction of medical knowledge graphs, clinical decision support systems, and automated medical question answering. Furthermore, the development process incorporated natural language processing and cross-lingual ontology mapping to improve the quality of the ontology and improved development efficiency. This workflow offers a methodological reference for the efficient development of other high-quality Chinese as well as non-English medical ontologies.",
        "keywords": "Diabetes mellitus; Chinese medical ontology; Ontology construction; Question answering; Clinical decision support",
        "released": 2024,
        "link": "https://doi.org/10.1186/s12911-023-02405-y"
    },
    {
        "title": "The development and validation of a global advanced development framework for the pharmacy workforce: A four-stage multi-methods approach",
        "abstract": "BackgroundStudies have indicated that a generalisable and translatable global framework is a useful tool for supporting career progression and recognising advanced practice.AimTo develop and validate a global advanced competency development framework as a tool to advance the pharmacy profession globally.MethodA four-stage multi-methods approach was adopted. In sequence, this comprised an assessment of initial content and a cultural validation of the advanced level framework. Following this, we conducted a transnational modified Delphi followed by an online survey sampling the global pharmacy leadership community. Finally, a series of case studies was constructed exemplifying the framework implementation.ResultsInitial validation resulted in a modified draft competency framework comprising 34 developmental competencies across six clusters. Each competency has three phases of advancement to support practitioner progression. The modified Delphi stage provided feedback on framework modifications related to cultural issues, including missing competencies and framework comprehensiveness. External engagement and case study stages provided further validity on the framework implementation and dissemination.ConclusionThe four-staged approach demonstrated transnational validation of a global advanced competency framework as a mapping and development tool for the pharmacy professions. Further study is needed to develop a global glossary of terminologies on advanced and specialist practice. Also, developing an accompanying professional recognition system and education and training programmes to support framework implementation is recommended.",
        "keywords": "Advancing practice; Competency-based education; Competency framework; International Pharmaceutical Federation (FIP); Pharmacy",
        "released": 2023,
        "link": "https://doi.org/10.1007/s11096-023-01585-x"
    },
    {
        "title": "Comparing the effectiveness of type of the traditional chinese exercises, frequency, intensity, time in osteoporosis: A protocol for systematic evaluation and network meta-analysis of randomised controlled trials",
        "abstract": "IntroductionAs populations age, osteoporosis has become a hot topic of global public concern. The beneficial effects of traditional Chinese exercises on the musculoskeletal system have been demonstrated. However, previous research findings on osteoporosis are inconsistent, and it is unclear which type of exercise and its frequency and duration have the best effect on osteoporosis. This study aims to investigate the most appropriate exercise modality for people with osteoporosis through systematic evaluation and network meta-analysis to guide clinical practice. Methods and analysisThe Cochrane Library, Web of Science, MEDLINE, Embase, China Biomedical Literature, China Knowledge Network, China Science and Technology Journal and Wanfang databases will be searched until January 2022. The language of the articles should be English or Chinese. All clinical randomised controlled trials on the effect of traditional Chinese exercises on osteoporosis will be included. We will use RevMan, Stata and GeMTC software to complete our network meta-analysis. We will perform risk of bias assessment, subgroup analysis and sensitivity analysis to correct the results. Finally, we will use the Grading of Recommendations Assessment, Development and Evaluation guideline development tool and Confidence in Network Meta-Analysis (CINeMA, a new method for assessing CINeMA results) approach to evaluate the reliability of our final results. Ethics and disseminationAll data for this study will be obtained from published studies, so no ethical review will be needed. We will publish the results of the study in a peer-reviewed journal. PROSPERO registration numberCRD42022323622.",
        "keywords": "COMPLEMENTARY MEDICINE; Musculoskeletal disorders; Bone diseases",
        "released": 2022,
        "link": "https://doi.org/10.1136/bmjopen-2022-063878"
    },
    {
        "title": "Threat-oriented security framework in risk management using multiagent system",
        "abstract": "Present day sophisticated and innovative attacks have resulted in exponentially increasing security problems. This paper therefore presents a three-phased threat-oriented security model to meet the above security challenges as a part of proactive risk management. This model is based on a spiral process for software development because it is a risk driven approach and provides an incremental method for a progressively growing system with decreasing risk. Integration of threat management during the development process in the proposed work provides necessary security cover against both unforeseen and known threats. Identification of these threats has been made possible by fusion of a threat modeling process and research honeytokens in conjunction with a statistical model in the first phase. Necessary security measures to mitigate the above identified threats have been adopted in the second phase using multiagent system planning. Risk reduction as a result of adoption of countermeasures has been evaluated in the third phase using meta-agents in association with fuzzy logic in a multiagent environment. The proposed proactive measures of this model have been demonstrated with a case study on “Online Banking” to show its feasibility and has been implemented using Java Agent Development Environment, Apache Tomcat Server, with MySQL Server at the backend. Copyright (C) 2012 John Wiley & Sons, Ltd.",
        "keywords": "threat-oriented security model; research honeytokens; statistical model; proactive risk management; multiagent system planning; meta-agents; fuzzy logic",
        "released": 2013,
        "link": "https://doi.org/10.1002/spe.2133"
    },
    {
        "title": "The appropriation of GitHub for curation",
        "abstract": "GitHub is a widely used online collaborative software development environment. this paper, we describe curation projects as a new category of GitHub project that collects, evaluates, and Preserves resources for software develoPers. We investigate: (1) what motivates software developers to curate resources; (2) why curation has occurred on GitHub; (3) how curated resources are used by software developers; and (4) how the GitHub platform could better support these practices. We conduct in-depth interviews with 16 software developers, each of whom hosts curation projects on GitHub. Our results suggest that the motivators that inspire software developers to curate resources on GitHub are similar to those that motivate them to participate in the development of open source projects. Convenient tools (e.g. Markdown syntax and Git version control system) and the opportunity to address professional needs of alarge number of peers attract developers to engage in curation projects on GitHub. Benefits of curating on GitHub include learning opportunities, support for develoPment work, and professional interaction. However, curation is limited by GitHub’s document structure, format, and a lack of key features, such as search. In light of this, we propose design possibilities to encourage and improve appropriations of GitHub for curation.",
        "keywords": "Curation; GitHub; Appropriation",
        "released": 2017,
        "link": "https://doi.org/10.7717/peerj-cs.134"
    },
    {
        "title": "A meta-design approach to the development of e-government services",
        "abstract": "This paper describes a meta-design approach to the development of online services for citizens of a government agency. The goal is to transfer the development of government-to-citizen services from professional software developers to administrative employees, without forcing employees to acquire any programming skills. The approach encompasses two main phases. The first phase analyzes the different perspectives of the stakeholders involved in service creation and usage - employees, citizens, software developers and human-computer interaction specialists - in order to derive a meta-model of e-govemment services. The latter applies the meta-model to design and develop an end-user development environment that properly supports employees in creating an instance of the service meta-model, which is then automatically interpreted to generate the service pages for citizens. A pilot application of the proposed approach is illustrated with reference to a specific class of e-government services offered by the Brescia Municipality, even though the approach is general enough to be applied to different kinds of e-government services and application domains. The results of the evaluation with a group of municipality employees provide initial feedback from the government field and show how to proceed along this research direction. (C) 2011 Elsevier Ltd. All rights reserved.",
        "keywords": "End-user development; Meta-design; e-government; Interaction design",
        "released": 2012,
        "link": "https://doi.org/10.1016/j.jvlc.2011.11.003"
    },
    {
        "title": "Evaluation of the effectiveness of developing real-world software projects as a motivational device for bridging theory and practice",
        "abstract": "While incorporating project-based exercise is a common practice in software engineering education, few studies have been conducted in investigating how real-world project development influences university students’ proactive learning and knowledge transformation. This study aims to evaluate the effectiveness of developing real-world projects with industry engagement in encouraging students to apply knowledge to practice and be more proactive in learning. Using a two-group, post-test quasi-experimental design, the performance between the students taking real-world project development and the students in the control group are compared using descriptive statistics, the independent samples (-test and Welch (-test, accordingly. Both the Spearman’s rank-order and Kendall’s tau-b are used to examine the relationship between students’ practical works and the level of knowledge transformation estimated by the students through online surveys. The results suggest that using real-world projects in the classroom can be an effective motivational device for proactive learning and knowledge transformation. Project-based exercise should be both comprehensive and keeping pace with technology development driven by the software industry evolution to be more effective. The direct interaction with stakeholders, dynamic requirements change, employment of Agile methods, self-organising teams, and using challenging real-world projects, are essential in simulating a real-world software development environment in the classroom.",
        "keywords": "Software engineering; project; development; theory; practice",
        "released": 2022,
        "link": "https://doi.org/10.1080/0309877X.2022.2070727"
    },
    {
        "title": "PharmaCORE: Optimizing medical pharmacology education with an innovative instructional dashboard",
        "abstract": "What was the educational challenge?Diminishing emphasis on pharmacology education in medical schools has resulted in a concerning lack of prescribing knowledge among physician graduates. These concerns mirror our graduates’ expressed dissatisfaction with the structure and quality of pharmacology educational experiences over the past 5 years. What was the solution?PharmaCORE, a web-based instructional dashboard, was developed as an interactive faculty development tool to enhance integration and instruction of pharmacology content in pre-clinical curriculum at a US medical school. How was the solution implemented?PharmaCORE was introduced in Spring 2022 for instructors teaching pharmacology in the pre-clinical curriculum. Instructors used the dashboard to assess coverage of specific drug topics throughout the curriculum and to apply tailored, learner-centered teaching strategies to optimize learner engagement and comprehension. What lessons were learned that are relevant to a wider global audience?The initial assessment indicated that the dashboard was user-friendly and positively influenced instructor awareness of pharmacology content and learner-centered teaching. This faculty development approach underscores the importance of skill-based mapping and maintaining learner-centered teaching standards to address other integrated subjects and broader curricular challenges. What are the next steps?This study lays the foundation for the broader applicability of instructional dashboards in tracking and addressing curricular challenges across pharmacology and other science subjects. Future steps include more personalized feedback for instructors, creating a student-accessible version, and ongoing monitoring of maintenance measures like milestone exams.",
        "keywords": "Pharmacology; curriculum map; faculty development; teaching & learning",
        "released": 2024,
        "link": "https://doi.org/10.1080/0142159X.2024.2342540"
    },
    {
        "title": "A novel direct load control testbed for smart appliances",
        "abstract": "The effort to continuously improve and innovate smart appliances (SA) energy management requires an experimental research and development environment which integrates widely differing tools and resources seamlessly. To this end, this paper proposes a novel Direct Load Control (DLC) testbed, aiming to conveniently support the research community, as well as analyzing and comparing their designs in a laboratory environment. Based on the LabVIEW computing platform, this original testbed enables access to knowledge of major components such as online weather forecasting information, distributed energy resources (e.g., energy storage, solar photovoltaic), dynamic electricity tariff from utilities and demand response (DR) providers together with different mathematical optimization features given by General Algebraic Modelling System (GAMS). This intercommunication is possible thanks to the different applications programming interfaces (API) incorporated into the system and to intermediate agents specially developed for this case. Different basic case studies have been presented to envision the possibilities of this system in the future and more complex scenarios, to actively support the DLC strategies. These measures will offer enough flexibility to minimize the impact on user comfort combined with support for multiple DR programs. Thus, given the successful results, this platform can lead to a solution towards more efficient use of energy in the residential environment.",
        "keywords": "demand response; direct load control; home energy management system; mixed-integer linear programming",
        "released": 2019,
        "link": "https://doi.org/10.3390/en12173336"
    },
    {
        "title": "A contest-oriented project for learning intelligent mobile robots",
        "abstract": "A contest-oriented project for undergraduate students to learn implementation skills and theories related to intelligent mobile robots is presented in this paper. The project, related to Micromouse, Robotrace (Robotrace is the title of Taiwanese and Japanese robot races), and line-maze contests was developed by the embedded control system research group of the Department of Electronic Engineering, Lunghwa University of Science and Technology, Taiwan. It targets both those students who have to earn credits for a one-year special topics course and those who are just interested in making robots, and it is designed to motivate them to learn digital motion control, path planning, attitude correction, curvature detection and maze-solving algorithms. The students begin by getting acquainted with the development environment of microcontrollers, the characteristics of different sensors, and servomotor control techniques. Having learned these basic skills, they acquire further specific advanced skills and proceed to design their own mobile robots to compete in contests. The special topics course students’ robots must pass examination by five teachers. Blogs and a wiki Web site for recording students’ progress and experiences enhance the project’s learning outcomes. Although not every student wins a prize in the contests, student feedback still shows that the contest-oriented project did motivate them to acquire the skills necessary to build and operate intelligent mobile robots.",
        "keywords": "Contest-oriented project; embedded control; intelligent robots; Micromouse; Robotrace",
        "released": 2013,
        "link": "https://doi.org/10.1109/TE.2012.2215328"
    },
    {
        "title": "Unconditionally stable numerical simulations of a new generalized reduced resistive magnetohydrodynamics model",
        "abstract": "Reduced-resistive magnetohydrodynamics (MHD) models are used in understanding different phenomenon in various domains, for example, astrophysics to model magnetotail or for solar arcades [Finn Bozkaya JM, Guzdar PN. Loss of equilibrium and reconnection in tearing of two dimensional equilibrias. Physics of Fluids B 1993; 5:2870-2876], modeling plasma confinements in reverse field pinch [Strauss HR. The dynamo effect in fusion plasmas. Physics of Fluids 1985; 28:2786-2792] and tokamaks [Strauss HR. Reduced MHD in nearly potential magnetic fields. Journal of Plasma Physics 1997; 57(1):83-87; Freidberg J. Plasma Physics and Fusion Energy. Cambridge University Press: Cambridge, 2008]. In this context, recently, a new generalized reduced-resistive MHD model, which can make use of an arbitrary density profile was proposed [Despres B, Sart R. Reduced resistive MHD in Tokamaks with general density. ESAIM: Mathematical Modelling and Numerical Analysis 2012; 46(5):1081-1106. EDP Sciences, SMAI, 2012 online 2011]. We in this work show that this proposed theoretical model can be realized numerically as well, and that it is very robust if the equation set is written in a very particular form using the properties of FEM. To illustrate these points, we pick the current hole configuration [Fujita T, Oikawa T, SuzukiT, Ide S, Sakamoto Y, Koide Y, Hatae T, Naito O, Isayama A, Hayashi N, Shirai H. Plasma equilibrium and confinement in a tokamak with nearly zero central current density in JT-60U. Physical Review Letters 2001; 87(11):245001; Hawkes NC, Stratton BC, Tala T, Challis CD, Conway G, DeAngelis R, Giroud C, Hobirk J, Joffrin E, Lomas P, Lotte P, Mailloux J, Mazon D, Rachlew E, Reyes-Cortes S, Solano E, Zastrow K-D. Observation of zero current density in the core of JET discharges with lower hybrid heating and current drive. Physical Review Letters 2001; 87(11):115001], which was modeled using reduced-resistive MHD and remodel it using different combinations of current sources and density profiles. Our model can be implemented with reasonable computational resources at the price of solving a well-posed global linear system and it is unconditionally stable. These features are also demonstrated as a part of our numerical experiments. Copyright (c) 2013 John Wiley & Sons, Ltd.",
        "keywords": "incompressible flow; MHD: magnetohydrodynamics; partial differential equations; generalized finite element; current hole; tokamak",
        "released": 2014,
        "link": "https://doi.org/10.1002/fld.3847"
    },
    {
        "title": "Non-adaptive programmability of random oracle",
        "abstract": "Random Oracles serve as an important heuristic for proving security of many popular and important cryptographic primitives. But, at the same time they are criticized due to the impossibility of practical instantiation. Programmability is one of the most important features behind the power of Random Oracles. Unfortunately, in the standard hash functions, the feature of programmability is limited. In recent years, there have been endeavors to restrict programmability of random oracles. However, we observe that the existing models allow adaptive programming, that is, the reduction can program the random oracle adaptively in the online phase of the security game depending on the query received from the adversary, and thus are quite far from the standard model. In this paper, we introduce a new feature called non-adaptive programmability of random oracles, where the reduction can program the random oracle only in the pre-processing phase. In particular, it might non-adaptively program the RO by setting a regular function as a post-processor on the oracle output. We call this new model Non-Adaptively-Programmable Random Oracle (NAPRO) and we show that this model is actually equivalent to so-called Non-Programmable Random Oracle (NPRO) introduced by Fischlin et al. [8], hence too restrictive. However, we also propose a slightly stronger model, called Weak-Non-Adaptively-Programmable Random Oracle (WNAPRO), where in addition to non-adaptive programming, the reduction is allowed to adaptively extract some “auxiliary information” from the RO and this “auxiliary information” interestingly plays crucial role in the security proof allowing several important RO proofs to go through! In particular we prove the following results in WNAPRO model. 1. RSA-Full-Domain Hash signature scheme (RSA-FDH), and Boneh Franklin ID-based encryption scheme (BF-IDE) are secure in the WNAPRO model. This is in sharp contrast to strong blackbox proofs of FDH schemes, where full programmability seems to be necessary. 2. Shoup’s Trapdoor-permutation based Key-encapsulation Mechanism (TDP-KEM) cannot be proven secure via blackbox reduction from ideal trapdoor-permutations in the WNAPRO model. (C) 2015 Elsevier B.V. All rights reserved.",
        "keywords": "Random oracle; Programmability; RSA-FDH; TDP-KEM",
        "released": 2015,
        "link": "https://doi.org/10.1016/j.tcs.2015.05.026"
    },
    {
        "title": "Puzzle: A mobile application development environment using a jigsaw metaphor",
        "abstract": "Objective: Create a visual mobile end user development framework, named Puzzle, which allows end users without IT background to create, modify and execute applications, and provides support for interaction with smart things, phone functions and web services. Methods: Design of an intuitive visual metaphor and associated interaction techniques for supporting end user development in mobile devices with iterative empirical validation. Results: Our results show that the jigsaw is an intuitive metaphor for development in a mobile environment and our interaction techniques required a limited cognitive effort to use and learn the framework. Integration of different modalities and usage of smart things was relevant for users. Conclusion: Puzzle has addressed the main objective. The framework further contributes to the research on mobile end user development in order to create an incentive for users to go beyond consuming content and applications to start creating their own applications. Practice: Usage of a mobile end user development environment has the potential to create a shift from the conventional few-to-many distribution model of software to a many-to-many distribution model. Users will be able to create applications that fit their requirements and share their achievements with peers. Implications: This study has indicated that the Puzzle visual environment has the potential to enable users to easily create applications and stimulate exploration of innovative scenarios through smartphones. (C) 2014 Elsevier Ltd. All rights reserved.",
        "keywords": "End user development; Ubiquitous computing; Mobile computing; Authoring tools; Metaphors; Interaction techniques",
        "released": 2014,
        "link": "https://doi.org/10.1016/j.jvlc.2014.03.005"
    },
    {
        "title": "Efficient model-driven prototyping for edge analytics",
        "abstract": "Software development cycles in the context of Internet of Things (IoT) applications require the orchestration of different technological layers, and involve complex technical challenges. The engineering team needs to become experts in these technologies and time delays are inherent due to the cross-integration process because they face steep learning curves in several technologies, which leads to cost issues, and often to a resulting product that is prone to bugs. We propose a more straightforward approach to the construction of high-quality IoT applications by adopting model-driven technologies (DIME and Pyrus), that may be used jointly or in isolation. The presented use case connects various technologies: the application interacts through the EdgeX middleware platform with several sensors and data analytics pipelines. This web-based control application collects, processes and displays key information about the state of the edge data capture and computing that enables quick strategic decision-making. In the presented case study of a Stable Storage Facility (SSF), we use DIME to design the application for IoT connectivity and the edge aspects, MongoDB for storage and Pyrus to implement no-code data analytics in Python. We have integrated nine independent technologies in two distinct Low-code development environments with the production of seven processes and pipelines, and the definition of 25 SIBs in nine distinct DSLs. The presented case study is benchmarked with the platform to showcase the role of code generation and the reusability of components across applications. We demonstrate that the approach embraces a high level of reusability and facilitates domain engineers to create IoT applications in a low-code fashion.",
        "keywords": "model driven development; internet of things; digital thread platform; domain specific languages; low-code/no-code; DIME; pyrus",
        "released": 2023,
        "link": "https://doi.org/10.3390/electronics12183881"
    },
    {
        "title": "Implementation of analytical tools in the scientometric platform for the formation of consolidated information",
        "abstract": "The article analyzes the main problems of the scientometric area by the criteria for HEIs scientists carried out using the data mining methods. The main purpose of any research is to share achievements and accomplishments with society. However, each time a scientist publishes a he/she will be curious about how it’s being received. Introduction of a category or criteria is a basic component of evaluating any activity, in particular scientific, and allows the unbiased consideration of the activities for building a balanced segmental circle. This approach was chosen to build the Odesa National University of Technology (ONUT) sciencetometric system, which allows researchers to have a quick and easy tool for identifying how much and what type of attention a research output has received. The software was developed in the PyCharm and DataGrip development environment. As a result of the research, a web application was created that fully meets the requirements of Odesa National University of Technology Scientific and Technical Library. The analysis of the subject area is carried out. The database was designed and developed using PostgreSQL DBMS version 10. Software solutions for the tasks were developed in the Atom environment using the Python programming language and the Django framework. The results are methods and functions that organize the operation of the system and the interaction of the application server with the database server.",
        "keywords": "computing information technology; information management system; scientific methods; web; database",
        "released": 2022,
        "link": "https://doi.org/10.33436/v32i3y202202"
    },
    {
        "title": "Global research on care-related burden and quality of life of informal caregivers for older adults: A bibliometric analysis",
        "abstract": "As global populations continue to undergo demographic aging, the role of caregivers in providing essential support and assistance to older adults has become increasingly prominent. This demographic shift has led to a growing reliance on informal caregivers, often family members, who take on the responsibilities of caring for older adults. This not only affects immediate family dynamics but also holds broader implications for societal sustainability. The primary objective of this bibliometric analysis is to comprehensively examine the worldwide research output related to the quality of life and caregiver burden among individuals providing care to older adults. By understanding the worldwide research output related to caregivers and their quality of life and burden, we can assess the long-term sustainability of caregiving practices. We retrieved studies with titles containing the terms “caregivers”, “burden”, “quality of life”, and “aged” from the Web of Science (WOS) database. The collected publications were then subjected to analysis using the “bibliometric” package in the R programming environment. A total of 44 publications from 2006-2023 were included in the analysis. Spain emerged as the leading contributor in terms of the number of publications, accounting for 21.9%, followed by the USA at 16.5% and China at 13.6%. The most prolific institution was Kaohsiung Medical University, Taiwan, responsible for 25% of the publications. Among the authors, Cura-Gonzalez I.D. had the highest number of articles, contributing four publications, or 9.1% of the total output. An analysis of co-occurring keywords revealed that the predominant focus of the research revolved around caregiver burden, quality of life, health, care, stress, and impact, reflecting enduring areas of interest within this field. This bibliometric analysis may serve as a tool to provide insights into the current state of research on caregiver burden and quality of life among those caring for older adults. The results of this study can contribute to the assessment of research strategies and the encouragement of global cooperation in the field of care for older adults. By considering the multidimensional nature of caregiving challenges and promoting international cooperation, strides can be made towards sustainable caregiving practices that ensure the wellbeing of both caregivers and the aging population, thus safeguarding the sustainability of healthcare systems worldwide.",
        "keywords": "bibliometric analysis; caregivers; burden; quality of life; aged",
        "released": 2024,
        "link": "https://doi.org/10.3390/su16031020"
    },
    {
        "title": "Optimization of the enterprise human resource management information system based on the internet of things",
        "abstract": "In this paper, the optimization of the enterprise HR information system is studied based on IoT first-off technology, the system demand phase is analysed, and the edge control system is designed and built. The hardware and software system and edge node management platform are implemented first, and then the communication scenarios between the edge layer of the system and the sensing layer, the edge layer, and the cloud layer are analysed, and the business type-driven link selection algorithm and the northbound multilink switching algorithm are designed and implemented, respectively, to guarantee the communication reliability between different layers of the system. Based on the implementation of the above functions, the edge control system can meet the intelligence, expandability, and security requirements of IoT applications. An in-depth investigation and research are launched mainly on the enterprise demand to determine the functional requirements and performance requirements of the enterprise and to achieve the basic logical structure; in the system design phase, the system architecture and other aspects of the design are realized. According to the conditions of the system function structure, a number of system module functions are designed in detail. The system is composed of the following modules, namely, personnel change management, organization management, and salary and benefits management. The system consists of the following modules, namely, personnel change management, organization management, compensation and benefits management, and personnel information management. The system modules run through the process of human resource management; in the system implementation stage, the system coding and page operation are realized based on the development tools and software development techniques. The system finally achieves the system design objectives and is put on a trial operation to meet its actual business requirements.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1155/2021/5592850"
    },
    {
        "title": "Use of portfolios in otolaryngology graduate medical education",
        "abstract": "Objectives/Hypothesis: Learning portfolios, as defined by the Accreditation Council for Graduate Medical Education (ACGME), are professional development tools for resident education. Moreover, the scope of portfolio use is expanding to become a component of the accreditation system, with likely mandatory implementation by 2016. The objective of this study is to describe the extent of portfolio use in otolaryngology training programs and resident attitudes toward portfolios. Study Design: Cross-sectional survey. Methods: All residents in ACGME-accredited otolaryngology programs were contacted via email linked to an online survey. One follow-up email was sent after initial notification. Results: Three hundred eighteen (22%) of the 1,431 invited residents responded to the survey, representing 65 of 103 ACGME-accredited otolaryngology training programs. Fifty-eight percent of the programs represented had residents who maintained a portfolio. When asked to what extent portfolios enhanced education, 39% of residents who kept a portfolio found them helpful, 27% were neutral, and 35% did not find them helpful, although 60% plan to use their portfolio after residency. For those residents who did not maintain a portfolio, 70% cited they did not maintain a portfolio because it is not a requirement in their program. Twenty-one percent of all respondents felt that portfolios should be mandatory, whereas 61% felt that portfolios should be encouraged, but not required. Conclusions: Although portfolios are encouraged by the ACGME, and will ultimately become mandatory, they are not yet fully integrated in otolaryngology training programs. Only a minority of residents in this study thought portfolios enhanced education.",
        "keywords": "Portfolio; graduate medical education; otolaryngology; academic medicine; Accreditation Council for Graduate Medical Education",
        "released": 2011,
        "link": "https://doi.org/10.1002/lary.21803"
    },
    {
        "title": "A realist review of digitally delivered child development assessment and screening tools: Psychometrics and considerations for future use",
        "abstract": "Background: Developmental screening improves the detection of developmental concerns, yet numerous children are not screened/assessed. Remote child developmental tool administration has been utilized to increase screening and assessment accessibility. Method: We conducted a realist review to: (1) identify existing multi-domain child development assessment and screening tools for children 0-5 years; (2) review psychometric data on their digital (i.e., only administered remotely) administration; and (3) explore contextual factors relevant to their digital administration. We searched APA PsycInfo, MEDLINE, CINAHL, and ERIC to identify tools and papers on their psychometrics. We referencesearched included articles and searched Google for relevant grey literature. Results: Of 33 multi-domain child development tools identified in objective one, five tools (in five studies) were delivered digitally and compared to traditional (e.g., paper) delivery (i.e., objective two). Studies evaluated within-group equivalence reliability (k = 2) and between-group equivalence (k = 3). Within-group equivalence reliability was established for the Vineland Adaptive Behavior Scales, and domains (e.g., gross motor) of the Ages and Stages Questionnaires 2nd edition (ASQ-2) and Revised Prescreening Denver Questionnaire (R-PDQ). Between group equivalence was demonstrated for Developmental Neuropsychological Assessment, 2nd Edition (NEPSY-II) subtests and Bayley Scales of Infant and Toddler Development, 3rd edition (Bayley-3) items. In another between group evaluation, web-based and paper versions of the ASQ-2 were deemed generally equivalent. Digital Bayley-3 inter-observer reliability ranged from 0.82 to 1.0. Examiner support, time, tool modifications, family resources, and comfort promotion supported digital administration. Conclusion: Digitally delivered ASQ-2, R-PDQ, Vineland, and Bayley-3 and NEPSY-II components show promise for equivalence with traditional administration.",
        "keywords": "Child development; Digital data collection; Equivalence reliability; Psychometrics; Early childhood",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.earlhumdev.2023.105818"
    },
    {
        "title": "The efficacy of antiviral treatment for chronic hepatitis b patients with normal ALT levels: A systematic review and meta-analysis",
        "abstract": "Context: When nucleos(t)ide analogues (NAs) were applied clinically to manage chronic hepatitis B virus infection, the prognosis of chronic hepatitis B (CHB) patients greatly improved. However, certain CHB patients with normal alanine aminotransferase (ALT) levels were not used to be considered as the population with the need for antiviral treatment. Objectives: This systematic review and meta-analysis collected and analyzed data from clinical trials to assess and compare the efficacy of antiviral treatment among patients with elevated and normal ALT levels. Methods: A systematic search was performed to gather studies published from 1990.01 to 2022.08 in PubMed and Web of Science databases. The quality of the literaturewasassessed, and16 studies were included for further analysis. Basic informationonincluded studies and study populations was collected. A meta-analysis was carried out to evaluate three major outcomes of viral response, hepatitis B envelope antigen (HBeAg) loss, and HBeAg seroconversion after NAs treatment based on data extracted from these studies. Odds ratios (ORs) with 95% confidence intervals (CIs) for all outcomes were calculated using fixed-effects models. Results: In the 16 relevant studies, 5,345 patients met the inclusion criteria, including 3,687 patients receiving NAs treatment. All patients were grouped into one with elevated ALT and another with normal ALT based on whether their pretreatment ALT levels > 1*upper limit of normal (ULN). For patients receiving lamivudine, the viral response showed no significant difference between the groups with elevated and normal ALT levels (pooled log OR: 0.51 [-0.23 - 1.26], P = 0.79); the pooled log OR for HBeAg loss was 1.19 (0.63 - 1.76, P = 0.03) and pooled log OR for HBeAg seroconversion was 2.19 (0.91 - 3.47, P = 0.40). For patients receiving first-line therapy with tenofovir disoproxil fumarate (TDF) and entecavir (ETV), the viral response showed no significant difference between the two groups: Pooled log OR (0.38 [-0.22 - 0.97], P = 0.10). The pooled log OR for HBeAg loss and HBeAg seroconversion was (-0.07 [-0.81 0.67], P = 0.68) and (0.40 [-0.84 - 1.63], P = 0.88), respectively. Conclusions: The efficacies of first-line therapy with TDF and ETV treatments were similar in groups with elevated and normal ALT levels for the outcomes of viral response and HBeAg loss. These findingsmaysupport further treatment of CHBpatients with normal ALT levels.",
        "keywords": "Alanine Aminotransferase (ALT); Chronic HBV Infection; Antiviral Agents; NAs",
        "released": 2022,
        "link": "https://doi.org/10.5812/hepatmon-129836"
    },
    {
        "title": "Recommendation mapping of the world health organization’s guidelines on tuberculosis: A new approach to digitizing and presenting recommendations",
        "abstract": "Objective: Having up-to-date health policy recommendations accessible in one location is in high demand by guideline users. We developed an easy to navigate interactive approach to organize recommendations and applied it to tuberculosis (TB) guidelines of the World Health Organization (WHO). Study Design: We used a mixed-methods study design to develop a framework for recommendation mapping with seven key methodological considerations. We define a recommendation map as an online repository of recommendations from several guidelines on a condition, providing links to the underlying evidence and expert judgments that inform them, allowing users to filter and cross tabulate the search results. We engaged guideline developers, users, and health software engineers in an iterative process to elaborate the WHO eTB recommendation map. Results: Applying the seven-step framework, we included 228 recommendations, linked to 103 guideline questions and organized the recommendation map according to key components of the health question, including the original recommendations and rationale ( https:// who.tuberculosis.recmap.org/ ). Conclusion: The recommendation mapping framework provides the entire continuum of evidence mapping by framing recommendations within a guideline questions’ population, interventions, and comparators domains. Recommendation maps should allow guideline developers to organize their work meaningfully, standardize the automated publication of guidelines through links to the GRADEpro guideline development tool, and increase their accessibility and usability. (c) 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ )",
        "keywords": "Evidence-based practice; Guideline; Tuberculosis; GRADE",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.jclinepi.2021.02.009"
    },
    {
        "title": "Decision tree algorithm-based model and computer simulation for evaluating the effectiveness of physical education in universities",
        "abstract": "In this paper, the forest algorithm and the decision tree algorithm are mainly used to analyze students’ physical education information, course exam results, and student learning data and relevant feature attributes from the online teaching platform. We aim to generate decision trees using the decision tree algorithm for the purpose of generating classification rules, based on which we can find factors that are important to students’ physical education performance and form data basis for improving teaching quality to help teaching management and teachers improve teaching methods and adjust teaching strategies. We specifically achieved this objective by constructing a model for assessing the effectiveness of student teaching, the steps of which include data collection and preparation, data preprocessing (data cleaning, conversion, integration), model construction (algorithm training), and algorithm optimization, as well as realizing the simulation results of the model. At the same time, the importance of the relevant attributes of the model is analyzed, and some measures are proposed to improve the universities: the standard of physical education teaching and the corresponding strategies for improving teaching methods. The mainstream development environment is chosen to ensure the complete operation of the project system that integrates learning, operation, and evaluation. The sports virtual simulation experimental teaching system realized in this paper has good functionality, stability, and application benefits in operation and use.",
        "keywords": "",
        "released": 2020,
        "link": "https://doi.org/10.1155/2020/8868793"
    },
    {
        "title": "Elaboration of screening scales for mental development problems detection in russian preschool children: Psychometric approach",
        "abstract": "Background: computer-based screenings are usually used for early detection of a child’s mental development problems. However, there are no such screenings in Russia yet. This study aimed to elaborate scales for rapid monitoring of mental development of 3-year-olds. Methods: 863 children took part in the study, among them 814 children of the group Norm, 49 children with developmental delay (DD), including 23 children with symptoms of autistic spectrum disorder (ASD). The multifactor study of mental development tool was used as a part of a software complex for longitudinal research for data collection. This study used a set of 233 tasks that were adequate for 3-year-olds. Exploratory and confirmatory factor analysis was used for the elaboration and factor validation of the scales. The structure of the relationship between scales and age was refined using structural equation modeling. Results: as a result of the research, screening scales were elaborated: “Logical reasoning”, “Motor skills”, “General awareness”, “Executive functions”. The factor validity and reliability of scales were proved. The high discriminability of the scales in distinguishing the “Norm” and “DD” samples was revealed. The developed test norms take into account the child’s age in days and allow identifying a “risk group” with an expected forecast accuracy of at least 90%. The obtained scales meet psychometric requirements for their application and allow creating an online screening system for wide application.",
        "keywords": "screening; markers of mental development; ASD; developmental delay; construction of scales; 3-year-olds",
        "released": 2020,
        "link": "https://doi.org/10.3390/diagnostics10090646"
    },
    {
        "title": "Integrating the OHIF viewer into XNAT: Achievements, challenges and prospects for quantitative imaging studies",
        "abstract": "Purpose: XNAT is an informatics software platform to support imaging research, particularly in the context of large, multicentre studies of the type that are essential to validate quantitative imaging biomarkers. XNAT provides import, archiving, processing and secure distribution facilities for image and related study data. Until recently, however, modern data visualisation and annotation tools were lacking on the XNAT platform. We describe the background to, and implementation of, an integration of the Open Health Imaging Foundation (OHIF) Viewer into the XNAT environment. We explain the challenges overcome and discuss future prospects for quantitative imaging studies. Materials and methods: The OHIF Viewer adopts an approach based on the DICOM web protocol. To allow operation in an XNAT environment, a data-routing methodology was developed to overcome the mismatch between the DICOM and XNAT information models and a custom viewer panel created to allow navigation within the viewer between different XNAT projects, subjects and imaging sessions. Modifications to the development environment were made to allow developers to test new code more easily against a live XNAT instance. Major new developments focused on the creation and storage of regions-of-interest (ROIs) and included: ROI creation and editing tools for both contour- and mask-based regions; a “smart CT” paintbrush tool; the integration of NVIDIA’s Artificial Intelligence Assisted Annotation (AIAA); the ability to view surface meshes, fractional segmentation maps and image overlays; and a rapid image reader tool aimed at radiologists. We have incorporated the OHIF microscopy extension and, in parallel, introduced support for microscopy session types within XNAT for the first time. Results: Integration of the OHIF Viewer within XNAT has been highly successful and numerous additional and enhanced tools have been created in a programme started in 2017 that is still ongoing. The software has been downloaded more than 3700 times during the course of the development work reported here, demonstrating the impact of the work. Conclusions: The OHIF open-source, zero-footprint web viewer has been incorporated into the XNAT platform and is now used at many institutions worldwide. Further innovations are envisaged in the near future.",
        "keywords": "XNAT; OHIF; web viewer; image visualisation; regions-of-interest; rapid reader",
        "released": 2022,
        "link": "https://doi.org/10.3390/tomography8010040"
    },
    {
        "title": "Towards a fictional collective programming scenario: An approach based on the EIF loop",
        "abstract": "In this paper, we base our research on a fictional collective programming scenario: A group of physically-distributed programmers try to collaboratively solve a programming problem in a web-based development environment, through a continually executing loop, consisting of three concurrent activities: exploration, integration, and feedback. Inexploration, a programmer is freely to submit a sequence of gradually improved solutions until achieving a correct one. All programmers’ latest submissions are integrated into a collective artifact throughintegration. And throughfeedback, any programmer who hasn’t achieved a correct solution is continuously pushed with personalized feedback information from the collective artifact, to help the programmer improve her/his submission. In order to facilitate the realization of this fictional scenario, we narrow the target problems to those introductory programming problems, design a genetic algorithm to integrate a set of syntax-correct programs into acollective program dependence graph(CPDG), and propose an automatic feedback generation method based on the CPDG and a programmer’s latest submission. The key idea is to generate feedback from mutual inspiration: Any programmer’s submission (even not correct) may possess information that could provide inspiration for others. We evaluate the proposed approach through a set of simulated experiments, as well as a set of real experiments. The results show that our approach has a precision of 90% and a recall of 80% in randomly generated data sets on average, and a precision of 69% and a recall of 77% in real student submissions on average.",
        "keywords": "Collective programming; Genetic algorithm; Software engineering; Collaborative learning",
        "released": 2020,
        "link": "https://doi.org/10.1007/s10664-020-09850-7"
    },
    {
        "title": "A meteorological data distribution system using remote method invocation technology",
        "abstract": "A meteorological data distribution system (MDDS) is presented. The system was implemented in the Spanish Juan Carlos I Antarctic station during the 2002-2003 campaign. Meteorological data are distributed to any point of the base using the LAN, which allows distributing data up to a 3.3-km radius from the acquisition point. The acquisition control procedures are also integrated in the same system. A wide range of difficulties have been overcome during the development of the MDDS, including the integration of heterogeneous components, the integrity of the message system, the future extension and scalability of the system, fault tolerance, process concurrency, and the temporary synchronization of all the applications involved. These have been achieved through three steps, namely 1) design of a complete object-oriented model of the system; 2) implementation of this model using a scalable and portable development environment, such as Java; and 3) use of an intercommunication application technology that fits all the system requirements (high level of abstraction and easy to program and to deploy) as the remote method invocation technology from Java. This development is part of a more ambitious project, called LabVir, that is devoted to implement distributed measurements in Spanish oceanographic vessels and in the Antarctic base using Web technology interfaces. In a more general framework, the LabVir project will make the experimentation tools accessible from any existing navigator, creating a distributed research environment, that is accessible and user friendly from any place with hypertext transfer protocol connectivity.",
        "keywords": "distributed systems; Java; meteorology; object oriented; remote method invocation (RMI)",
        "released": 2006,
        "link": "https://doi.org/10.1109/TIM.2006.881572"
    },
    {
        "title": "Design and implementation of an intelligent DNS management system",
        "abstract": "In this paper, we focus on the design and building of iDNS-MS by using knowledge-based system and ontological engineering technologies. iDNS-MS, developed by using web interface and expert system technology, is a unifying environment for providing plausible answers to help solve the complex DNS management problems or alleviate these DNS administration loadings. In iDNS-MS, we propose an ontology-driven model to elicit rules from a previously built DNS ontology and construct an objected-oriented knowledge base. The whole process of the model consists of ontology construction, knowledge class organization and facts/rules loading phases. Ontology construction phase is used to construct the domain ontology, knowledge class organization phase is used to organize the relationships between the knowledge classes, and facts/rules loading phase is used to fill in the facts/rules of knowledge classes extracted from domain experts. In addition, we adopt DRAMA/NORM development environment as the expert system shell to design and implement a unifying framework (e.g. DNS-related problem diagnosis, planning, tutoring, etc.) for supporting intelligent DNS management. According to our experimental results, the paradigm of using DNS ontology to build iDNS-MS works good and effective. iDNS-MS will benefit the sharing and reusing of global DNS knowledge, the reduction of people’s time to learn DNS management, the ease of DNS configuration and planning, and the improvement on DNS and network operation. (C) 2004 Elsevier Ltd. All rights reserved.",
        "keywords": "DNS ontology; ontology-driven; knowledge-based system; iDNS-MS",
        "released": 2004,
        "link": "https://doi.org/10.1016/j.eswa.2004.01.005"
    },
    {
        "title": "Insights in development and deployment of the GLA and NUTBAL decision support systems for grazinglands",
        "abstract": "The evolution of two decision support systems are traced from their roots in academia to deployment to technical advisors in USDA Natural Resource Conservation Service. The Grazing Lands Application (GLA) decision support system (DSS) was designed to provide forage inventories for grazing management of ranches. The other tool, NUTBAL, evolved as a stand alone DSS, emerging as a component of GLA when a supporting monitoring technology for nutritional profiling of free-ranging animals provided the user rapid estimates of diet quality from fecal scans with near infrared reflectance spectroscopy (NIRS). The adoption pattern of GLA and NUTBAL were quite different, with GLA experiencing less widespread adoption in USDA NRCS. The primary causes were (1) limited adoption rate of GLA within NRCS associated with changing culture in the information technology development group, (2) time overloading and staff reassignments for new programs, (3) changing software/hardware development environments imposed by the client disrupting development and System design and (4) large up front conversion of a largely paper-based system to a digital form. GLA was transformed to web-based delivery and streamlined to gain greater acceptance by users and ease time constraints on use of spatial tools. The NUTBAL system experienced more of a user driven evolution since it emerged from the GLA suite of tools and was supported by on-ranch monitoring systems capability of directly linking the livestock producer’s animals with the software. NUTBAL’s linkage to animal monitoring systems seems to have accelerated adoption rates. Ease of access to supporting input data coupled with early involvement of the target user and extensive analysis of the decision environment were critical to future success of these systems. Targeting technical advisors instead of livestock producers appears to be a more viable development track unless new innovations in DSS delivery systems can emerge using the internet. (C) 2002 Published by Elsevier Science Ltd.",
        "keywords": "DSS; assessment; adoption; software; nutrition; resource; management",
        "released": 2002,
        "link": "https://doi.org/10.1016/S0308-521X(02)00023-9"
    },
    {
        "title": "Efficacy and cost-effectiveness of antiviral regimens for entecavir-resistant hepatitis b: A systematic review and network meta-analysis",
        "abstract": "Background: Chronic hepatitis B (CHB) patients who had exposed to lamivudine (LAM) and telbivudine (LdT) had high risk of developing entecavir (ETV)-resistance after long-term treatment. We aimed to conduct a systematic review and a network meta-analysis on the efficacy and cost-effectiveness on antiviral regimens in CHB patients with ETV-resistance. Data sources: We searched PubMed, EMBASE and Web of Science for studies on nucleos(t)ide analogues (NAs) treatment [including tenofovir disoproxil fumarate (TDF)-based rescue therapies, adefovir (ADV)based rescue therapies and double-dose ETV therapy] in CHB patients with ETV-resistance. The network meta-analysis was conducted for 1-year complete virological response (CVR) and biological response (BR) rates using GeMTC and ADDIS. A cost-effective analysis was conducted to select an economic and effective treatment regimen based on the 1-year CVR rate. Results: A total of 6 studies were finally included in this analysis. The antiviral efficacy was estimated. On network meta-analysis, the 1-year CVR rate in ETV-TDF [odds ratio (OR) = 22.30; 95% confidence interval (CI): 2.78-241.93], LAM-TDF (OR = 70.67; 95% CI: 5.16-1307.45) and TDF (OR = 16.90; 95% CI: 2.28-186.30) groups were significantly higher than that in the ETV double-dose group; the 1-year CVR rate in the LAM-TDF group (OR = 14.82; 95% CI: 1.03-220.31) was significantly higher than that in the LAM/LdT-ADV group. The 1-year BR rate of ETV-TDF (OR = 28.68; 95% CI: 1.70-1505.08) and TDF (OR = 21.79; 95% CI: 1.43-1070.09) therapies were significantly higher than that of ETV double-dose therapy. TDFbased therapies had the highest possibility to achieve the CVR and BR at 1 year, in which LAM-TDF combined therapy was the most effective regimen. The ratio of cost/effectiveness for 1-year treatment was 8 526, 17 649, 20 651 Yuan in the TDF group, TDF-ETV group, and ETV-ADV group, respectively. Conclusions: TDF-based combined therapies such as ETV-TDF and LAM-TDF therapies were the first-line treatment if financial condition is allowed. (C) 2020 First Affiliated Hospital, Zhejiang University School of Medicine in China. Published by Elsevier B.V. All rights reserved.",
        "keywords": "Chronic hepatitis B; Drug resistance; Entecavir; Anitviral therapy",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.hbpd.2020.09.007"
    },
    {
        "title": "Mood themes the world",
        "abstract": "Apparatus theory (a hybrid of McLuhan and Derrida) hypothesizes that a civiliza-tion of electracy (the digital apparatus) must learn how to thrive in a lifeworld in which the visceral faculty of appetite is hegemonic. The dominant axis of behavior today is fantasy -anxi-ety (attraction/repulsion). We propose that world theming has created a vernacular discourse that may be raised to a second power of expression as vehicle of visceral intelligence. The immediate claim is that theming in digital media augments mood (ambiance) into a power of imagination, just as dialectic in writing augmented logic into a power of reason. Fantasy today is persuasive, just as logical entailment is (was) in the rational order of literacy. Decisions de-termining real events today are being made in worlds of mood.World theming is evident in the vernacular art practices arising from recent advanc-es in artificial intelligence. The availability of commodity GPUs, along with public access to advanced research via GitHub, Kaggle, Hugging Face, and the proliferation of forums such as Reddit, Discord, YouTube, and others, has resulted in a renaissance of public engagement with technology-informed creative practice. In addition, the general availability of Google’s previously internal-only development tool, Colab, in late 2017 provided access to cloud-based GPUs and storage systems accessible only to data scientists and academics. In early 2021 Ryan Murdock released a Colab notebook called Big Sleep that combined Ope-nAI’s recently published Contrastive Language-Image Pre-training (CLIP) with BigGAN. This model is a paradigmatic example of our observation. By early 2022, multiple derivations of this process incorporated alternative image generation techniques. This paper will demonstrate how the fundamental basis of these methods are distinctly electrate in their use of “theme” and emphasis on “mood” in world-building, including a case-study animation called Dissipative Off-ramps.",
        "keywords": "apparatus; theme; mood; artificial intelligence; electracy",
        "released": 2023,
        "link": "https://doi.org/10.25038/am.v0i29.556"
    },
    {
        "title": "Development and application of a chinese webpage suicide information mining system (sims)",
        "abstract": "Objectives This study aims at designing and piloting a convenient Chinese webpage suicide information mining system (SIMS) to help search and filter required data from the internet and discover potential features and trends of suicide. Methods SIMS utilizes Microsoft Visual Studio2008, SQL2008 and C# as development tools. It collects webpage data via popular search engines; cleans the data using trained models plus minimum manual help; translates the cleaned texts into quantitative data through models and supervised fuzzy recognition; analyzes and visualizes related variables by self-programmed algorithms. Results The SIMS developed comprises such functions as suicide news and blogs collection, data filtering, cleaning, extraction and translation, data analysis and presentation. SIMS-mediated mining of one-year webpage revealed that: peak months and hours of web-reported suicide events were June-July and 10-11 am respectively, and the lowest months and hours, September-October and 1-7 am; suicide reports came mostly from Soho, Tecent, Sina etc.; male suicide victims over counted female victims in most sub-regions but southwest China; homes, public places and rented houses were the top three places to commit suicide; poisoning, cutting vein and jumping from building were the most commonly used methods to commit suicide; love disputes, family disputes and mental diseases were the leading causes. Conclusions SIMS provides a preliminary and supplementary means for monitoring and understanding suicide. It proposes useful aspects as well as tools for analyzing the features and trends of suicide using data derived from Chinese webpages. Yet given the intrinsic “dual nature” of internet-based suicide information and the tremendous difficulties experienced by ourselves and other researchers, there is still a long way to go for us to expand, refine and evaluate the system.",
        "keywords": "Suicide; News; Blogs; Data mining; Support system",
        "released": 2014,
        "link": "https://doi.org/10.1007/s10916-014-0088-z"
    },
    {
        "title": "Artificial intelligence applications in k-12 education: A systematic literature review",
        "abstract": "Education is a vital part of the development of society and it is changing over time in terms of methods, content, concepts, and models. Recently, it has been increasingly prevalent to benefit from the potentialities of Artificial Intelligence (AI) in addressing educational issues. In this research, the current state of the art of the integration of AI in K-12 education was provided. Specifically, different parts of education in which AI was employed along with the related AI categories were discussed according to different K-12 grades and courses. Additionally, technologies and environments that contributed to employing AI in education were discussed. To this end, a systematic literature review was conducted on articles and conference papers published between 2011 and 2021 in the Web of Science and Scopus databases. As the result of the initial search, 2075 documents were extracted and based on inclusive criteria and 210 documents were identified for further investigation. AI applications were categorized into Student performance, Teaching, Selection, and Behavior tasks, and Other. Machine Learning (ML) and Intelligent Tutoring System (ITS) were the most common approaches among AI categories. Furthermore, high school-related applications were more frequent and STEM courses were substantially targeted by AI. In conclusion, the remarkable impact of AI on education was concluded. The current study reveals information about the potentialities offered by AI in K-12 education which aids researchers in implementing AI-based education systems. As for future works, other databases such as ACM library and Google Scholar can be investigated as well. Furthermore, exploring the 95 papers that were excluded due to inaccessibility to their full texts can be taken into account. Finally, the papers can be also investigated in terms of pedagogical approaches or development tools.",
        "keywords": "Artificial intelligence; Education; Task analysis; Systematics; Robots; Databases; Games; Artificial intelligence; machine learning; education; educational technology; review",
        "released": 2022,
        "link": "https://doi.org/10.1109/ACCESS.2022.3179356"
    },
    {
        "title": "Visual specification of measurements and redesigns for domain specific visual languages",
        "abstract": "Ensuring model quality is a key success factor in many computer science areas, and becomes crucial in recent software engineering paradigms like the one proposed by model-driven software development. Tool support for measurements and redesigns becomes essential to help developers improve the quality of their models. However, developing such helper tools for the wide variety of (frequently domain specific) visual notations used by software engineers is a hard and repetitive task that does not take advantage from previous developments, thus being frequently forgotten. In this paper we present our approach for the visual specification of measurements and redesigns for Domain Specific Visual Languages (DSVLs). With this purpose, we introduce a novel DSVL called SLAMMER that contains generalisations of some of the more used types of internal product measurements and redesigns. The goal is to facilitate the task of defining measurements and redesigns for any DSVL, as well as the generation of tools from such specification reducing or eliminating the necessity of coding. We rely on the use of visual patterns for the specification of the relevant elements for each measurement and redesign type. In addition, SLAMMER allows the specification of redesigns either procedurally or by means of graph transformation rules. These redesigns can be triggered when the measurements reach a certain threshold. These concepts have been implemented in the meta-model ling tool AToM(3). In this way, when a DSVL is designed, it is possible to specify measurements and redesigns that will become available in the final modelling environment generated for the language. As an example, we show a case study in the web modelling domain. (C) 2007 Elsevier Ltd. All rights reserved.",
        "keywords": "domain specific visual languages; meta-modelling; measurement; redesign; graph transformation; graphical pattern",
        "released": 2008,
        "link": "https://doi.org/10.1016/j.jvlc.2007.09.002"
    },
    {
        "title": "Prevalence and risk factors for road traffic injuries and mortalities in ethiopia: Systematic review and meta-analysis",
        "abstract": "Background Road traffic injuries, which are responsible for premature deaths and functional losses, are the leading causes of unintentional injuries in Ethiopia. As most studies on road traffic injuries, so far, have been either local or regional, it is believed that combining the regional or local data to get nationally representative information could help programme implementers in setting priorities. Objective The aim of this review was to estimate the proportion of road traffic injuries, mortality and risk factors for the problem among all age groups in Ethiopia. Data sources A systematic review of articles using MEDLINE/PubMed SCOPUS Web of Science and science direct was conducted. Additional studies were identified via manual search. Study selection Only studies that reported road traffic injuries and/or mortalities for all age groups were included in this review. Data synthesis All pooled analyses were based on random-effect models. Twenty-six studies for the prevalence of RTIs (n=37 424), 24 studies for road traffic injuries (RTI) mortality, (n=38 888), 9 studies for prevalence of fracture among RTIs (n=2817) and 5 studies for the prevalence of post-traumatic stress disorder (n=1733) met our inclusion criteria. Driving in the dark increased severity of injury by 1.77, 95% CI 1.60 to 1.95). The certainty of the evidence was assessed using GRADEpro Guideline Development Tool. Conclusion In this review, the burden of road traffic injuries and mortalities remains high in Ethiopia. Human factors are the most common causes of the problem in Ethiopia. The existing safety regulations should be re-evaluated and supported by continuous behavioural interventions. PROSPERO registration number CRD42019124406.",
        "keywords": "metanalysis; systematic review; mortality; public health",
        "released": 2021,
        "link": "https://doi.org/10.1136/injuryprev-2020-044038"
    },
    {
        "title": "Utilization and perceived effectiveness of a web-based faculty development seminar for international medical science educators",
        "abstract": "The goal of this study was to examine the utilization and perceived effectiveness of the International Association of Medical Science Educators (IAMSE) Webcast Audio Seminar Series (WAS) by participants at the individual and institutional levels. The Webcast Audio Seminar Series User Survey (WASUS) included multiple quantitative and qualitative measures of user perceptions of their experiences and overall quality. Data was collected using a 42-item survey that examined user identification, utilization, and perceived effectiveness of the IAMSE WAS as a faculty development tool. Quantitative measures were summarized using descriptive statistics, including frequencies, means, and standard deviations. Qualitative data was investigated using an iterative, inductive thematic coding method. Qualitative themes were summarized and applied to quantitative trends as explanatory mechanisms with the intent to provide a more nuanced narrative of the data. The survey was sent to all 2012-2017 WAS participants which provided a cross-sectional snapshot of WAS user perceptions over a substantial period of time. Fifty-two participants responded. Survey participants were asked to rate WAS sessions on twelve different quality components. Quality ratings were explored by user subscription types and user experiences with other web-based conferences. Users rated the WAS program very highly on all components from 2012 to 2017. The high level of perceived quality by users is likely an important reason why WAS participation has continued to grow since its implementation. Since the quality ratings were consistently high over a 5-year period in which the number of users also grew and organizers continue to add new interactive features for users, it is expected that this growth is sustainable.",
        "keywords": "Faculty development; Medical education; Web-based seminar; Pedagogy; Utilization; Effectiveness",
        "released": 2020,
        "link": "https://doi.org/10.1007/s40670-019-00824-z"
    },
    {
        "title": "Is soy intake related to age at onset of menarche? A cross-sectional study among adolescents with a wide range of soy food consumption",
        "abstract": "Background: Early onset of menarche may negatively influence the future health of adolescent girls. Several factors affect the timing of menarche but it is not clear if soy foods consumption around pubertal years plays a role; thus, we examined its relation to age at onset of menarche (AOM) in a high soy-consuming population. Methods: We conducted a cross-sectional study on 339 girls ages 12-18 years attending middle and high schools near two Seventh-day Adventist universities in California and Michigan using a web-based dietary questionnaire and physical development tool. Soy consumption (categorized as total soy, meat alternatives, tofu/traditional soy, and soy beverages) was estimated from the questionnaire, while AOM was self-reported. Data analyses included descriptive statistics, Cox proportional hazards ratios, Kaplan-Meier curves and Poisson regression with adjustment for relevant confounders. Results: Mean (SD) intakes were: total soy, 12.9 (14.4) servings/week; meat alternatives, 7.0 (8.9) servings/week; tofu/traditional soy foods, 2.1 (3.8) servings/week; soy beverages, 3.8 (6.3) servings/week. Mean AOM was 12.5 (1.4) y for those who reached menarche. Consumption of total soy and the 3 types of soy foods was not significantly associated with AOM and with the odds for early- or late-AOM. Adjustment for demographic and dietary factors did not change the results. Conclusion: Soy intake is not associated with AOM in a population of adolescent girls who have a wide range of, and relatively higher, soy intake than the general US population. Our finding suggests that the increasing popularity of soy in the US may not be associated with AOM.",
        "keywords": "Soy; Adolescence; Age at menarche; Pubertal timing; Puberty; Seventh-day Adventists",
        "released": 2014,
        "link": "https://doi.org/10.1186/1475-2891-13-54"
    },
    {
        "title": "Comparison of behavioral characteristics of dogs in the united states and japan",
        "abstract": "This study examined the difference in dog owning between Japan and the United States, and the effect of these differences on dogs’ behavioral characteristics. Behavioral evaluations of privately-owned dogs were obtained by using online questionnaire. We compared background and demographic information from the two countries and analyzed the effects of these differences on behavioral characteristics in dogs. The results indicated that there was a bias in the dog breeds kept in Japan compared to the United States and that Japanese dogs’ body weight was lower than the US dogs. The main source of dog acquisition was pet stores in Japan and breeders and/or shelters in the United States. Multiple linear regression analysis found that Japanese dogs showed more aggression to household members and higher energy, restlessness and fear of non-social stimuli than US dogs, while US dogs showed more fear of unfamiliar persons, separation-related behavior and excitability. US dogs also showed higher levels of trainability and attachment to owners. The lower dog’s body weight was, the higher the behavioral scores except for trainability were. When dogs that were obtained under 3 months of age were analyzed, the younger the dogs were when their owners obtained them, the higher the scores on some behavioral problem factors were. The higher rates of problem behaviors among Japanese dogs compared with US dogs suggest that the preference for small breed dogs and poor early development environment influenced the behavioral characteristics of dogs.",
        "keywords": "behavioral characteristics; canine; C-BARQ; questionnaire",
        "released": 2016,
        "link": "https://doi.org/10.1292/jvms.15-0253"
    },
    {
        "title": "Eye tracking technologies to visualize secure coding behavior",
        "abstract": "Secure coders’ experiences and performances vary greatly and any missed security flaws in source code may lead to costly consequences. Their behavior to analyze source code and develop mitigation techniques is not well understood. Our objective is to gain insight into the strategies and techniques from both novice and experienced developers. Proper understanding can help us to inform inexperienced coders to efficiently and accurately approach, discover, and mitigate security flaws. Our research relies upon eye tracking hardware and software to collect and analyze the eye gazes. Unlike existing approaches, we incorporate a wide range of tasks simultaneously reading documentation, writing code, and using security coding analysis tools. We analyze both static and dynamic (interactive) stimuli in a realistic software development environment. Our pictorial visualizations represent a coder’s eye gazes that visually demonstrates their behavior and patterns. In addition, we provide the full context of the stimuli that a participant observed. This allows for investigating the behavior at a range of tasks for a single participant and between participants. Our secure coding tasks include reading documentation, reading source code, and writing source code for a web application as well as utilizing security code scanning tools. Our contributions also include (1) novel visualization techniques to present transitions among components within and between applications, and (2) presentations of coders’ attention levels during secure coding by investigating the change of pupil sizes. The eye tracking collection and analysis techniques support both modifiable stimuli and stimuli presented in different sequences based upon individual participant’s behavior.",
        "keywords": "Cybersecurity secure coding; Hands-on education learning; Software development coding behaviors; Eye tracking; Eye tracking user -interactive stimuli; Software vulnerabilities",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.array.2022.100241"
    },
    {
        "title": "Links between environmental features and developmental outcomes of elite youth athletes: A cross-sectional study within the german talent pathway",
        "abstract": "From a holistic perspective, the talent development environment (TDE) influences not only the athletic development but also the development of personal skills as well as the wellbeing of elite youth athletes. Alongside research on the effects of broader environmental features on athlete-related talent development (TD) outcomes, the importance of the closer social climate (i.e., teammates, coaches, support staff) is also emphasized. Therefore, the purpose of this study was to examine the relationship between environmental features, the social climate and athlete-related TD outcomes (I). Additionally, an exploratory analysis was conducted to investigate the role of motivational climate and psychological safety in the relationship between environmental features and athleterelated TD outcomes (II). To this end, 345 German elite youth athletes completed an online survey assessing their perceptions of environmental features, coach-created motivational climate, and psychological safety as well as wellbeing, performance satisfaction, and life skills development. The results revealed that environmental features (especially long-term development focus, effective coach-athlete communication, and the social network), motivational climate and psychological safety were all significantly associated with the outcome variables. These findings underline the significance of environmental features for athlete-related TD outcomes of young talented athletes in German elite youth sports. However, the results indicated that motivational climate and psychological safety did not have significant indirect effects on the relationship between TDE features and athlete-related TD outcomes. Thus, it remains to be questioned if and what other processes influence this relationship.",
        "keywords": "Talent development environment; Motivational climate; Psychological safety; Wellbeing; Performance satisfaction; Life skills",
        "released": 2024,
        "link": "https://doi.org/10.1016/j.psychsport.2023.102569"
    },
    {
        "title": "Research on smart EFK algorithm for electric vehicle battery packs management system",
        "abstract": "Battery management system (BMS) is an important part of electric vehicle that can improve the efficiency of battery and ensure the safety during operating process. State of charge (SOC) estimation is one of kernel technology of BMS. High precision of SOC estimation can not only increase service life of battery, but also enhance energy utilization efficiency and cut down the operational cost. Based on fully understand working principle of battery, three-order RC equivalent circuit is chose to simulate the external characteristics. Introduce the working theory of that combination of extended Kalman filter (EKF) and open circuit voltage (OCV), and the whole recursive algorithm of estimation SOC, using the OCV-SOC look-up table to solve the problem of initial value of SOC to a certain degree. The hardware circuit and software of BMS are designed. Hardware circuit consists of current measurement circuit, voltage measurement circuit, temperature measurement circuit and equalization circuit. The software is designed by C as the development language, the Microsoft Visual Studio 2008 as development environment and SQL Server 2008 as database management system. The experiments are carried out after the experimental platform of system hardware and software is constructed. The system realizes the real-time online monitoring of BMS, SOC estimation and etc. The result verifies the reliability and feasibility. The precision requirement of SOC estimation by using EKF algorithm and OCV-SOC look-up table meets the national standard for electric vehicle.",
        "keywords": "Electric vehicle; battery management system; smart EFK algorithm; extended kalman filter; state of charge",
        "released": 2020,
        "link": "https://doi.org/10.3233/JIFS-179400"
    },
    {
        "title": "Scalable multimedia content analysis on parallel platforms using python",
        "abstract": "In this new era dominated by consumer-produced media there is a high demand for web-scalable solutions to multimedia content analysis. A compelling approach to making applications scalable is to explicitly map their computation onto parallel platforms. However, developing efficient parallel implementations and fully utilizing the available resources remains a challenge due to the increased code complexity, limited portability and required low-level knowledge of the underlying hardware. In this article, we present PyCASP, a Python-based framework that automatically maps computation onto parallel platforms from Python application code to a variety of parallel platforms. PyCASP is designed using a systematic, pattern-oriented approach to offer a single software development environment for multimedia content analysis applications. Using PyCASP, applications can be prototyped in a couple hundred lines of Python code and automatically scale to modern parallel processors. Applications written with PyCASP are portable to a variety of parallel platforms and efficiently scale from a single desktop Graphics Processing Unit (GPU) to an entire cluster with a small change to application code. To illustrate our approach, we present three multimedia content analysis applications that use our framework: a state-of-the-art speaker diarization application, a content-based music recommendation system based on the Million Song Dataset, and a video event detection system for consumer-produced videos. We show that across this wide range of applications, our approach achieves the goal of automatic portability and scalability while at the same time allowing easy prototyping in a high-level language and efficient performance of low-level optimized code.",
        "keywords": "Experimentation; Performance",
        "released": 2014,
        "link": "https://doi.org/10.1145/2517151"
    },
    {
        "title": "Environmental perspective and strategy selection for sustainable development of china’s sports industry: An analysis based on SWOT-PEST model",
        "abstract": "Sustainable development stands as a paramount strategic objective for the Chinese government, along with being a crucial research subject within the realm of China’s sports industry. This industry has transformed into a pivotal element and a significant domain in the construction of China’s athletic dominance, emerging as a dynamic catalyst for the nation’s high-quality economic advancement. Regrettably, the exploration of sustainable development within China’s sports industry has not received the requisite attention and profound investigation it deserves. The purpose of the study is to analyze the internal and external development environment of China’s sports industry from four aspects with the help of SWOT-PEST model: Political, Economic, Social and Technological, and make an exploratory study on the strategic choices for the sustainable development of China’s sports industry, suggesting: Strengthen the leading role of the Chinese government in the sports industry, increase policy support and financial investment; reduce the tax burden of sports enterprises and encourage sports enterprises to expand their scale; innovate sports products and services to promote the development of sports and health industry; Continue to strengthen the intelligent construction of sports infrastructure to improve operational efficiency and sports experience; strengthen international cooperation and exchanges, and learn from the experience and technology of other countries to improve international competitiveness; and utilize the digital economy to promote the development of the sports industry by organizing online tournaments and virtual spectator experiences, and other specific development strategies.",
        "keywords": "Sports industry in China; sustainable development; SWOT-PEST model; SWOT model; PEST model",
        "released": 2023,
        "link": "https://doi.org/10.47750/RAA/11.2.31"
    },
    {
        "title": "A SYSTEM OF REWARDS AND MOTIVATION FOR STUDENTS USING VIRTUAL CURRENCY",
        "abstract": "Motivating individuals in today’s world is an essential responsibility that falls on the shoulders of educators. Encouraging students to participate in various activities beyond just their studies is essential. This will help motivate them and keep them engaged. The study aims to develop a system of rewards and motivation for students using virtual currency and its implementation in the educational process of Zhytomyr Polytechnic State University. The developed system promotes additional external motivation for learning, scientific and creative activities. The article discusses the main stages of creating a mobile application. The Ionic Framework is the basis of the application’s architecture, utilizing Angular as its foundation. Ionic Framework 5 was used to write the client part. The application development environment is JetBrains WebStorm on JavaScript, CSS, and HTML. Firebase handles the server aspect of the application. The application uses the Firebase Google Cloud Firestore database with the Angular Fire project framework connected. The article describes the system design process using UML diagrams. The software product’s scenarios, general structure, and database description (which includes 16 tables) are provided. We have ensured that our features are developed with utmost priority given to the security of user data and real-time access to information. This allows to provide access only to relevant information without prior updating of the client part of the application. This article explains how to install and use the system on an Android operating system. It also covers the design of the application interface and provides a demonstration of how the system works. The system’s primary objective is to encourage users by granting them the power to handpick a task from their preferred category, successfully accomplish it, and ultimately earn virtual funds. These virtual funds can be converted into desirable items in a virtual store.",
        "keywords": "motivation; virtual currency; learning incentive; mobile application; PolyCoin",
        "released": 2023,
        "link": "https://doi.org/10.33407/itlt.v96i4.5285"
    },
    {
        "title": "Development and prospective evaluation of a multimedia teaching course on aortic valve replacement",
        "abstract": "Background: Participation in heart surgery requires procedural and factual knowledge and intensive preparation. There is evidence in the literature that multimedia driven learning has advantages in medical fields where an understanding of complex temporal and spatial events plays an important role. This work describes the development and evaluation of a multimedia driven, online teaching course on aortic valve replacement for students and residents. Methods: The instructional model followed a methodological approach with scalable information for different user groups. Various interactive multimedia development tools were employed. In a prospective study, 20 students and 10 residents were exposed to the program in a standardized environment. Both groups completed a 20-item multiple choice pre- and post-test. Psychometric evaluation with HILVE (Heidelberg inventory for the evaluation of teaching courses, 50 items) was performed. Results: The multimedia course integrates more than 200 high quality surgical video and audio sequences, inter-active 2 D and 3 D models, as well as illustrations and text. It is available at www.lamedica.de. Study time in the student group was 103 +/- 11 min and 70 +/- 10 min in the resident group. Mean number of correct responses to the knowledge pre-test was 6.23 +/- 2.71 in the student group and 12.2 +/- 2.66 in the resident group. Mean number of correct responses to the knowledge post-test was 15.24 +/- 2.06 in the student group (p < 0.0001 vs. pre-test) and 17 +/- 2.98 in the resident group (p = 0.004 vs. pretest). The HILVE test showed positive results for teaching conditions, the program’s instructional competence, student motivation and individual learning. Conclusion: Our data demonstrate that multimedia driven training can adapt to the individual needs of learners and improves procedural knowledge required for open heart surgery. Consequently, the whole course forms part of the training of residents and students.",
        "keywords": "multimedia teaching; aortic valve replacement; evaluation",
        "released": 2006,
        "link": "https://doi.org/10.1055/s-2005-865871"
    },
    {
        "title": "Churn prediction of mobile and online casual games using play log data",
        "abstract": "Internet-connected devices, especially mobile devices such as smartphones, have become widely accessible in the past decade. Interaction with such devices has evolved into frequent and short-duration usage, and this phenomenon has resulted in a pervasive popularity of casual games in the game sector. On the other hand, development of casual games has become easier than ever as a result of the advancement of development tools. With the resulting fierce competition, now both acquisition and retention of users are the prime concerns in the field. In this study, we focus on churn prediction of mobile and online casual games. While churn prediction and analysis can provide important insights and action cues on retention, its application using play log data has been primitive or very limited in the casual game area. Most of the existing methods cannot be applied to casual games because casual game players tend to churn very quickly and they do not pay periodic subscription fees. Therefore, we focus on the new players and formally define churn using observation period (OP) and churn prediction period (CP). Using the definition, we develop a standard churn analysis process for casual games. We cover essential topics such as preprocessing of raw data, feature engineering including feature analysis, churn prediction modeling using traditional machine learning algorithms (logistic regression, gradient boosting, and random forests) and two deep learning algorithms (CNN and LSTM), and sensitivity analysis for OP and CP. Play log data of three different casual games are considered by analyzing a total of 193,443 unique player records and 10,874,958 play log records. While the analysis results provide useful insights, the overall results indicate that a small number of well-chosen features used as performance metrics might be sufficient for making important action decisions and that OP and CP should be properly chosen depending on the analysis goal.",
        "keywords": "",
        "released": 2017,
        "link": "https://doi.org/10.1371/journal.pone.0180735"
    },
    {
        "title": "Comparison of 1L adjuvant auxiliary preparations with 2L solely polyethylene glycol plus ascorbic acid regime for bowel cleaning: A meta-analysis of randomized, controlled trials",
        "abstract": "The effectiveness of additional usage of adjuvants for bowel preparation is still unclear. This study compared 1L polyethylene glycol plus ascorbic acid with adjuvant drug regimens (1L PEG-AA, lower volume) with 2L polyethylene glycol plus ascorbic acid (2L PEG-A, low volume) to evaluate whether the adjuvants can be used to reduce the standard dosage of purgative further. The PubMed/MEDLINE, EMBASE, Cochrane Library, and Web of Science database were searched for randomized controlled trials (RCTs). The primary outcome was the efficacy of bowel preparation, and the secondary outcomes were patients’ tolerability and complication rate. The overall quality of evidence was assessed using the GRADEpro guideline development tool. Five RCTs with a total of 1013 patients from Korea were included. The majority of patients were outpatients from different hospitals. The pooled data showed no significant difference in the adequate bowel preparation rate (89.3% versus 89.4%, RR 1, 95% CI 0.95-1.05, I-2=47%) as well as in the complication rate (RR for nausea 1.22, 95% CI 0.89-1.65, I-2=49%; RR for bloating 0.96, 95% CI 0.73-1.28, I-2=0%; RR for vomiting 0.69, 95% CI 0.32-1.50, I-2=33%; RR for abdominal pain 1.01, 95% CI 0.61-1.69, I-2=0%). But a significantly higher willingness rate was observed in the lower volume (85.1% versus 67.9%, RR 1.25, 95% CI 1.14-1.38, I-2=46%). The quality of primary outcome evidence was moderate. The findings of this meta-analysis revealed that 1L PEG-AA may be a viable alternative to 2L PEG-A, with comparable effectiveness, better patient preference, and no statistically significant adverse event occurrence.",
        "keywords": "",
        "released": 2021,
        "link": "https://doi.org/10.1155/2021/6638858"
    },
    {
        "title": "Bibliometric mapping of research on magic towns of mexico",
        "abstract": "The tourism program “Pueblos Magicos” was created in 2001 by the Mexican Secretary of Tourism (Sectur), together with its brand, with the aim of promoting tourism by preserving secular and ancestral traditions, as well as revitalizing cities and towns that make great efforts to protect and safeguard their cultural wealth. In this context, the aim of this research work is to show the current state of scientific research carried out within the context of the Magic Towns of Mexico. The work methodology is based on the bibliometric analysis of the scientific production indexed in two main international databases: Web of Science and Scopus. The application of this technique will make it possible to obtain a scientific mapping of the production (growth, researchers in the subject, production impact through the number of citations, network analysis, etc.), with the aim of observing the evolution in the generation of knowledge regarding this tourism development tool that acts as a distinctive brand for tourism in Mexico. This mapping is useful for researchers as it provides information on the research carried out so far, allowing them to identify gaps to work on in their future research work. The systematic search process identified 52 articles. The results indicate that the research carried out in this context is incipient, with few researchers addressing the subject on a continuous basis and most of them being transient researchers with a single article. Most of the research was approached from the perspective of cultural heritage, cultural resources, inherited resources, cultural tourism, public policy, local development and sustainable tourism, and sustainable development. In light of the number of articles published, all of these can be considered to be incipient lines of research.",
        "keywords": "bibliometric analysis; science mapping; bibliometric network analysis; co-citation; magic towns of Mexico; cultural tourism",
        "released": 2021,
        "link": "https://doi.org/10.3390/land10080852"
    },
    {
        "title": "CIDA - THE EXPERT COMPANY INFORMATION DATABASE ADVISER",
        "abstract": "The development ana testing of an expert system to assist with the selection of online business databases for UK company information is described. The Company Information Database Adviser (CIDA) was designed for novice and end-user searchers and covers seven categories of company information: factual (directory) details, financial data, company news, company structure, mergers and acquisitions (M & A) activity, market information and company profiles. The development tool was the Leonardo expert system shell. Knowledge acquisition included three knowledge elicitation sessions with each of four experienced business information intermediaries. Outputs of knowledge acquisition were: (i) factual and evaluative information for over 200 database/host combinations, and (ii) rules, expressed diagrammatically, for database selection within each category of company information. These were used to build a prototype which was examined by intermediaries and further refined. The design is outlined and the rules governing the selection of company directory databases are described by way of illustration. Testing included: an evaluation of the user interface by novice users; field testing by staff and end-users at a prominent business school, and comparative performance trials against conventional database directories. CIDA’s database recommendations were in close agreement with those of business intermediaries across a range of independently generated queries. It was used far more successfully than conventional business database directories by trainee searchers (library and information students). End-user field trials were disappointing because of lack of participation. The study concludes that expertise in business database selection can successfully be distilled into a number of rules which can be applied by an expert system, but more research is needed into how databases are selected and used.",
        "keywords": "",
        "released": 1994,
        "link": "https://doi.org/10.1177/016555159402000403"
    },
    {
        "title": "Investigation of the physiological response to oxygen limited process conditions of <i>pichia pastoris</i> mut<SUP>+</SUP> strain using a two-compartment scale-down system",
        "abstract": "Inhomogeneities in production-scale bioreactors influence microbial growth and product quality due to insufficient mixing and mass transfer. For this reason, lots of efforts are being made to investigate the effects of gradients that impose stress in large-scale reactors in laboratory scale. We have implemented a scale-down model which allows separating a homogeneous part, a stirred tank reactor (STR), and a plug flow reactor (PFR) which mimics the inhomogeneous regimes of the large-scale fermenters. This scale-down model shows solutions to trigger oxygen limited conditions in the PFR part of the scale-down setup for physiological analysis. The goal of the study was to investigate the scale-up relevant physiological responses of Pichia pastoris strain to oxygen limited process conditions in the above mentioned two-compartment bioreactor setup. Experimental results with non-induced cultures show that the specific growth rate significantly decreased with increasing the exposure time to oxygen limitation. In parallel more by-products were produced. Examining physiological scalable key parameters, multivariate data analyses solely using on-line data revealed that different exposures to the oxygen limitation significantly affected the culture performance. This work with the small scale-downs setup reflects new approaches for a valuable process development tool for accelerating strain characterization or for verifying CFD simulations of large-scale bioreactors. As a novel methodological achievement, the combination of the two-compartment scale-down system with the proposed multivariate techniques of solely using online data is a valuable tool for recognition of stress effects on the culture performance for physiological bioprocess scale-up issues. (C) 2013, The Society for Biotechnology, Japan. All rights reserved.",
        "keywords": "Scale-down; Two-compartment reactor; Pichia pastoris; Stress response; Oxygen limitation",
        "released": 2013,
        "link": "https://doi.org/10.1016/j.jbiosc.2013.03.021"
    },
    {
        "title": "An easy-to-implement fuzzy expert package with applications using existing java classes",
        "abstract": "In this study, we present a computer package to implement fuzzy expert systems using Java. The purpose of this study is to provide an easy-to-understand and easy-to-implement computer package for those people who want to develop their own customized Java-based fuzzy expert applications without using existing end-user fuzzy expert software. The code size of the proposed package is less than 300 code lines where some existing Java classes developed by Sun Microsystems, Inc., are utilized to handle necessary geometrical manipulations. We demonstrate the ideas of how we implement fuzzy expert systems with the existing Java classes by providing code fragments. To validate the proposed package, we first developed a console application to compare the results of using the proposed package with those of using an existing commercial fuzzy expert software package, Fuzzy Inference Development Environment (FIDE). The proposed package is well-modularized in software modules with high maintainability and customizability. To validate this, we developed three additional applications with the proposed package including a window-based application and two web-based applications: JSP Model 1 and JSP Model 2 (MVC design pattern). Through the developments of these applications, we find that it is extremely simple and straightforward to build customized fuzzy expert applications with the code fragments provided in this study, saving a lot of programming works. In additions, the proposed package has good extensibilities to establish piecewise linear membership functions and to approximate non-linear membership functions. Meanwhile, UML is also utilized to describe the structures and behaviors of the proposed package. (C) 2011 Elsevier Ltd. All rights reserved.",
        "keywords": "Java; UML; Fuzzy expert system; Computer software; Object-oriented system",
        "released": 2012,
        "link": "https://doi.org/10.1016/j.eswa.2011.07.130"
    },
    {
        "title": "Re-envisioning the role of academic librarians for the digital learning environment: The case of UniSA online",
        "abstract": "Academic librarians cannot escape the implications of the knowledge economy and the pervasion of technology which effects everything that we do. Similarly, we must be prepared to teach our students how to cope in this knowledge society and how to develop the necessary information and digital literacy skills to be productive members of society in a digital environment. This article explores the first eighteen months of our experience as digital curriculum librarians in a large project at the University of South Australia (UniSA), UniSA Online. We have taken this opportunity to critically reflect on being embedded librarians within such a strategic and unique project. We examine the key cultural, pedagogical and technological challenges we have faced in delivering resources, support and services to the project team. The solutions we have adopted to overcome these challenges within an intensive course development environment are also outlined. The importance of building good relationships both within the project team, academics and with other library staff to deliver positive outcomes is discussed. We examine the pedagogical imperatives we have followed and the technological challenges we have faced to provide an active learning experience for our students in a digital learning environment. Our role as digital curriculum librarians is still evolving, however, we can observe some emerging trends within academic librarianship and comment on them, as we believe that the imperatives of the knowledge society will only become more prevalent into the future. We conclude by outlining which professional skills we need as academic librarians to evolve our roles and be successful in the digital world.",
        "keywords": "academic librarians; university libraries; embedded librarianship; information literacy; digital curriculum; online pedagogy; professional skills; digital learning environment",
        "released": 2019,
        "link": "https://www.webofscience.com/wos/woscc/full-record/WOS:000460760000011"
    },
    {
        "title": "Dynamically identifying and evaluating key barriers to promoting prefabricated buildings: Text mining approach",
        "abstract": "Prefabricated buildings (PBs) have been widely perceived as environmentally friendly buildings and advocated globally. In many countries, the promotion rate of PBs remains slow, hindered by various barriers. With the change of external development environment under different periods, there is also the temporal dynamic evolution in barriers. Many studies have adopted some traditional methods to explore these barriers, but only focused on the research-conducting periods and ignored the changes of obstacles in different periods, which is the so-called dynamic nature of barriers. This would lead to the noncorrespondence between the previously studied barriers and the new situation of different periods. In this regard, this study aims to explore barriers dynamically and efficiently. Taking the online discussions of Chinese construction practitioners (CPs) as the sample, text mining techniques were performed to identify the barriers at different stages in the period from 2016 to 2021. Then, with a proposed sentiment scoring model, CPs’ sentiment and attention level were combined to indirectly evaluate the impact degree of the corresponding barriers. In the end, the findings were verified in semistructured interviews. As a result, the changeable sentiments of CPs were tracked in the timeline, the evolution of key barriers at different stages and the internal driving force were revealed, and some new findings of barriers were identified. This paper not only makes a great theoretical contribution for PB studies, but also provides a research framework worthy of reference for researchers or government to regularly explore problems in the PB industry or other industries.",
        "keywords": "Prefabricated buildings (PB); Practitioners; Barriers; Dynamics; Text mining",
        "released": 2023,
        "link": "https://doi.org/10.1061/JCEMD4.COENG-13285"
    },
    {
        "title": "Design and research on industry user experience in internet financial network",
        "abstract": "With the development and progress of the Internet era, the mode and development environment of many industries are gradually changing and developing towards networking, the combination of online and offline has become the mainstream, but the development of the Internet is accompanied by hidden dangers of personal information security, especially for the financial industry, industry has become a very prominent user experience problem in the Internet finance industry. With the prosperity of Internet finance, many enterprises attach importance to the construction of Internet Finance websites, and improve the trust and experience of users through the design of websites. This paper analyzes the user trust experience and trust mechanism in Internet financial websites; Through the analysis of different user groups to compare the degree of trust in Internet Finance websites, compare the level of trust and coupling between traditional finance and Internet Finance in the hearts of users; Through the research, it is found that most white-collar social workers have high trust in Internet financial websites, and the elderly group has the lowest trust; In contrast to traditional finance, we can effectively improve the function-ality and value of financial products in the Internet Finance website, drive the emotional identity of users, and attract users visually from the website design, establishing and maintaining users’ trust in Internet finance industry websites can greatly enhance users’ trust in Internet finance; It expands the scope of financial services and simplifies service channels,it will have a positive impact on the development of Internet Finance and website design in the future.",
        "keywords": "Internet; Experience design; User industry; Finance network",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.ijleo.2022.170422"
    },
    {
        "title": "A visual interactive analytic tool for filtering and summarizing large health data sets coded with hierarchical terminologies (VIADS)",
        "abstract": "Vast volumes of data, coded through hierarchical terminologies (e.g., International Classification of Diseases, Tenth Revision-Clinical Modification [ICD10-CM], Medical Subject Headings [MeSH]), are generated routinely in electronic health record systems and medical literature databases. Although graphic representations can help to augment human understanding of such data sets, a graph with hundreds or thousands of nodes challenges human comprehension. To improve comprehension, new tools are needed to extract the overviews of such data sets. We aim to develop a visual interactive analytic tool for filtering and summarizing large health data sets coded with hierarchical terminologies (VIADS) as an online, and publicly accessible tool. The ultimate goals are to filter, summarize the health data sets, extract insights, compare and highlight the differences between various health data sets by using VIADS. The results generated from VIADS can be utilized as data-driven evidence to facilitate clinicians, clinical researchers, and health care administrators to make more informed clinical, research, and administrative decisions. We utilized the following tools and the development environments to develop VIADS: Django, Python, JavaScript, Vis.js, Graph.js, JQuery, Plotly, Chart.js, Unittest, R, and MySQL. VIADS was developed successfully and the beta version is accessible publicly. In this paper, we introduce the architecture design, development, and functionalities of VIADS. VIADS includes six modules: user account management module, data sets validation module, data analytic module, data visualization module, terminology module, dashboard. Currently, VIADS supports health data sets coded by ICD-9, ICD-10, and MeSH. We also present the visualization improvement provided by VIADS in regard to interactive features (e.g., zoom in and out, customization of graph layout, expanded information of nodes, 3D plots) and efficient screen space usage. VIADS meets the design objectives and can be used to filter, summarize, compare, highlight and visualize large health data sets that coded by hierarchical terminologies, such as ICD-9, ICD-10 and MeSH. Our further usability and utility studies will provide more details about how the end users are using VIADS to facilitate their clinical, research or health administrative decision making.",
        "keywords": "Data analytic tool; Data set filtering; Hierarchical terminology; Visualization; Human comprehension",
        "released": 2019,
        "link": "https://doi.org/10.1186/s12911-019-0750-y"
    },
    {
        "title": "The effects of chinese herbal medicines for treating diabetic foot ulcers: A systematic review of 49 randomized controlled trials",
        "abstract": "Objective: To assess the effects and associated risks of Chinese herbal medicine (CHM) for diabetic foot ulcer (DFU). Methods: We systematically searched seven electronic databases for randomized controlled trials (RCTs) about Chinese herbal medicines for treating diabetic foot ulcers. The methodological quality of RCTs was assessed by the Cochrane risk of bias tool. Data was synthesized using review manager (RevMan) 5.3. Meta-analysis was conducted if the data were available. A summary of finding table was generated by The GRADEpro Guideline Development Tool (GDT) online. Results: Forty-nine RCTs, all conducted in China, involving 3646 participants were included. Most of the included trials had unclear or high risk of bias. Twenty-six trials could be pooled in five Meta-analyses, the remaining trials could not be pooled due to the obvious clinical heterogeneity. Only low evidence showed CHM therapy may have 42%-60.4% participants healed completely after treatment, approximately twice (RR 1.42-1.76) as much as the healed rates in conventional therapy (or plus hot water foot bath) group. Majority of the included trials reported benefit of CHM group on shortening healing time (4-23 days) and reducing ulcer wound size (at least 2 cm(2)). No serious adverse events were reported related to the medication in all trials. Conclusion: Weak evidence showed benefit of CHM as add-on treatment of conventional therapy on increasing number of ulcer heals in patients with DFU. That’s about twice the healing rate of the conventional treatment (or plus hot water foot bath) group. With insufficient information, we could not draw confirmative conclusion on safety of CHM administration. These findings need to be tested in further large, rigorous trials.",
        "keywords": "Diabetic foot ulcer; Chinese herbal medicine; Randomized controlled trials; Systematic review; Meta-analyses",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.ctim.2019.03.007"
    },
    {
        "title": "Effectiveness of cognitive-based interventions for improving body image of patients having breast cancer: A systematic review and meta-analysis",
        "abstract": "Objective: Breast cancer patients often suffer from body image disturbance due to impairment of their body/ appearance resulting from surgeries, other cancer treatments, and/or their complications. Cognitive-based interventions (CBIs) have recently been adopted for patients having breast cancer but their effects on improving body image are uncertain. This systematic review aimed to examine the effects of CBIs on body image in these patients, identify the optimal dose, characteristics, and/or component(s) of an effective intervention for these patients to inform future research and practice.Methods: According to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework, ten online databases and five search engines were used to search for eligible studies. Quality appraisal of included studies and meta-analysis results were conducted using Rob 2 and Grading of Recommendations Assessment, Development and Evaluation profiler Guideline Development Tool, respectively. RevMan and comprehensive meta-analysis software were used to perform data analysis and synthesis.Results: Eleven eligible randomized controlled trials (RCTs) examining the effects of cognitive behavioral, acceptance and commitment, mindfulness, and self-compassion therapies were reviewed. Results of the metaanalysis showed that CBIs significantly reduced negative body image perception (Standardised Mean Difference, SMD = -0.49, 95% confidence interval [CI], [-0.87, -0.11], I2 = 81%, 6 RCTs, 758 participants), when compared to the control groups (mainly usual care) at immediately post-intervention; in which, CBT-based (SMD = -0.37, 95% CI (-0.60, -0.13), I2 = 0%) and group-based (SMD = -0.38, 95% CI (-0.62, -0.13), I2 = 0%) programs had more consistent and significant effects.Conclusions: In view of the highly heterogeneous and limited RCTs identified, high-quality controlled trials of CBIs for improving the body image of patients having breast cancer are suggested. Systematic review registration: PROSPERO, CRD42021259173.",
        "keywords": "Breast cancer; Body image; Cognitive -based interventions; Systematic review; Meta -analysis",
        "released": 2023,
        "link": "https://doi.org/10.1016/j.apjon.2023.100213"
    },
    {
        "title": "Experimental design of an adaptive LQG controller for battery charger/dischargers featuring low computational requirements",
        "abstract": "The growing use of DC/DC power converters has resulted in the requirement that their complex controllers be cheaper and smaller, thus using cost-effective implementations. For this purpose, it is necessary to decrease the computational burden in controller implementation to minimize the hardware requirements. This manuscript presents two methods for tuning an adaptive linear-quadratic-Gaussian voltage controller for a battery charger/discharger, implemented with a Sepic/Zeta converter, to work at any operating point. The first method is based on a lookup table to select, using the nearest method, both the state feedback vector and the observer gain vector, solving the Riccati’s differential equation offline for each practical operating point. The second method defines a polynomial function for each controller element that is based on the previous data corresponding to the system operating points. The adaptability of the two controllers to fixed voltage regulation and reference tracking was validated using simulations and experimental tests. The overshoot and settling time results were lower than 11% and 3.7 ms, which are in the same orders of magnitude of a control approach in which the equations are solved online. Likewise, three indices were evaluated: central processing unit capacity, cost, and performance. This evaluation confirms that the controller based on polynomial interpolation is the best option of the two examined methods due to the satisfactory balance between dynamic performance and cost. Despite the advantages of the controllers in being based on a lookup table and polynomial interpolation, the adaptive linear-quadratic-Gaussian has the benefit of not requiring an offline training campaign; however, the cost saving obtained with the lookup table controllers and polynomial interpolation controllers, due to the possible implementation on small-size microcontrollers with development tool simple and easy maintenance, will surely be desirable for a large number of deployed units, ensuring that those solutions are highly cost-effective.",
        "keywords": "adaptive controller; battery charger; discharger; Sepic; Zeta converter; DC bus regulation; cost-effective; comparison of performance",
        "released": 2023,
        "link": "https://doi.org/10.3390/wevj14060142"
    },
    {
        "title": "Getting more out of biomedical documents with GATE’s full lifecycle open source text analytics",
        "abstract": "This software article describes the GATE family of open source text analysis tools and processes. GATE is one of the most widely used systems of its type with yearly download rates of tens of thousands and many active users in both academic and industrial contexts. In this paper we report three examples of GATE-based systems operating in the life sciences and in medicine. First, in genome-wide association studies which have contributed to discovery of a head and neck cancer mutation association. Second, medical records analysis which has significantly increased the statistical power of treatment/outcome models in the UK’s largest psychiatric patient cohort. Third, richer constructs in drug-related searching. We also explore the ways in which the GATE family supports the various stages of the lifecycle present in our examples. We conclude that the deployment of text mining for document abstraction or rich search and navigation is best thought of as a process, and that with the right computational tools and data collection strategies this process can be made defined and repeatable. The GATE research programme is now 20 years old and has grown from its roots as a specialist development tool for text processing to become a rather comprehensive ecosystem, bringing together software developers, language engineers and research staff from diverse fields. GATE now has a strong claim to cover a uniquely wide range of the lifecycle of text analysis systems. It forms a focal point for the integration and reuse of advances that have been made by many people (the majority outside of the authors’ own group) who work in text processing for biomedicine and other areas. GATE is available online < 1 > under GNU open source licences and runs on all major operating systems. Support is available from an active user and developer community and also on a commercial basis.",
        "keywords": "",
        "released": 2013,
        "link": "https://doi.org/10.1371/journal.pcbi.1002854"
    },
    {
        "title": "Research capacity strengthening in low and middle income countries - an evaluation of the WHO/TDR career development fellowship programme",
        "abstract": "Between August 2012 and April 2013 the Career Development Fellowship programme of the Special Programme for Research and Training in Tropical Diseases (World Health Organization) underwent an external evaluation to assess its past performance and determine recommendations for future programme development and continuous performance improvement. The programme provides a year-long training experience for qualified researchers from low and middle income countries at pharmaceutical companies or product development partnerships. Independent evaluators from the Swiss Tropical and Public Health Institute and the Barcelona Institute for Global Health used a results-based methodology to review the programme. Data were gathered through document review, surveys, and interviews with a range of programme participants. The final evaluation report found the Career Development Fellowship to be relevant to organizers’ and programme objectives, efficient in its operations, and effective in its training scheme, which was found to address needs and gaps for both fellows and their home institutions. Evaluators found that the programme has the potential for impact and sustainability beyond the programme period, especially with the successful reintegration of fellows into their home institutions, through which newly-developed skills can be shared at the institutional level. Recommendations included the development of a scheme to support the re-integration of fellows into their home institutions post-fellowship and to seek partnerships to facilitate the scaling-up of the programme. The impact of the Professional Membership Scheme, an online professional development tool launched through the programme, beyond the scope of the Career Development Fellowship programme itself to other applications, has been identified as a positive unintended outcome. The results of this evaluation may be of interest for other efforts in the field of research capacity strengthening in LMICs or, generally, to other professional development schemes of a similar structure.",
        "keywords": "",
        "released": 2016,
        "link": "https://doi.org/10.1371/journal.pntd.0004631"
    },
    {
        "title": "Geographical trends for automatic milking systems research in non-pasture-based dairy farms: A scoping review",
        "abstract": "Automatic milking system (AMS) adoption in the United States is trending upward, with issues such as lower availability and increased cost of labor being fac-tors frequently listed as motives for AMS implementa-tion. In addition, more interest in precision dairy farm-ing by the new generation of farmers may also help increase AMS adoption. The objective of this scoping review was to characterize the nature of the literature investigating non-pasture-based AMS and the opportu-nities and challenges for future research. The eligibility criteria included studies published in or after the year 2000, with full text in English, of at least 500 words, examining various outcomes related to AMS in non-pasture-based dairy farms. Six electronic databases were searched: Biosis (Web of Science), CAB Abstracts (CAB Direct), Medline (PubMed), PubAg, AGRIS (FAO), and Scopus (Elsevier). The review focused on studies with objectives, characteristics, farms, and AMS information. A total of 4,292 titles and abstracts were screened, and 536 studies were finally included. Most of the studies were conducted in Europe (73.5%), among commercial herds (67.9%), comprising Holstein cows (57.7%), using Lely and DeLaval brands (45.4% vs. 39.7%), with free-flow traffic (52.7%). The main research topics investigated were milk production, milk composition, and AMS efficiency, followed by behavior and welfare, health disorders (especially mastitis), and nutrition in Europe and other regions. At the same time, in the United States, trends were similar, except for nutrition. Since 2016, there has been an increased interest in studies on energy and water consumption, technological development, environment (enteric emis-sions), reproduction, genetics, and longevity or culling. However, the small number of studies and unclear characterization of what is optimum for reproductive management, other health disorders, economics, and water and energy consumption suggest a need for fu-ture research.",
        "keywords": "dairy farming; voluntary milking; box robot; robotic milking",
        "released": 2023,
        "link": "https://doi.org/10.3168/jds.2023-23313"
    },
    {
        "title": "Developing a third-party analytics application using australia’s national personal health records system: Case study",
        "abstract": "Background: My Health Record (MyHR) is Australia’s national electronic health record (EHR) system. Poor usability and functionality have resulted in low utility, affecting enrollment and participation rates by both patients and clinicians alike. Similar to apps on mobile phone app stores, innovative third-party applications of MyHR platform data can enhance the usefulness of the platform, but there is a paucity of research into the processes involved in developing third-party applications that integrate and use data from EHR systems. Objective: The research describes the challenges involved in pioneering the development of a patient and clinician Web-based software application for MyHR and insights resulting from this experience. Methods: This research uses a case study approach, investigating the development and implementation of Actionable Intime Insights (AI(2)), a third-party application for MyHR, which translates Medicare claims records stored in MyHR into a clinically meaningful timeline visualization of health data for both patients and clinicians. This case study identifies the challenges encountered by the Personal Health Informatics team from Flinders University in the MyHR third-party application development environment. Results: The study presents a nuanced understanding of different data types and quality of data in MyHR and the complexities associated with developing secondary-use applications. Regulatory requirements associated with utilization of MyHR data, restrictions on visualizations of data, and processes of testing third-party applications were encountered during the development of the application. Conclusions: This study identified several processes, technical and regulatory barriers which, if addressed, can make MyHR a thriving ecosystem of health applications. It clearly identifies opportunities and considerations for the Australian Digital Health Agency and other national bodies wishing to encourage the development of new and innovative use cases for national EHRs.",
        "keywords": "computer software applications; electronic health record; software design; medication compliance",
        "released": 2018,
        "link": "https://doi.org/10.2196/medinform.7710"
    },
    {
        "title": "MDD <i>vs</i>. Traditional software development: A practitioner’s subjective perspective",
        "abstract": "Context: Today’s project managers have a myriad of methods to choose from for the development of software applications. However, they lack empirical data about the character of these methods in terms of usefulness, ease of use or compatibility, all of these being relevant variables to assess the developer’s intention to use them. Objective: To compare three methods, each following a different paradigm (Model-Driven, Model-Based and Code-Centric) with respect to their adoption potential by junior software developers engaged in the development of the business layer of a Web 2.0 application. Method: We have conducted a quasi-experiment with 26 graduate students of the University of Alicante. The application developed was a Social Network, which was organized around a fixed set of modules. Three of them, similar in complexity, were used for the experiment. Subjects were asked to use a different method for each module, and then to answer a questionnaire that gathered their perceptions during such use. Results: The results show that the Model-Driven method is regarded as the most useful, although it is also considered the least compatible with previous developers’ experiences. They also show that junior software developers feel comfortable with the use of models, and that they are likely to use them if the models are accompanied by a Model-Driven development environment. Conclusions: Despite their relatively low level of compatibility, Model-Driven development methods seem to show a great potential for adoption. That said, however, further experimentation is needed to make it possible to generalize the results to a different population, different methods, other languages and tools, different domains or different application sizes. (C) 2012 Elsevier B.V. All rights reserved.",
        "keywords": "MDD; MBD; Code-centric development; Experiment; Usefulness; Ease of use",
        "released": 2013,
        "link": "https://doi.org/10.1016/j.infsof.2012.07.004"
    },
    {
        "title": "Low-code development using requirements and knowledge representation models",
        "abstract": "Low-Code Development is a new software development paradigm which is typically used to generate web applications from high-level visual notations. These notations allow to express the User Interface and Application Logic (usersystem interactions) in a way which is understandable by the end-users. Using certain model-driven approaches, the low-code environments allow for generating the front-end layer and basic CRUD operations in the back-end. Yet still, non-standard domain logic (data processing) operations still necessitate the use of traditional programming. In this article we present a visual language, called RSL-DL that can be used to represent such non-standard domain logic in a visual form. It allows to capture domain knowledge with complex domain rules aligned with requirements models. The language synthesises and extends approaches found in knowledge representation (ontologies) and software modelling language engineering. The development environment of RSL-DL enables fully automatic generation of domain logic code by reasoning over and reusing domain knowledge. The environment includes a dedicated model editor and a transformation engine. The language’s abstract syntax is defined using a meta-model expressed in MOF. Its semantics is expressed with several translational rules that map RSL-DL models onto typical programming language constructs. The article presents a list of these rules in an informal way, and then introduces their formalisation using a graphical transformation notation. The RSL-DL environment includes an inference engine that enables processing queries to domain models and selecting appropriate invocations to generated code. It was also initially validated through studies that involve understandability, operability and complexity assessment. Based on these results, we conclude that declarative knowledge representations can be successfully used to produce imperative backend code with non-trivial logic.",
        "keywords": "low-code; model-driven web engineering; knowledge representation; ontologies; code generation",
        "released": 2024,
        "link": "https://doi.org/10.2298/CSIS230102024R"
    },
    {
        "title": "ASDB: A comprehensive omics database for anopheles sinensis",
        "abstract": "Anopheles sinensis is a key disease vector for human malaria and parasitic diseases such as malayan filariasis, and it is considered to be one of the most important malaria vectors in China and Southeast Asia. As high-throughput sequencing and assembly technology are widely used in An. sinensis, a lot of omics data have been generated, and abundant genome, mRNA transcriptome, miRNA transcriptome and resequencing results have been accumulated. In addition, lots of valuable morphological images and publications have been produced with the in-depth studies on An. sinensis. However, the increased quantity, variety, and structure complexity of the omics data create inconveniences for researchers to use and manage this information. We have built an An. sinensis omics database (ASDB, http://asdb.jungleran.com/) - a comprehensive and integrated database to promote scientific research on An. sinensis. Docker was used to deploy a development environment and Drupal to build ASDB. ASDB provides a Blast tool to do sequence alignment of genome sequence, gene sequence and protein sequence of An. sinensis. It also offers JBrowse (a next-generation genome visualization and analysis web platform) to facilitate researchers visualize the gene structure, non-coding RNA (include miRNA, snRNA, tRNA and so on) structure and genomic variation sites as desired. ASDB has integrated various latest omics data of An. Sinensis, including de novo genome and its annotation data, genome variation data (such as SNP and InDel), transcriptome and its expression value, miRNA expression value and miRNA-mRNA interaction, metagenomes. The database has also included the morphological images of different developmental stages and tissues, and important literatures associated with An. sinensis. ASDB provides a user-friendly search and displays pages. The integration of these resources will contribute to the study of basic biology and functional genome of An. sinensis.",
        "keywords": "Mosquito; Genome; Database; Drupal",
        "released": 2021,
        "link": "https://doi.org/10.1016/j.ygeno.2021.02.005"
    },
    {
        "title": "Implementation of evidence-based humanitarian programs in military-led missions: Part i. Qualitative gap analysis of current military and international aid programs",
        "abstract": "Background: A recent Department of Defense instruction mandates country-specific assessments, identification of interventions, and development of guidance for Department of Defense to plan, train, and prepare for the provision of humanitarian assistance in stability operations. It also directs the use of outcome-based measures of effectiveness and the establishment of processes facilitating transparency of information. Whereas this would align military-led projects closer to the standards of the international aid community, how this process will be developed and implemented within the military has not yet been determined. Methods: To begin developing an evidence-based program for military-led humanitarian aid, we conducted a qualitative gap analysis comparing information from a Web search of Department of Defense medical after-action reports, lessons learned, and expert interviews with the internationally accepted standards in humanitarian assistance impact assessment. Results: There is a major gap in the ability of the Department of Defense to assess the impact of humanitarian assistance in stability operations compared with international development standards. Of the 1000 Department of Defense after-action reports and lessons learned reviewed, only 7 (0.7%) reports refer to, but do not discuss, impact assessment or outcome-based measures of effectiveness. Conclusions: This investigation shows that the Department of Defense humanitarian assistance operations are, historically, recorded without documentation using quantifiable health data identifying which aid activities contributed directly to desired outcomes or favorable public opinion, and rarely are analyzed for effectiveness. As humanitarian assistance operations assume an ever greater role in US military strategy, it is imperative that we investigate useful impact assessment models to meet mission directives and, more important, to maximize coordination in a necessarily integrated and cooperative development environment. These findings provide baseline knowledge for the implementation of an evidence-based impact assessment process to validate future Department of Defense humanitarian assistance operations. (Disaster Med Public Health Preparedness. 2008; 2: 230-236)",
        "keywords": "humanitarian assistance; civil-military coordination; military medical missions; stability operations; disaster evaluation and monitoring; measures of effectiveness",
        "released": 2008,
        "link": "https://doi.org/10.1097/DMP.0b013e31818d3c80"
    },
    {
        "title": "A new efficient assembly language teaching aid for intel processors",
        "abstract": "Assembly 8086 language is an essential course that is taught in many universities offering computer engineering or science degree. However, the available software simulators that are used to write and run assembly programs do not fulfill the students’ expectations due to being difficult to deal with and lacking a lot of necessary GUI features. In this article, we introduce a new Assembly Language Teaching Aid (ALTA32 or ALTA8086) which is basically a simulator that helps much in delivering concepts related to assembly language programming course for Intel microprocessors (mu ps). Among current and relevant simulators, ALTA32 not only is the simplest and easiest to use, but also has the most fascinating user-friendly interface that exhibits constructive and informative features. Additionally, it offers multiple run modes with the option of writing programs directly or through loading existing files. It works efficiently in both 16-bit and 32-bit extensions considering real mode memory addressing. It supports all data and stack memory addressing modes along with all available directives to be used in both model and full-segment program approaches. Furthermore, to understand and recognize changes after execution, all affected fields are emphasized and shown using visual effects such as dynamic labels, blinking texts, as well as colored codes. On the other hand, it is widely known that the Complex Instruction Set Computing (CISC) architecture, which is also a register-memory architecture, is characterized by affording many instructions with various operands, affected flags, complex operations, and sources of syntax errors. Although it is really challenging to build up a simulator that completely supports that wide range of Intel assembly instructions taking into account the variety in their lengths and formats, we succeeded eventually to eliminate the code redundancy and make our code readable, maintainable, and scalable through programming all instructions based on the methodology of combining or grouping all instructions that almost act similarly to follow a certain efficient algorithm. As a programming environment, VB.Net has been used to develop the ALTA32 simulator. Throughout our experience in teaching assembly 8086 language programming course for CISC processors, we found many difficulties in getting the students familiar with well-known simulators such as Microsoft Macro Assembler (MASM8086 or MASM32) or 8086 mu p Emulator (EMU8086). Nevertheless, the feedback from our students about ALTA32 simulator was so impressive and the complaints were totally disappeared. (c) 2014 Wiley Periodicals, Inc. Comput Appl Eng Educ 23:217-238, 2015; View this article online at ; DOI",
        "keywords": "Intel 8086; assembly 8086; teaching aid; simulator",
        "released": 2015,
        "link": "https://doi.org/10.1002/cae.21591"
    },
    {
        "title": "Telomerase-related advances in hepatocellular carcinoma: A bibliometric and visual analysis",
        "abstract": "BACKGROUND As a critical early event in hepatocellular carcinogenesis, telomerase activation might be a promising and critical biomarker for hepatocellular carcinoma (HCC) patients, and its function in the genesis and treatment of HCC has gained much attention over the past two decades. AIM To perform a bibliometric analysis to systematically assess the current state of research on HCC-related telomerase. METHODS The Web of Science Core Collection and PubMed were systematically searched to retrieve publications pertaining to HCC/telomerase limited to “articles” and “reviews” published in English. A total of 873 relevant publications related to HCC and telomerase were identified. We employed the Bibliometrix package in R to extract and analyze the fundamental information of the publications, such as the trends in the publications, citation counts, most prolific or influential writers, and most popular journals; to screen for keywords occurring at high frequency; and to draw collaboration and cluster analysis charts on the basis of coauthorship and co-occurrences. VOSviewer was utilized to compile and visualize the bibliometric data. RESULTS A surge of 51 publications on HCC/telomerase research occurred in 2016, the most productive year from 1996 to 2023, accompanied by the peak citation count recorded in 2016. Up to December 2023, 35226 citations were made to all publications, an average of 46.6 citations to each paper. The United States received the most citations (n = 13531), followed by China (n = 7427) and Japan (n = 5754). In terms of national cooperation, China presented the highest centrality, its strongest bonds being to the United States and Japan. Among the 20 academic institutions with the most publications, ten came from China and the rest of Asia, though the University of Paris Cite, Public Assistance-Hospitals of Paris, and the National Institute of Health and Medical Research (INSERM) were the most prolific. As for individual contributions, Hisatomi H, Kaneko S, and Ide T were the three most prolific authors. Kaneko S ranked first by H-index, G-index, and overall publication count, while Zucman-Rossi J ranked first in citation count. The five most popular journals were the World Journal of Gastroenterology, Hepatology, Journal of Hepatology, Oncotarget, and Oncogene, while Nature Genetics, Hepatology, and Nature Reviews Disease Primers had the most citations. We extracted 2293 keywords from the publications, 120 of which appeared more than ten times. The most frequent were HCC, telomerase and human telomerase reverse transcriptase (hTERT). Keywords such as mutational landscape, TERT promoter mutations, landscape, risk, and prognosis were among the most common issues in this field in the last three years and may be topics for research in the coming years. CONCLUSION Our bibliometric analysis provides a comprehensive overview of HCC/telomerase research and insights into promising upcoming research.",
        "keywords": "Telomerase; Bibliometric analysis; Telomerase reverse transcriptase; Prognosis; Treatment; Hepatocellular carcinoma",
        "released": 2024,
        "link": "https://doi.org/10.3748/wjg.v30.i9.1224"
    },
    {
        "title": "Prediction and evaluation of healthy and unhealthy status of COVID-19 patients using wearable device prototype data",
        "abstract": "COVID-19 pandemic seriousness is making the whole world suffer due to inefficient medication and vaccines. The article prediction analysis is carried out with the dataset downloaded from the Application peripheral interface (API) designed explicitly for COVID-19 quarantined patients. The measured data is collected from a wearable device used for quarantined healthy and unhealthy patients. The wearable device provides data of temperature, heart rate, SPO2, blood saturation, and blood pressure timely for alerting the medical authorities and providing a better diagnosis and treatment. The dataset contains 1085 patients with eight features representing 490 COVID19 infected and 595 standard cases. The work considers different parameters, namely heart rate, temperature, SpO(2), bpm parameters, and health status.& nbsp;Furthermore, the real-time data collected can predict the health status of patients as infected and non infected from measured parameters. The collected dataset uses a random forest classifier with linear and polynomial regression to train and validate COVID-19 patient data. The google colab is an Integral development environment inbuilt with python and Jupyter notebook with scikit-learn version 0.22.1 virtually tested on cloud coding tools. The dataset is trained and tested in 80% and 20% ratio for accuracy evaluation and avoid overfitting in the model. This analysis could help medical authorities and governmental agencies of every country respond timely and reduce the contamination of the disease.& nbsp;The measured data provide a comprehensive mapping of disease symptoms to predict the health status. They can restrict the virus transmission and take necessary steps to control, mitigate and manage the disease.& nbsp;& nbsp;Benefits in scientific research with Artificial Intelligence (AI) to tackle the hurdles in analyzing disease diagnosis.& nbsp;& nbsp;The diagnosis results of disease symptoms can identify the severity of the patient to monitor and manage the difficulties for the outbreak caused. (C)& nbsp;2022 The Author(s). Published by Elsevier B.V.",
        "keywords": "Quarantine; <p>Warable electronic device</p>; Pandemic; Healthcare; AI model; Dataset",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.mex.2022.101618"
    },
    {
        "title": "EQUIDopa: A responsive web application for the levodopa equivalent dose calculator",
        "abstract": "Background and objective: Levodopa/carbidopa intestinal gel infusion is a treatment option for patients of advanced stage of Parkinson’s disease. This treatment requires the time-consuming and error-prone conversion of orally taken antiparkinson drugs into an equivalent replacing dose of levodopa/carbidopa, which is delivered via pump. In order to facilitate and speed up this conversion process, we developed a specific user-friendly application that would be available online or as a standalone mobile application. Such an application has now been written and designed in collaboration with a computer scientist and physician, who is also intended to be the final user. Methods: The levodopa equivalent dose calculations are based on previous studies and data reported in the literature. Two related on-line conversion applications were analyzed and evaluated. The primary objectives of the application were determined, and basic functionalities were outlined. The application was implemented using modern development tools and frameworks. Results: The application has proven to be effective and easy to use in our clinical setting. It can serve as a control for manual calculations made by medical staff. It can also be applied as a useful tool in levodopa/carbidopa intestinal gel infusion courses for the advanced treatment of Parkinson’s disease. Conclusion: The presented application could replace the tedious and error-prone levodopa equivalent dose conversion of antiparkinson drugs efficiently. In comparison to related on-line conversion applications, this specific application provides more functionality, reduces the risk of user errors, and can be run on a variety of devices. The conversion value provided by the application is only an estimate. The effect of the drugs is specific for each individual patient, which could not be considered in the calculations. Therefore, the responsibility of using the results of the application rests with the clinical judgment of the user. Nevertheless, the application can give us a good starting point for the initial pump setting, which would be adjusted further during the titration period, according to the patient’s response. (C) 2020 The Author(s). Published by Elsevier B.V.",
        "keywords": "Angular; Duodopa (R); Levodopa; Levodopa equivalent dose calculator; Parkinson’s disease; Responsive web application",
        "released": 2020,
        "link": "https://doi.org/10.1016/j.cmpb.2020.105633"
    },
    {
        "title": "The developmental needs and aspirations of teenagers, premises for the implementation of programs based on experiential learning",
        "abstract": "This article starts from a personal vision according to which any adolescent could develop personally and socially, anywhere and anytime, if he had an environment with offering learning experiences, he would acquire development tools, which he could use throughout life and would benefit from the support of the main educational partners (family, school, community) in his training. Starting from this vision, between December 2018 and June 2019, we initiated and carried out an exploratory study, within the Department of Educational Sciences, at the Faculty of Psychology and Educational Sciences of the “Babes-Bolyai” University, Cluj-Napoca under coordination of the University Professor Vasile Chis, who aimed to identify the educational needs of adolescents in Romania, for the design and implementation of educational programs, for their personal and social development. In the study, we asked teenagers to fill an online questionnaire, in which they were encouraged to answer questions openly and honestly, the answers being anonymous and the lack of answer variants being considered to be correct or wrong. Over 750 respondents provided answers to the questions “What would you like to learn if you could choose” and “What would you most like to do in life, if you had the time, energy, money, self-confidence and support of your family?”. The results of the study open the discussion on the need to approach education from a multilateral and personalized perspective, depending on the individual needs of the beneficiaries. In the context in which education for well-being begins in the first years of family life, continues in school, can be extended to nongovernmental organizations and is supported by the community and businesses throughout life, this work reflects the current needs of education, anchored to the future aspirations of adolescents in Romania. Based on the results of this study can be outlined a portrait of the Romanian adolescent, ideas for personalizing education and solutions for educational agents to design and implement programs and projects adapted to the educational needs of today’s adolescents, tomorrow’s adults.",
        "keywords": "adolescents’ needs and aspirations; educational agents; personalization of education; promotion of well-being; experiential learning",
        "released": 2021,
        "link": "https://doi.org/10.18662/rrem/13.1Sup1/389"
    },
    {
        "title": "Organizational career development versus employees’ career needs in hungary",
        "abstract": "The aim of the study is to prove that individuals still need the help of their employer to develop their career. Therefore, the paper investigates what kind of career support the employees expect from their employers and compare them to what they actually receive from the organization. The focus is on the career development tools applied by the organizations. Using quantitative research methods via an online survey of 1000 Hungarian employees, this study explores the extent of career development technic usage. It explores the solutions which are preferred by the individuals and also demonstrates the tools used by the organizations. Furthermore, the expected and perceived career support from different actors (managers, HR representatives, mentors, colleagues or external consultant) of the career process is measured and the perceived career opportunities are also evaluated. The most frequently applied tools are special work tasks, project work, networking opportunities, training opportunities and performance appraisal as a basis for career planning, as well as mentoring and coaching. The most often used career management solutions are applied mostly by the most employees; therefore, we can conclude that these are the most popular career interventions. All career management solutions were rated as useful by the respondents on average. However, the efficiency order set by respondents is different from the frequency of use. Career management tools that are evaluated most effective by employees are the following: training opportunities, projects to stimulate learning, networking opportunities, special tasks and promotion, succession planning. On average, respondents were partially informed and career prospects partly meet their expectations. Majority of the workers would expect help in developing their careers, but only few of them actually get support. In conclusion the employers should consider to apply certain career management tools more often as they do or they should involve in it more workers. For more effective cooperation, it would be worth to inform better their employees about career opportunities and give them more career support because they would require it.",
        "keywords": "career; career development; training and development; promotion",
        "released": 2019,
        "link": "https://doi.org/10.5937/StraMan1904003S"
    },
    {
        "title": "Omni-script: Device independent user interface development for omni-channel fintech applications",
        "abstract": "As the number of devices and platforms gradually increased that serve as a medium for online banking services, the number of replicated services for different platforms also increase. Each service has a different representation in a different platform even though targeting the same business objectives. This situation results in the problem of re-developing similar set of requirements for different devices and services which in turn results in multiplied effort in software development and degradation in the software quality because of the replicated code. The objective of this paper is to present a device independent user interface development approach that eliminates the replicated user interface development effort of the same set of services for different platforms/devices while increasing the presentation performance. In this context, we provide a basic technique that uses a json based user interface definition format, called omni-script, to separate the representation of banking services in different platforms/devices, so called channels. Omni-script is a pioneering solution being used in a real development environment in banking software domain that is experiencing a shift towards omni-channel user experience, where users are able to seamlessly continue using banking services in different platforms and devices. As the evaluation method, we have measured the actual development effort on a representative sample development team and a set of services. Additionally, we later deployed and measured the rendering time and the amount of data communication for the proposed approach in a real production environment. Production deployment resulted in a remarkable decrease in data communication and in loading times at the client side. As importantly, the proposed approach also resulted in a nearly 75% decrease in service development time by enabling analysts to produce user interface prototypes that can be integrated to service development with minimal effort.",
        "keywords": "Multimodal user interface definition; Omni-channel user experience; Omni-channel banking services",
        "released": 2019,
        "link": "https://doi.org/10.1016/j.csi.2019.01.003"
    },
    {
        "title": "AdaptiveVLE: An integrated framework for personalized online education using MPS JetBrains domain-specific modeling environment",
        "abstract": "This article contains the design and development of an Adaptive Virtual Learning Environment (AdaptiveVLE) framework to assist educators of all disciplines with creating adaptive VLEs tailored to their needs and to contribute towards the creation of a more generic framework for adaptive systems. Fully online education is a major trend in education technology of our times. However, it has been criticised for its lack of personalisation and therefore not adequately addressing individual students ; needs. Adaptivity and intelligence are elements that could substantially improve the student experience and enhance the learning taking place. There are several attempts in academia and in industry to provide adaptive VLEs and therefore personalise educational provision. All these attempts require a multiple-domain (multi-disciplinary) approach from education professionals, software developers, data scientists to cover all aspects of the system. An integrated environment that can be used by all the multiple-domain users mentioned above and will allow for quick experimentation of different approaches is currently missing. Specifically, a transparent approach that will enable the educator to configure the data collected and the way it is processed without any knowledge of software development and/or data science algorithms implementation details is required. In our proposed work, we developed a new language/framework using MPS JetBrains Domain-Specific Language (DSL) development environment to address this problem. Our work consists of the following stages: data collection configuration by the educator, implementation of the adaptive VLE, data processing, adaptation of the learning path. These stages correspond to the adaptivity stages of all adaptive systems such as monitoring, processing and adaptation. The extension of our framework to include other application areas such as business analytics, health analytics, etc. so that it becomes a generic framework for adaptive systems as well as more usability testing for all applications will be part of our future work.",
        "keywords": "DSL; Adaptive systems; Education; Adaptation models; Data collection; Classification algorithms; Adaptive VLE; DSL; classification algorithms; learning analytics; eLearning; adaptive systems",
        "released": 2020,
        "link": "https://doi.org/10.1109/ACCESS.2020.3029888"
    },
    {
        "title": "A toolkit for bulk PCR-based marker design from next-generation sequence data: Application for development of a framework linkage map in bulb onion (<i>allium cepa</i> l.)",
        "abstract": "Background: Although modern sequencing technologies permit the ready detection of numerous DNA sequence variants in any organisms, converting such information to PCR-based genetic markers is hampered by a lack of simple, scalable tools. Onion is an example of an under-researched crop with a complex, heterozygous genome where genome-based research has previously been hindered by limited sequence resources and genetic markers. Results: We report the development of generic tools for large-scale web-based PCR-based marker design in the Galaxy bioinformatics framework, and their application for development of next-generation genetics resources in a wide cross of bulb onion (Allium cepa L.). Transcriptome sequence resources were developed for the homozygous doubled-haploid bulb onion line “CUDH2150” and the genetically distant Indian landrace “Nasik Red”, using 454 (TM) sequencing of normalised cDNA libraries of leaf and shoot. Read mapping of “Nasik Red” reads onto “CUDH2150” assemblies revealed 16836 indel and SNP polymorphisms that were mined for portable PCR-based marker development. Tools for detection of restriction polymorphisms and primer set design were developed in BioPython and adapted for use in the Galaxy workflow environment, enabling large-scale and targeted assay design. Using PCR-based markers designed with these tools, a framework genetic linkage map of over 800cM spanning all chromosomes was developed in a subset of 93 F-2 progeny from a very large F-2 family developed from the “Nasik Red” x “CUDH2150” inter-cross. The utility of tools and genetic resources developed was tested by designing markers to transcription factor-like polymorphic sequences. Bin mapping these markers using a subset of 10 progeny confirmed the ability to place markers within 10 cM bins, enabling increased efficiency in marker assignment and targeted map refinement. The major genetic loci conditioning red bulb colour (R) and fructan content (Frc) were located on this map by QTL analysis. Conclusions: The generic tools developed for the Galaxy environment enable rapid development of sets of PCR assays targeting sequence variants identified from Illumina and 454 sequence data. They enable non-specialist users to validate and exploit large volumes of next-generation sequence data using basic equipment.",
        "keywords": "Marker; Onion; Genetic mapping; Next generation sequencing; SNP",
        "released": 2012,
        "link": "https://doi.org/10.1186/1471-2164-13-637"
    },
    {
        "title": "Demystifying the soft and hardened memory systems of modern FPGAs for software programmers through microbenchmarking",
        "abstract": "Both modern datacenter and embedded Field Programmable Gate Arrays (FPGAs) provide great opportunities for high-performance and high-energy-efficiency computing. With the growing public availability of FPGAs from major cloud service providers such as AWS, Alibaba, and Nimbix, as well as uniform hardware accelerator development tools (such as Xilinx Vitis and Intel oneAPI) for software programmers, hardware and software developers can now easily access FPGA platforms. However, it is nontrivial to develop efficient FPGA accelerators, especially for software programmers who use high-level synthesis (HLS). The major goal of this article is to figure out how to efficiently access the memory system of modern datacenter and embedded FPGAs in HLS-based accelerator designs. This is especially important for memory-bound applications; for example, a naive accelerator design only utilizes less than 5% of the available off-chip memory bandwidth. To achieve our goal, we first identify a comprehensive set of factors that affect the memory bandwidth, including (1) the clock frequency of the accelerator design, (2) the number of concurrent memory access ports, (3) the data width of each port, (4) the maximum burst access length for each port, and (5) the size of consecutive data accesses. Then, we carefully design a set of HLS-based microbenchmarks to quantitatively evaluate the performance of the memory systems of datacenter FPGAs (Xilinx Alveo U200 and U280) and embedded FPGA (Xilinx ZCU104) when changing those affecting factors, and we provide insights into efficient memory access in HLS-based accelerator designs. Comparing between the typically used soft and hardened memory systems, respectively, found on datacenter and embedded FPGAs, we further summarize their unique features and discuss the effective approaches to leverage these systems. To demonstrate the usefulness of our insights, we also conduct two case studies to accelerate the widely used K-nearest neighbors (KNN) and sparse matrix-vector multiplication (SpMV) algorithms on datacenter FPGAs with a soft (and thus more flexible) memory system. Compared to the baseline designs, optimized designs leveraging our insights achieve about 3.5x and 8.5x speedups for the KNN and SpMV accelerators. Our final optimized KNN and SpMV designs on a Xilinx Alveo U200 FPGA fully utilize its off-chip memory bandwidth, and achieve about 5.6x and 3.4x speedups over the 24-core CPU implementations.",
        "keywords": "Datacenter FPGAs; embedded FPGAs; memory system; HLS; benchmarking",
        "released": 2022,
        "link": "https://doi.org/10.1145/3517131"
    },
    {
        "title": "A data acquisition and control system for high-speed gamma-ray tomography",
        "abstract": "A data acquisition and control system (DACS) for high-speed gamma-ray tomography based on the USB (Universal Serial Bus) and Ethernet communication protocols has been designed and implemented. The high-speed gamma-ray tomograph comprises five 500 mCi Am-241 gamma-ray sources, each at a principal energy of 59.5 keV, which corresponds to five detector modules, each consisting of 17 CdZnTe detectors. The DACS design is based on Microchip’s PIC18F4550 and PIC18F4620 microcontrollers, which facilitates an USB 2.0 interface protocol and an Ethernet (IEEE 802.3) interface protocol, respectively. By implementing the USB- and Ethernet-based DACS, a sufficiently high data acquisition rate is obtained and no dedicated hardware installation is required for the data acquisition computer, assuming that it is already equipped with a standard USB and/or Ethernet port. The API (Application Programming Interface) for the DACS is founded on the National Instrument’s LabVIEW (R) graphical development tool, which provides a simple and robust foundation for further application software developments for the tomograph. The data acquisition interval, i.e. the integration time, of the high-speed gamma-ray tomograph is user selectable and is a function of the statistical measurement accuracy required for the specific application. The bandwidth of the DACS is 85 kBytes s(-1) for the USB communication protocol and 28 kBytes s(-1) for the Ethernet protocol. When using the iterative least square technique reconstruction algorithm with a 1 ms integration time, the USB-based DACS provides an online image update rate of 38 Hz, i.e. 38 frames per second, whereas 31 Hz for the Ethernet-based DACS. The off-line image update rate (storage to disk) for the USB-based DACS is 278 Hz using a 1 ms integration time. Initial characterization of the high-speed gamma-ray tomograph using the DACS on polypropylene phantoms is presented in the paper.",
        "keywords": "high speed gamma-ray tomograph; data acquisition and control; USB interface protocol; Ethernet interface protocol; LabVIEW API",
        "released": 2008,
        "link": "https://doi.org/10.1088/0957-0233/19/9/094012"
    },
    {
        "title": "Mycophenolate mofetil in the treatment of steroid-dependent or frequently relapsing nephrotic syndrome in children: A meta-analysis",
        "abstract": "Objectives: This meta-analysis aims to evaluate the efficacy and safety of the mycophenolate mofetil (MMF) in the treatment of steroid-dependent nephrotic syndrome (SDNS) or frequently relapsing nephrotic syndrome (FRNS) in children. Methods: We searched for the studies especially the randomized controlled trials in PubMed, Cochrane Library, Embase, China National Knowledge Infrastructure, and Wan Fang database. The data were analyzed by Review Manager 5.3 software. We used the GRADE pro-Guideline Development Tool online software to evaluate the quality of evidence. Results: Finally, we identified 620 studies, of which we included five randomized controlled trials and one prospective cohort study with 447 children. The results showed the following: (1) the relapse-free survival rate within 1 year-the MMF group was superior to the levamisole group [ratio difference (RD) = 0.13, 95% CI (0.02, 0.24), P = 0.02] but not to the calcineurin inhibitors (CNIs) group [RD = -0.27, 95%CI (-0.40, -0.14), P < 0.0001]; (2) the number of relapses within 1 year-the MMF group was less than that in the CNIs and levamisole group [mean difference (MD) = -0.26, 95%CI (-0.45, -0.08), P = 0.005]; (3) the cumulative prednisone dosage-the MMF group was lower than that in the control group [standardized mean difference (SMD) = -0.32, 95%CI (-0.53, -0.11), P = 0.003]; (4) incidence of adverse reactions-there was no significant difference between the MMF group and the control group [RD = 0.02, 95%CI (-0.04, 0.09), P = 0.46]. Conclusion: The therapy of mycophenolate mofetil in the treatment of SDNS or FRNS in children has a certain advantage in reducing the number of relapses and cumulative prednisone dosage within 1 year when compared with the CNIs and levamisole. However, due to the limited quantity and quality of the included studies, the conclusions above need to be confirmed by more high-quality randomized controlled trials.",
        "keywords": "mycophenolate mofetil; frequently relapsing nephrotic syndrome; steroid-dependent nephrotic syndrome; children; meta-analysis",
        "released": 2021,
        "link": "https://doi.org/10.3389/fped.2021.671434"
    },
    {
        "title": "Automated end user-centred adaptation of web components through automated description logic-based reasoning",
        "abstract": "Context: This paper addresses one of the major end-user development (EUD) challenges, namely, how to pack today’s EUD support tools with composable elements. This would give end users better access to more components which they can use to build a solution tailored to their own needs. The success of later end-user software engineering (EUSE) activities largely depends on how many components each tool has and how adaptable components are to multiple problem domains. Objective: A system for automatically adapting heterogeneous components to a common development environment would offer a sizeable saving of time and resources within the EUD support tool construction process. This paper presents an automated adaptation system for transforming EUD components to a standard format. Method: This system is based on the use of description logic. Based on a generic UML2 data model, this description logic is able to check whether an end-user component can be transformed to this modelling language through subsumption or as an instance of the UML2 model. Besides it automatically finds a consistent, non-ambiguous and finite set of XSLT mappings to automatically prepare data in order to leverage the component as part of a tool that conforms to the target UML2 component model. Results: The proposed system has been successfully applied to components from four prominent EUD tools. These components were automatically converted to a standard format. In order to validate the proposed system, rich intemet applications (RIA) used as an operational support system for operators at a large services company were developed using automatically adapted standard format components. These RIAs would be impossible to develop using each EUD tool separately. Conclusion: The positive results of applying our system for automatically adapting components from current tool catalogues are indicative of the system’s effectiveness. Use of this system could foster the growth of web EUD component catalogues, leveraging a vast ecosystem of user-centred SaaS to further current EUSE trends. (C) 2014 Elsevier B.V. All rights reserved.",
        "keywords": "D.2 [Software engineering]; D.3 [Programming languages]; H.5 [Information interfaces and presentation]; K.4 [Computers and society]; J.4 [Social and behavioural sciences]",
        "released": 2015,
        "link": "https://doi.org/10.1016/j.infsof.2014.05.021"
    },
    {
        "title": "Extensible tooling for reactive programming based on",
        "abstract": "Reactive programming uses dedicated language concepts such as signals, data bindings, and constraints, so developers can better express behavior that is triggered by data changes and user interactions. As applications also contain aspects that cannot be easily expressed through reactive programming, reactive concepts are often integrated into more generally applicable imperative programming languages. Although such language integrations are readily available, working on reactive code with tools designed for imperative code is hard, because without dedicated tool support implementation details may leak unintentionally. There are special tools for reactive programming available, however, they are expensive to make. Further, a tool typically supports only a single language concept and cannot be applied to others even though they build on similar ideas. Consequently, control flow or data flow cannot be followed between concepts. We propose to leverage the commonalities found in reactive programming concepts to create reusable tool components for data gathering and visualization. To do so we create a toolset working on a generalization of reactive programming concepts, Active Expressions. By building upon the generic tool components, tool developers can create tool support for specific reactive concepts. Furthermore, multiple reactive concepts and their potentially complex interaction can be explored in one shared environment. We implemented the approach in the Lively4 Web-based JavaScript development environment using its Active Expression framework. Our toolset gathers relevant data about the reactive system and visualizes it using code annotations, an overview tree, an event timeline, and a dependency graph. We evaluate the reusability of this toolset by adapting it to two more concepts: signals, and implicit layer activation, known from context-oriented programming. We found that most of the functionality provided by the toolset can be reused, thus, reducing the implementation effort. Further, we show that multiple reactive concepts can be supported by the same common toolset. Programmers can use and debug multiple different reactive concepts simultaneously, without requiring new tools for each one. For future work, we believe our common toolset provides a starting point for researching the interplay between multiple reactive programming concepts.",
        "keywords": "Programming Tools; Development Environments; Reactive Programming; Active Expressions; Lively Kernel",
        "released": 2024,
        "link": "https://doi.org/10.5381/jot.2024.23.1.a4"
    },
    {
        "title": "Systematic review and meta-analysis of randomized controlled trials of perioperative outcomes and prognosis of transurethral en-bloc resection vs. Conventional transurethral resection for non-muscle-invasive bladder cancer",
        "abstract": "Objectives: This article aimed to perform a systematic review and meta-analysis of randomized controlled trials (RCTs) of perioperative outcomes and prognosis of transurethral en-bloc resection versus conventional transurethral resection for non-muscle-invasive bladder cancer (NMIBC). Methods: We searched MEDLINE, Web of Science, and the Cochrane Controlled Register of Trials (CENTRAL) to find eligible RCTs. The studies were classified by version 2 of the Cochrane risk-of-bias tool for randomized trials. Review Manager 5.4.0 was used to evaluate the data. The certainty of the evidence was assessed using the Guideline Development Tool by GRADEpro GDT. Results: Seven RCTs with 1142 patients was included in the present study. The results indicated that bladder perforation (OR = 0.17; 95% CI 0.05 to 0.67; P = 0.01), obturator nerve reflex (OR = 0.03; 95% CI 0.01 to 0.13; P < 0.00001), residual tumor (OR = 0.24; 95% CI 0.08 to 0.77; P = 0.02) and repeat transurethral resection of bladder tumor (re-TURBT) (OR = 0.54; 95% CI 0.34 to 0.85; P = 0.008) were significantly reduced in the en-bloc resection group than the conventional resection group. However, there were no significant differences in hemoglobin deficit (p = 0.31), urethral stricture (p = 0.47), and detrusor muscle presence (P = 0.16) between both groups. Besides, resection time (p = 0.25), operative time (p = 0.20), catheter dwell time (p = 0.24), and length of hospital stay (p = 0.16) were similar in the two groups. Meanwhile, en-bloc resection yielded no advantage for the 3-month (P = 0.11), 6-month (P = 0.05), 1-year (P = 0.61), 2-year (P = 0.53), and 3-year (P = 0.26) tumor recurrence rates. Conclusions: Our meta-analysis shows that transurethral en-bloc resection is associated with comparable outcomes to conventional transurethral resection for recurrence-free survival in NMIBC patients. En-bloc resection is more feasible and safer than conventional resection for NMIBC, with fewer intraoperative complications, less residual tumor, and less re-TURBT.",
        "keywords": "Non-muscle-invasive bladder cancer; En-bloc resection of bladder tumor; Conventional transurethral resection of bladder tumor; Meta-analysis",
        "released": 2022,
        "link": "https://doi.org/10.1016/j.ijsu.2022.106777"
    },
    {
        "title": "Romanian complex data center for dense seismic network",
        "abstract": "In 2002, the National Institute for Earth Physics started the development of its own real-time digital seismic network. This now consists of 86 seismic stations, of which 32 are broad-band sensors, 52 stations are equipped with short-period sensors, and two seismic arrays, all of which transmit data in real time to the National Data Center (NDC) and the Eforie Nord (EFOR) seismic observatory. EFOR is the back-up for the NDC, and it is also a monitoring center for Black Sea tsunamis. The seismic stations are equipped with Quanterra Q330 and K2 digitizers, broad-band seismometers (STS2, CMG40T, CMG 3ESP, CMG3T) and Episensor Kinemetrics acceleration sensors (+/- 2g). SeedLink is a part of Seiscomp2.5 and Antelope, which are the software packages used for data acquisition in real time and data exchange. Communication from the digital seismic stations to the NDC in Bucharest and EFOR is assured by five providers (GPRS, VPN, satellite, radio and internet). AntelopeTM 4.11 is used for acquisition and data processing at these two data centers for the reception and processing of the data, which runs on two workstations: one for real-time processing and the other for off-line processing. A Seiscomp 3 server works as the back-up for the Antelope 4.11. This acquisition and analysis systems for the seismic data produce information about the local and global parameters of earthquakes. In addition, Antelope is used for manual processing (e. g. association events, creation of a database, sending seismic bulletins, and calculation of magnitude and peak ground acceleration and velocity), generation of ShakeMap products, and interactions with global data centers. The NDC has developed tools to make all of this information easily available across the internet, and also to lay the grounds for a more modular and flexible development environment. This will enable centralizing of the data from software such as Antelope, which is using a dedicated database system (Datascope; a database system based on text files), to the more general-purpose database, MySQL. This acts like a hub between the different acquisition and analysis systems used at the NDC, while also providing better connectivity at no expense to security. Mirroring certain data to MySQL also allows the NDC to easily share information with the public, via the new application that is being developed, and also to mix in data collected from the public (e. g. information about the damage after an earthquake, which can be used to produce macroseismic intensity indices that are then stored in the database and also made available via the web application). For internal use, there is also a web application that uses the data stored in the database to display earthquake information, like location, magnitude and depth, in semi real time, thus aiding the personnel on duty. Another use for the data collected is to create and maintain contact lists to which the datacenter sends notifications (SMS and email), based on the parameters of an earthquake. For the future development, one of the NDC plans is to develop the means to cross-check the data generated between the different acquisition and analysis systems (e. g. comparing data generated by Antelope with data generated by Seiscomp).",
        "keywords": "",
        "released": 2011,
        "link": "https://doi.org/10.4401/ag-4809"
    },
    {
        "title": "A study on the development of one source multi use cross-platform based on zero coding",
        "abstract": "This study is to research and develop a native application based development tool which can perfectly satisfy both OSMU and cross-platform, and secure stable H/W control and execution speed. Additionally, this study is to enable developers in beginner or intermediate level to develop an application with the “zero coding methodology” reflected tool developed by this study and not using the conventional program coding methods, eventually providing the environment that anyone, who can use MS office programs, can easily develop an application. The explosively increasing demand for the development of mobile apps is due to all works on typical PCs that should be converted to the smart basis. Although functions to be implemented are the same, different apps should be developed for each platform to result in cost, period and task forces increased a few times. The web-based application platform emerging as a solution for cross platforms is not capable of embodying the desired One Source Multi Use due to speed, UI, and technical incompleteness. Therefore if a “native-based OSMU development platform” is implemented which can develop native apps operating in various mobile operating systems, their high development productivity will contribute to remarkably reduced task forces, costs and time for development and maintenance. Also it will be possible to preoccupy the market of a potential as great as the exiting operating systems, for example, Android, iOS, Windows 8, etc. In order to develop and makes fullest use of smart app authoring tool different approaches should be appropriately employed. Firstly, it is required to have a close relation with legacy systems. Secondly, developers are fully qualified to use office programs. Thirdly, different programs should be also differently developed for business uses. This study aims to commercialize the authoring tool technology of the GUI method without coding and the source technology for driving apps in the cross platform environment. The high development productivity of one source multi use cross-platform based on Zero Coding will contribute to remarkably reduced task forces, costs and time for development and maintenance. The final goal of this paper is to establish operating platform specialized for business under predominant operating systems such as Google’s Android or Apple’s iOS, and to create making tools for application programs based on GUI, in which anyone can easily learn and implement by this platform.",
        "keywords": "Cross-platform; Osmu(one source multi use); Zero coding; Mobile app; Native app",
        "released": 2015,
        "link": "https://doi.org/10.1007/s11042-014-1886-5"
    },
    {
        "title": "Efficacy and safety of traditional chinese medicine for intracranial hemorrhage by promoting blood circulation and removing blood stasis: A systematic review and meta-analysis of randomized controlled trials",
        "abstract": "Background: Although blood-activating Chinese medicine (BACM) has been reported as adjuvant therapy for intracranial hemorrhage (ICH) in China, high-quality evidence is still lacking. Our study aimed to collect the latest high-quality randomized controlled trials (RCTs) and to evaluate the efficacy and safety of BACM for ICH. Methods: RCTs published between January 2015 and March 2022 were searched in databases, including China National Knowledge Infrastructure (CNKI), China Science and Technology Journal Database (VIP), Sino-Med, Wanfang, PubMed, Web of Science, Cochrane Library, and Embase without language restrictions. Eligible RCTs were included and both primary (clinical efficacy evidenced by decreased neurological deficit scores) and secondary outcomes (increased Barthel index, decreased NIHSS, hematoma volume, the volume of cerebral edema, the incidence of side effects, and mortality) were analyzed. The quality of included RCTs was assessed using the Cochrane risk of bias tool. In the meta-analysis, the pooled results were analyzed using Review Manager 5.3 and STATA14.0. Finally, The GRADEpro GDT software (Guideline Development Tool) was used to summarize the results. Sensitivity and subgroup analyses were conducted based on the follow-up time. Results: Fifteen RCTs, involving 1,579 participants, were included for analysis in our study. The pooled outcomes indicated that BACM combined with western medicine treatment (WMT) was superior to WMT alone for patients with ICH, demonstrated by the improvements in efficacy (RR = 1.22 (95% CI, [1.13 to 1.32], p < 0.001), neurological functions (MDNIHSS = -2.75, 95% CI [-3.74 to -1.76], p < 0.001), and activities of daily living (MDBarthel index = 5.95, 95% CI [3.92 to 7.98], p < 0.001), as well as decreased cerebral hematoma, cerebral edema (MD cerebral hematoma = -2.94, 95% CI [-3.50 to -2.37, p < 0.001 and MDcerebral edema = -2.66, 95% CI [-2.95 to -2.37], p < 0.001), side effects and mortality (RR = 0.84 (95% CI [0.60 to 1.19], p = 0.330 and RR = 0.51 (95% CI, [0.16 to 1.65], p = 0.260). In addition, Conioselinum anthriscoides “Chuanxiong “ [Apiaceae], Camellia reticulata Lindl. [Theaceae], and Bupleurum sibiricum var. jeholense (Nakai) C.D.Chu [Apiaceae]) were the most frequently used herbs in the treatment of ICH. Recently, there was a trend toward the extensive use of another two herbs, including Rheum palmatum L. [Polygonaceae], Astragalus mongholicus Bunge [Fabaceae]) for ICH. Conclusion: BACM combined with WMT seems to be superior to WMT alone for patients with ICH. Further high-quality RCTs are warranted to confirm the efficacy and safety of BACM.",
        "keywords": "intracranial hemorrhage; traditional Chinese medicine; blood stasis; systematic review; meta-analysis",
        "released": 2022,
        "link": "https://doi.org/10.3389/fphar.2022.942657"
    },
    {
        "title": "GeoSRM - online geospatial safety risk model for the GB rail network",
        "abstract": "RSSB and the University of Southampton’s GeoData Institute have collaborated to research and develop a toolkit for managing large volumes of rail risk data. The pilot system encompasses concepts of highly complex geospatial big data’, open standards, open source development tools and methodologies, and enables stakeholders to filter, analyse and visualise risk across the rail network, for a range of risk models. These include train derailments, suicides and passenger slip, trips and falls, and feature a wide range of spatially dependent parameters that affect the causal, escalation and consequence mechanisms. The risk has been calculated to a high resolution, splitting 2,100,000 m of track typically into 10 m sections. By creating geospatial representations of risk, the tool can help to identify risk hotspots and in this way contribute to the improvement of rail safety. Once scaled up to a National level and full range of risk models, the tool will deliver a powerful capability, unique across Europe. Further research is extending the prototype to incorporate live and historic environmental and related rail incident data to augment and improve the risk model.",
        "keywords": "railway safety; geographic information systems; risk analysis; public domain software; Big Data; online geospatial safety risk model; GB rail network; GeoSRM; RSSB; pilot system; rail risk data; geospatial big data; open source development tools; rail network; train derailments; geospatial representations; rail safety; University of Southampton GeoData Institute",
        "released": 2016,
        "link": "https://doi.org/10.1049/iet-its.2015.0038"
    },
    {
        "title": "Generalizable layered blockchain architecture for health care applications: Development, case studies, and evaluation",
        "abstract": "Background: Data coordination across multiple health care facilities has become increasingly important for many emerging health care applications. Distrust has been recognized as a key barrier to the success of such applications. Leveraging blockchain technology could provide potential solutions tobuild trust between data providers and receivers by taking advantage of blockchain properties such as security, immutability, anonymity, decentralization, and smart contracts. Many health technologies have empirically proven that blockchain designs fit well with the needs of health care applications with certain degrees of success. However, there is a lack of robust architecture to provide a practical framework for developers to implement applications and test the performance of stability, efficiency, and scalability using standard blockchain designs. A generalized blockchain model is needed for the health care community to adopt blockchain technology and develop applications in a timely fashion. Objective: This study aimed at building a generalized blockchain architecture that provides data coordination functions, including data requests, permission granting, data exchange, and usage tracking, for a wide spectrum of health care application developments. Methods: An augmented, 3-layered blockchain architecture was built on a private blockchain network. The 3 layers, from bottom to top, are as follows: (1) incorporation of fundamental blockchain settings and smart contract design for data collection; (2) interactions between the blockchain and health care application development environment using Nodejs and web3 js; and (3) a flexible development platform that supports web technologies such as HTML, https, and various programing languages. Two example applications, health information exchange (HIE) and clinical trial recruitment, were developed in our design to demonstrate the feasibility of the layered architecture. Case studies were conducted to test the performance in terms of stability, efficiency, and scalability of the blockchain system. Results: A total of 331,142 simulated HIE requests from accounts of 40,000 patients were successfully validated through this layered blockchain architecture with an average exchange time of 11.271 (SD 2.208) seconds. We also simulated a clinical trial recruitment scenario with the same set of patients and various recruitment criteria to match potential subjects using the same architecture. Potential subjects successfully received the clinical trial recruitment information and granted permission to the trial sponsors to access their health records with an average time of 3.07 seconds. Conclusions: This study proposes a generalized layered blockchain architecture that offers health technology community blockchain features for application development without requiring developers to have extensive experience with blockchain technology. The case studies tested the performance of our design and empirically proved the feasibility of the architecture in 2 relevant health application domains.",
        "keywords": "blockchain; smart contract; health information exchange; electronic health records; health care application",
        "released": 2020,
        "link": "https://doi.org/10.2196/19029"
    },
    {
        "title": "The development of a hypertension prevention and financial-incentive mHealth program using a “no-code” mobile app builder: Development and usability study",
        "abstract": "Background: Regular physical activity (PA) is a key lifestyle component for hypertension prevention. Previous studies have shown that mobile health (mHealth) apps can be an effective tool for improving PA behaviors. However, adherence to and poor engagement with these apps is a challenge. A potential solution to overcome this challenge may be to combine financial incentives with innovative behavior theory, such as the Multiprocess Action Control (M-PAC) framework. Currently, there is a lack of PA financial incentive-driven M-PAC mHealth programs aimed at hypertension prevention. Objective: We aimed to describe the process of developing an 8-week mHealth PA and financial-incentive hypertension education program (Healthy Hearts) and to evaluate usability of the Healthy Hearts program. Methods: The first 2 stages of the Integrate, Design, Assess, and Share framework were used to guide the development of the Healthy Hearts program. The development process consisted of 2 phases. In phase 1, the research team met to discuss implementing the M-PAC framework to adopt an existing web-based hypertension prevention program to a mobile app. The app was developed using a no-code app development platform, Pathverse (Pathverse Inc), to help decrease overall development time. In phase 2, we created a prototype and conducted usability testing to evaluate lesson 1 of the Healthy Hearts program to further enhance the user experience. We used semistructured interviews and the mHealth App Usability Questionnaire to evaluate program acceptability and usability. Results: Intervention development among the research team successfully created an 8-week financial-incentive hypertension education program for adults aged 40-65 years who did not currently meet the Canadian Physical Activity Guidelines (<150 minutes of moderate to vigorous PA per week). This program lasted 8 weeks and comprised 25 lessons guided by the M-PAC framework. The program used various behavior change techniques to further support PA adherence. Usability testing of the first lesson was successful, with 6 participants recruited for 2 rounds of testing. Feedback was gathered to enhance the content, layout, and design of the Healthy Hearts program to prepare the mHealth program for feasibility testing. Results of round 1 of usability testing suggested that the content delivered in the lessons was long. Therefore, the content was divided into multiple lessons before round 2 of usability testing, where feedback was only on design preferences. A minimum viable product was created with these results. Conclusions: The iterative development process and the usability assessments suggested by the Integrate, Design, Assess, and Share framework enabled participants to provide valuable feedback on the content, design, and layout of the program before advancing to feasibility testing. Furthermore, the use of the “no-code” app development tool enabled our team to rapidly make changes to the app based on user feedback during the iterative design process.",
        "keywords": "mobile health; mHealth; usability study; financial incentive; physical activity; mobile phone; smartphone",
        "released": 2023,
        "link": "https://doi.org/10.2196/43823"
    },
    {
        "title": "Using MATLAB software with tomcat server and java platform for remote image analysis in pathology",
        "abstract": "Background: The Matlab software is a one of the most advanced development tool for application in engineering practice. From our point of view the most important is the image processing toolbox, offering many built-in functions, including mathematical morphology, and implementation of a many artificial neural networks as AI. It is very popular platform for creation of the specialized program for image analysis, also in pathology. Based on the latest version of Matlab Builder Java toolbox, it is possible to create the software, serving as a remote system for image analysis in pathology via internet communication. The internet platform can be realized based on Java Servlet Pages with Tomcat server as servlet container. Methods: In presented software implementation we propose remote image analysis realized by Matlab algorithms. These algorithms can be compiled to executable jar file with the help of Matlab Builder Java toolbox. The Matlab function must be declared with the set of input data, output structure with numerical results and Matlab web figure. Any function prepared in that manner can be used as a Java function in Java Servlet Pages (JSP). The graphical user interface providing the input data and displaying the results (also in graphical form) must be implemented in JSP. Additionally the data storage to database can be implemented within algorithm written in Matlab with the help of Matlab Database Toolbox directly with the image processing. The complete JSP page can be run by Tomcat server. Results: The proposed tool for remote image analysis was tested on the Computerized Analysis of Medical Images (CAMI) software developed by author. The user provides image and case information (diagnosis, staining, image parameter etc.). When analysis is initialized, input data with image are sent to servlet on Tomcat. When analysis is done, client obtains the graphical results as an image with marked recognized cells and also the quantitative output. Additionally, the results are stored in a server database. The internet platform was tested on PC Intel Core2 Duo T9600 2.8GHz 4GB RAM server with 768x576 pixel size, 1.28Mb tiff format images reffering to meningioma tumour (x400, Ki-67/MIB-1). The time consumption was as following: at analysis by CAMI, locally on a server -3.5 seconds, at remote analysis -26 seconds, from which 22 seconds were used for data transfer via internet connection. At jpg format image (102 Kb) the consumption time was reduced to 14 seconds. Conclusions: The results have confirmed that designed remote platform can be useful for pathology image analysis. The time consumption is depended mainly on the image size and speed of the internet connections. The presented implementation can be used for many types of analysis at different staining, tissue, morphometry",
        "keywords": "",
        "released": 2011,
        "link": "https://doi.org/10.1186/1746-1596-6-S1-S18"
    },
    {
        "title": "Data-driven onboard inter-turn short circuit fault diagnosis for electric vehicles by using real-time simulation environment",
        "abstract": "Various fault detection methods, particularly focused on onboard Condition-Based Monitoring (CBM) in Electrical Machines and Drives (EMDs), face limitations such as sensitivity to load variations, slow fault detection, and the absence of fully automated solutions. AI and Data-Driven methods offer flexible alternatives, utilizing historical data for pattern and anomaly identification. Among Electrical Signature Analysis techniques for electrical motor diagnostics, the Space Vector Theory (SVT) is extensively used, while Park’s Vector based diagnostic solutions lack real-time Inter-Turn Short Circuit (ITSC) fault severity assessment, with available techniques often limited to binary classifiers. Implementing AI with SVT for real-time Electric Vehicle (EV) use is underdeveloped, hindered by data scarcity and diverse dataset collection challenges. Real-time simulation, accurate fault modeling, and hardware limitations pose challenges, especially for embedding AI models into processors. To achieve intelligent onboard diagnosis for ITSC fault severity in this paper, a multi-modal approach model is proposed, employing MobileNetV2 to classify Park’s Vector trajectories based on the fault features related to the number of shorted turns. Performance assessments encompass both the standard MobileNetV2 and the proposed multi-modal approach model across various fault severity levels. Furthermore, to address the challenge of limited data availability, an accelerated real-time AI development environment is designed using an FPGA to generate synthetic fault pattern datasets, aligning with the standards of the Electric Vehicle industry. For modeling PMSM with ITSC faults, a fault circuit model is employed. The dataset of 900 Park’s Vector trajectory images is automatically generated by varying the torque request from 10 to 100 Nm with a 10 Nm resolution. At each torque operating point, the motor currents are recorded by adjusting the number of shorted turns. Simulation results confirm the outstanding performance of MobileNetV2 in binary classification, achieving an accuracy of 99.26 %. In case of 5-class ITSC fault severity classification, the prediction accuracy reaches only 72.55 %. The here proposed multi-modal MobileNetV2 model excels, achieving a remarkable accuracy of 99.163 % in the 3-class fault severity classification and 84.907 % in the 5-class classification. These results support the superiority of the proposed multi-modal MobileNetV2 model, which is trained on the generated rich dataset. It outperforms existing Park’s Vector Analysis based ITSC fault detection methods, particularly in early ITSC fault detection as it can detect faults from 6 shorted turns. Additionally, it allows for online fault severity assessment during transient operation and meets stringent requirements for onboard applications. Altogether, the results of investigations prove the presence and extractability of fine detail information in Park’s Vector trajectories, for assessing ITSC fault severity. This contributes to a deeper understanding and analysis of faults in electrical motors through the use of Park’s Vector trajectories.",
        "keywords": "Circuit faults; Fault diagnosis; Induction motors; Feature extraction; Analog circuits; Stator windings; Integrated circuit modeling; Rotating electrical machines and drives; PMSM; Park’s vector analysis; electric vehicles; condition-based monitoring; real-time simulation; inter-turn short circuit; fault severity; classification; artificial intelligence",
        "released": 2023,
        "link": "https://doi.org/10.1109/ACCESS.2023.3344483"
    },
    {
        "title": "WikiBuild: A new application to support patient and health care professional involvement in the development of patient support tools",
        "abstract": "Active patient and public involvement as partners in their own health care and in the development of health services is key to achieving a health care system that is responsive to patients’ needs and values. It promotes better use of the health care system, and improves health outcomes, quality of life and patient satisfaction. By involving patients and health care professionals as partners in the creation and updating of patient health support tools, wikis-highly accessible, interactive vehicles of communication-have the potential to empower users to implement these support tools in daily life. Acknowledging the potential of wikis, and recognizing that they capitalize on the free and open access to information, scientists, opinion leaders and patient advocates have suggested that wikis could help decision-making constituencies improve the delivery of health care. They might also decrease its cost and improve access to knowledge within developing countries. However, little is known about the efficacy of wikis in helping to attain these goals. There is also a need to know more about the intention of patients and health care workers to use wikis, in what circumstances and what factors will influence their use of wikis. In this issue of the Journal of Medical Internet Research, Gupta et al describe how they developed and tested a new wiki-inspired application to improve asthma care. The researchers involved patients with asthma, primary care physicians, pulmonologists and certified asthma educators in the construction of an asthma action plan. Their paper-entitled “WikiBuild: a new online collaboration process for multistakeholder tool development and consensus building”-is the first description of a wiki-inspired technology built to involve patients and health care professionals in the development of a patient support tool. This innovative study has made important contributions toward how wikis could be generalized to involve multiple stakeholders in the development of other knowledge translation tools such as clinical practice guidelines or decision aids. More specifically, Gupta et al have uncovered potential action mechanisms toward increasing usage of these tools by patients and health care professionals. These are decreasing hierarchical influences, increasing usability and adapting a tool to local context. More research is now needed to determine if the use of the resulting wiki-developed plan will actually be higher than a plan developed using other methods. Furthermore, there is also a need to assess the intention of participants to continue using wiki-based processes on an ongoing basis. It is in this dynamic and continuous retroaction loop that the support tool users-both patients and health care professionals-can adapt and improve the product after its real-life shortcomings are revealed and as new evidence becomes available. As such, a wiki would be more than a simple patient support development tool, but could also become a dynamic and interactive repository and delivery tool that would facilitate ongoing and sustainable patient and professional engagement.",
        "keywords": "Medical informatics; patient-centered care; wikis; collaborative writing applications; knowledge translation; patient and public involvement",
        "released": 2011,
        "link": "https://doi.org/10.2196/jmir.1961"
    },
    {
        "title": "Marketing model of tourism enterprises based on new media environment",
        "abstract": "New media is a scientific and technological product under the background of the new era, and now new media technology has been widely used in all aspects of social development, and it has spawned the arrival of the new media era. In the new media environment, the survival and development environment faced by enterprises have undergone earth-shaking changes; if you still follow the traditional marketing methods, it will not only reduce the effectiveness of marketing but also because of the deviation from the development requirements of the times be eliminated, bringing threats to the survival and development of enterprises. The development of new media, represented by the Internet and mobile phones, has brought great changes to various industries in the world including tourism. The market size of the travel network is expanding, and online sales are soaring. At the same time, all kinds of tourism organizations often have blind optimism in the construction of tourism networks and lack of reasonable application means, so that tourism network marketing is far from exerting its potential in reality; on the one hand, it cannot provide transaction services that satisfy both supply and demand; on the other hand, it also affects the economic benefits of tourism organizations. Compared with the rapid development of the tourism media industry, the theoretical research related to the actual situation lags significantly. Although the number of related studies at home and abroad is increasing, most of them stay at the level of explanation and introduction, and the analysis of the tourism industry chain is not enough. The breadth and depth of the study are sorely lacking. The future letter needs to summarize the successes and failures of the development of the tourism media industry from a theoretical height and carry out theoretical improvement to guide practice, and it also needs to strengthen forward-looking research, summarize the laws and models, and put forward practical implementation plans to improve the overall development level of the tourism and media industry. Aiming at the problem of new media marketing strategy in shopping tourism scenic spots, this paper applies descriptive research, questionnaire survey, statistical processing, mathematical model, and other research methods to analyze the characteristics of new media such as prominence of personalization, increased audience selectivity, and diversified forms of expression presented by new media compared with traditional media, as well as the differences between the two in terms of communication status, dominant state, and audience state. 4I Marketing theory is applied to new media marketing, the changes in the marketing environment and marketing needs under the background of new media, and the extensive impact of these changes on the marketing mix strategy; the basic strategies and expected effects of new media marketing of tourism enterprises are studied from the aspects of product, price, channel, and promotion; the IPA model is constructed, and the empirical analysis of the shopping tourist attractions of Changshu Garment City in China is carried out. rough the research of this paper, in theory, the understanding of new media marketing is enriched, and in fact, it provides guidance for optimizing the new media marketing strategy of tourist attractions.",
        "keywords": "",
        "released": 2022,
        "link": "https://doi.org/10.1155/2022/5273167"
    },
    {
        "title": "Chronotherapy versus conventional statins therapy for the treatment of hyperlipidaemia",
        "abstract": "Background Elevated levels of total cholesterol and low-density lipoprotein play an important role in the development of atheromas and, therefore, in cardiovascular diseases. Cholesterol biosynthesis follows a circadian rhythm and is principally produced at night (between 12: 00 am and 6: 00 am). The adjustment of hypolipaemic therapy to biologic rhythms is known as chronotherapy. Chronotherapy is based on the idea that medication can have different effects depending on the hour at which it is taken. Statins are one of the most widely used drugs for the prevention of cardiovascular events. In usual clinical practice, statins are administered once per day without specifying the time when they should be taken. It is unknown whether the timing of statin administration is important for clinical outcomes. Objectives To critically evaluate and analyse the evidence available from randomised controlled trials regarding the effects of chronotherapy on the effectiveness and safety of treating hyperlipidaemia with statins. Search methods We searched the CENTRAL, MEDLINE, Embase, LILACS, ProQuest Health & Medical Complete, OpenSIGLE, Web of Science Conference Proceedings, and various other resources including clinical trials registers up to November 2015. We also searched the reference lists of relevant reviews for eligible studies. Selection criteria We included randomised controlled trials (RCTs), enrolling people with primary or secondary hyperlipidaemia. To be included, trials must have compared any chronotherapeutic lipid-lowering regimen with statins and any other statin lipid-lowering regimen not based on chronotherapy. We considered any type and dosage of statin as eligible, as long as the control and experimental arms differed only in the timing of the administration of the same statin. Quasi-randomised studies were excluded. Data collection and analysis We used the standard methodological procedures expected by Cochrane. We extracted the key data from studies in relation to participants, interventions, and outcomes for safety and efficacy. We calculated odds ratios (OR) for dichotomous data and mean differences (MD) for continuous data with 95% confidence intervals (CI). Using the GRADE approach, we assessed the quality of the evidence and we used the GRADEproGuideline Development Tool to import data from Review Manager to create “ Summary of findings” tables. Main results This review includes eight RCTs (767 participants analysed in morning and evening arms). The trials used different lipid-lowering regimens with statins (lovastatin: two trials; simvastatin: three trials; fluvastatin: two trials; pravastatin: one trial). All trials compared the effects between morning and evening statin administration. Trial length ranged from four to 14 weeks. We found a high risk of bias in the domain of selective reporting in three trials and in the domain of incomplete outcome data in one trial of the eight trials included. None of the studies included were judged to be at low risk of bias. None of the included RCTs reported data on cardiovascular mortality, cardiovascular morbidity, incidence of cardiovascular events, or deaths from any cause. Pooled results showed no evidence of a difference in total cholesterol (MD 4.33, 95% CI -1.36 to 10.01), 514 participants, five trials, mean follow-up 9 weeks, low-quality evidence), low-density lipoprotein cholesterol (LDL-C) levels (MD 4.85 mg/dL, 95% CI -0.87 to 10.57, 473 participants, five trials, mean follow-up 9 weeks, low-quality evidence), high-density lipoprotein cholesterol (HDL-C) (MD 0.54, 95% CI -1.08 to 2.17, 514 participants, five trials, mean follow-up 9 weeks, low-quality evidence) or triglycerides (MD -8.91, 95% CI -22 to 4.17, 510 participants, five trials, mean follow-up 9 weeks, low-quality evidence) between morning and evening statin administration. With regard to safety outcomes, five trials (556 participants) reported adverse events. Pooled analysis found no differences in statins adverse events between morning and evening intake (OR 0.71, 95% CI 0.44 to 1.15, 556 participants, five trials, mean follow-up 9 weeks, low-quality evidence). Authors’ conclusions Limited and low-quality evidence suggested that there were no differences between chronomodulated treatment with statins in people with hyperlipidaemia as compared to conventional treatment with statins, in terms of clinically relevant outcomes. Studies were short termand therefore did not report on our primary outcomes, cardiovascular clinical events or death. The review did not find differences in adverse events associated with statins between both regimens. Taking statins in the evening does not have an effect on the improvement of lipid levels with respect to morning administration. Further high-quality trials with longer-term follow-up are needed to confirm the results of this review.",
        "keywords": "",
        "released": 2016,
        "link": "https://doi.org/10.1002/14651858.CD009462.pub2"
    }
]