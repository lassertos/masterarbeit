@article{10.1504/ijlt.2023.131313,
author = {Trakosas, Dimitrios and Tikva, Christina and Tambouris, Efthimios},
title = {Visual programming and computational thinking environments for K-9 education: a systematic literature review},
year = {2023},
issue_date = {2023},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {1},
issn = {1477-8386},
url = {https://doi.org/10.1504/ijlt.2023.131313},
doi = {10.1504/ijlt.2023.131313},
abstract = {Teaching programming and computational thinking to young students has gained increasing attention in recent years. This great attention is attributed partially to the emergence of easy-to-use visual programming environments. These environments help students focus on the logic and concepts of programming and at the same time enhance their engagement. It has been shown that the characteristics of visual programming environments influence students' engagement with programming. However, there is still no systematic investigation of these characteristics. This study aims to provide insights on the characteristics of visual programming environments for K-9 education based on a systematic literature review of 83 empirical studies on K-9 teaching and learning programming. These characteristics are analysed based on the following four levels: a) functional features; b) student experience; c) teacher experience; d) disadvantages. Finally, herein we discuss the features that a programming environment for K-9 education could have to improve the experience of students and teachers.},
journal = {Int. J. Learn. Technol.},
month = {jan},
pages = {94–121},
numpages = {27},
keywords = {visual programming environments, computational thinking environments, K-9 education}
}

@article{10.1007/s10515-023-00410-z,
author = {Anwar, Zeeshan and Afzal, Hammad},
title = {Mining crowd sourcing repositories for open innovation in software engineering},
year = {2024},
issue_date = {May 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00410-z},
doi = {10.1007/s10515-023-00410-z},
abstract = {Various development tools have been introduced and the choice of suitable development tool depends on the particular context like the type of application to be developed, the development process and application domain, etc. The real challenge is to deliver new features at the right time with a faster development cycle. The selection of suitable development tools will help developers to save time and effort. In this research, we will explore software engineering repositories (like StackOverflow) to collect feedback from developers about development tools. This will explore which features in a development tool are most important, which features are missing, and which features require changes. The answers to these questions can be found by mining the community question-answering sites (CQA). We will use user feedback to innovate the new features in the development tool. Various techniques of Big Data, Data Mining, Deep Learning, and Transformers including Generative Pre-Training Transformer will be used in our research. Some of the major techniques include (i) data collection from CQA sites like StackOverflow, (ii) data preprocessing (iii) categories the data into various topics using topic modeling (iv) sentiment analysis of data to get positive or negative aspects of features (v) ranking of users and their feedback. The output of this research will categorize the users feedback into various ideas, this will help organizations to decide which features are required, which features are not required, which features are difficult or confusing, and which new features should be introduced into a new release.},
journal = {Automated Software Engg.},
month = {jan},
numpages = {6},
keywords = {Open innovation, Community question answering, Topic modeling, Sentiment analysis, Quality assessment, Crowd sourcing}
}

@inproceedings{10.5555/582910.786750,
author = {Wyeth, Peta and Purchase, Helen C.},
title = {Programming without a Computer: A New Interface for Children under Eight},
year = {2000},
isbn = {0769505155},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Electronic Blocks are a new programming interface, designed for children aged between three and eight years. The Electronic Blocks programming environment includes sensor blocks, action blocks and logic blocks. By connecting these blocks children can program structures that interact with the environment. The Electronic Block programming interface design is based on principles of developmentally appropriate practices in early childhood education. As a result the blocks provide young children with a programming environment that allows them to explore quite complex programming principles. The simple syntax of the blocks provides opportunities for young children unavailable through the use of traditional programming languages. The blocks allow children to create and use simple code structures. The Electronic Block environment provides a developmentally appropriate environment for planning overall strategies for solving a problem, breaking a strategy down into manageable units, and systematically determining the weakness of the solution. Electronic Blocks are the physical embodiment of computer programming. They have the unique dynamic and programmable properties of a computer minus its complexity.},
booktitle = {Proceedings of the First Australasian User Interface Conference},
pages = {141},
keywords = {Early Childhood Education, Electronic Building Blocks, Physical Programming},
series = {AUIC '00}
}

@article{10.1145/1868358.1868361,
author = {K\"{o}lling, Michael},
title = {The Greenfoot Programming Environment},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
url = {https://doi.org/10.1145/1868358.1868361},
doi = {10.1145/1868358.1868361},
abstract = {Greenfoot is an educational integrated development environment aimed at learning and teaching programming. It is aimed at a target audience of students from about 14 years old upwards, and is also suitable for college- and university-level education. Greenfoot combines graphical, interactive output with programming in Java, a standard, text-based object-oriented programming language. This article first describes Greenfoot and then goes on to discuss design goals and motivations, strengths and weaknesses of the system, and its relation to two environments with similar goals, Scratch and Alice.},
journal = {ACM Trans. Comput. Educ.},
month = {nov},
articleno = {14},
numpages = {21},
keywords = {Greenfoot, programming education, programming environment}
}

@inproceedings{10.5555/1114280.1114372,
author = {Moore, Shirley and Cronk, David and Wolf, Felix and Purkayastha, Avi and Teller, Patricia and Araiza, Robert and Aguilera, Maria Gabriela and Nava, Jamie},
title = {Performance Profiling and Analysis of DoD Applications Using PAPI and TAU},
year = {2005},
isbn = {0769524966},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Large scientific applications developed as recently as five to ten years ago are often at a disadvantage in current computing environments. Due to frequent acquisition decisions made for reasons such as priceperformance, in order to continue production runs it is often necessary to port large scientific applications to completely different architectures than the ones on which they were developed. Since the porting step does not include optimizations necessary for the new architecture, performance often suffers due to various architectural features. The Programming Environment and Training (PET) Computational Environments (CE) team has developed and deployed different procedures and mechanisms for collection of performance data and for profiling and optimizations of these applications based on that data. The paper illustrates some of these procedures and mechanisms.},
booktitle = {Proceedings of the 2005 Users Group Conference on 2005 Users Group Conference},
pages = {394},
series = {DOD_UGC '05}
}

@article{10.1109/TKDE.2008.144,
author = {Pahl, Claus and Kenny, Claire},
title = {Interactive Correction and Recommendation for Computer Language Learning and Training},
year = {2009},
issue_date = {June 2009},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {21},
number = {6},
issn = {1041-4347},
url = {https://doi.org/10.1109/TKDE.2008.144},
doi = {10.1109/TKDE.2008.144},
abstract = {Active learning and training is a particularly effective form of education. In various domains, skills are equally important to knowledge. We present an automated learning and skills training system for a database programming environment that promotes procedural knowledge acquisition and skills training. The system provides meaningful knowledge-level feedback such as correction of student solutions and personalized guidance through recommendations. Specifically, we address automated synchronous feedback and recommendations based on personalized performance assessment. At the core of the tutoring system is a pattern-based error classification and correction component that analyzes student input in order to provide immediate feedback and in order to diagnose student weaknesses and suggest further study material. A syntax-driven approach based on grammars and syntax trees provides the solution for a semantic analysis technique. Syntax tree abstractions and comparison techniques based on equivalence rules and pattern matching are specific approaches.},
journal = {IEEE Trans. on Knowl. and Data Eng.},
month = {jun},
pages = {854–866},
numpages = {13},
keywords = {Applications and Expert Knowledge-Intensive Systems, Artificial intelligence, Artificial intelligence—applications and expert knowledge-intensive systems, Data Structures, Education, Query languages, applications and expert knowledge-intensive systems, data structures, education, programming languages, query languages.}
}

@inproceedings{10.1145/3544548.3580981,
author = {Dietz, Griffin and Tamer, Nadin and Ly, Carina and Le, Jimmy K and Landay, James A.},
title = {Visual StoryCoder: A Multimodal Programming Environment for Children’s Creation of Stories},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580981},
doi = {10.1145/3544548.3580981},
abstract = {Computational thinking (CT) education reaches only a fraction of young children, in part because CT learning tools often require expensive hardware or fluent literacy. Block-based programming environments address these challenges through symbolic graphical interfaces, but users often need instructor support to advance. Alternatively, voice-based tools provide direct instruction on CT concepts but can present memory and navigation challenges to users. In this work, we present Visual StoryCoder, a multimodal tablet application that combines the strengths of each of these approaches to overcome their respective weaknesses. Visual StoryCoder introduces children ages 5–8 to CT through creative storytelling, offers direct instruction via a pedagogical voice agent, and eases use through a block-like graphical interface. In a between-subjects evaluation comparing Visual StoryCoder to a leading block-based programming app for this age group (N = 24), we show that Visual StoryCoder is more understandable to independent learners, leads to higher-quality code after app familiarization, and encourages personally meaningful projects.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {96},
numpages = {16},
keywords = {children, computational thinking, multimodal interface, storytelling},
location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
series = {CHI '23}
}

@inproceedings{10.1145/3368308.3415397,
author = {Johnson, Jeremiah W.},
title = {Benefits and Pitfalls of Jupyter Notebooks in the Classroom},
year = {2020},
isbn = {9781450370455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368308.3415397},
doi = {10.1145/3368308.3415397},
abstract = {Jupyter notebooks are widely used in industry and in academic research, but have only begun to make inroads into the classroom. The design of the Jupyter notebook is in many ways well suited for teaching subjects in information technology and computer science, but it is a tool that departs significantly from a standard text editor or integrated development environment, and thus carries with it several unique advantages as well as several surprising potential pitfalls. As use of Jupyter notebooks has grown, so has criticism of the notebook, for varied reasons: notebooks can behave in unexpected ways, they can be difficult to reproduce, they open up potential security issues, and they may encourage poor coding practices. A set of best practices to guide instructors and help addressing these concerns when using Jupyter notebooks in the classroom is currently lacking. This paper addresses the strengths and weaknesses of the Jupyter notebook for education, drawing on existing literature as well as the author's experience teaching a range of courses with Jupyter notebooks for over five years, and recommends a set of best practices for teaching with the Jupyter notebook.},
booktitle = {Proceedings of the 21st Annual Conference on Information Technology Education},
pages = {32–37},
numpages = {6},
keywords = {ipython, jupyter notebooks, literate programming},
location = {Virtual Event, USA},
series = {SIGITE '20}
}

@book{10.5555/3055864,
author = {Valle, Andrea},
title = {Introduction to SuperCollider},
year = {2016},
isbn = {3832540172},
publisher = {Logos Verlag},
address = {DEU},
abstract = {Originally developed by James McCartney in 1996 and now an open source project, SuperCollider is a software package for the synthesis and control of audio in real time. Currently, it represents the state of the art in the field of audio programming: there is no other software available that is equally powerful, efficient or flexible. Yet, SuperCollider is often approached with suspicion or awe by novices, but why? One of the main reasons is the use of a textual user interface. Furthermore, like most software packages that deal with audio, SuperCollider prerequisites a series of skills, ranging from expertise in analog/digital signal processing, to musical composition, to computer science. However, as the beginner overcomes these initial obstacles and understands the powerful flexibility of SuperCollider, what once were seen as weaknesses become its strengths. SuperCollider's features also mean versatility in advanced software applications, generality in terms of computer modelling, and expressivity in terms of symbolic representations. This book aims at providing a brief overview of, and an introduction to, the SuperCollider programming environment. It also intends to informally present, by employing SuperCollider, a series of key notions relevant to what is broadly referred to as computer music. Andrea Valle is a researcher/aggregate professor in film, photography and television at the University of Turin-DAMS, and is active as a musician and composer. He has been a SuperCollider user since 2005.}
}

@phdthesis{10.5555/932896,
author = {Kerkiz, Nabil Fouad and Bouldin, Dan},
title = {Development and experimental evaluation of partitioning algorithms for adaptive computing systems},
year = {2000},
isbn = {0493035788},
publisher = {The University of Tennessee},
abstract = {Multi-FPGA systems offer the potential to deliver higher performance solutions than traditional computers for some low-level computing tasks. This requires a flexible hardware substrate and an automated mapping system. CHAMPION is an automated mapping system for implementing image processing applications in multi-FPGA systems under development at the University of Tennessee. CHAMPION will map applications in the Khoros Cantata graphical programming environment to hardware. The work described in this dissertation involves the automation of the CHAMPION back-end design flow, which includes the partitioning problem, netlist to structural VHDL conversion, synthesis and placement and routing, and host code generation. The primary goal is to investigate the development and evaluation of three different k-way partitioning approaches. In the first and the second approaches, we discuss the development and implementation of two existing algorithms. The first approach is a hierarchical partitioning method based on topological ordering (HP). The second approach is a recursive algorithm based on the Fiduccia and Mattheyses bipartitioning heuristic (RP). We extend these algorithms to handle the multiple constraints imposed by adaptive computing systems. We also introduce a new recursive partitioning method based on topological ordering and levelization (RPL). In addition to handling the partitioning constraints, the new approach efficiently addresses the problem of minimizing the number of FPGAs used and the amount of computation, thereby overcoming some of the weaknesses of the HP and RP algorithms.},
note = {AAI9996361}
}

@inproceedings{10.1145/3545947.3569611,
author = {Malan, David J. and Carter, Jonathan and Liu, Rongxin and Zenke, Carter},
title = {Providing Students with Standardized, Cloud-Based Programming Environments at Term's Start (for Free)},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3569611},
doi = {10.1145/3545947.3569611},
abstract = {For CS50 at Harvard, we have long provided students with a standardized programming environment, to avoid start-of-term technical difficulties that might otherwise arise if students had to install and configure compilers, interpreters, and debuggers on their own Macs and PCs. (For many students, "hello, world" is challenge enough on day 0, without also encountering "command not found" at the same time!) We originally provided students with shell accounts on a university-managed cluster of systems. We then transitioned to a cloud-based equivalent so as to manage the systems ourselves, root access and all. We transitioned thereafter to client-side virtual machines, to scale to more students and enable GUI-based assignments. We have since transitioned to web-based environments, complete with code tabs, terminal windows, and file explorers, initially implemented atop AWS Cloud9 and now, most recently, GitHub Codespaces, an implementation of Visual Studio (VS) Code in the cloud, free for teachers and students alike. In this workshop, we'll discuss the pedagogical and technological advantages and disadvantages of every approach and focus most of our time, hands-on, on using and configuring GitHub Codespaces itself for teaching and learning. Along the way, attendees will learn how to create their own Docker images and "devcontainers" for their own classes and any languages they teach. Attendees will learn what is possible educationally by writing their own VS Code extensions as well. And how, at term's end, to "offboard" students to VS Code itself on their own Macs and PCs, so as to continue programming independent of Codespaces.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1183},
numpages = {1},
keywords = {cli, code, code editor, command-line interface, container, docker, editor, graphical user interface, gui, ide, integrated development environment, programming, terminal window, text editor, web app, web application},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3626253.3633427,
author = {Malan, David J. and Liu, Rongxin and Zenke, Carter and Lloyd, Doug},
title = {Providing Students with Standardized, Cloud-Based Programming Environments at Term's Start (for Free)},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633427},
doi = {10.1145/3626253.3633427},
abstract = {For CS50 at Harvard, we have long provided students with a standardized programming environment, to avoid start-of-term technical difficulties that might otherwise arise if students had to install and configure compilers, interpreters, and debuggers on their own Macs and PCs. (For many students, "hello, world" is challenge enough on day 0, without also encountering "command not found" at the same time!) We originally provided students with shell accounts on a university-managed cluster of systems. We then transitioned to a cloud-based equivalent so as to manage the systems ourselves, root access and all. We transitioned thereafter to client-side virtual machines, to scale to more students and enable GUI-based assignments. We have since transitioned to web-based environments, complete with code tabs, terminal windows, and file explorers, initially implemented atop AWS Cloud9 and now, most recently, GitHub Codespaces, an implementation of Visual Studio (VS) Code in the cloud, free for teachers and students alike. In this workshop, we'll discuss the pedagogical and technological advantages and disadvantages of every approach and focus most of our time, hands-on, on using and configuring GitHub Codespaces itself for teaching and learning. Along the way, attendees will learn how to create their own Docker images and "devcontainers" for their own classes and any languages they teach. Attendees will learn what is possible educationally by writing their own VS Code extensions as well. And how, at term's end, to "offboard" students to VS Code itself on their own Macs and PCs, so as to continue programming independent of Codespaces.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1903},
numpages = {1},
keywords = {cli, code, code editor, command-line interface, container, docker, editor, graphical user interface, gui, ide, integrated development environment, programming, terminal window, text editor, web app, web application},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@article{10.1145/3427596,
author = {Kim, Han Sung and Kim, Soohwan and Na, Wooyoul and Lee, Woon Jee},
title = {Extending Computational Thinking into Information and Communication Technology Literacy Measurement: Gender and Grade Issues},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
url = {https://doi.org/10.1145/3427596},
doi = {10.1145/3427596},
abstract = {As Information and Communication Technology (ICT) literacy education has recently shifted to fostering computing thinking ability as well as ICT use, many countries are conducting research on national curriculum and evaluation. In this study, we measured Korean students’ ICT literacy levels by using the national measurement tool that assesses abilities of the IT (Information Technology) area and the CT (Computational Thinking) area. A research team revised an existing ICT literacy assessment tool for the IT test and developed a new CT test environment in which students could perform actual coding through a web-based programming tool such as Scratch. Additionally, after assessing ICT literacy levels, differences in ICT literacy levels by gender and grade were analyzed to provide evidence for national education policies. Approximately 23,000 elementary and middle school students participated in the 2018 national assessment of ICT literacy, accounting for 1% of the national population of students. The findings demonstrated that female students had higher literacy levels in most sub-factors of IT and CT areas. Additionally, in the areas of strengths and weaknesses, the ratio of below-basic achievement among male students was at least two times greater than that of female students. Nonetheless, male students scored higher on CT automation, a coding item that involved problem solving using Scratch. Looking at the difference according to grade level, the level improved as the school year increased in elementary school, but there was no difference in middle school. When analyzing the detailed elements of middle school students, the automation factor of seventh grade students was found to be higher than eighth and ninth grade students. Based on these results, this study discussed some implications for ICT and computing education in elementary and middle schools.},
journal = {ACM Trans. Comput. Educ.},
month = {jan},
articleno = {5},
numpages = {25},
keywords = {21st century abilities, ICT literacy, computational thinking, elementary education, secondary education}
}

@inproceedings{10.1145/1384271.1384395,
author = {Kiesmueller, Ulrich and Brinda, Torsten},
title = {How do 7th graders solve algorithmic problems? a tool-based analysis},
year = {2008},
isbn = {9781605580784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1384271.1384395},
doi = {10.1145/1384271.1384395},
abstract = {Informatics education, not only in higher but also in secondary education, is often assisted by special learning software to teach the fundamental ideas of algorithms [2]. In this context pupils also learn the basics of programming using didactically reduced, textbased or visual programming languages. Therefore in Germany, in some federal countries (for example Bavaria), where the basics of algorithms are already taught in the 7th grade (age 12 to 13 years), age-based learning and programming environments, such as Karel, the robot and Kara, the programmable ladybug [1], are used. Although the design of these environments is age-based, working with them to solve algorithmic problems often causes problems in the classroom. These tools give feedback to the learners based on the analysis of a current solution attempt without taking the previous problem solving process into account. The system messages are often rather technical and therefore hardly helpful especially for weaker learners to enable them to correct arisen problems by themselves. In order to give optimal support to pupils in these situations and therefore improve the learning processes, the learner-system interaction of the used educational software environments should be enhanced and better be adapted to the learners? individual problem solving strategies.The main objective of this research project is to find out, to what extent the automated diagnosis of a problem solving strategy of a learner is possible, and to what extent this knowledge can be used to enhance the learner-system interaction. Starting from the advantages and disadvantages of standardized process observation methods, two software-based research instruments for the system supported diagnosis of the individual proceedings, using the learning environment Kara, were designed and implemented. With the first component learner-system interactions are recorded, the second one provides functions to analyse the collected data. Using test-cases gives a first idea of the quality of the solution attempts.The requirements for the software components resulted from several test scenarios with a small number of participants with different qualification in computer science (from novices to graduating computer science students). During these tests each individual was observed by a researcher and additionally interviewed afterwards. A first version of the implemented instruments was tested in case studies with more than 100 participants (12 to 13 years old) from Bavarian grammar schools to evaluate the suitability for daily use. During the studies the learners were asked to solve three given tasks in a session of 45 minutes, provided by the Kara system, individually (one pupil per computer), but communication between the test persons was allowed. The tasks required knowledge of the control structures (sequence, selection, iteration).The results of these studies indicate that it is possible to identify and to evaluate different problem solving patterns with the help of the developed instruments. To identify different types of learners? strategies it is necessary to combine the various kinds of visualizations of the collected data. To support automatic categorization pattern-recognition methods will be used. The collected ordinal (test-case results) and nominal data can be used for analyses of the correlation between different factors (for example number of error messages or program executions compared with the assessment of the solution attempt) with methods of descriptive statistics.},
booktitle = {Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education},
pages = {353},
numpages = {1},
keywords = {algorithms, didactics of informatics, kara, problem solving process, secondary computer science education, tool-based analysis},
location = {Madrid, Spain},
series = {ITiCSE '08}
}

@article{10.5555/767598.767646,
author = {Ury, Gary},
title = {Workshop on moving to visual basic.net},
year = {2003},
issue_date = {April 2003},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {18},
number = {4},
issn = {1937-4771},
abstract = {This workshop and accompanying paper will discuss and demonstrate some of the strengths and weaknesses of the new VB.Net object-oriented programming language. It is appropriate for anyone who is contemplating a course in the new language or anyone who just wants to know more about VB.Net architecture. Database connectivity and web applications will be demonstrated in addition to some fundamental navigational and Interactive Development Environment (IDE) issues. Finally, the authors will share their experiences in developing and teaching VB.Net as a second or third language to junior and senior CS/IS majors and minors at Northwest Missouri State University.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {332–335},
numpages = {4}
}

@inproceedings{10.1145/3291280.3291787,
author = {Katchapakirin, Kantinee and Anutariya, Chutiporn},
title = {An Architectural Design of ScratchThAI: A conversational agent for Computational Thinking Development using Scratch},
year = {2018},
isbn = {9781450365680},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291280.3291787},
doi = {10.1145/3291280.3291787},
abstract = {Scratch is a visual, block-based programming language, adopted as a computational thinking development tool in elementary education among many countries. Thailand has also recently included Scratch as part of the computing science course in its basic education. However, Thailand is facing a shortage of ICT teachers who are skillful in Scratch programming, especially in small provincial schools. This research aims to overcome the shortage by developing ScratchThAI, a Scratch tutorial chatbot. It is designed to assist young learners directly through a messaging platform. By giving supports through a textual conversation, more relevant advice, knowledge, and resources could be provided precisely. Different levels of each computational thinking concept are extracted and evaluated by the designed assessment algorithm. Extra predefined exercises are assigned based on the analyzed learner's strengths and weaknesses in order to actively improving the learner's understanding. Moreover, gamification is incorporated to engage and motivate young learners in computational thinking development.},
booktitle = {Proceedings of the 10th International Conference on Advances in Information Technology},
articleno = {7},
numpages = {7},
keywords = {AI in Education, Computational thinking development, Educational Technology, Game-Based Learning, Personalized Learning, Scratch Tutoring Chatbot, Virtual Scratcher, Virtual Teaching Assistant},
location = {<conf-loc>, <city>Bangkok</city>, <country>Thailand</country>, </conf-loc>},
series = {IAIT '18}
}

@inproceedings{10.1145/1275604.1275610,
author = {Franklin, Diana and Seng, John},
title = {Experiences with the Blackfin architecture for embedded systems education},
year = {2005},
isbn = {9781450347341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1275604.1275610},
doi = {10.1145/1275604.1275610},
abstract = {In the course of a major curriculum change at California Polytechnic State University, the embedded processing course was redesigned. During this process, the course had the opportunity to purchase new hardware. Analog Device's Black-fin processor was chosen based mostly on cost, but also on performance, development environment, and documentation.We first present our goals in the class. We then give an overview of the Blackfin architecture and how the Blackfin fits in with many of our goals. We then present the implementation of an expansion board developed to interface with Blackfin's EZ-KIT Lite board.We present our experiences with this setup in the hopes that others who might be thinking of a similar curricular change can learn from our successes and failures. We outline the strengths and weaknesses of the Blackfin architecture as an educational platform, followed by a discussion of our experiences and a presentation of the support materials we developed to accompany the course, including lecture material and laboratories. Finally, we discuss our future directions for our uses with the board.},
booktitle = {Proceedings of the 2005 Workshop on Computer Architecture Education: Held in Conjunction with the 32nd International Symposium on Computer Architecture},
pages = {3–es},
location = {Madison, Wisconsin},
series = {WCAE '05}
}

@inproceedings{10.1145/3419635.3419727,
author = {Yuan, Menghui},
title = {Research on the Construction of Virtual Reality Engine for Party School Teaching Application},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419727},
doi = {10.1145/3419635.3419727},
abstract = {In the current situation of the development of science and technology, virtual technology arises at the historic moment. As a kind of computer simulation technology, it has the simulation ahead of the real world. Because of its many advantages, virtual technology is already being used in fields such as education and medicine. However, there are still some shortcomings in the current virtual technology. Although the development tools are equipped, there are still some disadvantages in the process of using them, which hinders the developers to improve their work efficiency significantly. Among the prospects, this technology is the most widely used in education, and this paper aims to develop a virtual reality application engine that is convenient for party school teaching. In the education and learning of the party school, the design and r&amp;d of VR products should be completed under the support of the model and theory of education, and should respond to the characteristics of the discipline. This paper expounds a series of processes from design to r&amp;d of VR products. At the beginning, I analyses the subject and investigated the market prospect of VR education, so as to get a product with the same learning efficiency and teaching efficiency. Secondly, the depth plane analysis is carried out on the mathematical knowledge needed in the construction of 3d model, and the emphasis is put on the introduction of quaternions, transformation matrix, homogeneous coordinates and the functions of these knowledge in 3d application.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {472–475},
numpages = {4},
keywords = {Party school teaching, VR education, Virtual reality technology},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@phdthesis{10.5555/927446,
author = {Hung, Shih-Hao and Davidson, Edward S.},
title = {Optimizing parallel applications},
year = {1998},
isbn = {0591944391},
publisher = {University of Michigan},
address = {USA},
abstract = {While parallel computing offers an attractive perspective for the future, developing efficient parallel applications today is a labor-intensive process that requires an intimate knowledge of the machines, the applications, and many subtle machine-application interactions. Optimizing applications so that they can achieve their full potential on parallel machines is often beyond the programmer's or the compiler's ability; furthermore its complexity will not be reduced with the increasingly complex computer architectures of the foreseeable future. In this dissertation, we discuss how application performance can be optimized systematically. We show how insights regarding machine-application pairs and the weaknesses in their delivered performance can be derived by characterizing the machine, the application, and the machine-application interactions. We describe a general performance tuning scheme that can be used for selecting and applying a broad range of performance tuning actions to solve major performance problems in a structured sequence of steps, and discuss the interrelationship among and between performance problems and performance tuning actions. To guide programmers in performance tuning, we developed a goal-directed performance tuning methodology that employs hierarchical performance bounds to characterize the delivered performance quantitatively and explain where potential performance is lost. To reduce the complexity of performance tuning, we developed an innovative performance modeling scheme to quickly derive machine-application interactions from abstract representations of the machine and application of interest.Collectively, this dissertation unifies a range of research work done within the Parallel Performance Project at the University of Michigan over the past seven years and significantly improves the state-of-the-art in parallel application development environments.},
note = {AAI9840559}
}

@inproceedings{10.1109/DIGITEL.2007.31,
author = {Hamid, Siti Hafizah Ab and Fung, Leong Yu},
title = {Learn Programming by Using Mobile Edutainment Game Approach},
year = {2007},
isbn = {0769528015},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DIGITEL.2007.31},
doi = {10.1109/DIGITEL.2007.31},
abstract = {The concept of edutainment or "education and entertainment" is not new in a learning environment and its purpose is to make the learning process more enjoyable. This project integrated mobile game technology into a Mobile Learning system to make the learning process more fun and effective for its student end-users. The current generation prefers to spend time in front of the computer playing games instead of studying, so the project merged the technology so they can still play games but study at the same time. Even though mobile technology has many weaknesses (e.g., limited screen size), the project used this technology because it can be played anywhere, anytime, by any mobile device. The main end-users of the project are students or anyone who is learning the subject. For the time being, the project focuses on C++ programming as its domain. Basically, once a student executes the application, he or she can play the game by using C++ programming knowledge. The game starts by displaying a question. The player is represented by a small car that can shoot by using bullets. Within a time limit, the player must shoot the correct answer to a practical programming question. If the player shoots the wrong answer, the wrong answer will try to shoot the player back. The game was developed using J2ME as its development tool and Adobe Photoshop as its graphic design tool.},
booktitle = {Proceedings of the The First IEEE International Workshop on Digital Game and Intelligent Toy Enhanced Learning},
pages = {170–172},
numpages = {3},
series = {DIGITEL '07}
}

@phdthesis{10.5555/221842,
author = {Jeong, Jong Sik},
title = {An intelligent computer-assisted rater trainer with fault tree analysis for isolating},
year = {1995},
publisher = {University of Alabama},
address = {USA},
abstract = {The training of raters is one practical way for enhancing the accuracy of performance ratings. Traditional methods for presenting the training, such as lecture, group discussion, and practice/feedback, are subject to certain weaknesses. A new ICART training method, which is an application of knowledge-based systems, was suggested to overcome the weaknesses of these traditional training methods. The objectives of this study were three-fold: (1) preparing a proposal for training raters, (2) developing the ICART system according to proposal guidelines, and (3) examining whether the developed ICART system may improve the accuracy of performance ratings.The fault tree analysis technique was used to effectively define the training content. During this analysis of the problem domain, biases that occurred in judgmental activities were considered as rating-error sources. Based on a questionnaire survey and factor analysis, relationships between rating errors and error sources were explored. The results of the fault tree analysis defined the scope and the order of training tasks.Based on the defined training content, the ICART system was developed as a prototype in accordance with formalities of intelligent tutoring systems. The system was composed of three functional modules: a domain-expert knowledge module, a teaching knowledge module, and a user-interface module. Production systems and hypertext techniques were utilized to represent knowledge for the modules in the system. The Exsys Professional shell was used as a development tool.The developed ICART system was tested in the situation of evaluating an individual performing a briefing. A total of twenty-eight ROTC cadets participated in two scheduled training sessions. The control group was trained by the traditional lecture and practice method; the experimental group was trained by interacting with the ICART system. Five accuracy components, elevation, differential elevation, stereotype accuracy, differential accuracy, and overall accuracy, were used as dependent variables for analyzing training effects. It was found that training by the ICART system improved accuracy components of differential elevation and overall accuracy.},
note = {UMI Order No. GAX94-29236}
}

@phdthesis{10.5555/1834920,
author = {Fyllingness, Jennifer Lynne},
advisor = {Gardiner, John Jacob},
title = {Internet as a training tool in small tourism and hospitality businesses in norway},
year = {2009},
isbn = {9781109465778},
publisher = {Seattle University},
abstract = {Training and development are necessary components to insure the success of small businesses in the changing global economy. Previous small- and medium-size enterprise (SME) research has shown that the owners and employees of small businesses receive less formal education than employees at larger firms due to high costs, inconvenient locations, inflexible schedules, lack of human resources, and owner attitudes. Technology and the advancement of internet-based learning may be a solution to many of these potential barriers. This descriptive study examines the current adoption and attitudes towards the Internet as a formal and informal training and development tool by small tourism and hospitality businesses (STHB) in Northern Norway. The study provides an introduction to SMEs and the attitudes and barriers they face in pursuing training, the advantages and disadvantages of internet-based training and a discussion of formal versus informal training methods followed by a description of the methodology and results. For this study a web-based survey link was e-mailed to 615 STHBs in Northern Norway in June and July 2008. The findings of this research have significance in the context of STHBs in Northern Norway, but given the low response rate of 24.2% (N=149), cannot be generalized to other contexts. The researcher found that the majority of small tourism and hospitality owner-managers who had participated in training in the last 2 years perceived a variety of positive outcomes and that those owner-managers who had previously participated in internet-based training had a positive experience. Overall, the owner-managers had a positive view of information communication technologies (ICTs) and internet-based training and the majority indicated they were interested in utilizing the internet for training and development. The researcher also found that the STHBs utilized information communication technologies within their businesses for a variety of informal educational purposes including e-mail, internet searches, word processing, billing/checking, marketing/advertising and e-commerce. The least used of the ICT options provided were social networking programs. Overall, the researcher found that small tourism and hospitality businesses in Northern Norway were interested in internet-based training opportunities if the classes offered are relevant for their business and relatively inexpensive.},
note = {AAI3383804}
}

@inproceedings{10.5555/874069.875990,
author = {Berndtsson, Johan},
title = {Designing an Intranet from Scratch to Sketch: Experiences from Techniques used in the IDEnet Project},
year = {1999},
isbn = {0769500013},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The choice of techniques to support system design is important in order to achieve a satisfactory result with regards to the quality of the future system. In the IDEnet development project we chose to work with techniques used within, or inspired by, three different research areas, Sociology, Participatory Design and Hu-man Computer Interaction. The paper discusses the use of one technique from each of these research areas, ranging from 'scratch to sketch' in the development of an Intranet (IDEnet) at the Department of Computer Science and Business Administration (IDE), University College of Karlskrona/Ronneby in Sweden. The advantages and disadvantages for the use of each technique for system design are also discussed.},
booktitle = {Proceedings of the Thirty-Second Annual Hawaii International Conference on System Sciences-Volume 2 - Volume 2},
pages = {2019},
keywords = {Card Sorting, Future Workshop with Democratic Dialogue, Informal interviewing, Information Systems Design, Intranet, Techniques},
series = {HICSS '99}
}

@inproceedings{10.1145/1937117.1937126,
author = {Cav\'{e}, Vincent and Budimli\'{c}, Zoran and Sarkar, Vivek},
title = {Comparing the usability of library vs. language approaches to task parallelism},
year = {2010},
isbn = {9781450305471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1937117.1937126},
doi = {10.1145/1937117.1937126},
abstract = {In this paper, we compare the usability of a library approach with a language approach to task parallelism. There are many practical advantages and disadvantages to both approaches. A key advantage of a library-based approach is that it can be deployed without requiring any change in the tool chain, including compilers and IDEs. However, the use of library APIs to express all aspects of task parallelism can lead to code that is hard to understand and modify. A key advantage of a language-based approach is that the intent of the programmer is easier to express and understand, both by other programmers and by program analysis tools. However, a language-based approach usually requires the standardization of new constructs and (possibly) of new keywords. In this paper, we compare the java.util.concurrent (j.u.c) library [14] from Java 7 and the Habanero-Java (HJ) [16] language, supported by our experiences in teaching both models at Rice University.},
booktitle = {Evaluation and Usability of Programming Languages and Tools},
articleno = {9},
numpages = {6},
location = {Reno, Nevada},
series = {PLATEAU '10}
}

@inproceedings{10.1109/ISSRE.2004.21,
author = {Davidsson, Martin and Zheng, Jiang and Nagappan, Nachiappan and Williams, Laurie and Vouk, Mladen},
title = {GERT: An Empirical Reliability Estimation and Testing Feedback Tool},
year = {2004},
isbn = {0769522157},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISSRE.2004.21},
doi = {10.1109/ISSRE.2004.21},
abstract = {Software testing is an integral part of the software development process. Some software developers, particularly those who use the Extreme Programming test-driven development practice, continuously write automated tests to verify their code. We present a tool to complement the feedback loops created by continuous testing. The tool combines static source code metrics with dynamic test coverage for use throughout the development phase to predict a reliability estimate based on a linear combination of these values. Implemented as an open source plug-in to the Eclipse IDE, the tool facilitates the rapid transition between unit test case completions and testing feedback. The color-coded results highlight inadequate testing efforts as well as weaknesses in overall program structure. To illustrate the tool's efficacy, we share the results of its use on university software engineering course projects.},
booktitle = {Proceedings of the 15th International Symposium on Software Reliability Engineering},
pages = {269–280},
numpages = {12},
series = {ISSRE '04}
}

@book{10.5555/524373,
author = {Henderson, Chuck},
title = {Mastering Macromedia Director 5 for Windows and Mac, with CD-ROM (Mastering)},
year = {1996},
isbn = {0782118348},
publisher = {SYBEX Inc.},
address = {USA},
edition = {5th},
abstract = {From the Book: This book will provide you with a thorough understanding of Director. It also focuses on the dramatic new flood of Xtras and XObjects that, thanks to Macromedia's Open Architecture (MOA), may well elevate Director to the lofty status of a true application development environment that goes far beyond C++. The long heralded 5GL (fifth generation language) that was hype ten years ago is today's Director. Where will it go How high will it fly You'll never know unless you've been there! Mastering Macromedia Director 5 and the accompanying Directions CD-ROM will guide you on your first steps down a path toward becoming a player in the media revolution. You can use this book and CD-ROM to start preparing for the big changes that will undoubtedly rock our culture and challenge our perceptions about media before we ring out the millennium. Why This Book This book is intended for the quick learner. Someone who can seize Director as the opportunity of a lifetime and run with it. As difficult as Director and Lingo (Director's companion scripting language) can be at times, Director is nevertheless the quickest route to professional quality and highly creative multimedia authoring. I have endeavored to write a companion text to Macromedia's own documentation. A book that would provide an additional perspective to concepts and procedures that might otherwise be unnecessarily complicated. I have also attempted to write a book that, once basic concepts were thoroughly understood, could metamorphose into a quick-reference tool. In short, I have tried to write a book for myself. A book that you can pick up, thumb through to a specific section, andquickly get that, "Oh yeah, I remember how you do that!" feeling. The fast changing nature of software in general, and Internet and multimedia applications specifically, doom any printed work on the subject to almost instant obsolescence. For this reason, I have attempted to include those subjects and concepts- Director, Lingo, and Shockwave (Director for the Internet) that will have enduring value. Nevertheless, I implore you to use this book only as a starting point. Learn to prowl the Internet for the latest and most compelling new techniques and extensions that allow you to get the most out of Director, for therein lies the future of the multimedia industry and, if you so choose, a career in multimedia application development. Places to constantly browse include: www.macromedia.com - with thousands of pages of technical notes, tips and techniques, this web site is the Holy Grail of Director devotees - Macromedia's own web site. www.macromedia.com/AYS/Tech.support/Technotes/Director/index.html - more specifically, check out the tech notes available concerning Director at the Macromedia site. hakatai.mcli.dist.maricopa.edu/director/index.html - this Arizona community college has the most potent following of Lingo hackers and splicers in the world. hakatai.mcli.dist.maricopa.edu/director/digest/today.txt - check out daily Lingo expository, DirectL, the Director listserv of the airwaves. www.blarge.net/~cagles/lingo/ - Kirt Cagle always waxing poetic on the vagaries of Lists and such. www..shocker.com/shocker/cool.html - want shocked Try the best of the best Shockwave sites from Shocker! Www.gmatter.com - the most comprehensive source for new Xtras is g/matter, Inc. Check out XtraNet, a networking Xtra with unknown potential for integrating Director movies and Internet/Intranet works. www.hyperstand.com - New Media's own home page with tracks to the MacroMedia User Jourinal's web pages. How This Book Is Organized This book has 22 chapters divided into four parts. An appendix is also included. The book begins with a discourse on what we may well back on as the greatest media revolution in all of history, what I have called the multimedia/Internet revolution. This epic change in the way we communicate, entertain, educate, and play might well compete with reality itself, and lead to unimagined worlds that are limited, not by technology, but by our imaginations."Part 1: The Essentials of Director" (Chapters 1-10) continues by presenting an all encompassing overview of the premiere multimedia author tool, Director. Tutorials, demonstrations, and straightforward discussions lead you through the first ten chapters."Part 2: The Linguistics of Lingo" (Chapters 11-16) transports readers to the world of Director's embedded scripting language, Lingo. Once you have mastered Lingo-from controlling objects to becoming intimate with the elements of Lingo style-you have mastered Director."Part 3: Putting It All Together" (Chapters 17-20) combines every aspect of Director you have learned throughout the first 16 chapters and demonstrates how you can use all these elements to create multimedia products. You will also explore Director's Internet version called Shockwave."Part 4: Xtras" (Chapters 21-22) is over 200 pages dedicated entirely to revealing dozens of new third-party extensions to Director called Xtras and their still valid predecessors, XObjects.Finally, Appendix A is an extensive listing of Lingo Command References.Mastering Macromedia Director 5uses various conventions to help you find the information you need quickly and effortlessly. Tips, Notes, and Warnings are placed strategically throughout the book to help you focus in on important information quickly. Long but important or interesting digressions are set aside as boxed text, called sidebars. Because building any multimedia product-whether it is for CD-ROM distribution or for Internet/Intranet distribution - must be capable ( running on both Microsoft's Windows 95 and NT (Windows), an Apple's Macintosh Operating System (MacOS), I have tailored the book to cross-platforrn developers. In situations where significant differences in the two systems can be found, there will be notes and side bars that draw the distinction. Throughout the book I have used the following generalized conventions to distinguish between Macintosh, Power Mac, Power Mac Clones, Windows 95, and Windows NT:MacOS refers to any machine running under the Apple Macintosh Operating System. Except where noted, there is no difference between the Power PC compatible versions and 680x0 versions of the MacOSWindows refers to any machine running under the Microsoft Windows 95 and NT operating systems. Except where noted, there is no difference ( (from the standpoint of the Director author) between Windows 95 and Windows NT. References to Windows 3.1 have, for the most part, not been included. About the CD-ROM Inside the back cover of this book you will find the Directions CD-ROM. This disc contains cross-platform versions of all the tutorials and examples of techniques and Lingo code used throughout the book You will also find a save-disabled version of Director 5.0 (for both the MacOS and Windows) and save-disabled versions of many other Macromedia multimedia tools for both the MacOS and Windows. The save-disabled version of Director will allow you to open and experiment with all the tutorials and demonstration Director movies on the CD-ROM. The only drawback is you won't be able to save any of your changes. Finally, we have endeavored to include as many demonstrations of Xtras and XObjects as possible. Open these demonstrations using the save-disabled version of Director and test drive these Xtras before you purchase them.Good luck! And remember to experiment, and then experiment some more. It's only through actually trying the processes and techniques described herein that you will discover the multimedia modalities that work for you. Ultimately, you must become your own teacher.}
}

@phdthesis{10.5555/2019765,
author = {Mulhern, Anne},
advisor = {Fischer, Charles and Liblit, Ben},
title = {Polytypic proving},
year = {2010},
isbn = {9781124220956},
publisher = {University of Wisconsin at Madison},
address = {USA},
abstract = {Formal methods are a class of techniques for automatically verifying software correctness. They are a common topic in computer science research. However, they are less well-known in software development and in undergraduate computer science education. As society's reliance on software increases so does the potential severity of the consequences of software failure. Formal methods are the surest way of guaranteeing software correctness. Moreover, the study of formal methods leads to better informal understanding of software correctness and thus to better software. Unfortunately, the study and use of formal methods can be difficult and this impedes their adoption in software engineering and the computer science curriculum. Coq is an automated proof-assistant for developing certified programs and proofs. It is powerful and expressive and has been used in a large variety of significant developments. Experienced developers are accustomed to picking up new languages rapidly. Unfortunately, it is difficult for a new user to learn the use of any proof-assistant with anything like the same rapidity. Furthermore, developers have become accustomed to sophisticated IDEs with tools for refactoring and for auto-generating code. Very little such support is available for proof-assistants. Coq suffers from both these drawbacks. Our work addresses these problems in several ways. We provide two alternative approaches that facilitate general induction and recursion. We demonstrate methods for enhancing structural recursion principles to increase their generality and transparency. We demonstrate graphical techniques for improved proof visualization and an impact analysis tool for predicting the consequences of a proof refactoring.},
note = {AAI3424020}
}

@book{10.5555/579266,
author = {Unhelkar, Bhuvan},
title = {Process Quality Assurance for Uml-Based Projects},
year = {2002},
isbn = {0201758210},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: Purpose of this Book The convenor of the OOSIG (object-oriented Special Interest Group) of the Australian Computer Society is occasionally referred to as Chairperson. For past two years, this honorary and honourable title has been conferred upon me and, as the title suggests, it provides me with — amongst other things — the unenviable job of moving and organising chairs before the monthly meeting starts and ensuring they are stacked back against the wall after the meeting in the societys office is over. Getting the flipcharts and whiteboard ready, booking the room, sending the invitations, organising coffee and keeping the data projector light bulb from blowing up are some things keep the adrenaline level of the chairperson always on high. However, I had no such challenges to face when Canada-based Bran Selic, kindly addressed my SIG. Many members turned up to listen to one of the original contributors to the Unified Modelling Languages meta-model, particularly to the behind-the-scene stories. One of the interesting features of Brans talk was the candid highlighting of the strengths and weaknesses of the UML. Couple of reasons for UMLs popularity, as emerged during the talk were: UML is a standard and therefore accepted within the larger IT community, and UML came on the IT scene at the right time. The UML fills the void that existed in software development — a modelling mechanism that enables capture and expression of requirements, documentation of design, facilitates architectural discussion and supports software implementation. The modelling capabilities of the UML, supported by CASE tools, are widely usedin numerous practical software projects. Further, professional training courses on business analysis, architecture, design and testing are routinely based on the UML standard. The UML is also popular in the academic world as many university courses use the standard notations and diagrams of the UML to teach the students principles of software engineering. Finally, through relatively less-technical and more business-focused works, object technology and the UML have shown to be capable of being used in non-software related work, such as modelling business processes (BPR), business workflows, and even software workflows. Despite its popularity, however, the UML literature still needs discussion on and application of quality with UML. While we have some excellent literature on the processes of software development it seems to fall short of separate and detailed discussions on quality. On the other hand, works like Binders focus on the technical aspects of testing, using the UML notations, do not provide the process-aspect of improving the quality of software development. Indeed, none of this literature deserves any criticism for the lack of quality discussion — because these literary works do not purport to be discussing quality. The focus of these respectable and popular works is either development or testing. This book is written with an aim of directly addressing the paucity of literature in the area of process quality assurance for UML-based projects. Good Quality is all about satisfying the needs of the user. However, good is a highly subjective term. The reference-point against which quality is judged depends on time, place, and situation — all of which can change! Hence, the essential ingredients in producing good quality are: A product that satisfies the changing needs of the user A process that enables creation, verification and validation of such a product A common mechanism to establish communication Continuous improvement of the process of producing product When applied to software development, these quality requirements translate into producing a software product that would evolve, scale and change, according to the needs of its users — primarily the business. Not only do we need a process for developing such a software product, we also need significant checking and crosschecking of the models and processes that have built the software product. There is a need to ensure the syntactical correctness, semantic consistency and aesthetic symmetry in the models that will be used to produce good quality software. There is also a need to create, follow and check all necessary process steps in order to achieve maturity of processes that will result in good quality software products. Furthermore, these process steps must be executed iteratively, incrementally and sufficiently. Process steps should also be malleable enough to suit various development environments, and various types and sizes of projects. The specific and significant areas of quality related work required in a process incorporating the UML are addressed in this book. The quality techniques discussed in this book include how to organize the overall quality function, the process steps to be followed in creation of UML diagrams, the steps in verification and validation of these diagrams, when to conduct such verificat how the interpret the results of quality activities, who should create and validate the UML diagrams, and how to create a quality control (testing) strategy. Because of the process focus in this book, the techniques of creation of UML diagrams is assumed to be known to the readers. Summary of the Book This book is divided into 6 chapters as summarized below. Chapter 1: The Quality Game In this background chapter on quality assurance we discuss the elusive nature of quality in the context of software. Modelling, particularly with the UML, is shown as means to improve communication and quality and is conducted in the three distinct yet related modelling spaces of Problem, Solution and Background. Process is discussed in the context of its three dimensions of technology (what), methodology (how) and sociology (who). This is followed by discussion on the various checks (syntax, semantics and aesthetics) needed to validate and verify UML-based models and the checks of necessity, sufficiency and malleability needed for a good quality process. Organization of the quality function, and its application to various types of projects (development, integration, package implementation, outsourcing, data warehousing, and educational) as well as various sizes (small, medium, large) of projects are also discussed here. Chapter 2: Quality Environment: Managing the Quality function Process aspect of quality encompasses the management functions of creating and managing a quality environment. This is because software quality is not just verifying and validating what has been produced but also a sustained effort at following the discipline of producing models and software. This discipline encompasses the process or the steps involved in producing good models and good software. This part of this book comprehensively considers organization and execution of the quality function with detailed emphasis on the process of developing UML based software. In other words we discuss how the quality function is organized and carried out in UML-based projects. The people issues (who) is also given due relevance in this part of the book. Chapter 3: Quality Process Architecture This chapter discusses what constitutes such a process, and how it will be helpful in enhancing quality in a UML-based project. This chapter does not propose a new process, but discusses a most generic Process including the Technological, Methodological and Sociological dimensions — what constitutes a process, and what are its major dimensions of a process is described here. The technological dimension of a process deal with the what, the methodological dimension with the how and the sociological dimension with the who, of an overall process. These dimensions are described with common workday examples. Furthermore, the generic process also describes the most commonly used activities and tasks that should be there in any process. These activities and tasks, and the related roles and deliverables, are described with the aim of improving the discipline in a process, resulting in enhanced quality of UML-based deliverables and eventually the software product. Chapter 4: Enacting the Quality Software Process In this chapter we discuss the enactment of an example process including practical issues of configuring an iterative, incremental and parallel project plan, based on the process-components discussed in the previous chapter, are discussed here. We also discuss practical issues of tracking the progress of a project as well as modifying the project plan based on that tracking. An iterative and incremental project plan will facilitate better absorption of changes than a sequential project plan. Creation and management of such a changing plan, derived from the malleability aspect of the process, are also discussed here. This chapter discusses what happens when the rubber hits the road in terms of application of a process. Chapter 5: Estimates and Metrics for UML-based Projects This chapter discusses the important issues of measurements and estimates in UML-based software projects. Starting with an argument for the need to make good estimates, and how good metrics help in making good estimates, this chapter delves into the importance of these measures and estimates in improving the quality of models and processes in the project. Technical measures related to sizes and complexities of the UML artefacts and diagrams is also discussed. Estimates for the example implementation project using the UML are shown with a view to demonstrate the application and significance of metrics in a practical project. Chapter 6: Testing the product This chapter will discuss in detail the quality control and testing aspect of a quality lifecycle. While we discussed process quality in previous chapters, quality control, or testing, is a major process-component dedicated to verifying and validating the results of our efforts thus far in creating models and following a process. Good quality control is inherently negative as it is aimed at breaking everything in a system — its logic, its execution, its performance. Thus, although Quality control is an integral part of quality assurance, but is not synonymous with it. This separation is given its due importance in this separate part of this book. CD &amp; Potential Web Support The CD contains details of the chapters, diagrams, and a set of templates that can be customised for use in projects. Suggested metrics for improving quality (e.g. size of use cases, effort in creating classes) are also incorporated in the CD. Evaluation copies of relevant process tools that deal with quality process have also been provided, with permissions. Literary Audience There are a large number of books written on UML and similarly on processes. Their scope encompasses both academic research and practical applications. This book attempts to synergies the application of quality processes in UML-based projects. With the process focus, the reader is expected to be familiar with UML and its modelling techniques as the book does not purport to discuss the modelling techniques of the UML. However, a person responsible for quality assurance will find this work self-sufficient and may even be encouraged after reading this material to extend their understanding further in to UML. Semantics This author firmly believes in gender-neuter language. Person is therefore used wherever possible. However, in order to maintain simplicity of reading he has been used as freely, and has been balanced by equal, if not more, use of she. Terms like programmer and quality manager, unless otherwise mentioned, represent roles performed by actors. These terms dont tie down real people like you and me who, in a short span of time, can jump from the role of a programmer to a quality manager to a director and back. It is also recognised that people may be playing more than one role at a time. For example, a business analyst may also be a part-time academic or a researcher. We throughout the text primarily refers to the reader and the author — you and me. Occasionally, we refers to the general IT community of which the author is a member. We also refers to the teams in which the author has worked. Therefore, although this is a single author book, you may encounter we as a reference by the author to himself, as well as to the IT community. Real conversations, as you and I are having through this work, cannot be statically typed. Mapping to a Workshop The practical aspects of UML and Quality, displayed in this book, have been popular in seminars and conferences. Amongst many presentation, particular noteworthy are its acceptance as a tutorial at the UML2001 conference in Toronto, Canada and the two-day seminar series in Mumbai, Bangalore and Delhi, in India. Here is a generic outline of the two-day workshop based on this book. For the education and academic community, each chapter in this book can correspond to a 3-hour lecture topic, with earlier part of the semester used in simply creating the UML-based models based on the case study. Acknowledgements Encouragement and support can take various forms —a word of encouragement here, hint of a smile there! And then there are those detailed discussions and arguments with honest reviewers of the manuscript on what should be included and how it should be presented. This is interspersed with the arduous task of typing sections of the manuscript, drawing the figures and the rather trying job of proofreading someone elses writing. All this has come to me through many wonderful people whom I acknowledge here gratefully: Anurag Agarwal Rajeev Arora Craig Bates Paul Becker Christopher Biltoft Bhargav Bhatt Graham Churchley Kamlesh Chaudhary Sandra Cormack Joanne Curry Sanjeev Dandekar Edward DSouza Con DiMeglio Julian Edwards Nandu Gangal Athula Ginige David Glanville Mark Glikson Nitin Gupta Brian Henderson-Sellers Murray Heke Ivar Jacobson Sudhir Joshi Ashish Kumar Vijay Khandelwal Akshay Kriplani Yi-chen Lan Chinar &amp; Girish Mamdapur Javed Matin Sid Mishra Rahul Mohod Navyug Mohnot Narayana Munagala Karin Ovari Les Parcell Chris Payne Andrew Powell Abhay Pradhan Amit Pradhan Anand Pradhan Prabhat Pradhan Rajesh Pradhan Tim Redrup Tracey Reeve Prashant Risbud James Ross Magdy Serour Bran Selic Ashish Shah Paresh Shah Prince &amp; Nithya Soundararajan Pinku Talati Amit Tiwary Murat Tanik Asha Unhelkar Sunil Vadnerkar Suresh Velgapudi John Warner Houman Younessi Paul Becker, my editor at Addison-Wesley, has provided invaluable support in this work and deserves special recognition. Bearing with my delayed submissions, passing encouraging comments when the progress was slow and accommodating my changes up to the last minute are some of the traits of this considerate editor that are gratefully acknowledged. Finally, my family makes all this possible by just being around me even, and especially, when I am mentally lost. I am grateful to my wife Asha, my daughter Sonki Priyadarshini whose view on quality took a jagged turn as she stepped into her teens, my son Keshav Raja who can appreciate quality in cars, bikes and planes — which is the ability of these tools of the kindergarten trade to withstand punishment meted out by rather tough 6 year olds. Finally, this work acknowledges all trademarks of respective organisations, whose names andor tools have been used in this book. Specifically, I acknowledge the trademarks of Rational (for ROSETM), TogetherSoft (for TogetherControlCenterTM), Object-oriented Pty Ltd (for ProcessMentorTM) and eTrackTM. Critiques It reflects a healthy state of affairs within the IT world, and especially the UML and process community, if work of this nature receives its due share of criticism. All criticisms have an underlying rationale and that they should all be accepted in a positive and constructive vein. All comments on this work, both positive and negative will be accepted positively. Thus, to all my prospective critics, whose criticisms will not only enrich my own knowledge and understanding of the quality topics discussed in this book, but which will also add to the general wealth of knowledge available to the IT community, I wish to say a big thank you in advance. Bhuvan UnhelkarSydney, July 2001}
}

@book{10.5555/558527,
author = {Mullin, Eileen and Rubin, Jared T.},
title = {Free-Commerce: The Ultimate Guide to E-Business on a Budget},
year = {2001},
isbn = {0130337676},
publisher = {Prentice Hall PTR},
address = {USA},
abstract = {From the Book: Foreword Why Free Whatever you may think of Bill Gates, chairman of Microsoft and arguably one of the world's wealthiest individuals, you must give him credit for seeing-and capitalizing on-the rise of the personal computer. Therefore, it was more than mere press release fodder when the bespectacled billionaire proclaimed the Internet the most significant advancement since the personal computer. He was wrong, though, or at least uncharacteristically understated. The Net has been far more significant than the PC. Although the PC has been instrumental in driving adoption of the Net, particularly in the United States, the Internet and World Wide Web were born independently of it. While most people access the Internet from Windows today, the PC's Net suitability and Microsoft's Internet strategy were late to blossom. The Net holds promise for having its greatest impact yet outside the U.S. through non-PC devices, such as advanced mobile phones, which are more common in Japan and Europe. The real reason why the Net is far more important than the PC ever was, though, is that the PC failed to fundamentally change the economy. The PC may have improved productivity in virtually all industries. It may have created sea changes in many industries, such as publishing and film production. It may have even virtually created intrinsic industries, such as software development. However, the PC failed to tear down walls among industries. It did not raise the questions of whether music artists should distribute their music without their labels' involvement. It did not cause CNN and the New York Times to compete in the same medium. It did not give rise to retailers like Amazon.com that have Wal-Mart in their sights. It did not cause NBC to question whether it was a media or commerce company. Calling the Internet a force in the evolution of the PC is like saying that television's major impact was changing radio. LOOKING, HOOKING, AND BOOKING There is another way the Internet has changed our thinking about the economy. The Net's unique combination of direct marketing potential and wildfire distribution combined with new financial valuation models and a glut of startups, all desperate to rise above the fray, has led to a new way of thinking about customers. This is a world that has seen the tremendous success that companies like America Online have had once they have acquired customers. All AOL has really done, though, is replicate what financial and telecommunications companies have known for years. Once you have a billing relationship with customers, it is relatively simple to extract additional revenue from them through premium services, cross-sold services, merchandise, bundles, and other marketing techniques. The difference in the Net space, though, in spite of the market correction of early 2000, is that investors have been willing to forego the billing relationship for extended periods on the logic that it is only after getting someone's attention that you can sell them goods and services. Microsoft bought Hotmail, which provided free Web-based e-mail, for over $400 million because it had acquired millions of users who might be more easily converted into customers. Regardless of whether their revenue model includes advertising (as many sites profiled in this book do) every company with a Web presence must understand the principles of media. That was only in the first wave, though. The Internet has certainly blurred boundaries across media and channels, but motivating consumers is still a difficult task, particularly since the low bandwidth available to the Web makes it difficult to elicit the kinds of emotional responses possible in television. The rise of online retail, the transformation of online media, and the investment in crossover online services have pushed the Net from the domain of the consumer out to meeting the needs of businesses, particularly small businesses. AN EVOLUTION OF SERVICES The first wave of excitement around the Net came around selling access to consumers. Early Internet service providers such as Netcom seemed poised to dominate the landscape. Then came flat-rate pricing, which wiped away the juicy margins in the access business. Furthermore, as we'll discover, distracted Internet service providers were so focused on survival that they slowly saw most of their value-added services, such as e-mail and home pages, usurped by more nimble Web sites. Selling goods to consumers marked the second phase. Amazon.com started with a franchise in book sales and has slowly branched out to music, video, toys, and even consumer electronics. Amazon's blitzkrieg quickly forced consolidation in sectors ranging from CD sales to beauty supplies, where the retailer has a major investment in Drugstore.com. Undeterred in its strategy of horizontal domination, Amazon has embraced consumer-to-consumer auctions and even launched zShops, which leads smaller vessels of commerce into its raging river. What is often overlooked about Amazon, though, is that its storefront masks a services play. Books were merely an early vehicle through which to develop tools that provide one of the best online shopping experiences today, including excellent customer service, recommendations driven by collaborative personalization, and a huge selection. And what is selection if not a way to avoid running around, i.e., a convenience service Despite Amazon's success, though, the bloom is off online retail's rose. The threat of shopping bots that compare prices across Web sites; the eventuality of established branded retailers moving online; the reality of lowballers such as Buy.com, who are willing to trade profits for advertising revenue; and the twist of reverse auctioneers such as Mercata, which allow consumers to aggregate demand, have made selling goods online something less than the frictionless mecca it once seemed. Similarly, in the media side of the Web, major portals such as Yahoo!, which has established itself as one of the few successful online media companies (and only through strong offline ties, at that), have made a fundamental shift in the past few years. Most of the attention on its transition over the past few years has focused on the migration from a search site pointing consumers to resources to a directory focusing on internal resources. The more significant change, however, has been Yahoo!'s development of a series of communications and transactional services, all under a unified login-Yahoo!'s answer to Amazon's one-click ordering. Yahoo! itself has started offering its own wallet, from which its members can shop at sites from such retail stalwarts as Macy's and Nordstrom's. AN OVERLAP OF MARKETS The relatively distinct market for consumer goods has little overlap in terms of corporate competition. Outside of uniforms, most corporations have little use for clothing. Outside of functions, they have little need for food. And outside of desks, they have little use for furniture. A floor of 200 people, each of whom may have his or her own VCR at home, may share a single VCR in a corporation. Likewise, the average consumer has little use for copy machines, and the largest consumer PC companies in the world derive only a small fraction of their total revenue from the consumer market. Moreover, these markets account only for finished goods, leaving out major sectors of the economy focused on components and parts. On the other hand, while some services, such as health care, education, and entertainment, remain largely consumer-focused, what consumers spend on some of their most expensive services is dwarfed by what corporations spend on those services in real estate, telecommunications, insurance, financial services, and travel. We are at a crossroads in the Net's evolution. It's not that the Internet economy has given up on the consumer. It's that business models-ever craving the steady renewable revenue that services can bring-are evolving to a point where it's easier to make the case to the business customer than to the consumer. The disadvantage is that while changes in the consumer Internet economy led to a crumbling of walls among industry and through the supply chain, the business Internet economy will have relatively low impact in comparison. ENTER THE ENTREPRENEUR As an entrepreneur, you represent a very attractive target to the provider of free services. You're willing to take action and responsibility, and may even influence the spending of others, such as employees and customers. Furthermore, while the key to motivating customers is often to evoke an emotional reaction, capitalism's unadorned invisible fingers-seeking opportunity, saving costs, and improving efficiency-guide entrepreneurs. Sites such as eToys had to invest millions of dollars to buy their servers, develop their Web sites, get online, host their sites, promote their brands, and communicate with their customers. They also had to spend millions to shore up their own organizational needs. Today, technology has advanced and organizations have launched to greatly ease the pain of starting a robust commercial site. Web content management systems, team development environments, and commerce servers have allowed new online stores to get up and running faster. Application service providers and commerce service providers let you rent programs and outsource development so you don't have to hire as much staff to build your own engines of e-commerce. Nevertheless, we won't kid you. To launch a world-class commerce site still costs millions of dollars because the bar has been raised. Now, for example, staying on customers' radar involves personalized opt-in direct mail campaigns, auctions, and sophisticated data management. However, while the Internet may not quite have allowed all of the smallest guys to unseat the biggest guys, it has allowed the smallest guys to take on the somewhat bigger guys. SOHO (small officehome office) is really where free-commerce has bloomed, allowing entrepreneurs to concentrate on their skills and look to other resources to mind the shop. SELLING YOUR SOUL: BEST OF BREED VS. INTEGRATION Even though some services profiled in Free-Commerce have a strong offline component, Web-based services have, in general, started to push the limits of Web development. Desktop.com, for example, has made extensive use of JavaScript to create an online experience that behaves more like a traditional Windows desktop than a Web page. Other advanced developments lurk behind the interface. As has often happened when weighing costs and benefits, decisions need to be made between choosing the best of breed and an integrated experience. For example, in the PC market, the best-of-breed components won. Apple's Macintosh may not have had the best video cards, the biggest hard disk, or the widest selection of software, but its integrated experience allowed a user experience that many argue the PC still has not matched. Nevertheless, the PC far outsold the Mac. Advocates of Microsoft Office argue that each of its applications were best of breed, but in the productivity applications space, the packaging together of word processor, spreadsheet, presentation program, and database unseated the former leaders of those categories-WordPerfect, Lotus 1-2-3, Harvard Presents, and dBase. With that in mind, let's return to Yahoo!, which along with other portals, has built some of the Web's most comprehensive service offerings. Among its prodigious collection are Yahoo! Messenger, Yahoo! Finance, Yahoo! Mail, and Yahoo! Auctions. Registering for one of these services registers you for all of them, but they have other ties. For example, Yahoo! Calendar can be accessed easily from within Yahoo! Mail, which can be monitored from My Yahoo!, the site's personalized front door. Undoubtedly, moving from Yahoo! service to service is more convenient than having to log in to several different services on different sites. But while Yahoo!'s services are generally good, they are not necessarily the best. Other free mail programs, for example, offer the option to have text read over the phone or free mail forwarding. Yahoo! Messenger has far fewer members than the equivalent service by America Online, while Yahoo! Auctions has a far smaller audience than eBay. Datek and Realtimequotes.com offer free real-time market information; Yahoo! Finance's quotes are delayed by 20 minutes. The competitive nature of the Web means that no one should own an advantage for too long. The integrated sites are certainly worth registering for and provide an easy way to experiment with services you might not ordinarily try. They also tend to be more established compared to many startups. In general, it's not very difficult to switch among services online, but sites use features such as personalization to raise those barriers. THE PRICE OF FREEDOM Almost all the free services have a price, and that's personal information. Often times, it can be as innocuous as your name and e-mail address, but it often includes more sensitive items like your address, phone number, and income. Surprisingly, few ask much about the specifics of your business, apart from company size and industry. Critics contend that privacy online is like a nuclear plant. Everyone enjoys the cheap power until someone's space gets contaminated. Increasingly, Web sites offer privacy policies that describe what they will and won't do with data you provide, but there have been instances of companies breaching their policies intentionally or with the unsolicited help of hackers. Worse than having your free service provider abuse your trust is having one of its partners solicit you after buying or trading for their member list. Efforts to enforce strict privacy standards have ranged from industry organizations such as TrustE to largely stillborn technical standards with obscure acronyms like P3P. Technology companies such as Zero Knowledge Systems and eNonymous have also been sprung up to anonymize the online experience. In fact, they don't even know who their own users are. If your privacy is tantamount to you, your safest bet is to avoid these services. A less extreme measure is to limit your relationships to those you trust or to companies that have more to lose than you by violating their own policies. In general, that means larger companies like America Online and Microsoft, but smaller companies also have their reputation on the line. The main difference between the Net and other channels, though, is the speed with which information travels. Don't do or divulge anything online that you wouldn't want the rest of the world to know about, although determined snoops can probably get the information through other means. If other companies, like banks and telcos, were held to the same strict standards privacy advocates are proposing for the Net, we could kiss junk mail and telemarketing goodbye. Privacy concerns also tend to be overstated by the media and are a price of progress. The telephone represented a major threat to privacy. And yet, as we complain about telemarketers who are far more annoying than direct e-mail marketers, we rush to adopt cellular telephones to ensure we have even less privacy. ABOUT FREE-COMMERCE There may be other prices to pay for free services. Some companies explicitly request the right to send you e-mail as part of the registration. Many of the free PCs discussed in Chapter 1 are tied to contracts. These have parallels in the physical world, where cellular carriers like AT&amp;T Wireless have offered free cellphones in the past in exchange for annual or even multi-year contracts. Nevertheless, the shift of the Net to services and the attractiveness of the small business as a target customer have allowed you to get things done on the Net. You can get online, find the right software, build your Web site, attract and retain customers, and stay organized throughout it all-all without spending a penny. There are over 150 free services profiled in Free-Commerce, each with information on what they offer, what they'll ask for, and even some inexpensive premium services they offer for you big spenders. The book seeks to provide access to the best and most useful services to you in running your business; we didn't include many free consumer services because they just couldn't be tied back to productivity gains. Whether you already run a small business with a few employees or have thought about launching a home office, embracing free-commerce will let you focus on what you need to, including the bottom line. Ross Scott Rubin Vice President &amp; Chief Research Fellow, Jupiter Research}
}

